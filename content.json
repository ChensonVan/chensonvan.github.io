[{"title":"Centos7å®‰è£…Nvidaé©±åŠ¨å’Œæ˜¾å¡é©±åŠ¨","date":"2019-03-07T09:54:49.000Z","path":"2019/03/07/Centos7å®‰è£…Nvidaé©±åŠ¨å’Œæ˜¾å¡é©±åŠ¨/","text":"1. å®‰è£…1.1 å®‰è£…æ–¹å¼æœ‰ä¸‰ç§ CUDAï¼ˆ.runï¼‰ä¸‹è½½ä»¥åå®‰è£…å¸¦ä¸Šæ˜¾å¡é©±åŠ¨ï¼Œ ä¸‹è½½å¦‚ä¸‹ 1234567891011121314# æŸ¥çœ‹æ˜¾å¡ç‰ˆæœ¬lspci | grep -i nvidia# ä¸‹è½½# https://www.nvidia.cn/Download/index.aspx?lang=cn å¯åœ¨è¿™ä¸ªç½‘ç«™çœ‹å‡ºå¯¹åº”çš„ç‰ˆæœ¬wget http://cn.download.nvidia.com/tesla/410.79/NVIDIA-Linux-x86_64-410.79.run# å®‰è£…sh NVIDIA-Linux-x86_64-410.79.run# å¸è½½sh NVIDIA-Linux-x86_64-410.79.run --uninstallnvidia-uninstall æ˜¾å¡é©±åŠ¨.runæ–‡ä»¶å®‰è£… é›†æˆè½¯ä»¶åŒ…å®‰è£…ï¼ˆyumç­‰ï¼‰ 12345# ç¡®è®¤æ˜¯å¦ä¸ºyumå®‰è£…yum list installed | grep nvidia# å¸è½½yum remove nvidia-detect.x86_64 1.2 å®‰è£…æ­¥éª¤ æ·»åŠ EIRepoæº 1234rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # æ˜¯å¦æŒ‡å®šç‰ˆæœ¬ï¼Ÿrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm å®‰è£…æ˜¾å¡é©±åŠ¨æ£€æŸ¥ 1yum install nvidia-detect è¿è¡Œæ˜¾å¡æ£€æµ‹é©±åŠ¨ 1234nvidia-detect -v# è¿è¡Œç»“æœkmod-nvidia ç³»ç»Ÿå‡†å¤‡å·¥ä½œ 12345678910111213# æ³¨æ„è¿™æ˜¯å‡çº§ç³»ç»Ÿç›¸å…³çš„åŒ…ï¼Œè€—æ—¶è¾ƒé•¿yum -y updateyum -y groupinstall \"GNOME Desktop\" \"Development Tools\"# å®‰è£…åŸºç¡€åŒ…yum install kernel-devel kernel-doc kernel-headers gcc* glibc* glibc-*# å®‰è£…åˆšæ‰æ£€æµ‹åˆ°çš„é©±åŠ¨yum install kmod-nvidia yum -y install epel-releaseyum -y install dkms ç¼–è¾‘grubæ–‡ä»¶ 123456vim /etc/default/grub# åœ¨â€œGRUB_CMDLINE_LINUXâ€ä¸­æ·»åŠ # rd.driver.blacklist=nouveau nouveau.modeset=0# éšåç”Ÿæˆé…ç½®grub2-mkconfig -o /boot/grub2/grub.cfg åˆ›å»ºblacklist 12345# /lib/modprobe.d/dist-blacklist.confvim /etc/modprobe.d/blacklist.conf# æ·»åŠ , å±è”½é»˜è®¤å¸¦æœ‰çš„nouveaublacklist nouveau æ›´æ–°é…ç½® (é‡å»ºinitramfs image) 12mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.imgdracut /boot/initramfs-$(uname -r).img $(uname -r) é‡å¯ 1reboot ç¡®è®¤ç¦ç”¨äº†nouveau 12# è‹¥æ— è¾“å‡ºåˆ™ç¦ç”¨æˆåŠŸlsmod | grep nouveau å¼€å§‹å®‰è£…é©±åŠ¨ 1sh NVIDIA-Linux-x86_64-410.79.run æŸ¥çœ‹æ˜¾å¡ä½¿ç”¨æƒ…å†µ 12# nvidia-smiæ˜¯ç”¨æ¥æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µçš„nvidia-smi 1.3 å¸¸ç”¨æŒ‡ä»¤ æŸ¥çœ‹å®‰è£…çš„nvidiaæ¨¡å—å‘½ä»¤ 12# rpm -qarpm -qa|grep -i nvid|sort æŸ¥çœ‹ç³»ç»Ÿç‰ˆæœ¬ 123456uname -alsb_release -a# ä»¥ä¸‹æ–¹æ³•é€‚ç”¨äºRedHat,CentOScat /etc/redhat-release 2. å®‰è£…CUDA æ£€æµ‹GPUæ˜¯å¦OK 1lspci | grep -i nvidia ä¸‹è½½cuda 12# https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=CentOS&amp;target_version=7&amp;target_type=runfilelocalwget https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda_10.0.130_410.48_linux å®‰è£…gccã€g++ç¼–è¯‘å™¨ 12yum install gccyum install g++ å®‰è£…kernel-develå’Œkernel-headers 12yum install kernel-develyum install kernel-headers èµ‹äºˆæƒé™ï¼Œå’Œå®‰è£… 12chmod 755 cuda_9.0.176_384.81_linux.run./cuda_9.0.176_384.81_linux.run å®‰è£…è¿‡ç¨‹ 1234567891011121314install NVIDIA Accelerated Graphics Driver ... -no # ä¸éœ€è¦å†å®‰è£…æ˜¾å¡é©±åŠ¨Install OpenGL ... -no # è¿™ä¸ªç»„ä»¶ä¸€å®šä¸è¦å®‰è£…ï¼Œå¦‚æœå‡ºç°é€‰æ‹©ï¼Œé€‰ NoInstall CUDA 9.0 Toolkit ... -yesToolkit location /usr/local/cuda-9.0 ... [Enter] # é»˜è®¤å®‰è£…ä½ç½®å³å¯Install a symbolic link at ... -yesInstall CUDA 9.0 Samples ... -yes # å®‰è£…ä¸€äº›ä¾‹ç¨‹Enter CUDA Samples Location ... [Enter] # é»˜è®¤å®‰è£…ä½ç½®...Finished Driver : Not SelectedToolkit : Installed in /usr/local/cuda-9.0Samples : Installed in /root, but missing recommended libraries***WARNING: Incomplete installation! å‡ºç°è­¦å‘Šï¼Œå®‰è£…ä¸å®Œå…¨ï¼Œä½†æ˜¯æ²¡æœ‰å½±å“ é…ç½®ç³»ç»Ÿè·¯å¾„ 123456# vim /etc/profile...export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH# source /etc/profile ; ä½¿ç¯å¢ƒå˜é‡ç«‹å³ç”Ÿæ•ˆ CUDAæµ‹è¯• 12345678# cuda ; æŒ‰ä¸¤ä¸‹ tab é”®cudafe cuda-gdb cuda-install-samples-9.0.shcudafe++ cuda-gdbserver cuda-memcheck# nvcc --versionnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2017 NVIDIA CorporationBuilt on Fri_Sep__1_21:08:03_CDT_2017Cuda compilation tools, release 9.0, V9.0.176 3. å®‰è£…cuDNN ä¸‹è½½ 1234567wget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/cudnn-9.0-linux-x64-v7.4.2.24.tgz# Runtime Libyarywget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/RHEL7_3-x64/libcudnn7-devel-7.4.2.24-1.cuda9.0.x86_64.rpm# Developer Librarywget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/RHEL7_3-x64/libcudnn7-devel-7.4.2.24-1.cuda9.0.x86_64.rpm 4. Anacondaå®‰è£… ä¸‹è½½ 123# æ‰€æœ‰å†å²ç‰ˆæœ¬ï¼šhttps://repo.continuum.io/archive/# anaconda 3.5.1wget https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh è·¯å¾„åŠ å…¥åˆ°ç³»ç»Ÿé…ç½®æ–‡ä»¶ä¸­ ç”Ÿæˆç§˜é’¥ 12345from notebook.auth import passwdpasswd()Enter password: kn88888Verify password: kn88888'sha1:25f17927d3a2:74b098a697b5e7bd76931badf00df1000eccfe17' ç”Ÿæˆjupyterçš„é…ç½®æ–‡ä»¶ 12[root@s3-aidc-dl-prod-01 ~]# jupyter notebook --generate-configWriting default config to: /root/.jupyter/jupyter_notebook_config.py ä¿®æ”¹é…ç½®æ–‡ä»¶ 12345678910111213vim /root/.jupyter/jupyter_notebook_config.py# çº¦174è¡Œc.NotebookApp.ip = &apos;localhost&apos;# çº¦240è¡Œc.NotebookApp.port = 8888# çº¦220è¡Œc.NotebookApp.open_browser = False# çº¦229è¡Œc.NotebookApp.password = &apos;sha1:25f17927d3a2:74b098a697b5e7bd76931badf00df1000eccfe17&apos; ä½¿ç”¨sshç§æœ‰é€šé“ 12ssh -N -f -L [remote port]:localhost:[local port] -p [ssh port] -l [username] [å…¬ç½‘IP]ssh -N -f -L 7864:10.105.50.178:8888 fanchangxun@182.254.211.45 -p36000 å®‰è£…JupyterHub 1conda install -c conda-forge jupyterhub jupyter é…ç½® 5. References centos7 nvidiaé©±åŠ¨å®‰è£…å¤±è´¥é—®é¢˜çš„è§£å†³åŠæ³• æˆ‘çš„AIä¹‹è·¯ â€”â€” ä»è£¸æœºæ­å»ºGPUç‰ˆæœ¬çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒ ã€å·²è§£å†³ã€‘æ±‚åŠ©ï¼ŒNå¡å®˜æ–¹é©±åŠ¨å®‰è£…ä¸èƒ½ CentOS 7 å®‰è£… Cuda çš„ç»å† Centos7 é‡è£…è‹±ä¼Ÿè¾¾æ˜¾å¡é©±åŠ¨+Cuda9.0+Cudnn7 centos7ç³»ç»Ÿ å®‰è£…NVIDIAæ˜¾å¡é©±åŠ¨ CentOS 7.0å®‰è£…Nvidiaé©±åŠ¨ Centos7 ä¸Šä¸ºkaldiå®‰è£…/å¸è½½ nvidiaæ˜¾å¡é©±åŠ¨å’ŒCUDA CUDAä¹‹nvidia-smiå‘½ä»¤è¯¦è§£ CentOS 7 å®‰è£… NVIDIA æ˜¾å¡é©±åŠ¨å’Œ CUDA Toolkit DKMSç®€ä»‹ 6. Error Logs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114ERROR: Failed to run `/usr/sbin/dkms build -m nvidia -v 410.79 -k 3.10.0-862.11.6.el7.x86_64`: Error! echo Your kernel headers for kernel 3.10.0-862.11.6.el7.x86_64 cannot be found at /lib/modules/3.10.0-862.11.6.el7.x86_64/build or /lib/modules/3.10.0-862.11.6.el7.x86_64/source. # é”™è¯¯ï¼šGoogle failed to run /usr/sbin/dkms centos# è§£å†³æ–¹æ¡ˆsudo apt-get install dkms build-essential linux-headers-$(uname -r)ERROR: Failed to install the kernel module through DKMS. No kernel module was installed; please try installing again without DKMS, or check the DKMS logs for more information.ERROR: Installation has failed. Please see the file &apos;/var/log/nvidia-installer.log&apos; for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.nvidia-installer log file &apos;/var/log/nvidia-installer.log&apos;creation time: Mon Dec 24 15:39:55 2018installer version: 410.79PATH: /data1/anaconda3/bin:/usr/local/services/java/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/services/go/bin:/root/binnvidia-installer command line: ./nvidia-installerUnable to load: nvidia-installer ncurses v6 user interfaceUsing: nvidia-installer ncurses user interface-&gt; Detected 28 CPUs online; setting concurrency level to 28.-&gt; Installing NVIDIA driver version 410.79.-&gt; There appears to already be a driver installed on your system (version: 410.79). As part of installing this driver (version: 410.79), the existing driver will be uninstalled. Are you sure you want to continue? (Answer: Continue installation)-&gt; Would you like to register the kernel module sources with DKMS? This will allow DKMS to automatically build a new module, if you install a different kernel later. (Answer: Yes)-&gt; Installing both new and classic TLS OpenGL libraries.-&gt; Installing both new and classic TLS 32bit OpenGL libraries.-&gt; Install NVIDIA&apos;s 32-bit compatibility libraries? (Answer: No)-&gt; Will install GLVND GLX client libraries.-&gt; Will install GLVND EGL client libraries.-&gt; Skipping GLX non-GLVND file: &quot;libGL.so.410.79&quot;-&gt; Skipping GLX non-GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLX non-GLVND file: &quot;libGL.so&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so.410.79&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so.1&quot;-&gt; Uninstalling the previous installation with /usr/bin/nvidia-uninstall.Looking for install checker script at ./libglvnd_install_checker/check-libglvnd-install.sh executing: &apos;/bin/sh ./libglvnd_install_checker/check-libglvnd-install.sh&apos;... Checking for libglvnd installation. Checking libGLdispatch... Checking libGLdispatch dispatch table Checking call through libGLdispatch All OK libGLdispatch is OK Checking for libGLX libGLX is OK Checking for libEGL libEGL is OK Checking entrypoint library libOpenGL.so.0 Checking call through libGLdispatch Checking call through library libOpenGL.so.0 All OK Entrypoint library libOpenGL.so.0 is OK Checking entrypoint library libGL.so.1 Checking call through libGLdispatch Checking call through library libGL.so.1 All OK Entrypoint library libGL.so.1 is OK Found libglvnd libraries: libGL.so.1 libOpenGL.so.0 libEGL.so.1 libGLX.so.0 libGLdispatch.so.0 Missing libglvnd libraries: libglvnd appears to be installed.Will not install libglvnd libraries.-&gt; Skipping GLVND file: &quot;libOpenGL.so.0&quot;-&gt; Skipping GLVND file: &quot;libOpenGL.so&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so.1.2.0&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so.1&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so.2.1.0&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so.2&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so&quot;-&gt; Skipping GLVND file: &quot;libGLdispatch.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1.7.0&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libGL.so&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1.1.0&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libEGL.so&quot;-&gt; Skipping GLVND file: &quot;./32/libGL.so.1.7.0&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libGL.so&quot;-&gt; Skipping GLVND file: &quot;./32/libGLX.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so&quot;-&gt; Skipping GLVND file: &quot;./32/libEGL.so.1.1.0&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libEGL.so&quot;Will install libEGL vendor library config file to /usr/share/glvnd/egl_vendor.d-&gt; Searching for conflicting files:-&gt; done.-&gt; Installing &apos;NVIDIA Accelerated Graphics Driver for Linux-x86_64&apos; (410.79): executing: &apos;/usr/sbin/ldconfig&apos;...-&gt; done.-&gt; Driver file installation is complete.-&gt; Installing DKMS kernel module:ERROR: Failed to run `/usr/sbin/dkms build -m nvidia -v 410.79 -k 3.10.0-862.11.6.el7.x86_64`: Error! echoYour kernel headers for kernel 3.10.0-862.11.6.el7.x86_64 cannot be found at/lib/modules/3.10.0-862.11.6.el7.x86_64/build or /lib/modules/3.10.0-862.11.6.el7.x86_64/source.-&gt; error.ERROR: Failed to install the kernel module through DKMS. No kernel module was installed; please try installing again without DKMS, or check the DKMS logs for more information.ERROR: Installation has failed. Please see the file &apos;/var/log/nvidia-installer.log&apos; for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.","tags":[{"name":"Centos7","slug":"Centos7","permalink":"http://chenson.com/tags/Centos7/"},{"name":"Nvida","slug":"Nvida","permalink":"http://chenson.com/tags/Nvida/"},{"name":"GPU","slug":"GPU","permalink":"http://chenson.com/tags/GPU/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-RNN","date":"2019-02-23T09:40:26.384Z","path":"2019/02/23/æ·±åº¦å­¦ä¹ ç¬”è®°-RNN/","text":"1. ä¸€äº›é¢„å¤‡çŸ¥è¯†1.1 Markov Chain 2. å¾ªç¯ç¥ç»ç½‘ç»œ - RNN (Recurrent Neural Network)3. é—¨æ§å¾ªç¯å•å…ƒ - GRN (Gated Recurrent Neural Networks)4. é•¿çŸ­æœŸè®°å¿† - LSTM (Long and Short-Term Memory)","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.com/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.com/tags/CNN/"}]},{"title":"hexoé”™è¯¯è®°å½•","date":"2019-02-23T06:30:57.000Z","path":"2019/02/23/hexoé”™è¯¯è®°å½•/","text":"è®²å®è¯ï¼Œæˆ‘ä¸æ˜¯ç‰¹åˆ«äº†è§£hexoã€‚å¤§æ¦‚å°±çŸ¥é“æ˜¯ä¸ªç”¨æ¥ç”Ÿæˆé™æ€çš„åšå®¢åŒ…orå·¥å…·ï¼Ÿä¾èµ–äº†npm(node package manager)ã€‚ç„¶åå¹³å¸¸ä½¿ç”¨çš„æ—¶å€™æœ‰æ—¶ä¼šé‡åˆ°ä¸€äº›æ¯”è¾ƒå¤´å¤§çš„é—®é¢˜ï¼Œåœ¨æ²¡æœ‰åŠ¨hexoå’Œnpmçš„æƒ…å†µä¸‹ã€‚ æ‰€ä»¥ä¸ºäº†ä»¥åé‡åˆ°ç±»ä¼¼çš„é—®é¢˜æ–¹ä¾¿å¤„ç†ï¼Œå¯¹hexoçš„é”™è¯¯å¤„ç†åšäº†ä¸€ä¸ªç®€å•çš„è®°å½•ã€‚ 1. hexo not found in XXX é—®é¢˜æè¿° å¤§æ¦‚æ˜¯ä¸€ä¸ªå¤šæœˆæ²¡å†™æ–‡ç« äº†ï¼Œåœ¨è¿è¡Œhexo n titleçš„æ—¶å€™æŠ¥äº†è¿™ä¸ªé”™è¯¯ï¼Œè¯´æ˜¯æœ¬åœ°æœ¬åœ°æ²¡æœ‰å‘ç°hexoï¼Ÿï¼Ÿï¼Ÿåæ­£æ˜¯ä¸€è„¸æ‡µé€¼ è§£å†³æ–¹æ¡ˆ å°è¯•è¿‡æœªæˆåŠŸçš„æ–¹æ³• æ›´æ–°npmå’Œnode å¸è½½nodeï¼Œé‡è£… æœ€ç»ˆæˆåŠŸçš„æ–¹æ³• 12345678# å¼ºåˆ¶åˆ é™¤åšå®¢ä¸‹node_modulesæ–‡ä»¶å¤¹$ rm -rf node_modules# é‡è£…$ npm install # ç»“æœå¤±è´¥ï¼Œæ”¹ç”¨äº†æ·˜å®çš„é•œåƒåæˆåŠŸ$ npm install cnpm -g --registry=https://registry.npm.taobao.org æ³¨æ„ï¼Œç»å¸¸ä¼šæŠ¥permission deniedçš„Errorï¼Œæ”¹ç”¨sudoè¿è¡ŒæŒ‡ä»¤ã€‚ ä¹‹åhexo n titleå¯ä»¥æ­£å¸¸ç”Ÿæˆæ–°çš„æ–‡ç« ã€‚ 2. hexo Error: Cannot find module â€˜highlight.js é—®é¢˜æè¿° æ¥è‡ªä¸Šä¸€ä¸ªé—®é¢˜çš„å­é—®é¢˜ï¼Œè™½ç„¶å¯ä»¥ç”Ÿæˆæ–°çš„æ–‡ç« ï¼Œä½†æ˜¯åœ¨gendrateç”Ÿæˆé™æ€åšå®¢æ–‡ä»¶çš„æ—¶å€™å¤±è´¥äº†ï¼ŒæŠ¥äº†æ²¡æœ‰highlights.jsè¿™ä¸ªæ¨¡å—çš„é”™è¯¯ã€‚ çœ‹é—®é¢˜åº”è¯¥æ˜¯npmæ²¡æœ‰å®‰è£…ä¸Šhighlightsè¿™ä¸ªæ¨¡å—ã€‚ è§£å†³æ–¹æ¡ˆ å°è¯•è¿‡æœªæˆåŠŸçš„æ–¹æ³• å®‰è£…highlights.jsæ¨¡å— 1$ npm install highlight.js --save æŠ¥é”™å…³äºfseventsæ¨¡å—çš„é—®é¢˜ï¼ŒçœŸçš„æ˜¯å¤´å¤§ğŸ˜¡ï¼ï¼ï¼ é‡è£…npmçš„æ¨¡å— 123$ sudo npm cache clean -f$ sudo npm install -g n$ sudo n stable å®‰è£…fseventsæ¨¡å— 123$ sudo npm i fsevents # å¤±è´¥$ sudo npm install -D fsevents # å¤±è´¥ åŒæ ·æŠ¥é”™Error: EACCES: permission denied, mkdir &#39;/Users/Chenson/Dropbox/PROJECTS/myBlog/node_modules/fsevents/build 1$ sudo npm install -g fsevents # å¤±è´¥ æ³¨æ„åˆ°é‡Œé¢æœ‰ä¸€ä¸ªæŠ¥é”™ npm WARN babel-eslint@10.0.1 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself. ä¼¼ä¹æ‰‹åŠ¨å®‰è£…å‡ ä¸ªåŒ…ï¼Œè¯•ä¸€ä¸‹çœ‹ 1$ sudo npm install eslint å¦ˆäº†ä¸ªé¸¡ï¼ŒåˆæŠ¥ä¸Šé¢åŒæ ·çš„é”™è¯¯ï¼Œè¦ç–¯äº†ğŸ˜ ã€‚ åŒæ ·è¿˜æ˜¯é”™è¯¯gyp ERR! stack Error: EACCES: permission denied, mkdir node_modules/fsevents/build æœ€ç»ˆæˆåŠŸçš„æ–¹æ³• çœ‹ä¸Šé¢å°è¯•çš„æ–¹æ³•ï¼Œå…³é”®åœ¨äºæƒé™é—®é¢˜ï¼Œå°è¯•æ”¹äº†node_moulesçš„æƒé™ä¸º777ï¼Œä¾æ—§å¤±è´¥ã€‚ åé¢å‚è€ƒç­”æ¡ˆInstallation error: permission denied for node-sasï¼Œé‡Œé¢ä»¥ä¸ºè€å“¥è¯´è®¾ç½®nodeçš„æƒé™ä¸ºroot 12345# éå¸¸é‡è¦ï¼ï¼ï¼$ npm config set user root$ sudo npm install eslint $ sudo npm install fsevents$ sudo npm install highlight.js --save æœ€åè¿™ä¿©å±…ç„¶å®‰è£…ä¸Šäº†ï¼ ç„¶åä¹Ÿå¯ä»¥æ­£å¸¸è¿è¡ŒæŒ‡ä»¤hexo gï¼Œç”Ÿæˆé™æ€åšå®¢ã€‚","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chenson.com/tags/hexo/"}]},{"title":"è¯»ä¹¦ç¬”è®°-å¢é•¿é»‘å®¢","date":"2019-02-23T05:19:54.000Z","path":"2019/02/23/è¯»ä¹¦ç¬”è®°-å¢é•¿é»‘å®¢/","text":"","tags":[{"name":"è¯»ä¹¦ç¬”è®°","slug":"è¯»ä¹¦ç¬”è®°","permalink":"http://chenson.com/tags/è¯»ä¹¦ç¬”è®°/"},{"name":"æ™ºèƒ½è¥é”€","slug":"æ™ºèƒ½è¥é”€","permalink":"http://chenson.com/tags/æ™ºèƒ½è¥é”€/"}]},{"title":"è¯»ä¹¦ç¬”è®°-æ•°æ®åŒ–è¿è¥çš„æ–°ç°è±¡ä¸æ–°å‘å±•","date":"2019-02-23T05:15:17.000Z","path":"2019/02/23/è¯»ä¹¦ç¬”è®°-æ•°æ®åŒ–è¿è¥çš„æ–°ç°è±¡ä¸æ–°å‘å±•/","text":"","tags":[{"name":"è¯»ä¹¦ç¬”è®°","slug":"è¯»ä¹¦ç¬”è®°","permalink":"http://chenson.com/tags/è¯»ä¹¦ç¬”è®°/"},{"name":"æ™ºèƒ½è¥é”€","slug":"æ™ºèƒ½è¥é”€","permalink":"http://chenson.com/tags/æ™ºèƒ½è¥é”€/"}]},{"title":"è¯»ä¹¦ç¬”è®°-é‡æ„å¤§æ•°æ®ç»Ÿè®¡","date":"2019-02-23T04:41:05.000Z","path":"2019/02/23/è¯»ä¹¦ç¬”è®°-é‡æ„å¤§æ•°æ®ç»Ÿè®¡/","text":"","tags":[{"name":"Big Data","slug":"Big-Data","permalink":"http://chenson.com/tags/Big-Data/"},{"name":"è¯»ä¹¦ç¬”è®°","slug":"è¯»ä¹¦ç¬”è®°","permalink":"http://chenson.com/tags/è¯»ä¹¦ç¬”è®°/"}]},{"title":"Sparkç¬”è®°-åº”ç”¨ç¯‡","date":"2019-01-24T09:36:58.000Z","path":"2019/01/24/Sparkç¬”è®°-åº”ç”¨ç¯‡/","text":"","tags":[]},{"title":"Sparkç¬”è®°-åŸºç¡€æ“ä½œç¯‡","date":"2019-01-24T09:36:35.000Z","path":"2019/01/24/Sparkç¬”è®°-åŸºç¡€æ“ä½œç¯‡/","text":"æœ¬ç¯‡ä¸»è¦æ˜¯ä¸ºäº†ç†Ÿæ‚‰PySparkçš„åŸºæœ¬æ“ä½œï¼Œæ²¡å•¥ç‰¹åˆ«çš„å†…å®¹ã€‚ 1. å‡†å¤‡å·¥ä½œ å¯åŠ¨Hadoop å¯åŠ¨Spark åœ¨Jupyterä¸­æµ‹è¯•PySpark 123456789101112131415161718import pysparkfrom pyspark import SparkContext as scfrom pyspark import SparkConffrom pyspark.sql import SparkSession# conf = SparkConf().setAppName('spark-basic-oprations') \\ .setMaster('local[*]')sc = sc.getOrCreate(conf)# spark = SparkSession.builder \\ .appName('spark-basic-oprations') \\ .master('local[*]') \\ .config('spark.some.config.option', 'some-value') \\ .config('spark.debug.maxToStringFields', '50') \\ .getOrCreate() 2. RDDç›¸å…³æ“ä½œ2.1 RDDçš„åˆ›å»º è¯»å–å¤–éƒ¨æ•°æ® æ•°æ®ä½äºHDFSä¸Š æ•°æ®ä½äºæœ¬åœ°ç£ç›˜ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ‰€æœ‰çš„worker nodesä¸Šéƒ½éœ€è¦èƒ½å¤Ÿè®¿é—®è¿™ä¸ªæœ¬åœ°è·¯å¾„ ä»£ç  12345678# æ•°æ®ä½äºHDFSä¸Šhdfs_file = '/user/hadoop/google_news.txt'# æ•°æ®ä½äºæœ¬åœ°ç£ç›˜ï¼Œæ ¼å¼ä¸º(file:// + æœ¬åœ°çš„ç»å¯¹è·¯å¾„)local_file = 'file:///usr/local/spark/data/google_news.txt'lines = sc.textFile(hdfs_file)lines = sc.textFile(local_file) textFileçš„æ•°æ®å¯ä»¥æ˜¯æ–‡ä»¶åã€ç›®å½•å’Œå‹ç¼©æ–‡ä»¶ç­‰ã€‚åé¢è¿˜å¯ä»¥è·Ÿä¸€ä¸ªå‚æ•°ï¼Œç”¨æ¥æŒ‡å®šåˆ†åŒºçš„æ•°ç›® è¯»å–Driverä¸­å·²ç»å­˜åœ¨çš„æ•°æ®é›†åˆ 123nums = list(10)data = sc.parallelize()data.collect() åˆ›å»ºKey-Values-Pair RDD è¿™ä¸ªè·ŸMapReduceä¸­çš„Key-Vlaueé”®å€¼å¯¹æ˜¯å·®ä¸å¤šçš„ï¼Œç»å¸¸éƒ½æ˜¯ (key, value) è¿™æ ·çš„æ ¼å¼ã€‚ä¹‹æ‰€ä»¥ä»‹ç»è¿™æ ·çš„ä¸€ç§æ ¼å¼ï¼Œæ˜¯å› ä¸ºè¿™ä¸ªä¹‹åå¾ˆå¤šéƒ½ä¼šå°†æ•°æ®è½¬æ¢æˆç±»ä¼¼çš„æ•°æ®æ ¼å¼ï¼Œç„¶åå¯¹RDDåšå„ç§transformationåšç»Ÿè®¡ï¼Œæ¯”å¦‚reduceByKeyã€groupByKeyç­‰ã€‚ 123lines = sc.textFile(local_file)kv_pairs = lines.flatMap(lambda x : x.split(' ')).map(lambda x : (x, 1))kv_pairs.take(10) 2.2 RDDå¸¸ç”¨æ“ä½œ Transformation map(func): å°†æ¯ä¸ªå…ƒç´ ä¼ é€’åˆ°å‡½æ•°funcä¸­ï¼Œå¹¶å°†ç»“æœè¿”å›ä¸ºä¸€ä¸ªæ–°çš„æ•°æ®é›† filter(func): ç­›é€‰å‡ºæ»¡è¶³å‡½æ•°funcçš„å…ƒç´ ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®é›† 1234567lines = sc.textFile(local_file)# åˆ¤æ–­æ˜¯å¦ä¸ºç©ºè¡Œï¼Œè¿™é‡Œè¿”å›å€¼ä¸ºTrue/Falselines.filter(lambdas x : len(x) &gt; 0).collect()# è¿‡æ»¤éç©ºè¡Œçš„ï¼Œè¿™é‡Œè¿”å›éç©ºçš„å†…å®¹lines.filter(lambdas x : len(x) &gt; 0).collect() flatMap(func): ä¸map()ç›¸ä¼¼ï¼Œä½†æ¯ä¸ªè¾“å…¥å…ƒç´ éƒ½å¯ä»¥æ˜ å°„åˆ°0æˆ–å¤šä¸ªè¾“å‡ºç»“æœ reduceByKey(func): åº”ç”¨äº (key, value) é”®å€¼å¯¹çš„æ•°æ®é›†æ—¶ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„ (key, value) å½¢å¼çš„æ•°æ®é›†ï¼Œå…¶ä¸­çš„æ¯ä¸ªå€¼æ˜¯å°†æ¯ä¸ªkeyä¼ é€’åˆ°å‡½æ•°funcä¸­è¿›è¡Œèšåˆã€‚å¦‚æœä¹‹å‰æœ‰äº†è§£è¿‡MapReduceçš„è¯ï¼Œå‘ç°å’Œé‡Œé¢çš„Reduceæ“ä½œä¼šæ¯”è¾ƒç›¸ä¼¼ã€‚ groupByKey(): åº”ç”¨äº (key, value) é”®å€¼å¯¹çš„æ•°æ®é›†æ—¶ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„(key, iterable)å½¢å¼çš„æ•°æ®é›†ï¼Œè¿™ä¸ªå’ŒreduceByKeyæœ‰é‚£ä¹ˆä¸€ç‚¹åŒºåˆ«ï¼Œå…¥å‚ä¸éœ€è¦funcã€‚ sortByKey(): å­—é¢æ„æ€ï¼Œæ ¹æ®é”®æ’åºçš„RDD mapValues(func): å…¥å‚çš„funcåªåº”ç”¨äºvalueä¸Šï¼Œä¸å¯¹keyåšä¿®æ”¹ã€‚ join: è¿™ä¸ªæ“ä½œæ¥è‡ªäºdatabaseä¸­ï¼Œæ‰€ä»¥è¿˜å¯ä»¥ç»†åˆ†ä¸ºå†…è¿æ¥()ã€å³å¤–è¿æ¥(rightOuterJoin)å’Œå·¦å¤–è¿æ¥(leftOuterJoin)ç­‰ã€‚ Action count(): è¿”å›æ•°æ®é›†ä¸­çš„å…ƒç´ ä¸ªæ•° collect(): ä»¥æ•°ç»„çš„å½¢å¼è¿”å›æ•°æ®é›†ä¸­çš„æ‰€æœ‰å…ƒç´  first(): è¿”å›æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´  take(): ä»¥æ•°ç»„çš„å½¢å¼è¿”å›æ•°æ®é›†ä¸­çš„å‰nä¸ªå…ƒç´  reduce(func): é€šè¿‡å‡½æ•°func(è¾“å…¥ä¸¤ä¸ªå‚æ•°å¹¶è¿”å›ä¸€ä¸ªå€¼)èšåˆæ•°æ®é›†ä¸­çš„å…ƒç´  1234567lines = sc.textFile(local_file)# è®¡ç®—å•è¡Œçš„é•¿åº¦lines_length = lines.map(lambdas x : len(x))# è®¡ç®—æ€»é•¿åº¦total_length = lines_length.reduce(lambda a, b : a + b) foreach(func): å°†æ•°æ®é›†ä¸­çš„æ¯ä¸ªå…ƒç´ ä¼ é€’åˆ°å‡½æ•°funcä¸­è¿è¡Œ 2.3 RDDæŒä¹…åŒ– éæŒä¹…åŒ– 1234567891011data = [\"Hadoop\", \"Spark\", \"Hive\"]rdd = sc.parallelize(data)# éæŒä¹…åŒ–ï¼Œé‡å¤è®¡ç®—äº†## è¡ŒåŠ¨æ“ä½œï¼Œè§¦å‘ä¸€æ¬¡çœŸæ­£ä»å¤´åˆ°å°¾çš„è®¡ç®—print(rdd.count()) # &gt;&gt; 3## è¡ŒåŠ¨æ“ä½œï¼Œè§¦å‘ä¸€æ¬¡çœŸæ­£ä»å¤´åˆ°å°¾çš„è®¡ç®—print(', '.join(rdd.collect())) # &gt;&gt;&gt; Hadoop, Spark, Hive æŒä¹…åŒ– persist() persistæ¥å—ä¸€ä¸ªå‚æ•° MEMORY_ONLY è¡¨ç¤ºå°†RDDä½œä¸ºååºåˆ—åŒ–çš„å¯¹è±¡å­˜å‚¨äºJVMä¸­ï¼Œå¦‚æœå†…å­˜ä¸è¶³ï¼Œå°±è¦æŒ‰ç…§LRUåŸåˆ™æ›¿æ¢ç¼“å­˜ä¸­çš„å†…å®¹ MEMORY_AND_DISK è¡¨ç¤ºå°†RDDä½œä¸ºååºåˆ—åŒ–çš„å¯¹è±¡å­˜å‚¨åœ¨JVMä¸­ï¼Œå¦‚æœå†…å­˜ä¸è¶³ï¼Œè¶…å‡ºçš„åˆ†åŒºå°†ä¼šè¢«å­˜æ”¾åœ¨ç¡¬ç›˜ä¸Š ä»£ç  12345678910111213# æŒä¹…åŒ–data = [\"Hadoop\", \"Spark\", \"Hive\"]rdd = sc.parallelize(data)# ä¼šè°ƒç”¨persist(MEMORY_ONLY)ï¼Œä½†æ˜¯ï¼Œè¯­å¥æ‰§è¡Œåˆ°è¿™é‡Œï¼Œå¹¶ä¸ä¼šç¼“å­˜rddï¼Œè¿™æ˜¯rddè¿˜æ²¡æœ‰è¢«è®¡ç®—ç”Ÿæˆï¼Œæ¥ä¸‹æ¥ç¬¬ä¸€æ¬¡actionçš„æ—¶å€™æ‰ä¼šç¼“å­˜ç»“æœrdd.cache() # ç¬¬ä¸€æ¬¡è¡ŒåŠ¨æ“ä½œï¼Œè§¦å‘ä¸€æ¬¡çœŸæ­£ä»å¤´åˆ°å°¾çš„è®¡ç®—ï¼Œè¿™æ—¶æ‰ä¼šæ‰§è¡Œä¸Šé¢çš„rdd.cache()ï¼ŒæŠŠè¿™ä¸ªrddæ”¾åˆ°ç¼“å­˜ä¸­print(rdd.count()) # &gt;&gt;&gt; 3# ç¬¬äºŒæ¬¡è¡ŒåŠ¨æ“ä½œï¼Œä¸éœ€è¦è§¦å‘ä»å¤´åˆ°å°¾çš„è®¡ç®—ï¼Œåªéœ€è¦é‡å¤ä½¿ç”¨ä¸Šé¢ç¼“å­˜ä¸­çš„rddprint(', '.join(rdd.collect())) # &gt;&gt;&gt; Hadoop, Spark, Hive ç§»é™¤æŒä¹…åŒ– å¯ä»¥ä½¿ç”¨unpersist()æ–¹æ³•æ‰‹åŠ¨åœ°æŠŠæŒä¹…åŒ–çš„RDDä»ç¼“å­˜ä¸­ç§»é™¤ 2.4 åˆ†åŒº RDDæ˜¯å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œé€šå¸¸RDDå¾ˆå¤§ï¼Œä¼šè¢«åˆ†æˆå¾ˆå¤šä¸ªåˆ†åŒºï¼Œåˆ†åˆ«ä¿å­˜åœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚RDDåˆ†åŒºçš„ä¸€ä¸ªåˆ†åŒºåŸåˆ™æ˜¯ä½¿å¾—åˆ†åŒºçš„ä¸ªæ•°å°½é‡ç­‰äºé›†ç¾¤ä¸­çš„CPUæ ¸å¿ƒ(core)æ•°ç›®ã€‚ å¯¹äºä¸åŒçš„Sparkéƒ¨ç½²æ¨¡å¼è€Œè¨€(åˆ†åˆ«æ˜¯æœ¬åœ°æ¨¡å¼ã€Standaloneæ¨¡å¼ã€YARNæ¨¡å¼ã€Mesosæ¨¡å¼)ï¼Œéƒ½å¯ä»¥é€šè¿‡è®¾ç½®spark.default.parallelismè¿™ä¸ªå‚æ•°çš„å€¼ï¼Œæ¥é…ç½®é»˜è®¤çš„åˆ†åŒºæ•°ç›®ï¼Œä¸€èˆ¬è€Œè¨€ï¼š æœ¬åœ°æ¨¡å¼ï¼šé»˜è®¤ä¸ºæœ¬åœ°æœºå™¨çš„CPUæ•°ç›®ï¼Œè‹¥è®¾ç½®äº†local[N],åˆ™é»˜è®¤ä¸ºNï¼› Apache Mesosï¼šé»˜è®¤çš„åˆ†åŒºæ•°ä¸º8ï¼› Standaloneæˆ–YARNï¼šåœ¨â€œé›†ç¾¤ä¸­æ‰€æœ‰CPUæ ¸å¿ƒæ•°ç›®æ€»å’Œâ€å’Œâ€œ2â€äºŒè€…ä¸­å–è¾ƒå¤§å€¼ä½œä¸ºé»˜è®¤å€¼ï¼› 2.5 å…±äº«å˜é‡åœ¨Sparkåˆ†å¸ƒå¼è®¡ç®—ä¸­ï¼Œä¸åŒçš„è®¡ç®—èŠ‚ç‚¹ä¸Šçš„ä¸åŒäººç‰©ä¼šè°ƒç”¨åŒä¸€ä¸ªå‡½æ•°ï¼Œé‡Œé¢ä¼šè®¾è®¡åˆ°ä¸€äº›æ•°æ®çš„ä½¿ç”¨ã€‚ å¦‚æœæ¯ä¸ªèŠ‚ç‚¹åªæ˜¯è¿›è¡Œè¯»æ“ä½œï¼Œé‚£ä¹ˆå¯ä»¥å°†è¯¥ä»½æ•°æ®æ‹·è´åˆ°æ¯å°èŠ‚ç‚¹çš„ä¸åŒä»»åŠ¡ä¸Šã€‚ å¦‚æœéœ€è¦åœ¨ä¸åŒèŠ‚ç‚¹ä¸åŒä»»åŠ¡ä¹‹é—´ï¼Œè¯¥å˜é‡ï¼Œå³å­˜åœ¨è¯»å†™çš„æ“ä½œã€‚æ­¤æ—¶åº”è¯¥è®¾ç½®è¿™äº›å˜é‡ä¸ºå…±äº«å˜é‡ã€‚ æ ¹æ®ä»¥ä¸Šéœ€æ±‚ï¼ŒSparkä¸­æä¾›äº†ä¸¤ç§ç±»å‹çš„å˜é‡ å¹¿æ’­å˜é‡(broadcast variables) æ¯å°æœºå™¨ç¼“å­˜ä¸€ä¸ªåªè¯»å˜é‡(éæ¯ä¸ªä»»åŠ¡)ï¼Œè¿™æ ·å¯ä»¥éå¸¸é«˜æ•ˆçš„ç»™æ¯ä¸ªèŠ‚ç‚¹æä¾›ä¸€ä¸ªå¤§çš„è¾“å…¥æ•°æ®é›†çš„å‰¯æœ¬ã€‚ éœ€è¦è¢«ç¼“å­˜çš„æ•°æ®åœ¨è¢«å¹¿æ’­åï¼Œä¼šå…ˆç»“æœåºåˆ—åŒ–ï¼Œç„¶ååœ¨è¢«è°ƒç”¨çš„æ—¶å€™å†è¿›è¡Œååºåˆ—åŒ–ã€‚ Question ä¸ºä»€ä¹ˆhaodoopéœ€è¦åºåˆ—åŒ–æ•°æ®ï¼Ÿ å½“éœ€è¦å¤šæ¬¡å¤šåœ°æ–¹ä½¿ç”¨çš„æ—¶å€™ï¼Œå¤šæ¬¡çš„ååºåˆ—åŒ–æ˜¯å¦å½±å“æ•ˆç‡ï¼Ÿ å…·ä½“æ“ä½œ 1234broadcast_data = sc.broadcast([1, 2, 3])broadcast_data.value&gt;&gt;&gt; [1, 2, 3] ä¸Šé¢çš„SparkContext.broadcast(v)æ˜¯åœ¨æ™®é€šçš„å˜é‡ä¸­åˆ›å»ºä¸€ä¸ªå¹¿æ’­å˜é‡ ç´¯åŠ å™¨(accumulators) 3. DataFrameç›¸å…³æ“ä½œ3.1 Dataframe v.s. RDD RDD RDDæ˜¯åˆ†å¸ƒå¼çš„Javaå¯¹è±¡çš„é›†å’Œï¼Œå¦‚ä¸Šå›¾çš„RDDæ˜¯ä»¥ä¸ºPersonä¸ºç±»å‹å‚æ•°ï¼Œä½†Personç±»çš„å†…éƒ¨ç»“æœå¯¹äºRDDæ˜¯ä¸å¯çŸ¥çš„(æœ‰æ ¡éªŒæœºåˆ¶ä¹ˆï¼Ÿ) DataFrame DFçœ‹ç€å’ŒPythonä¸­çš„DataFrameå·®ä¸å¤šï¼Œæ˜¯ä»¥RDDä¸ºåŸºç¡€çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œä¹Ÿæ˜¯åˆ†å¸ƒå¼çš„Rowå¯¹è±¡çš„é›†å’Œï¼Œæä¾›äº†æ•°æ®çš„è¯¦ç»†ç»“æ„ä¿¡æ¯ï¼Œå³Schemaã€‚ åŒæ—¶RDDä¹Ÿæ˜¯æƒ°æ€§æœºåˆ¶çš„ï¼Œåªè®°å½•äº†å„ç§çš„é€»è¾‘è½¬æ¢çš„è·¯çº¿ï¼Œå³DAGå›¾ã€‚ç­‰éœ€è¦è®¡ç®—ç»“æœçš„æ—¶å€™ï¼Œå†è®¡ç®—ã€‚ 3.2 åˆ›å»ºDataFrame ç›´æ¥åˆ›å»º 12345678910111213141516171819202122232425262728spark = SparkSession.builder \\ .appName('spark-basic-oprations') \\ .master('local[*]') \\ .config('spark.some.config.option', 'some-value') \\ .config('spark.debug.maxToStringFields', '50') \\ .getOrCreate() df = spark.read.text(file)df.show()# å’Œ Python ä¸­çš„DataFrameå¾ˆç›¸ä¼¼&gt;&gt;&gt; +--------------------+ | value| +--------------------+ |As of 2013, Googl...| | | |The service cover...| | | |On December 1, 20...| | | |The layout of Goo...| | | |On July 14, 2011,...| | | |Additionally in J...| | | |In June 2017, the...| +--------------------+ RDDè½¬æ¢æˆDataFrame åˆ©ç”¨åå°„æœºåˆ¶æ¨æ–­åŒ…å«ç‰¹å®šç±»å‹å¯¹è±¡çš„RDDçš„Schemaï¼Œé€‚ç”¨å¯¹å·²çŸ¥çš„æ•°æ®ç»“æ„ ä½¿ç”¨ç¼–ç¨‹æ¥å£ï¼Œæ„é€ ä¸€ä¸ªSchemaå¹¶å°†å…¶åº”ç”¨åœ¨å·²çŸ¥çš„RDDä¸Š 3.2 DataFrameå¸¸ç”¨æ“ä½œä¸‹é¢è¿™äº›æ“ä½œå’ŒPythonä¸­é˜ŸDataFrame 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# æ‰“å°æ¨¡å¼ä¿¡æ¯df.printSchema()&gt;&gt;&gt; root |-- age: long (nullable = true) |-- name: string (nullable = true) # é€‰æ‹©å¤šåˆ—df.select(df.name,df.age + 1).show()&gt;&gt;&gt; +-------+---------+ | name|(age + 1)| +-------+---------+ |Michael| null| | Andy| 31| | Justin| 20| +-------+---------+ # æ¡ä»¶è¿‡æ»¤df.filter(df.age &gt; 20 ).show()&gt;&gt;&gt; +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ # åˆ†ç»„èšåˆdf.groupBy(\"age\").count().show()&gt;&gt;&gt; +----+-----+ | age|count| +----+-----+ | 19| 1| |null| 1| | 30| 1| +----+-----+ # æ’åºdf.sort(df.age.desc()).show()&gt;&gt;&gt; +----+-------+ | age| name| +----+-------+ | 30| Andy| | 19| Justin| |null|Michael| +----+-------+ # å¤šåˆ—æ’åºdf.sort(df.age.desc(), df.name.asc()).show()&gt;&gt;&gt; +----+-------+ | age| name| +----+-------+ | 30| Andy| | 19| Justin| |null|Michael| +----+-------+ # å¯¹åˆ—è¿›è¡Œé‡å‘½ådf.select(df.name.alias(\"username\"),df.age).show()&gt;&gt;&gt; +--------+----+ |username| age| +--------+----+ | Michael|null| | Andy| 30| | Justin| 19| +--------+----+ 4. Spark Streaming æ•°æ®æ€»ä½“ä¸Šå¯ä»¥åˆ†ä¸ºé™æ€æ•°æ®å’Œæµæ•°æ®ã€‚å¯¹é™æ€æ•°æ®å’Œæµæ•°æ®çš„å¤„ç†ï¼Œå¯¹åº”ç€ä¸¤ç§æˆªç„¶ä¸åŒçš„è®¡ç®—æ¨¡å¼ï¼šæ‰¹é‡è®¡ç®—å’Œå®æ—¶è®¡ç®—ã€‚ æ‰¹é‡è®¡ç®—ä»¥â€œé™æ€æ•°æ®â€ä¸ºå¯¹è±¡ï¼Œå¯ä»¥åœ¨å¾ˆå……è£•çš„æ—¶é—´å†…å¯¹æµ·é‡æ•°æ®è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œè®¡ç®—å¾—åˆ°æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚Hadoopå°±æ˜¯å…¸å‹çš„æ‰¹å¤„ç†æ¨¡å‹ï¼Œç”±HDFSå’ŒHBaseå­˜æ”¾å¤§é‡çš„é™æ€æ•°æ®ï¼Œç”±MapReduceè´Ÿè´£å¯¹æµ·é‡æ•°æ®æ‰§è¡Œæ‰¹é‡è®¡ç®—ã€‚ æµæ•°æ®å¿…é¡»é‡‡ç”¨å®æ—¶è®¡ç®—ï¼Œå®æ—¶è®¡ç®—æœ€é‡è¦çš„ä¸€ä¸ªéœ€æ±‚æ˜¯èƒ½å¤Ÿå®æ—¶å¾—åˆ°è®¡ç®—ç»“æœï¼Œä¸€èˆ¬è¦æ±‚å“åº”æ—¶é—´ä¸ºç§’çº§ã€‚å½“åªéœ€è¦å¤„ç†å°‘é‡æ•°æ®æ—¶ï¼Œå®æ—¶è®¡ç®—å¹¶ä¸æ˜¯é—®é¢˜ï¼›ä½†æ˜¯ï¼Œåœ¨å¤§æ•°æ®æ—¶ä»£ï¼Œä¸ä»…æ•°æ®æ ¼å¼å¤æ‚ã€æ¥æºä¼—å¤šï¼Œè€Œä¸”æ•°æ®é‡å·¨å¤§ï¼Œè¿™å°±å¯¹å®æ—¶è®¡ç®—æå‡ºäº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œé’ˆå¯¹æµæ•°æ®çš„å®æ—¶è®¡ç®—â€”â€”æµè®¡ç®—ï¼Œåº”è¿è€Œç”Ÿã€‚ å¤„ç†Spark Streamingï¼Œå…¶ä»–æµè®¡ç®—æ¡†æ¶æœ‰ Twitter Storm(å¯å®ç°æ¯«ç§’çº§æµè®¡ç®—)ã€Yahoo S4ã€‚ æµè®¡ç®—å¤„ç†è¿‡ç¨‹åŒ…æ‹¬å¦‚ä¸‹å‡ éƒ¨åˆ†ï¼š æ•°æ®å®æ—¶é‡‡é›† å¸¸è§çš„å¼€æºåˆ†å¸ƒå¼æ—¥å¿—é‡‡é›†ç³»ç»Ÿ Facebookçš„Scribe LinkedInçš„Kafka æ·˜å®çš„TimeTunnel åŸºäºHadoopçš„Chukwaå’ŒFlume å®é™…å®æ—¶è®¡ç®— å®æ—¶æŸ¥è¯¢æœåŠ¡ 4.1 åˆ›å»ºStreamingContextå¯¹è±¡1234567891011121314# åœ¨pysparkä¸­åˆ›å»ºfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextssc = StreamingContext(sc, 1)# ç¼–å†™ä¸€ä¸ªç‹¬ç«‹çš„Spark Streamingç¨‹åºï¼Œè€Œä¸æ˜¯åœ¨pysparkä¸­è¿è¡Œfrom pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextconf = SparkConf().setAppName('spark-streaming').setMaster('local[*]') sc = SparkContext(conf=conf)ssc = StreamingContext(sc, 1) ç›‘å¬æœ¬åœ°æ–‡ä»¶æµ ä¸Šé¢ä¾‹å­ä¸­ï¼Œå¼€å¯sscçš„ç›‘å¬åï¼Œæ¯éš”5ç§’(ä¹‹å‰åˆå§‹åŒ–è®¾å®šçš„å€¼)ä¼šæŸ¥è¯¢ä¸€æ¬¡ï¼Œä½†åªå¤„ç†æ–°å¢çš„æ–‡ä»¶ï¼Œå¯¹å·²ç»å­˜åœ¨çš„å†å²æ–‡ä»¶ä¸åšå¤„ç†ã€‚ ç›‘å¬å¥—æ¥å­—æµ 12345678910111213141516171819202122from __future__ import print_functionimport sysfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextif __name__ == \"__main__\": if len(sys.argv) != 3: print(\"Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;\", file=sys.stderr) exit(-1) sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\") ssc = StreamingContext(sc, 1) lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2])) counts = lines.flatMap(lambda line: line.split(\" \")) \\ .map(lambda word: (word, 1)) \\ .reduceByKey(lambda a, b: a + b) counts.pprint() ssc.start() ssc.awaitTermination() ç›‘å¬RDDé˜Ÿåˆ—æµ 12345678910111213141516171819202122232425import time from pyspark import SparkContextfrom pyspark.streaming import StreamingContext if __name__ == \"__main__\": sc = SparkContext(appName=\"PythonStreamingQueueStream\") ssc = StreamingContext(sc, 1) # Create the queue through which RDDs can be pushed to # a QueueInputDStream rddQueue = [] for i in range(5): rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)] # Create the QueueInputDStream and use it do some processing inputStream = ssc.queueStream(rddQueue) mappedStream = inputStream.map(lambda x: (x % 10, 1)) reducedStream = mappedStream.reduceByKey(lambda a, b: a + b) reducedStream.pprint() ssc.start() time.sleep(6) ssc.stop(stopSparkContext=True, stopGraceFully=True) ç›‘å¬Kafka ç›‘å¬Flume","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.com/tags/Spark/"},{"name":"PySpark","slug":"PySpark","permalink":"http://chenson.com/tags/PySpark/"}]},{"title":"Sparkç¬”è®°-åŸç†ç¯‡","date":"2019-01-24T08:28:11.000Z","path":"2019/01/24/Sparkç¬”è®°-åŸç†ç¯‡/","text":"è¿˜åœ¨UNSWè¯»ç ”çš„æ—¶å€™ï¼Œä¿®è¿‡ä¸€é—¨COMP9313ï¼Œå¤§æ•°æ®ç›¸å…³è¯¾ç¨‹çš„æ—¶å€™æœ‰å­¦è¿‡ä¸€ç‚¹Sparkï¼Œä½†é‚£ä¼šåªæ˜¯åšè¿‡ä¸€äº›ç®€å•çš„Projectsï¼Œç®€å•çš„åº”ç”¨è¿‡Sparkå’ŒMapReduceè¿™ä¿©è®¡ç®—æ¡†æ¶ï¼Œå¹¶æ²¡æœ‰å®é™…å·¥ç¨‹ä¸Šçš„ç»éªŒã€‚æœ€è¿‘å› ä¸ºå·¥ä½œçš„åŸå› ï¼Œéœ€è¦ä½¿ç”¨Sparkåšä¸€äº›ç‰¹å¾å·¥ç¨‹çš„è®¡ç®—ï¼Œå› ä¸ºæ•°æ®é‡è¿˜æ˜¯æŒºå¤§çš„ï¼Œå¥½å‡ åäº¿æ¡çš„æ•°æ®å§ã€‚ç”¨SQLçš„è¯ï¼Œä¸æ˜¯é‚£ä¹ˆçš„çµæ´»ï¼Œç”¨Pythonè®¡ç®—è¿™äº›çš„è¯ï¼Œä¼°è®¡è¿˜æ˜¯æŒºå‘›çš„ï¼Œæ‰€ä»¥Sparkæ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚ å…³äºå­¦ä¹ è®¡åˆ’ï¼Œæ‰“ç®—å…ˆç†Ÿæ‚‰ä¸€ä¸‹Sparkçš„ä¸€äº›åŸºæœ¬åŸç†å’ŒPySparkçš„åŸºæœ¬æ“ä½œï¼Œç„¶åæ¥ä¸‹æ¥å†è®°å½•ä¸€ä¸‹åšç‰¹å¾å·¥ç¨‹ç›¸å…³çš„å†…å®¹ã€‚ 1. Sparkçš„è®¾è®¡ä¸è¿è¡ŒåŸç†1.1 Sparkç”Ÿæ€ç³»ç»Ÿ Spark Core - åŸºæœ¬åŠŸèƒ½æ¨¡å—ï¼Œå¦‚å†…å­˜è®¡ç®—ã€ä»»åŠ¡è°ƒåº¦ã€éƒ¨ç½²æ¨¡å¼ã€æ•…éšœæ¢å¤ã€å­˜å‚¨ç®¡ç†ç­‰ Spark SQL - å¯ä»¥ç›´æ¥æ“ä½œSparkä¸­çš„RDD Spark Streaming - ç”¨äºè®¡ç®—æµæ•°æ®çš„ï¼Œæ”¯æŒé«˜ååé‡ã€å¯å®¹é”™å¤„ç†çš„å®æ—¶æµæ•°æ®å¤„ç†ï¼Œå…¶æ ¸å¿ƒæ€è·¯æ˜¯å°†æµå¼è®¡ç®—åˆ†è§£æˆä¸€ç³»åˆ—çŸ­å°çš„æ‰¹å¤„ç†ä½œä¸š Spark MLLib - æœºå™¨å­¦ä¹ çš„åŒ…ï¼Œé«˜çº§çš„æ¥å£æœ‰ML GraphX - ç”¨äºå›¾è®¡ç®— 1.2 SparkåŸºæœ¬æ¦‚å¿µ RDD - å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(Resilient Distributed Dataset)ï¼Œæ˜¯åˆ†å¸ƒå¼å†…å­˜çš„ä¸€ä¸ªæŠ½è±¡æ¦‚å¿µï¼Œæä¾›äº†ä¸€ç§é«˜åº¦å—é™çš„å…±äº«å†…å­˜æ¨¡å‹ï¼ˆé«˜åº¦å—é™ä¸æ˜¯å¾ˆç†è§£ï¼‰ DAG - æœ‰å‘æ— ç¯å›¾ (Directed Acyclic Graph)ï¼Œä¸»è¦åæ˜ äº†RDDä¹‹é—´çš„ä¾èµ–å…³ç³» Executor - è¿è¡Œåœ¨å·¥ä½œèŠ‚ç‚¹(Work Node)ä¸Šçš„ä¸€ä¸ªè¿›ç¨‹ åº”ç”¨ - ç”¨æˆ·ç¼–å†™çš„Sparkåº”ç”¨ç¨‹åº ä»»åŠ¡ - è¿è¡Œåœ¨Excutorä¸Šçš„å·¥ä½œå•å…ƒ ä½œä¸š - ä¸€ä¸ªä½œä¸šåŒ…å«å¤šä¸ªRDDåŠä½œç”¨äºç›¸åº”RDDä¸Šçš„å„ç§æ“ä½œ é˜¶æ®µ - ä½œä¸šçš„åŸºæœ¬è°ƒåº¦å•ä½ï¼Œä¸€ä¸ªä½œä¸šä¼šåˆ†ä¸ºå¤šç»„ä»»åŠ¡ï¼Œæ¯ç»„ä»»åŠ¡è¢«ç§°ä¸ºâ€œé˜¶æ®µâ€ï¼Œæˆ–è€…ä¹Ÿè¢«ç§°ä¸ºâ€œä»»åŠ¡é›†â€ 1.3 Sparkæ¶æ„è®¾è®¡ä¸€ä¸‹ä¸¤éƒ¨åˆ†å°±çœ‹çœ‹å°±å¥½äº†ï¼ŒåˆæœŸä¸ä¸€å®šéœ€è¦å¾ˆæ·±å…¥çš„äº†è§£ã€‚åˆæœŸçš„é‡ç‚¹æˆ‘è§‰å¾—åº”è¯¥æ˜¯å…ˆç†è§£RDDéƒ¨åˆ†ã€‚ 1.4 Sparkè¿è¡ŒåŸºæœ¬æµç¨‹ 1.5 Sparkæ ¸å¿ƒéƒ¨åˆ†ä¹‹ä¸€RDDSparkç›¸æ¯”MapReduceæ›´åŠ é«˜æ•ˆï¼Œå¾ˆé‡è¦ä¸€ç‚¹æ˜¯å»ºç«‹åœ¨ç»Ÿä¸€çš„æŠ½åƒRDDä¹‹ä¸Šã€‚MapReduceåœ¨è®¡ç®—çš„æ—¶å€™ï¼Œä¼šæ‹†åˆ†æˆå¤šä¸ªMapså’ŒReducesï¼Œè¿™äº›éƒ½ä¼šè¢«åˆ†é…åœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šï¼Œæ¯ä¸ªèŠ‚ç‚¹ä¼šå°†è‡ªå·±è®¡ç®—å¥½çš„ä¸­é—´ç»“æœå†™å…¥åˆ°HDFSä¸Šã€‚é‚£ä¹ˆè¿™ä¸ªè¿‡ç¨‹å°±é€ æˆå¾ˆå¤šçš„é‡å¤è®¡ç®—ã€æ•°æ®å¤åˆ¶ã€ç£ç›˜IOå’Œåºåˆ—åŒ–å¼€é”€ã€‚è€Œåœ¨Sparkä¸­ï¼ŒRDDçš„å­˜åœ¨å°±ä¼šé¿å…äº†ä¸Šé¢æåˆ°çš„è¿™å‡ ç‚¹ï¼Œä»è€Œä½¿Sparkæ›´åŠ é«˜æ•ˆã€‚ é‚£ä¹ˆä»€ä¹ˆæ˜¯RDDå‘¢ï¼Ÿä»æœ€åä¸€ä¸ªå•è¯çœ‹ï¼Œè¿™ä¸ªæ˜¯ä¸€ä¸ªDatasetï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªæ•°æ®é›†åˆï¼Œä¸€ä¸ªåˆ†å¸ƒå¼å¯¹è±¡é›†åˆ(ä¸æ˜¯å¾ˆç†è§£)ã€‚åœ¨ä¸€å¼€å§‹æåˆ°RDDçš„æ—¶å€™ï¼Œè¯´äº†RDDæä¾›äº†ä¸€ç§é«˜åº¦å—é™çš„å…±äº«å†…å­˜æ¨¡å‹ï¼Œå¯¹é«˜åº¦å—é™ä¸æ˜¯å¾ˆç†è§£ï¼Œçœ‹äº†ä¸‹å¥½åƒæ˜¯å› ä¸ºè¿™ä¸ªé›†åˆæ˜¯åªè¯»çš„ã€‚å³ä¸€ä¸ªRDDé‡Œé¢æ˜¯å¯ä»¥æœ‰å¤šä¸ªè®°å½•åˆ†åŒºçš„ï¼Œä¹Ÿå¯ä»¥åˆ†å¸ƒåœ¨é›†ç¾¤ä¸­ä¸åŒçš„èŠ‚ç‚¹ä¸Šï¼Œä½†æ˜¯è¿™äº›æ•°æ®é›†æ˜¯åªè¯»çš„ï¼Œä¸èƒ½ç›´æ¥ä¿®æ”¹ã€‚(æœ‰ç‚¹åƒC++ä¸­çš„å…±äº«å†…å­˜æ¨¡å‹) 1.5.1 RDDçš„æ“ä½œ Transformation è½¬æ¢çš„æ„æ€ï¼Œä»å­—é¢ä¸Šç†è§£å°±æ˜¯è¿™ä¸ªè½¬åˆ°é‚£ä¸ªï¼Œé‚£ä¸ªè½¬åˆ°å¦å¤–ä¸€ä¸ªã€‚å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªç±»ä¼¼æµç¨‹å›¾ä¸€æ ·ï¼Œé‚£ä¹ˆæ¯èµ°ä¸€æ­¥å°±æ˜¯æŒ‡å®šäº†å…¶ä¾èµ–å…³ç³»ï¼ŒRDDä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»(ä¸å¤ªç†è§£ä¸ºä»€ä¹ˆæ˜¯RDDä¹‹é—´ï¼Œè€Œä¸æ˜¯RDDä¹‹å†…)ã€‚ å¸¸è§çš„Transformationæœ‰Mapã€Filterã€Groupbyã€Joinç­‰ï¼Œå…¶è¾“å…¥æ˜¯ä¸€ä¸ªRDDï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªRDDã€‚ Questions: å¦‚æœå¯¹ä¸€ä¸ªRDDè¿ç»­æ‰§è¡ŒMapã€Filterã€Joinç­‰ï¼Œé‚£ä¹ˆä¸­é—´è¿‡ç¨‹ä¼šæœ‰å¤šå°‘ä¸ªRDDå‘¢ï¼Ÿå¦‚æœæœ‰å¤šä¸ªè¿™äº›RDDä¼šä¸ä¼šå¾ˆå å†…å­˜ç­‰ï¼Ÿ Action å­—é¢æ„æ€å°±æ˜¯è¡ŒåŠ¨ï¼Œå¤§è‡´å°±æ˜¯éœ€è¦è¾“å‡ºä¸€ä¸ªç»“æœã€‚å¸¸è§çš„Actionæœ‰Countï¼ŒCollectç­‰ï¼Œå…¶è¾“å…¥æ˜¯ä¸€ä¸ªRDDï¼Œä½†æ˜¯è¾“å‡ºä¸ºä¸€ä¸ªå€¼æˆ–è€…ç»“æœã€‚ 1.5.2 RDDçš„æ‰§è¡Œæµç¨‹ RDDè¯»å…¥å¤–éƒ¨æˆ–è€…å†…å­˜çš„é›†åˆè¿›è¡Œåˆ›å»º(è¯»å®Œä¹‹åå½“å‰åªæ˜¯ä¸€ä¸ªRDD)ï¼› ç»è¿‡å¤šä¸ªTransformationsï¼Œæ¯æ¬¡è½¬æ¢éƒ½ä¼šäº§ç”Ÿä¸åŒçš„RDDï¼Œä¾›ä¸‹ä¸€æ¬¡Transformationä½¿ç”¨ï¼› æœ€åä¸€ä¸ªæ˜¯Actionæ“ä½œï¼Œç„¶åè¾“å‡ºåˆ°å¤–éƒ¨æ•°æ®æºæˆ–è€…Scalaé›†åˆæˆ–è€…æ ‡é‡ã€‚ å›ç­”ä¸€ä¸‹ä¸Šé¢çš„å›°æƒ‘ï¼ŒRDDé‡‡ç”¨çš„æƒ°æ€§è°ƒç”¨ï¼Œå­—é¢æ„æ€æ˜¯åªæœ‰åœ¨éœ€è¦çš„æ—¶å€™æ‰ä¼šå»åšï¼Œå³åœ¨Actionçš„æ—¶å€™ï¼Œæ‰ä¼šå»è®¡ç®—ä¹‹å‰çš„Transformationsæ“ä½œã€‚é‚£ä¹ˆæ„å‘³ç€ä¸ä¼šè¾“å‡ºåˆ°å¤–éƒ¨æ•°æ®æºæˆ–è€…Scalaé›†åˆç­‰ï¼Œä¸ä¼šå®é™…è®¡ç®—å‡ºä¸­é—´ç»“æœï¼Œå‡å°‘æ•°æ®IOç­‰æ“ä½œã€‚ æ–°çš„å›°æƒ‘ï¼šæ‰€ä»¥RDDåªæ˜¯ä¸€ä¸ªè½¬æ¢è®°å½•çš„é›†åˆï¼Œé‚£ä¹‹å‰æ€ä¹ˆè¯´æ˜¯åˆ†å¸ƒå¼å¯¹è±¡é›†åˆï¼Œåˆ°åº•ä¼šä¸ä¼šå ç”¨HDFSç£ç›˜æˆ–è€…å†…å­˜å‘¢ï¼Ÿ 1.5.3 RDDç‰¹æ€§ é«˜æ•ˆçš„å®¹é”™æ€§(åˆ†å¸ƒå¼å…±äº«å†…å­˜ã€é”®å€¼å­˜å‚¨ã€å†…å­˜æ•°æ®åº“) ä¸­é—´ç»“æœæŒä¹…åŒ–åˆ°å†…å­˜(Sparkæ˜¯åŸºäºå†…å­˜è®¡ç®—çš„ï¼Œä¸­é—´æ•°æ®æ˜¯æŒä¹…åŒ–åˆ°å†…å­˜ï¼Œè€Œä¸æ˜¯ç£ç›˜ï¼Œé¿å…è¯»å†™IOå¼€é”€ï¼Œä½†å½“Tçº§çš„æ•°æ®æ˜¯å¦‚ä½•å†…å­˜è¶³å¤Ÿå­˜ä¸‹æ•°æ®çš„å‘¢ï¼Ÿ) å­˜æ”¾çš„æ•°æ®å¯ä»¥æ˜¯Javaå¯¹è±¡ï¼Œé¿å…äº†ä¸å¿…è¦çš„å¯¹è±¡åºåˆ—åŒ–å’Œååºåˆ—åŒ–å¼€é”€(è®°å¾—Hadoopä¸­æ˜¯æœ‰è‡ªå·±çš„ä¸€å¥—åºåˆ—åŒ–çš„ï¼Œå’ŒJavaä¸ä¸€æ ·) 1.5.4 RDDä¹‹é—´çš„ä¾èµ–å…³ç³» çª„ä¾èµ–(Narrow Dependency) ä¸€ä¸ªæˆ–å¤šä¸ªçˆ¶RDDåˆ†åŒºå¯¹åº”ä¸€ä¸ªå­RDDåˆ†åŒºï¼Œå³ä¸€å¯¹ä¸€æˆ–è€…å¤šå¯¹ä¸€ï¼› ç”Ÿæˆçª„ä¾èµ–å…³ç³»çš„Transformationsæœ‰Mapï¼ŒFilterï¼ŒUnionç­‰ å®½ä¾èµ–(Wide Dependency) ä¸€ä¸ªçˆ¶RDDåˆ†åŒºå¯¹åº”å¤šä¸ªå­RDDåˆ†åŒºï¼Œå³ä¸€å¯¹å¤šï¼› ç”Ÿæˆå®½ä¾èµ–å…³ç³»çš„Transformationsæœ‰Groupbyï¼ŒSortbykeyç­‰ Joinæ¯”è¾ƒç‰¹æ®Šï¼Œä¸¤è€…éƒ½æœ‰å¯èƒ½æ˜¯ã€‚ Questionsï¼š ä¸æ˜¯å¾ˆç†è§£ä¸åŒTransformationsç”Ÿæˆçš„ä¾èµ–å…³ç³»ä¸åŒ å…¶å®ä¸Šé¢çš„ç†è§£ç‚¹åº”è¯¥æ˜¯åˆ†åŒºï¼Œçˆ¶åˆ†åŒºå’Œå­åˆ†åŒºçš„å…³ç³»ã€‚æ ¹æ®MapReduceçš„è®¡ç®—æ¡†æ¶ï¼ŒGroupbyçš„æ“ä½œè‚¯å®šæ˜¯ä¼šé€ æˆä¸€å¯¹å¤šçš„åˆ†åŒºã€‚åŒæ—¶åœ¨MapReduceä¸­Mapç»™æˆ‘æ„Ÿè§‰ä¹Ÿæ˜¯ä¼šæœ‰ä¸€å¯¹å¤šï¼Œæ˜¯å¦è¿™é‡Œçš„Mapå’ŒMapReduceä¸­çš„Mapä¸å¤ªä¸€æ ·ï¼Ÿ 2. References å­é›¨å¤§æ•°æ®ä¹‹Sparkå…¥é—¨æ•™ç¨‹(Pythonç‰ˆ)","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.com/tags/Spark/"}]},{"title":"jupyterhubé…ç½®å¤šç”¨æˆ·","date":"2019-01-13T12:42:12.000Z","path":"2019/01/13/jupyterhubé…ç½®å¤šç”¨æˆ·/","text":"æœ€è¿‘ç»„é‡Œç”³è¯·äº†ä¸€å°å¸¦GPUçš„æœåŠ¡å™¨ï¼ŒNvidiaçš„M40ï¼Œè™½ç„¶æ€§èƒ½ä¸€èˆ¬ï¼Œä½†ç©ä¸€ç©ä¹Ÿå¤Ÿç”¨äº†ã€‚å› ä¸ºæ˜¯Centosçš„ç³»ç»Ÿï¼Œç»„é‡Œçš„å°ä¼™ä¼´è§‰å¾—å®‰è£…ä¸å¦‚Ubuntuæ–¹ä¾¿ï¼Œéƒ½ä¸æƒ³é¼“æ£ç¯å¢ƒï¼Œç„¶åå°±ç”±æˆ‘æ¥å®‰è£…äº†ã€‚ä¹‹å‰å·²ç»å®‰è£…äº†CUDAå’Œæ˜¾å¡é©±åŠ¨ï¼Œå› ä¸ºå®‰è£…æ—¶å€™çš„ç¬”è®°æ²¡æ•´ç†å¥½ï¼Œå°±å…ˆæ•´ç†å®‰è£…jupyterå¤šç”¨æˆ·çš„æ•™ç¨‹ã€‚åç»­æœ‰ç©ºçš„æ—¶å€™å†å†™å‰ç½®çš„ç¯å¢ƒæ•™ç¨‹å§ï¼ˆå…¶å®å¦‚æœä¸€å¼€å§‹å°±ç»™ä¸€ä¸ªå¹²å‡€çš„ç¯å¢ƒï¼Œå®‰è£…è¶…ç®€å•çš„ï¼‰ã€‚ 1. ç³»ç»Ÿç¯å¢ƒæˆ‘çš„ç³»ç»Ÿï¼šCentOS Linux release 7.3.1611 (Core) 1234567# æŸ¥çœ‹ç³»ç»Ÿç‰ˆæœ¬lsb_release -a# ä»¥ä¸‹äºŒç§æ–¹æ³•é€‚ç”¨äºRedHat,CentOScat /etc/redhat-releaserpm -q centos-releaserpm -q redhat-release 2. å®‰è£…æ­¥éª¤2.1 å®‰è£…ä¾èµ–åŒ…1234yum install sqlite-devel npm nodejs-legacy zlib-devel openssl-devel# è¿™ä¸ªå¼€å¯äº†ï¼Œå¯¼è‡´æˆ‘åé¢ä¸€ç›´æŠ¥é”™ï¼Œå¤´ç–¼ï¼Œå°±killç›¸å…³è¿›ç¨‹å°±okäº†npm install -g configurable-http-proxy 2.2 å®‰è£…Anaconda312345wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.shbash Anaconda3-4.4.0-Linux-x86_64.sh# å®‰è£…å®Œååº”è¯¥ä¼šæç¤ºä½ æ˜¯å¦è¦å°†anaconda3è·¯å¾„åŠ å…¥åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­å»ï¼Œå½“ç„¶é€‰æ‹©æ˜¯å•¦ï¼Œç„¶åæ›´æ–°ç¯å¢ƒå˜é‡source ~/.bashrc æ£€æµ‹æ˜¯å¦å®‰è£…æˆåŠŸ 1conda list 2.3 å®‰è£…jupyterhub1conda install -c conda-forge jupyterhub æ£€æµ‹æ˜¯å¦å®‰è£…æˆåŠŸ 12jupyterhub -hconfigurable-http-proxy -h 2.4 åˆ›å»ºjupyterhubçš„é…ç½®æ–‡ä»¶123# jupyterhubæ˜¯åœ¨anaconda3/binè·¯å¾„ä¸‹# è¿™æ¡æŒ‡ä»¤ä¼šç”Ÿæˆä¸€ä¸ªjupyterhub_config.pyçš„é…ç½®æ–‡ä»¶jupyterhub --generate-config ä¿®æ”¹é…ç½®æ–‡ä»¶ 123456789101112131415161718# ipå’Œä½¿ç”¨çš„ç«¯å£å·195:c.JupyterHub.ip = '10.105.xx.xxx'219:c.JupyterHub.port = 9999294:c.JupyterHub.statsd_prefix = 'jupyterhub'375:c.Spawner.cmd = ['jupyterhub-singleuser'] # ç®¡ç†ç”¨æˆ·623: #c.Authenticator.admin_users = &#123;'root'&#125; # ç™½åå•ç”¨æˆ·673:c.Authenticator.whitelist =&#123;'user_name_1','user_name_2','user_name_3', ..., 'user_name_x'&#125;708:c.LocalAuthenticator.create_system_users = True722:c.PAMAuthenticator.encoding = 'utf8'# éå¿…è¦749:c.InteractiveShellApp.exec_lines = ['import os;os.environ.update(&#123;\"JAVA_HOME\": \"/usr/local/jdk1.8.0_74\"&#125;)'] æ³¨æ„ï¼šè¿™é‡Œåœ¨ç™½åå•å’Œç®¡ç†ç”¨æˆ·éƒ½æåˆ°çš„è®¿é—®çš„ç”¨æˆ·ï¼Œè¿™é‡Œçš„ç”¨æˆ·å®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬CentosæœåŠ¡å™¨çš„ç”¨æˆ·ï¼Œå¦‚æœæˆ‘ä»¬é…ç½®çš„ç”¨æˆ·æ²¡åœ¨ç³»ç»Ÿå½“ä¸­ï¼Œåˆ™ä¼šè‡ªåŠ¨åˆ›å»ºï¼Œåˆ›å»ºåéœ€è¦æˆ‘ä»¬æ‰‹å·¥çš„æŒ‡å®šç”¨æˆ·å¯†ç ï¼Œç”¨äºç”¨æˆ·åæœŸç™»å½•jupyterhubã€‚ 2.5 å¼€å¯æœåŠ¡1jupyterhub --config=/etc/jupyterhub/jupyterhub.py --no-ssl ç„¶è€Œå¤±è´¥äº† TAT ä¸€ç›´æŠ¥403é”™è¯¯ï¼Œç„¶ågoogleäº†ä¸€ä¸‹è¿™ä¸ªé—®é¢˜ Jupyterhub sevice unavailable error and http :403 forbidden. æ ¹æ®ä¸Šè¿°çš„é—®é¢˜æ‰¾åˆ°äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆ @carlurips aux | grep configurable-http-proxyif you find any processes, shut them down ç„¶åå†å¯åŠ¨ä¸€å¼€å§‹çš„é‚£æ¡æŒ‡ä»¤å°±å¯ä»¥å®Œç¾è¿è¡Œäº†ã€‚ å¯èƒ½æœ‰äº›ç«¥é‹ä¸çŸ¥é“æ€ä¹ˆåœ¨æœ¬åœ°è®¿é—®æœåŠ¡å™¨ä¸Šçš„æœåŠ¡ï¼Œé‚£ä¹ˆæ­¤æ—¶å°±éœ€è¦å°†ç«¯å£è¿›è¡Œè½¬å‘å•¦ï¼Œè¿è¡Œå¦‚ä¸‹æŒ‡ä»¤ 12# ä¸‹é¢çš„æ„æ€æ˜¯å¤§æ¦‚æˆ‘å…ˆç™»å…¥åˆ°fanchangxnè¿™ä¸ªè·³æ¿æœºï¼Œç„¶åå†è¿åˆ°10.105è¿™å°æœåŠ¡å™¨ä¸Šï¼ŒåŒæ—¶å°†è¿™å°æœºå™¨ä¸Šçš„ç«¯å£7864å’Œæœ¬åœ°ç«¯å£7865æŒ‚è½½åˆ°ä¸€èµ·ssh -N -f -L 7865:10.105.xx.xxx:7864 fanchangxun@182.254.xx.xxx -p36000 å®Œç¾ç™»å…¥ æ­¤æ—¶å‘ç°è¦è´¦å·å’Œå¯†ç æ€ä¹ˆåŠï¼Ÿä¼¼ä¹åˆšåˆšåªæ˜¯é…ç½®äº†ç”¨æˆ·åå­—ï¼Œæ²¡æœ‰è¦æ±‚é…ç½®å¯†ç å•Šã€‚ ç”±äºä¹‹å‰æˆ‘ä¸€ç›´æ˜¯åœ¨rootè´¦å·ä¸‹è¿è¡Œçš„ï¼Œè€Œæˆ‘ä»¬æœ¬æ¬¡é…ç½®çš„ç›®çš„æ˜¯å…è®¸å¤šç”¨æˆ·éš”ç¦»è¿è¡Œè‡ªå·±çš„ä»£ç ï¼Œè€Œå…¶ä»–ç”¨æˆ·æ²¡æœ‰æƒé™æŸ¥çœ‹/ä¿®æ”¹/è¿è¡Œã€‚æ‰€ä»¥å…ˆåœ¨æœ¬å°æœºå™¨ä¸‹åˆ›å»ºä¸€äº›ç”¨æˆ·å’Œå¯†ç ï¼ˆéœ€åœ¨rootè´¦å·ä¸‹ï¼‰ 12345678910111213# æ·»åŠ ç”¨æˆ·adduser user1# è®¾ç½®å¯†ç ï¼Œå¤ªç®€å•ç³»ç»Ÿä¸ºæŠ¥ä¸€ä¸ªbad passwordçš„warnningï¼Œå¯ä»¥å¿½ç•¥# å¯¹äºrootç”¨æˆ·æ¥è¯´ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹å…¶ä»–ç”¨æˆ·çš„å¯†ç passwd user1# ç„¶åè¾“å…¥å¯†ç ï¼Œæ¯”å¦‚ pwd12345678# æ£€æŸ¥æ˜¯å¦åˆ›å»ºæˆåŠŸcut -d: -f1 /etc/passwd# å¦‚æœä¸å°å¿ƒæŠŠè´¦å·åˆ›å»ºé”™äº†ï¼Œéœ€è¦åˆ é™¤ä¹Ÿæ˜¯okçš„(æ°¸ä¹…åˆ é™¤)userdel user1 åˆ›å»ºæˆåŠŸåï¼Œç³»ç»Ÿå°±ä¼šåœ¨/homeä¸‹åˆ›å»ºç›¸åº”çš„user1è¿™ä¸ªæ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªå°±æ˜¯ä»¥åuser1ç”¨çš„æ–‡ä»¶å¤¹ï¼Œç„¶åç”¨åˆšåˆšçš„è´¦å·å’Œå¯†ç ç™»å…¥ä¸Šè¿°çš„jupyterå°±okäº†ï¼ˆå‰æå·²ç»æ·»åŠ åˆ°äº†whitelisté‡Œé¢å»ï¼‰ï¼Œåœ¨é‡Œé¢åˆ›å»ºæ–‡ä»¶å¤¹æˆ–æ˜¯notebookï¼Œå°±ä¼šåœ¨å¯¹åº”çš„/home/user1ä¸‹åˆ›å»ºç›¸åº”çš„æ–‡ä»¶å¤¹ç­‰ã€‚ 2.4 å¼€å¯åå°è¿è¡Œå’Œå…³é—­å› ä¸ºè¿™æ ·çš„æœåŠ¡é€šå¸¸éœ€è¦24hè¿è¡Œçš„ï¼Œæ‰€ä»¥éœ€è¦åå°è¿è¡Œï¼Œé€šå¸¸å¯ä»¥ç”¨nohupã€screenéƒ½å¯ä»¥ã€‚ 12345678# åå°è¿è¡Œnohup jupyterhub --config=/data1/jupyterhub/jupyterhub_config.py --no-ssl &gt; /data1/jupyterhub/nohup.out 2&gt;&amp;1 &amp;# æŸ¥çœ‹æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œåªå¯¹å½“å‰ç»ˆç«¯ç”Ÿæ•ˆjobs# åˆ é™¤è¿è¡Œçš„ä»»åŠ¡ï¼Œåé¢ä¸ºä»»åŠ¡å·fg num_of_jobs 12345# ç„¶è€Œjobsåªçœ‹å½“å‰ç»ˆç«¯ç”Ÿæ•ˆçš„ï¼Œå…³é—­ç»ˆç«¯åï¼Œåœ¨å¦ä¸€ä¸ªç»ˆç«¯jobså·²ç»æ— æ³•çœ‹åˆ°åå°è·‘å¾—ç¨‹åºäº†ï¼Œæ­¤æ—¶åˆ©ç”¨psï¼ˆè¿›ç¨‹æŸ¥çœ‹å‘½ä»¤ï¼‰# a:æ˜¾ç¤ºæ‰€æœ‰ç¨‹åº # u:ä»¥ç”¨æˆ·ä¸ºä¸»çš„æ ¼å¼æ¥æ˜¾ç¤º # x:æ˜¾ç¤ºæ‰€æœ‰ç¨‹åºï¼Œä¸ä»¥ç»ˆç«¯æœºæ¥åŒºåˆ†ps aux | egrep node 12# æˆ–è€…æŸ¥çœ‹å“ªä¸ªç«¯å£å·è¢«å ç”¨lsof -i:7864 12# æˆ–è€…netstat -ap | grep 7864 12# æ€æ­»è¿›ç¨‹kill -9 è¿›ç¨‹å· ç„¶è€Œä¸çŸ¥ä¸ºä½•ä¸€ç›´æ€ä¸æ‰è¿™ä¸ªè¿›ç¨‹ï¼Œåªèƒ½å»çœ‹çœ‹ä»–çš„çˆ¶è¿›ç¨‹ï¼Œæ€æ‰çˆ¶è¿›ç¨‹è¯•è¯•çœ‹ å…ˆæ€äº†4773è¿›ç¨‹ï¼Œå†æŠŠ17959è¿›ç¨‹æ€äº†å°±OKäº†ï¼Œå®Œç¾ã€‚ 3. å®‰è£…å¸¸ç”¨æ’ä»¶3.1 nb_extension12345678910111213# ä¸ºjupyterå®‰è£…extensionpip install jupyter_contrib_nbextensionsconda install -c conda-forge jupyter_contrib_nbextensions# å®‰è£…jupyter nb_extensionjupyter contrib nbextension install --user# å¼€å¯/å…³é—­extensionsjupyter nbextension enable &lt;nbextension require path&gt;jupyter nbextension disable &lt;nbextension require path&gt;# e.gjupyter nbextension enable codefolding/main 3.2 notedownä½¿ç”¨notedownæ’ä»¶æ¥è¯»å†™githubæºæ–‡ä»¶ 12# å®‰è£…nodtedownæ’ä»¶pip install https://github.com/mli/notedown/tarball/master ä¿®æ”¹jupyterhub_config.pyæ–‡ä»¶ 12# å°†é…ç½®æ·»åŠ åˆ°æ–‡ä»¶çš„æœ«å°¾c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager' ä½¿ç”¨æ–¹æ³• 1234# ç›¸äº’è½¬åŒ–notedown input.md &gt; output.ipynbnotedown input.ipynb --to markdown --strip &gt; output.mdnotedown input.ipynb --to markdown &gt; output_with_outputs.md åœ¨IPythonä¸­ä½¿ç”¨ 12345678import reimport sysimport argparsefrom IPython.nbformat.v3.rwbase import NotebookReaderfrom IPython.nbformat.v3.nbjson import JSONWriterimport IPython.nbformat.v3.nbbase as nbbase 4. References Jupyterhubå®‰è£…æµç¨‹ å¦‚ä½•å»ºç«‹ä¸€å€‹åœ˜éšŠç”¨çš„ Jupyter-Hub CentOS 7ä¸­æ·»åŠ ä¸€ä¸ªæ–°ç”¨æˆ·å¹¶æˆæƒ centosç³»ç»Ÿæ·»åŠ /åˆ é™¤ç”¨æˆ·å’Œç”¨æˆ·ç»„ Installing jupyter_contrib_nbextensions Github-notedown","tags":[{"name":"jupyterhub","slug":"jupyterhub","permalink":"http://chenson.com/tags/jupyterhub/"}]},{"title":"Mixed-Logistic-Regressionæ¨¡å‹åˆæ¢","date":"2018-12-22T09:05:55.000Z","path":"2018/12/22/Mixed-Logistic-Regressionæ¨¡å‹åˆæ¢/","text":"","tags":[{"name":"MLR","slug":"MLR","permalink":"http://chenson.com/tags/MLR/"}]},{"title":"å¤§æ•°æ®æ•°ä»“å»ºè®¾ç¬”è®°","date":"2018-12-20T05:02:15.000Z","path":"2018/12/20/å¤§æ•°æ®æ•°ä»“å»ºè®¾ç¬”è®°/","text":"è™½ç„¶ä¸æ˜¯ä»äº‹å¤§æ•°æ®å¼€å‘çš„ï¼Œä½†æˆ‘è§‰å¾—è¿˜æ˜¯æœ‰å¿…è¦å¯¹å¤§æ•°æ®è¿™è¾¹æœ‰ä¸€äº›äº†è§£ï¼Œç®€å•çš„å¯èƒ½æ˜¯å¦‚ä½•ä½¿ç”¨å¤§æ•°æ®çš„ä¸€äº›å·¥å…·ï¼Œæ¯”å¦‚æ˜¯Hiveã€Sparkã€Prestoç­‰ï¼Œä»¥åŠå¦‚ä½•æ—¥æ›´ä¸€å¼ å®½è¡¨ã€‚æ‰€ä»¥å°±æŒ‰ç…§æˆ‘å¹³å¸¸æ¥è§¦å’Œäº†è§£çš„åšäº†ä¸ªç®€å•çš„æ€»ç»“ã€‚ 1. ä¸ºä»€ä¹ˆè¦å»ºè®¾æ•°æ®ä»“åº“ æ•°æ®é›†æˆ ä¸åŒä¸šåŠ¡ç³»ç»Ÿæ•°æ®é›†ä¸­ã€ç»Ÿä¸€ç®¡ç† é¢å‘ä¸»é¢˜ å¯¹ä¸šåŠ¡æ•°æ®åˆ†ä¸»é¢˜å»ºè®¾â€“å‚ä¸äººã€è¡Œä¸ºåŸŸã€é»‘åå• æ—¶é—´ç»´åº¦ æŒ‰å¤©å­˜å‚¨å†å²å¿«ç…§æ•°æ®ï¼Œæ•°æ®å¯è¿½æº¯ ç›¸å¯¹ç¨³å®š æå‡è®¡ç®—èƒ½åŠ› æ•°æ®é¢„æ¸…æ´—å¤„ç†ï¼Œé¢„åŠ å·¥å­˜å‚¨ æ”¯æŒå¤šæ–¹é¢åº”ç”¨ OLAPã€BI 2. æ•°æ®ä»“åº“è§„åˆ’2.1 æ•°æ®ä»“åº“åˆ†å±‚ä»‹ç»ä¸ºä»€ä¹ˆè¦å¯¹æ•°æ®è¿›è¡Œå±‚æ¬¡è®¾è®¡å‘¢ï¼Ÿä¸»è¦æœ‰ä¸€ä¸‹åŸå› ï¼š æ¸…æ™°æ•°æ®ç»“æ„ï¼šæ¯ä¸€ä¸ªæ•°æ®åˆ†å±‚éƒ½æœ‰å®ƒçš„ä½œç”¨åŸŸï¼Œè¿™æ ·æˆ‘ä»¬åœ¨ä½¿ç”¨è¡¨çš„æ—¶å€™èƒ½æ›´æ–¹ä¾¿åœ°å®šä½å’Œç†è§£ã€‚ æ•°æ®è¡€ç¼˜è¿½è¸ªï¼šç®€å•æ¥è®²å¯ä»¥è¿™æ ·ç†è§£ï¼Œæˆ‘ä»¬æœ€ç»ˆç»™ä¸šåŠ¡è¯šä¿¡çš„æ˜¯ä¸€èƒ½ç›´æ¥ä½¿ç”¨çš„å¼ ä¸šåŠ¡è¡¨ï¼Œä½†æ˜¯å®ƒçš„æ¥æºæœ‰å¾ˆå¤šï¼Œå¦‚æœæœ‰ä¸€å¼ æ¥æºè¡¨å‡ºé—®é¢˜äº†ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°å®šä½åˆ°é—®é¢˜ï¼Œå¹¶æ¸…æ¥šå®ƒçš„å±å®³èŒƒå›´ã€‚ å‡å°‘é‡å¤å¼€å‘ï¼šè§„èŒƒæ•°æ®åˆ†å±‚ï¼Œå¼€å‘ä¸€äº›é€šç”¨çš„ä¸­é—´å±‚æ•°æ®ï¼Œèƒ½å¤Ÿå‡å°‘æå¤§çš„é‡å¤è®¡ç®—ã€‚ æŠŠå¤æ‚é—®é¢˜ç®€å•åŒ–ï¼šè®²ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡åˆ†è§£æˆå¤šä¸ªæ­¥éª¤æ¥å®Œæˆï¼Œæ¯ä¸€å±‚åªå¤„ç†å•ä¸€çš„æ­¥éª¤ï¼Œæ¯”è¾ƒç®€å•å’Œå®¹æ˜“ç†è§£ã€‚è€Œä¸”ä¾¿äºç»´æŠ¤æ•°æ®çš„å‡†ç¡®æ€§ï¼Œå½“æ•°æ®å‡ºç°é—®é¢˜ä¹‹åï¼Œå¯ä»¥ä¸ç”¨ä¿®å¤æ‰€æœ‰çš„æ•°æ®ï¼Œåªéœ€è¦ä»æœ‰é—®é¢˜çš„æ­¥éª¤å¼€å§‹ä¿®å¤ã€‚ å±è”½åŸå§‹æ•°æ®çš„å¼‚å¸¸ã€‚ å±è”½ä¸šåŠ¡çš„å½±å“ï¼Œä¸å¿…æ”¹ä¸€æ¬¡ä¸šåŠ¡å°±éœ€è¦é‡æ–°æ¥å…¥æ•°æ® 2.2 ç®€å•ä»‹ç»è¿™é‡Œä¸»è¦ç®€å•ä»‹ç»ä¸Šé¢çš„åˆ†å±‚æ¶æ„ï¼Œå®é™…ä¸Šå¯¹äºå¤§ç‚¹çš„å…¬å¸ï¼Œå¯èƒ½æ•°ä»“åœ¨è®¾è®¡ä¸Šè¿˜æ›´ä¸ºè¯¦ç»†ä¸€äº›ï¼Œä¸ä¼šå¦‚ä¸Šé¢é‚£æ¬ç®€å•ã€‚è¿™é‡Œå¯èƒ½å°±æŒ‰ç…§æˆ‘å¹³å¸¸å·¥ä½œä¸­æ¥è§¦çš„æ¶æ„æ¥ä»‹ç»äº†ã€‚ æºæ•°æ®å±‚ - BDM è¿™éƒ¨åˆ†ä¸€èˆ¬ç”±æ•°æ®ç½‘å…³å»æ‹‰å»æ•°æ®ï¼Œå°½é‡ä¿ç•™æœ€ã¡³çš„æ•°æ®ä¿¡æ¯ï¼Œä¸€èˆ¬ä¸ºæœ€åŸå§‹çš„jsonæ•°æ®ã€‚ æ•°æ®å‡†å¤‡å±‚ - ODS(Operational Data Store) è¿™éƒ¨åˆ†çš„æ•°æ®ä¸€èˆ¬æ˜¯ç”±BDMé‚£è¾¹çš„æ•°æ®è§£ææ¥äº†ï¼Œä¼šå¯¹jsonè¿›è¡Œæ‹†è§£ï¼Œå°†jsonæ‹†è§£æˆç»“æ„åŒ–çš„è¡¨ã€‚åœ¨æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å°½å¯èƒ½ä¼šä¿ç•™jsonä¸­æ‰€æœ‰çš„ä¿¡æ¯ï¼Œæ¯”å¦‚è¡¨åå’Œå­—æ®µå€¼ä¹‹ç±»çš„ã€‚ä¸è¿‡ä¹Ÿæœ‰æ—¶å€™ä¼šå¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæ¯”å¦‚è¯´æ•°æ®è„±æ•å¯èƒ½åœ¨è¿™ä¸€å±‚å°±å¼€å§‹å®Œæˆäº†ã€‚ ä¸ºäº†ä¿ç•™åŸå§‹æ•°æ®ï¼Œé€šå¸¸è¿™è¾¹æ˜¯æ¯å¤©å…¨é‡æ›´æ–°çš„ï¼Œä¹Ÿå°±æ˜¯ä¼šä¿ç•™äº†å†å²å˜æ›´çš„æ•°æ®ï¼ŒæŒ‰å¤©åˆ†åŒºå‚¨å­˜ã€‚è¿™ä¸ªéå¸¸é‡è¦æ–¹ä¾¿ä»¥åå›æº¯æ•°æ®ã€‚ å­˜å‚¨ å°½å¯èƒ½ä¿ç•™åŸå§‹æ•°æ®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­—æ®µåç§° æ•°æ®å¤„ç† ç»“æ„åŒ–ï¼Œæ‹†è¡¨ï¼ˆjsonå¤æ‚ç»“æ„ï¼‰ è„±æ•ï¼ˆèº«ä»½è¯ã€æ‰‹æœºå·æ‹†å­—æ®µï¼‰ å»å™ªã€å»é‡ç­‰ã€‚ ç¦æ­¢ ç­›é€‰ã€æ±‡æ€»ã€èˆå»å­—æ®µ æ•°æ®æ˜ç»†å±‚ - DWD è¿™ä¸€å±‚ä¼šåœ¨å­˜å‚¨æ–¹é¢å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»å­˜å‚¨ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ•°æ®åšä¸€äº›ç®€å•çš„å¤„ç†ï¼Œæ¯”å¦‚å»å™ªå•Šï¼Œæ ¼å¼åŒ–å¤„ç†å•Šä¹‹ç±»çš„ã€‚ å­˜å‚¨ åˆ†ä¸»é¢˜ã€å…¨é‡æ•°æ® æ•°æ®å¤„ç† å­—æ®µå‘½åè§„èŒƒåŒ–ã€å…³é”®ç»´åº¦æ˜ å°„ æ—¶é—´ç»´åº¦çš„å¤„ç† ç»´åº¦è½¬ä»£ç†é”®ã€ç©ºå€¼å¤„ç†ã€éå¿…è¦å­—æ®µçš„èˆå¼ƒä¸è½¬æ¢ è„æ•°æ®å¤„ç† ä¸šåŠ¡è¡¨åˆå¹¶ å¢é‡è½¬å…¨é‡ è½»åº¦æ±‡æ€»å±‚ - DWS è¿™ä¸€å±‚å¯¹å·²ç»åˆ†ç±»å’Œå¤„ç†å¥½çš„æ•°æ®åšä¸€äº›è½»åº¦çš„æ±‡æ€»ã€‚ å­˜å‚¨ åˆ†ä¸»é¢˜ã€ç»´åº¦æ±‡æ€»æ•°æ® æ•°æ®å¤„ç† ç»´åº¦æ±‡æ€»ã€ä¸åŒé¢—ç²’åº¦çš„ç»´åº¦æ±‡æ€» å„ç±»ä¸šåŠ¡ç»Ÿè®¡åº¦é‡å€¼çš„è®¡ç®— ä¸šåŠ¡å£å¾„é€»è¾‘çš„å®ç° æ•°æ®é›†å¸‚å±‚ - DM ä½œä¸ºä¸€ä¸ªè°ƒåŒ…ä¾ ï¼Œç»å¸¸æ¥è§¦çš„å¯èƒ½å°±æ˜¯è¿™éƒ¨åˆ†äº†ã€‚å› ä¸ºè¿™éƒ¨åˆ†åŸºæœ¬å°±æ˜¯å·²ç»æ±‡æ€»å¥½çš„å®½è¡¨ï¼Œé€šå¸¸éƒ½æ˜¯å¯ä»¥ç›´æ¥å…¥æ¨¡å‹çš„æ•°æ®äº†ï¼Œæˆ–å¯¹è¿™éƒ¨åˆ†æ•°æ®ç®€å¤„ç†ä¸€ä¸‹å°±å¯ä»¥ç›´æ¥ç”¨çš„ã€‚è¿™ä¸€å±‚é€šå¸¸æ˜¯T+1çš„æ—¥æ›´å®½è¡¨ï¼Œå®½è¡¨é€»è¾‘é€šå¸¸ç”±æ¨¡å‹è¿™è¾¹å¼€å‘å¥½æä¾›ç»™å¤§æ•°æ®å¼€å‘ï¼Œæˆ–è€…æ˜¯è¿è¥ã€ä¸šåŠ¡ã€BIé‚£è¾¹æ ¹æ®ä»–ä»¬çš„éœ€æ±‚ï¼Œå°†é€»è¾‘æ•´ç†å¥½ç»™å¤§æ•°æ®å¼€å‘ã€‚ å­˜å‚¨ è·¨ä¸»é¢˜æ¨¡å‹å®ç°ã€å®½è¡¨ æ•°æ®å¤„ç† è·¨ä¸»é¢˜çš„æ±‡æ€»ç»Ÿè®¡è®¡ç®— å±•å¼€ç»´åº¦å½¢æˆå®½è¡¨ ä¸šåŠ¡å£å¾„é€»è¾‘çš„å®ç° 3. æ¶æ„æ€»è§ˆæ•´ä¸ªæ•°ä»“å°±æ˜¯æ ¹æ®ä¸Šé¢çš„åˆ†å±‚æ¥è®¾è®¡çš„ï¼Œä»ç½‘å…³åˆ°ODSåˆ°DWDåˆ°DWSæœ€ååˆ°DMã€‚ 4. DMå±‚å®½è¡¨è®¾è®¡ä¸ç»´æŠ¤12 5. References å¤§æ•°æ®ç¯å¢ƒä¸‹è¯¥å¦‚ä½•ä¼˜é›…åœ°è®¾è®¡æ•°æ®åˆ†å±‚","tags":[{"name":"Big Data","slug":"Big-Data","permalink":"http://chenson.com/tags/Big-Data/"}]},{"title":"æ‹’ç»å›¾åºŠï¼Œä»æˆ‘åšèµ·","date":"2018-12-01T04:29:20.000Z","path":"2018/12/01/æ‹’ç»å›¾åºŠï¼Œä»æˆ‘åšèµ·/","text":"å‰æ®µæ—¶é—´æƒ³æ•´ç†ä¸€ä¸‹ä»¥å‰çš„åšæ–‡ç¬”è®°ï¼Œå› ä¸ºå¾ˆå¤šéƒ½æ˜¯æ—©æœŸè‡ªå·±è®°å½•çš„ç¬”è®°ï¼Œæ¡ç†æ¯”è¾ƒä¹±ã€‚æƒ³ç¨å¾®æ•´ç†ä¸€ä¸‹ï¼Œæ–¹ä¾¿ä»¥åè‡ªå·±å›é¡¾å¤ä¹ ä»¥åŠä»–äººé˜…è¯»ã€‚ä½†å‘ç°ä¸€ä»¶æ¯”è¾ƒå¤´å¤§çš„äº‹æƒ…å°±æ˜¯ï¼Œä»¥å‰æ’å›¾éƒ½æ˜¯ä¸Šä¼ åˆ°å›¾åºŠä¸Šç„¶ï¼Œè€Œç°åœ¨å¾ˆå¤šå›¾ç‰‡éƒ½å·²ç»æŒ‚æ‰äº†ï¼Œé“¾æ¥ä¸å¯è®¿é—®ã€‚è¿™ç®€ç›´å°±æ˜¯ç¾éš¾æ€§çš„ä¸€å¹•ã€‚ æ—©æœŸä¹‹æ‰€ä»¥ç”¨å›¾åºŠæ˜¯å› ä¸ºæ–¹ä¾¿ï¼Œå› ä¸ºChromeæœ‰ä¸ªæ’ä»¶å¯ä»¥ç›´æ¥ä¸Šä¼ ï¼Œé‡‡ç”¨æœ¬åœ°çš„è¯éœ€è¦æŠŠå›¾ç‰‡ä¿å­˜åˆ°æœ¬åœ°è·¯å¾„ç„¶åå¼•ç”¨å¯¹åº”çš„å›¾ç‰‡ï¼Œè‡³äºå¦‚ä½•åŒæ­¥åˆ°githubä¸Šä¸å½±å“æ˜¾ç¤ºæ•ˆæœä¹Ÿæ˜¯æ¯”è¾ƒéº»çƒ¦çš„ã€‚å¥½åœ¨ç°åœ¨Typoraæ›´æ–°äº†å‡ ä¸ªç‰ˆæœ¬åï¼Œå¯¹äºæ’å…¥å›¾ç‰‡å¯ä»¥æ—¶å€™éå¸¸å‹å¥½ï¼ˆè¿™é‡Œè£‚å¢™æ¨èTyporaï¼Œæœ€å¥½ç”¨çš„MarkDownå·¥å…·ï¼Œæ²¡æœ‰ä¹‹ä¸€ï¼‰ã€‚å…·ä½“ä½¿ç”¨æ­¥éª¤å¦‚ä¸‹ï¼š 1. å®‰è£…æ­¥éª¤ å®‰è£…æ’ä»¶ï¼Œä¿®æ”¹Hexoé…ç½® ä¿®æ”¹ä¸»é¡µé…ç½®æ–‡ä»¶_config.ymlé‡Œé¢çš„post_asset_folder : true åœ¨hexoç›®å½•ä¸‹æ‰§è¡Œå‘½ä»¤npm install hexo-asset-image --saveï¼Œå³å®‰è£…ä¸€ä¸ªå¯ä»¥ä¸Šä¼ æœ¬åœ°å›¾ç‰‡çš„æ’ä»¶ å®‰è£…æˆåŠŸåï¼Œä»¥åå†æ–°å»ºåšæ–‡hexo n â€œXXXâ€çš„æ—¶å€™ï¼Œé™¤äº†ç”Ÿæˆå¯¹åº”çš„.mdæ–‡ä»¶çš„åŒæ—¶ï¼Œä¹Ÿä¼šåœ¨åŒä¸€å±‚ç›®å½•ä¸‹ï¼Œå»ºä¸€ä¸ªåŒåçš„æ–‡ä»¶å¤¹ï¼Œç”¨äºå­˜æ”¾è¿™ç¯‡åšæ–‡å¼•ç”¨çš„æœ¬åœ°å›¾ç‰‡ã€‚ 12345XXX.mdXXX|--img1.jpg|--img2.jpg|--img3.jpg ç”Ÿæˆé™æ€é¡µé¢ï¼Œå¯¹åº”çš„htmlæ˜¯ 1&lt;img src=\"/year/month/day/XXX/img1.jpg\" alt=\"img1\"&gt; ä¿®æ”¹Typoraé…ç½®ï¼Œå…·ä½“å¦‚å›¾ é…ç½®å®Œä¹‹åï¼Œå¯ä»¥åœ¨æˆªå›¾å®Œä¹‹åï¼Œå°†å‰ªåˆ‡æ¿çš„å›¾ç‰‡ç›´æ¥å¤åˆ¶åˆ°å¯¹åº”æ–‡ä»¶ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹å†…ï¼Œå³æˆ‘ä»¬ä¸Šé¢æ–°å»ºå¥½çš„æ–‡ä»¶å¤¹ã€‚æœ‰æœ¨æœ‰éå¸¸æ–¹ä¾¿ï¼ï¼ï¼å½“ç„¶è¿™é‡Œä¹Ÿå¯ä»¥è®¾ç½®å…¶ä»–è·¯å¾„ï¼Œåªæ˜¯æˆ‘è¿™é‡Œè®¾ç½®çš„æ˜¯è¿™æ ·ï¼Œä¸ºäº†ä¾¿äºå›¾ç‰‡ç®¡ç†ã€‚ å‘å¸ƒæ–‡ç«  æ–‡ç« å†™å¥½ä¹‹åï¼Œæ‰§è¡ŒæŒ‡ä»¤hexo g; hexo dä¹‹åï¼Œå°±ä¼šç”Ÿæˆå¯¹åº”çš„é™æ€é¡µé¢ï¼Œæ”¾åˆ°publicå¯¹åº”çš„æ–‡ä»¶å¤¹ä¸‹ã€‚OKï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å¼€è‡ªå·±çš„ç½‘ç«™çœ‹ä¸€ä¸‹å°±å¯ä»¥å•¦ã€‚è¿™ä¸ªæ—¶å€™ä½ ä¼šæƒŠå–œçš„å‘ç°ï¼Œå›¾ç‰‡å…¨éƒ½æ— æ³•æ˜¾ç¤ºã€‚å“ˆå“ˆå“ˆï¼Œä¸ºä»€ä¹ˆå˜ï¼Ÿ é‚£ä¹ˆçœ‹ä¸€ä¸‹åŸå§‹makrdownç›®å½•ä¸‹çš„ç»“æ„ å†çœ‹ä¸€ä¸‹å¯¹åº”publicä¸‹çš„ç›®å½•ç»“æ„ å‘ç°å›¾ç‰‡å¯¹åº”çš„æ–‡ä»¶å¤¹æ˜¯ä¸ä¸€æ ·çš„ï¼ˆæ­¤æ—¶å¯å›å¤´çœ‹ç”Ÿæˆå¯¹åº”çš„htmlè·¯å¾„åœ°å€ï¼‰åŸå§‹mdæ˜¯å¼•ç”¨åŒåæ–‡ä»¶å¤¹ä¸‹çš„å›¾ç‰‡ï¼Œè€Œhtmlå´å’Œåº”ç”¨çš„å›¾ç‰‡åœ¨åŒä¸€å±‚ç›®å½•ä¸‹ï¼Œæ‰€ä»¥è¯»å–å¤±è´¥äº†ã€‚ ç”Ÿæˆçš„htmlè·¯å¾„ 1&lt;img src=\"/year/month/day/XXX/img1.jpg\" alt=\"img1\"&gt; ä½†æˆ‘æ¯”è¾ƒæƒ³è¦çš„è·¯å¾„æ˜¯ 1&lt;img src=\"/year/month/day/XXX/year-month-day-XXX/img1.jpg\" alt=\"img1\"&gt; åŸå§‹å›¾ç‰‡å’Œmdæ–‡ç« æ‰€åœ¨çš„è·¯å¾„æ˜¯ 12PATH-TO-BLOG/source/_posts/year-month-day-XXX/img1.jpgPATH-TO-BLOG/source/_posts/year-month-day-XXX.md å½“ç„¶æˆ‘ä»¬å¯ä»¥æ„šè ¢çš„è®¾ç½®æœ¬åœ°çš„mdæ–‡ä»¶ï¼ŒæŠŠå›¾ç‰‡å‰é¢çš„XXXå…ˆå»äº†ï¼Œä½†æ˜¯æ¨ä¸Šå»çš„æ—¶å€™ï¼Œå›¾ç‰‡çš„ç›¸å¯¹è·¯å¾„å°±æ˜¯æ­£å¸¸çš„ã€‚è¿™æ ·è™½ç„¶çº¿ä¸Šå¯ä»¥æ­£å¸¸æ˜¾ç¤ºï¼Œä½†æ˜¯åŒæ—¶ä¼šå¯¼è‡´çº¿ä¸‹æ— æ³•æ˜¾ç¤ºå›¾ç‰‡ï¼Œæ‰€ä»¥è¿™æ–¹æ³•çœŸçš„æ˜¯æœ‰ç‚¹è ¢ã€‚ ç½‘ä¸Šæ‰¾äº†ä¼šè§£å†³æ–¹æ¡ˆä¼¼ä¹éƒ½æ²¡æœ‰æˆåŠŸï¼Œå¯èƒ½æ˜¯æˆ‘çš„æ“ä½œé—®é¢˜ï¼Œæ²¡è®¾ç½®å¥½ã€‚äºæ˜¯å°±è‡ªå·±å†™äº†ä¸ªç®€å•è„šæœ¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ è¿™æ®µè„šæœ¬ä¸»è¦å°±æ˜¯åœ¨publicä¸‹å¯¹åº”blogæ‰€åœ¨çš„æ–‡ä»¶ä¸‹æ–°å»ºä¸€ä¸ªæ—¥æœŸ-æ–‡ç« åçš„æ–‡ä»¶å¤¹ï¼Œç„¶åå°†å›¾ç‰‡ç§»åˆ°è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œé‚£ä¹ˆå›¾ç‰‡å’Œæ–‡ç« çš„ç›¸å¯¹è·¯å¾„å°±å’Œmdæ–‡ä»¶é‡Œé¢æ˜¯ä¸€è‡´çš„äº†ï¼Œæ‰€ä»¥åœ¨æ›´æ–°hexoæ¨ä¸Šå»ï¼Œå›¾ç‰‡å°±æ˜¾ç¤ºæ­£å¸¸å•¦ã€‚ä»¥åå†™æ–‡ç« çš„æ—¶å€™åªéœ€è¦åœ¨éƒ¨ç½²hexohexo dçš„ä¹‹å‰ï¼Œè¿è¡Œä¸€ä¸‹ä¸‹é¢ä»£ç å³å¯ã€‚å¦ˆå¦ˆä»¥åå†ä¹Ÿä¸ç”¨æ‹…å¿ƒæˆ‘å›¾ç‰‡æŒ‚æ‰äº†ï¼ï¼ï¼ hexo g python relocated_img.py hexo d è¿è¡Œä»£ç åçš„æ–‡ä»¶å¤¹ç»“æ„ å…·ä½“ä»£ç å¦‚ä¸‹ï¼š 12345678910111213141516171819202122232425262728293031import os_PUBLIC_ = 'public'_PATH_ = os.getcwd()list_year = [year for year in os.listdir(_PUBLIC_) if year.isdigit() and len(year) == 4]for year in list_year: p_year = os.path.join(_PATH_, _PUBLIC_, year) list_month = [month for month in os.listdir(p_year) if month.isdigit() and len(month) == 2] for month in list_month: p_month = os.path.join(_PATH_, _PUBLIC_, year, month) list_day = [day for day in os.listdir(p_month) if day.isdigit() and len(day) == 2] for day in list_day: p_day = os.path.join(_PATH_, _PUBLIC_, year, month, day) list_blog = [blog for blog in os.listdir(p_day) if not blog.startswith('.')] for blog in list_blog: p_blog = os.path.join(_PATH_, _PUBLIC_, year, month, day, blog) list_file = os.listdir(p_blog) dir_img = '-'.join([year, month, day, blog]) dir_img = os.path.join(p_day, blog, dir_img) if not os.path.exists(dir_img): os.makedirs(dir_img) for file in list_file: if file.endswith('.png'): p_file = os.path.join(_PATH_, _PUBLIC_, year, month, day, blog, file) cmd = f'mv &#123;p_file&#125; &#123;dir_img&#125;' os.system(cmd) 2. Bugsè™½ç„¶è¿™æ–¹æ³•æŒºå¥½ç”¨çš„ï¼Œä½†å‘ç°æœ‰ä¸ªå°bugï¼Œå°±æ˜¯å›¾ç‰‡åœ¨æ”¾åˆ°å¯¹åº”çš„XXXæ–‡ä»¶å¤¹ä¸‹ï¼Œä¼šé‡å¤ä¸€å¼ ã€‚å¦‚æœåœ¨Typoraä¸­è®¾è®¾ç½®æŒ‡å®šçš„custom foldä¸‹å€’ä¸ä¼šï¼Œè®¾ç½®æˆåŒåæ–‡ä»¶å¤¹ä¸‹å°±ä¼šå¤šå‡ºä¸€å¼ ï¼Œç›®å‰æ²¡æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œæš‚ä¸”å…ˆè¿™æ ·å§ã€‚ å½“ç„¶å¦å¤–ä¸€ä¸ªå¯èƒ½çš„é—®é¢˜å°±æ˜¯ä¼šå ç”¨githubçš„ç©ºé—´ï¼Œä½†é‡å°çš„æƒ…å†µä¸‹é—®é¢˜ä¸å¤§ã€‚ 3. References hexoåšå®¢å›¾ç‰‡é—®é¢˜","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chenson.com/tags/hexo/"},{"name":"Typora","slug":"Typora","permalink":"http://chenson.com/tags/Typora/"}]},{"title":"èšç±»ç®—æ³•ï¼šK-MeansåŠæ‰©å±•ç®—æ³•K-Modesã€K-Prototypeåˆæ¢","date":"2018-11-27T05:23:30.000Z","path":"2018/11/27/èšç±»ç®—æ³•ï¼šK-MeansåŠæ‰©å±•ç®—æ³•K-Modesã€K-Prototypeåˆæ¢/","text":"ç”±äºæœ€è¿‘æ­£åœ¨å‚ä¸çš„è‡ªåŠ¨åŒ–å»ºæ¨¡å¹³å°éœ€è¦ç”¨åˆ°è¿™ä¸€ç®—æ³•ï¼Œä½†sklearné‡Œé¢çš„èšç±»ç®—æ³•åªæ”¯æŒæ•°å€¼å‹çš„ï¼Œæ— æ³•ç”¨åˆ°ç±»åˆ«å‹çš„ç‰¹å¾ä¸Šï¼Œæ‰€ä»¥å°±ç ”ç©¶äº†K-Modeså’ŒK-Prototypesè¿™ä¸¤ä¸ªç®—æ³•ã€‚å…·ä½“ç®—æ³•çš„æ€æƒ³å’Œæºç å¦‚ä¸‹ã€‚ 1. k-Means Algorightm K-Means K-Means++ K-Means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321class KMeans(BaseEstimator, ClusterMixin, TransformerMixin): \"\"\"K-Means clustering Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : &#123;'k-means++', 'random' or an ndarray&#125; Method for initialization, defaults to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. n_init : int, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, default: 300 Maximum number of iterations of the k-means algorithm for a single run. tol : float, default: 1e-4 Relative tolerance with regards to inertia to declare convergence precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances verbose : int, default 0 Verbosity mode. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. Attributes ---------- cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : Labels of each point inertia_ : float Sum of squared distances of samples to their closest cluster center. Examples -------- &gt;&gt;&gt; from sklearn.cluster import KMeans &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], ... [4, 2], [4, 4], [4, 0]]) &gt;&gt;&gt; kmeans = KMeans(n_clusters=2, random_state=0).fit(X) &gt;&gt;&gt; kmeans.labels_ array([0, 0, 0, 1, 1, 1], dtype=int32) &gt;&gt;&gt; kmeans.predict([[0, 0], [4, 4]]) array([0, 1], dtype=int32) &gt;&gt;&gt; kmeans.cluster_centers_ array([[1., 2.], [4., 2.]]) See also -------- MiniBatchKMeans Alternative online implementation that does incremental updates of the centers positions using mini-batches. For large scale learning (say n_samples &gt; 10k) MiniBatchKMeans is probably much faster than the default batch implementation. Notes ------ The k-means problem is solved using Lloyd's algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, 'How slow is the k-means method?' SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That's why it can be useful to restart it several times. \"\"\" def __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=1e-4, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto'): self.n_clusters = n_clusters self.init = init self.max_iter = max_iter self.tol = tol self.precompute_distances = precompute_distances self.n_init = n_init self.verbose = verbose self.random_state = random_state self.copy_x = copy_x self.n_jobs = n_jobs self.algorithm = algorithm def _check_test_data(self, X): X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES) n_samples, n_features = X.shape expected_n_features = self.cluster_centers_.shape[1] if not n_features == expected_n_features: raise ValueError(\"Incorrect number of features. \" \"Got %d features, expected %d\" % ( n_features, expected_n_features)) return X def fit(self, X, y=None, sample_weight=None): \"\"\"Compute k-means clustering. Parameters ---------- X : array-like or sparse matrix, shape=(n_samples, n_features) Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) \"\"\" random_state = check_random_state(self.random_state) self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \\ k_means( X, n_clusters=self.n_clusters, sample_weight=sample_weight, init=self.init, n_init=self.n_init, max_iter=self.max_iter, verbose=self.verbose, precompute_distances=self.precompute_distances, tol=self.tol, random_state=random_state, copy_x=self.copy_x, n_jobs=self.n_jobs, algorithm=self.algorithm, return_n_iter=True) return self def fit_predict(self, X, y=None, sample_weight=None): \"\"\"Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" return self.fit(X, sample_weight=sample_weight).labels_ def fit_transform(self, X, y=None, sample_weight=None): \"\"\"Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" # Currently, this just skips a copy of the data if it is not in # np.array or CSR format already. # XXX This skips _check_test_data, which may change the dtype; # we should refactor the input validation. return self.fit(X, sample_weight=sample_weight)._transform(X) def transform(self, X): \"\"\"Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by `transform` will typically be dense. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) return self._transform(X) def _transform(self, X): \"\"\"guts of transform method; no input validation\"\"\" return euclidean_distances(X, self.cluster_centers_) def predict(self, X, sample_weight=None): \"\"\"Predict the closest cluster each sample in X belongs to. In the vector quantization literature, `cluster_centers_` is called the code book and each value returned by `predict` is the index of the closest code in the code book. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to predict. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return _labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[0] def score(self, X, y=None, sample_weight=None): \"\"\"Opposite of the value of X on the K-means objective. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- score : float Opposite of the value of X on the K-means objective. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return -_labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[1] k_means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239def k_means(X, n_clusters, sample_weight=None, init='k-means++', precompute_distances='auto', n_init=10, max_iter=300, verbose=False, tol=1e-4, random_state=None, copy_x=True, n_jobs=1, algorithm=\"auto\", return_n_iter=False): \"\"\"K-means clustering algorithm. Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- X : array-like or sparse matrix, shape (n_samples, n_features) The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. verbose : boolean, optional Verbosity mode. tol : float, optional The relative increment in the results before declaring convergence. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. return_n_iter : bool, optional Whether or not to return the number of iterations. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). best_n_iter : int Number of iterations corresponding to the best results. Returned only if `return_n_iter` is set to True. \"\"\" if n_init &lt;= 0: raise ValueError(\"Invalid number of initializations.\" \" n_init=%d must be bigger than zero.\" % n_init) random_state = check_random_state(random_state) if max_iter &lt;= 0: raise ValueError('Number of iterations should be a positive number,' ' got %d instead' % max_iter) # avoid forcing order when copy_x=False order = \"C\" if copy_x else None X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32], order=order, copy=copy_x) # verify that the number of samples given is larger than k if _num_samples(X) &lt; n_clusters: raise ValueError(\"n_samples=%d should be &gt;= n_clusters=%d\" % ( _num_samples(X), n_clusters)) tol = _tolerance(X, tol) # If the distances are precomputed every job will create a matrix of shape # (n_clusters, n_samples). To stop KMeans from eating up memory we only # activate this if the created matrix is guaranteed to be under 100MB. 12 # million entries consume a little under 100MB if they are of type double. if precompute_distances == 'auto': n_samples = X.shape[0] precompute_distances = (n_clusters * n_samples) &lt; 12e6 elif isinstance(precompute_distances, bool): pass else: raise ValueError(\"precompute_distances should be 'auto' or True/False\" \", but a value of %r was passed\" % precompute_distances) # Validate init array if hasattr(init, '__array__'): init = check_array(init, dtype=X.dtype.type, copy=True) _validate_center_shape(X, n_clusters, init) if n_init != 1: warnings.warn( 'Explicit initial center position passed: ' 'performing only one init in k-means instead of n_init=%d' % n_init, RuntimeWarning, stacklevel=2) n_init = 1 # subtract of mean of x for more accurate distance computations if not sp.issparse(X): X_mean = X.mean(axis=0) # The copy was already done above X -= X_mean if hasattr(init, '__array__'): init -= X_mean # precompute squared norms of data points x_squared_norms = row_norms(X, squared=True) best_labels, best_inertia, best_centers = None, None, None if n_clusters == 1: # elkan doesn't make sense for a single cluster, full will produce # the right result. algorithm = \"full\" if algorithm == \"auto\": algorithm = \"full\" if sp.issparse(X) else 'elkan' if algorithm == \"full\": kmeans_single = _kmeans_single_lloyd elif algorithm == \"elkan\": kmeans_single = _kmeans_single_elkan else: raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\" \" %s\" % str(algorithm)) if n_jobs == 1: # For a single thread, less memory is needed if we just store one set # of the best results (as opposed to one set per run per thread). for it in range(n_init): # run a k-means once # å¯é€‰ï¼škmeans_single_lloyd or kmeans_single_elkan labels, inertia, centers, n_iter_ = kmeans_single( X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, precompute_distances=precompute_distances, tol=tol, x_squared_norms=x_squared_norms, random_state=random_state) # determine if these results are the best so far if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia best_n_iter = n_iter_ else: # parallelisation of k-means runs seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(kmeans_single)(X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, tol=tol, precompute_distances=precompute_distances, x_squared_norms=x_squared_norms, # Change seed to ensure variety random_state=seed) for seed in seeds) # Get results with the lowest inertia labels, inertia, centers, n_iters = zip(*results) best = np.argmin(inertia) best_labels = labels[best] best_inertia = inertia[best] best_centers = centers[best] best_n_iter = n_iters[best] if not sp.issparse(X): if not copy_x: X += X_mean best_centers += X_mean distinct_clusters = len(set(best_labels)) if distinct_clusters &lt; n_clusters: warnings.warn(\"Number of distinct clusters (&#123;&#125;) found smaller than \" \"n_clusters (&#123;&#125;). Possibly due to duplicate points \" \"in X.\".format(distinct_clusters, n_clusters), ConvergenceWarning, stacklevel=2) if return_n_iter: return best_centers, best_labels, best_inertia, best_n_iter else: return best_centers, best_labels, best_inertia _kmeans_single_lloyd 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127def _kmeans_single_lloyd(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): \"\"\"A single run of k-means, assumes preparation completed prior. Parameters ---------- X : array-like of floats, shape (n_samples, n_features) The observations to cluster. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,) The weights for each observation in X. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (k, p) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. tol : float, optional The relative increment in the results before declaring convergence. verbose : boolean, optional Verbosity mode x_squared_norms : array Precomputed x_squared_norms. precompute_distances : boolean, default: True Precompute distances (faster but takes more memory). random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). n_iter : int Number of iterations run. \"\"\" random_state = check_random_state(random_state) sample_weight = _check_sample_weight(X, sample_weight) best_labels, best_inertia, best_centers = None, None, None # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) if verbose: print(\"Initialization complete\") # Allocate memory to store the distances for each sample to its # closer center for reallocation in case of ties distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype) # iterations for i in range(max_iter): centers_old = centers.copy() # labels assignment is also called the E-step of EM labels, inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, centers, precompute_distances=precompute_distances, distances=distances) # computation of the means is also called the M-step of EM if sp.issparse(X): centers = _k_means._centers_sparse(X, sample_weight, labels, n_clusters, distances) else: centers = _k_means._centers_dense(X, sample_weight, labels, n_clusters, distances) if verbose: print(\"Iteration %2d, inertia %.3f\" % (i, inertia)) if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia center_shift_total = squared_norm(centers_old - centers) if center_shift_total &lt;= tol: if verbose: print(\"Converged at iteration %d: \" \"center shift %e within tolerance %e\" % (i, center_shift_total, tol)) break if center_shift_total &gt; 0: # rerun E-step in case of non-convergence so that predicted labels # match cluster centers best_labels, best_inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, best_centers, precompute_distances=precompute_distances, distances=distances) return best_labels, best_inertia, best_centers, i + 1 _kmeans_single_elkan 123456789101112131415161718192021222324252627def _kmeans_single_elkan(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): if sp.issparse(X): raise TypeError(\"algorithm='elkan' not supported for sparse input X\") random_state = check_random_state(random_state) if x_squared_norms is None: x_squared_norms = row_norms(X, squared=True) # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) centers = np.ascontiguousarray(centers) if verbose: print('Initialization complete') checked_sample_weight = _check_sample_weight(X, sample_weight) centers, labels, n_iter = k_means_elkan(X, checked_sample_weight, n_clusters, centers, tol=tol, max_iter=max_iter, verbose=verbose) if sample_weight is None: inertia = np.sum((X - centers[labels]) ** 2, dtype=np.float64) else: sq_distances = np.sum((X - centers[labels]) ** 2, axis=1, dtype=np.float64) * checked_sample_weight inertia = np.sum(sq_distances, dtype=np.float64) return labels, inertia, centers, n_iter 2. K-Modes Algorithm step1ï¼šéšæœºç¡®å®škä¸ªèšç±»ä¸­å¿ƒ$C_1$, $C_2$ â€¦ $C_k$ï¼Œ$C_i$æ˜¯é•¿åº¦ä¸ºMçš„å‘é‡ï¼Œ$C_i$ = [$C_{1i}$, $C_{2i}$, â€¦ , $C_{mi}$] step2ï¼šå¯¹äºæ ·æœ¬$x_j$ (j=1,2,â€¦,N)ï¼Œåˆ†åˆ«æ¯”è¾ƒå…¶ä¸kä¸ªä¸­å¿ƒä¹‹é—´çš„è·ç¦» è¿™é‡Œçš„**è·ç¦»ä¸ºä¸åŒå±æ€§å€¼çš„ä¸ªæ•°**ï¼Œå‡å¦‚$x_1$=[1, 2, 1, 3], $C_1$=[1, 2, 3, 4]ï¼Œé‚£ä¹ˆx1ä¸C1ä¹‹é—´çš„è·ç¦»ä¸º2 step3ï¼šå°†$x_j$åˆ’åˆ†åˆ°è·ç¦»æœ€å°çš„ç°‡ï¼Œåœ¨å…¨éƒ¨çš„æ ·æœ¬éƒ½è¢«åˆ’åˆ†å®Œæ¯•ä¹‹åï¼Œé‡æ–°ç¡®å®šç°‡ä¸­å¿ƒï¼Œå‘é‡$C_i$ä¸­çš„æ¯ä¸€ä¸ªåˆ†é‡éƒ½æ›´æ–°ä¸ºç°‡iä¸­çš„ä¼—æ•° step4ï¼šé‡å¤æ­¥éª¤äºŒå’Œä¸‰ï¼Œç›´åˆ°æ€»è·ç¦»ï¼ˆå„ä¸ªç°‡ä¸­æ ·æœ¬ä¸å„è‡ªç°‡ä¸­å¿ƒè·ç¦»ä¹‹å’Œï¼‰ä¸å†é™ä½ï¼Œè¿”å›æœ€åçš„èšç±»ç»“æœ KModes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152class KModes(BaseEstimator, ClusterMixin): \"\"\"k-modes clustering algorithm for categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. cat_dissim : func, default: matching_dissim Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : &#123;'Huang', 'Cao', 'random' or an ndarray&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centroids. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. verbose : int, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, cat_dissim=matching_dissim, init='Cao', n_init=1, verbose=0, random_state=None, n_jobs=1): self.n_clusters = n_clusters # ç°‡çš„ä¸ªæ•° self.max_iter = max_iter # self.cat_dissim = cat_dissim # ç±»åˆ«é—´è·ç¦»çš„è®¡ç®—æ–¹æ³• self.init = init # åˆå§‹ä¸­å¿ƒç‚¹çš„é€‰å–æ–¹æ³• &#123;'Huang', 'Cao', 'random' # or an ndarray&#125; self.n_init = n_init # è¿è¡Œæ¬¡æ•°å–æœ€ä½³å€¼ self.verbose = verbose # self.random_state = random_state # self.n_jobs = n_jobs # if ((isinstance(self.init, str) and self.init == 'Cao') or hasattr(self.init, '__array__')) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method and algorithm are deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, **kwargs): \"\"\"Compute k-modes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] \"\"\" random_state = check_random_state(self.random_state) self._enc_cluster_centroids, self._enc_map, self.labels_,\\ self.cost_, self.n_iter_ = k_modes(X, self.n_clusters, self.max_iter, self.cat_dissim, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def fit_predict(self, X, y=None, **kwargs): \"\"\"Compute cluster centroids and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). \"\"\" return self.fit(X, **kwargs).predict(X, **kwargs) def predict(self, X, **kwargs): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" if self.verbose and self.cat_dissim == ng_dissim: print(\"Ng's dissimilarity measure was used to train this model, \" \"but now that it is predicting the model will fall back to \" \"using simple matching dissimilarity.\") X = check_array(X, dtype=None) X, _ = encode_features(X, enc_map=self._enc_map) return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return decode_centroids(self._enc_cluster_centroids, self._enc_map) else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_modes 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def k_modes(X, n_clusters, max_iter, dissim, init, n_init, verbose, random_state, n_jobs): \"\"\"k-modes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-modes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values X = check_array(X, dtype=None) # Convert the categorical values in X to integers for speed. # Based on the unique values in X, we can make a mapping to achieve this. X, enc_map = encode_features(X) n_points, n_attrs = X.shape assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = unique results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_modes_single)(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best] k_modes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, random_state): \"\"\" X : n_clusters : n_points : n_points, n_attrs = X.shape n_attrs : max_iter : Maximum number of iterations of the k-modes algorithm for a single run. dissim : func, default: matching_dissim, &#123;matching_dissim, ng_dissim&#125; Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : inti_no : verbose : random_state : \"\"\" random_state = check_random_state(random_state) # _____ INIT _____ # åˆå§‹åŒ–ï¼Œé€‰å–ä¸­å¿ƒç‚¹ if verbose: print(\"Init: initializing centroids\") # 1. method huang if isinstance(init, str) and init.lower() == 'huang': centroids = init_huang(X, n_clusters, dissim, random_state) # 2. method cao elif isinstance(init, str) and init.lower() == 'cao': centroids = init_cao(X, n_clusters, dissim) # 3. éšæœºé€‰å– elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = X[seeds] # 4. ä½¿ç”¨æä¾›å¥½çš„array elif hasattr(init, '__array__'): # Make sure init is a 2D array. if len(init.shape) == 1: init = np.atleast_2d(init).T assert init.shape[0] == n_clusters, \\ \"Wrong number of initial centroids in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[0], n_clusters) assert init.shape[1] == n_attrs, \\ \"Wrong number of attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[1], n_attrs) centroids = np.asarray(init, dtype=np.uint16) else: raise NotImplementedError if verbose: print(\"Init: initializing clusters\") # rows : n_clusters, cols : n_points membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # cl_attr_freq is a list of lists with dictionaries that contain the # frequencies of values per cluster and attribute. # rows : n_clusters, cols : n_attrs cl_attr_freq = [[defaultdict(int) for _ in range(n_attrs)] for _ in range(n_clusters)] for ipoint, curpoint in enumerate(X): # Initial assignment to clusters # è¿”å›è·ç¦»æœ€è¿‘çš„ä¸­å¿ƒç‚¹ clust = np.argmin(dissim(centroids, curpoint, X=X, membship=membship)) membship[clust, ipoint] = 1 # Count attribute values per cluster. for iattr, curattr in enumerate(curpoint): cl_attr_freq[clust][iattr][curattr] += 1 # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(n_attrs): if sum(membship[ik]) == 0: # Empty centroid, choose randomly centroids[ik, iattr] = random_state.choice(X[:, iattr]) else: centroids[ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_modes_iter(X, centroids, cl_attr_freq, membship, dissim, random_state) # All points seen in this iteration labels, ncost = _labels_cost(X, centroids, dissim, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, cost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, cost)) return centroids, labels, cost, itr 3. K-prototype Algorithm KPrototypes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160class KPrototypes(kmodes.KModes): \"\"\"k-protoypes clustering algorithm for mixed numerical/categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. num_dissim : func, default: euclidian_dissim Dissimilarity function used by the algorithm for numerical variables. Defaults to the Euclidian dissimilarity function. cat_dissim : func, default: matching_dissim Dissimilarity function used by the kmodes algorithm for categorical variables. Defaults to the matching dissimilarity function. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. init : &#123;'Huang', 'Cao', 'random' or a list of ndarrays&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If a list of ndarrays is passed, it should be of length 2, with shapes (n_clusters, n_features) for numerical and categorical data respectively. These are the initial centroids. gamma : float, default: None Weighing factor that determines relative importance of numerical vs. categorical attributes (see discussion in Huang [1997]). By default, automatically calculated from data. verbose : integer, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. gamma : float The (potentially calculated) weighing factor. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, num_dissim=euclidean_dissim, cat_dissim=matching_dissim, init='Huang', n_init=10, gamma=None, verbose=0, random_state=None, n_jobs=1): super(KPrototypes, self).__init__(n_clusters, max_iter, cat_dissim, init, verbose=verbose, random_state=random_state, n_jobs=n_jobs) self.num_dissim = num_dissim self.gamma = gamma self.n_init = n_init if isinstance(self.init, list) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method is deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, categorical=None): \"\"\"Compute k-prototypes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] categorical : Index of columns that contain categorical data \"\"\" random_state = check_random_state(self.random_state) # If self.gamma is None, gamma will be automatically determined from # the data. The function below returns its value. self._enc_cluster_centroids, self._enc_map, self.labels_, self.cost_,\\ self.n_iter_, self.gamma = k_prototypes(X, categorical, self.n_clusters, self.max_iter, self.num_dissim, self.cat_dissim, self.gamma, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def predict(self, X, categorical=None): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. categorical : Index of columns that contain categorical data Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) Xcat, _ = encode_features(Xcat, enc_map=self._enc_map) return _labels_cost(Xnum, Xcat, self._enc_cluster_centroids, self.num_dissim, self.cat_dissim, self.gamma)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return [ self._enc_cluster_centroids[0], decode_centroids(self._enc_cluster_centroids[1], self._enc_map) ] else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_prototypes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677def k_prototypes(X, categorical, n_clusters, max_iter, num_dissim, cat_dissim, gamma, init, n_init, verbose, random_state, n_jobs): \"\"\"k-prototypes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-prototypes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values if categorical is None or not categorical: raise NotImplementedError( \"No categorical data selected, effectively doing k-means. \" \"Present a list of categorical columns, or use scikit-learn's \" \"KMeans instead.\" ) if isinstance(categorical, int): categorical = [categorical] assert len(categorical) != X.shape[1], \\ \"All columns are categorical, use k-modes instead of k-prototypes.\" assert max(categorical) &lt; X.shape[1], \\ \"Categorical index larger than number of columns.\" ncatattrs = len(categorical) nnumattrs = X.shape[1] - ncatattrs n_points = X.shape[0] assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) # Convert the categorical values in Xcat to integers for speed. # Based on the unique values in Xcat, we can make a mapping to achieve this. Xcat, enc_map = encode_features(Xcat) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = list(_split_num_cat(unique, categorical)) init[1], _ = encode_features(init[1], enc_map) # Estimate a good value for gamma, which determines the weighing of # categorical values in clusters (see Huang [1997]). if gamma is None: gamma = 0.5 * Xnum.std() results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_prototypes_single)(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) # Note: return gamma in case it was automatically determined. return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best], gamma k_prototypes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132def k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, random_state): # For numerical part of initialization, we don't have a guarantee # that there is not an empty cluster, so we need to retry until # there is none. random_state = check_random_state(random_state) init_tries = 0 # åˆå§‹åŒ–å‡ ä¸ªä¸­å¿ƒç‚¹ # å¯èƒ½å­˜åœ¨å¤±è´¥çš„æƒ…å†µ while True: init_tries += 1 # _____ INIT _____ if verbose: print(\"Init: initializing centroids\") if isinstance(init, str) and init.lower() == 'huang': centroids = kmodes.init_huang(Xcat, n_clusters, cat_dissim, random_state) elif isinstance(init, str) and init.lower() == 'cao': centroids = kmodes.init_cao(Xcat, n_clusters, cat_dissim) elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = Xcat[seeds] elif isinstance(init, list): # Make sure inits are 2D arrays. init = [np.atleast_2d(cur_init).T if len(cur_init.shape) == 1 else cur_init for cur_init in init] assert init[0].shape[0] == n_clusters, \\ \"Wrong number of initial numerical centroids in init \" \\ \"(&#123;&#125;, should be &#123;&#125;).\".format(init[0].shape[0], n_clusters) assert init[0].shape[1] == nnumattrs, \\ \"Wrong number of numerical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[0].shape[1], nnumattrs) assert init[1].shape[0] == n_clusters, \\ \"Wrong number of initial categorical centroids in init (&#123;&#125;, \" \\ \"should be &#123;&#125;).\".format(init[1].shape[0], n_clusters) assert init[1].shape[1] == ncatattrs, \\ \"Wrong number of categorical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[1].shape[1], ncatattrs) centroids = [np.asarray(init[0], dtype=np.float64), np.asarray(init[1], dtype=np.uint16)] else: raise NotImplementedError(\"Initialization method not supported.\") if not isinstance(init, list): # Numerical is initialized by drawing from normal distribution, # categorical following the k-modes methods. meanx = np.mean(Xnum, axis=0) stdx = np.std(Xnum, axis=0) centroids = [ meanx + random_state.randn(n_clusters, nnumattrs) * stdx, centroids ] if verbose: print(\"Init: initializing clusters\") # è®¡ç®—å¯¹åº”ç»„å†…çš„æˆå‘˜ membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # Keep track of the sum of attribute values per cluster so that we # can do k-means on the numerical attributes. cl_attr_sum = np.zeros((n_clusters, nnumattrs), dtype=np.float64) # Same for the membership sum per cluster cl_memb_sum = np.zeros(n_clusters, dtype=int) # cl_attr_freq is a list of lists with dictionaries that contain # the frequencies of values per cluster and attribute. cl_attr_freq = [[defaultdict(int) for _ in range(ncatattrs)] for _ in range(n_clusters)] for ipoint in range(n_points): # Initial assignment to clusters # è®¡ç®—åˆå§‹çš„å½’å±ç±»ï¼Œåˆ†åˆ«ç”±ç±»åˆ«çš„å’Œæ•°å€¼å‹æ•°æ®ç»„æˆ # gammaæ˜¯ç±»åˆ«ç‰¹å¾çš„æƒé‡ clust = np.argmin( num_dissim(centroids[0], Xnum[ipoint]) + gamma * cat_dissim(centroids[1], Xcat[ipoint], X=Xcat, membship=membship) ) membship[clust, ipoint] = 1 cl_memb_sum[clust] += 1 # Count attribute values per cluster. for iattr, curattr in enumerate(Xnum[ipoint]): cl_attr_sum[clust, iattr] += curattr for iattr, curattr in enumerate(Xcat[ipoint]): cl_attr_freq[clust][iattr][curattr] += 1 # If no empty clusters, then consider initialization finalized. # å¦‚æœæ¯ä¸€ç»„çš„æˆå‘˜ä¸ªæ•°éƒ½å¤§äº0çš„è¯ï¼Œåˆ™ç®—æ˜¯åˆå§‹åŒ–æˆåŠŸï¼Œå¦åˆ™éœ€è¦é‡æ–°åˆå§‹åŒ– if membship.sum(axis=1).min() &gt; 0: break # TODO: å¦‚æœä¸ä¿®æ”¹éšæœºç§å­æˆ–è€…å…¶ä»–çš„æ•°æ®ï¼Œæ¯æ¬¡è·‘çš„ç»“æœéƒ½æ˜¯ä¸€æ ·çš„ if init_tries == MAX_INIT_TRIES: # Could not get rid of empty clusters. Randomly # initialize instead. init = 'random' elif init_tries == RAISE_INIT_TRIES: raise ValueError( \"Clustering algorithm could not initialize. \" \"Consider assigning the initial clusters manually.\" ) # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(nnumattrs): centroids[0][ik, iattr] = cl_attr_sum[ik, iattr] / cl_memb_sum[ik] for iattr in range(ncatattrs): centroids[1][ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_prototypes_iter(Xnum, Xcat, centroids, cl_attr_sum, cl_memb_sum, cl_attr_freq, membship, num_dissim, cat_dissim, gamma, random_state) # All points seen in this iteration labels, ncost = _labels_cost(Xnum, Xcat, centroids, num_dissim, cat_dissim, gamma, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run: &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, ncost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, ncost)) return centroids, labels, cost, itr","tags":[{"name":"Cluster","slug":"Cluster","permalink":"http://chenson.com/tags/Cluster/"}]},{"title":"å…³äºPythonçš„Mixinæ¨¡å¼åˆæ¢","date":"2018-11-19T23:25:13.000Z","path":"2018/11/20/å…³äºPythonçš„Mixinæ¨¡å¼åˆæ¢/","text":"1. ç»å…¸ç±» / æ–°å¼ç±» ç»å…¸ç±» (Python2.2ä¹‹å‰çš„ç‰ˆæœ¬) ç»å…¸ç±»æ˜¯ä¸€ç§æ²¡æœ‰ç»§æ‰¿çš„ç±»ï¼Œå®ä¾‹ç±»å‹éƒ½æ˜¯typeç±»å‹(???)ã€‚å¦‚æœç»å…¸ç±»è¢«ä½œä¸ºçˆ¶ç±»ï¼Œå­ç±»åœ¨è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ—¶å°±ä¼šå‡ºé”™ã€‚è¿™æ—¶å€™MROçš„æ–¹æ³•ä¸ºDFSï¼ˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆå­èŠ‚ç‚¹é¡ºåºï¼šä»å·¦åˆ°å³ï¼‰ï¼‰ 1234import inspect # inspect.getmroï¼ˆAï¼‰å¯ä»¥æŸ¥çœ‹ç»å…¸ç±»çš„MROé¡ºåºclass A: def __init__(self): print('This is a classic class.') http://python.jobbole.com/85685/ æ–°å¼ç±» ä¸ºäº†ä½¿ç±»å’Œå†…ç½®ç±»å‹æ›´åŠ ç»Ÿä¸€(???)ï¼Œå¼•å…¥äº†æ–°å¼ç±»ã€‚æ–°å¼ç±»çš„æ¯ä¸ªç±»éƒ½ç»§æ‰¿äºä¸€ä¸ªåŸºç±»ï¼Œå¯ä»¥æ˜¯è‡ªå®šä¹‰ç±»æˆ–è€…å…¶å®ƒç±»ï¼Œé»˜è®¤æ‰¿äºobjectã€‚å­ç±»å¯ä»¥è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°ã€‚ è¿™æ—¶æœ‰ä¸¤ç§MROçš„æ–¹æ³• å¦‚æœæ˜¯ç»å…¸ç±»MROä¸ºDFSï¼ˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆå­èŠ‚ç‚¹é¡ºåºï¼šä»å·¦åˆ°å³ï¼‰ï¼‰ã€‚ å¦‚æœæ˜¯æ–°å¼ç±»MROä¸ºBFSï¼ˆå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆå­èŠ‚ç‚¹é¡ºåºï¼šä»å·¦åˆ°å³ï¼‰ï¼‰ã€‚ 123class A(object): def __init__(self): print('This is a new-style class.') 1234567891011121314151617181920class D(object): pass class E(object): pass class F(object): pass class C(D, F): pass class B(E, D): pass class A(B, C): pass if __name__ == '__main__': print A.__mro__ 2. å¤šé‡ç»§æ‰¿super https://blog.csdn.net/qwertyupoiuytr/article/details/56439134 3. Mix-inMixinè¡¨ç¤ºçš„æ˜¯Mix-inï¼Œè¡¨ç¤ºè¿™ä¸ªç±»æ˜¯ä½œä¸ºåŠŸèƒ½æ·»åŠ åˆ°å­ç±»ä¸­ï¼Œè€Œä¸æ˜¯ä½œä¸ºçˆ¶ç±»ï¼Œå®ƒçš„ä½œç”¨åŒJavaä¸­çš„æ¥å£ã€‚ æ™®é€šç±»å¤šé‡ç»§æ‰¿ä¸‹ï¼Œåªèƒ½æœ‰ä¸€ä¸ªæ™®é€šçˆ¶ç±»å’Œè‹¥å¹²ä¸ªMixinç±»ï¼ˆä¿æŒä¸»å¹²å•ä¸€ï¼‰ Mixinç±»ä¸èƒ½ç»§æ‰¿æ™®é€šç±»ï¼ˆé¿å…é’»çŸ³ç»§æ‰¿ï¼Œé‡å¤è°ƒç”¨ï¼‰ Mixin ç±»åº”è¯¥å•ä¸€èŒè´£ï¼ˆå‚è€ƒ Java çš„ interface è®¾è®¡ï¼ŒMixin å’Œæ­¤æå…¶ç›¸ä¼¼ï¼Œåªä¸è¿‡é™„å¸¦å®ç°è€Œå·²ï¼‰ ä½¿ç”¨Mixinç±»å®ç°å¤šé‡ç»§æ‰¿è¦éå¸¸å°å¿ƒ é¦–å…ˆå¿…é¡»æ˜¯è¡¨ç¤ºä¸€ç§åŠŸèƒ½ï¼Œè€Œä¸æ˜¯æŸä¸ªç‰©å“ å¿…é¡»è´£ä»»å•ä¸€ï¼Œå¦‚æœæœ‰å¤šä¸ªåŠŸèƒ½ï¼Œé‚£ä¹ˆå°±å†™å¤šä¸ªMixinç±» ç„¶åä¸èƒ½ä¾èµ–äºå­ç±»å®ç° å³ä½¿å­ç±»æ²¡æœ‰ç»§æ‰¿è¿™ä¸ªMixinç±»ï¼Œä¹Ÿå¯ä»¥ç…§æ ·å·¥ä½œï¼Œå°±æ˜¯ç¼ºå¤±æŸä¸ªåŠŸèƒ½","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"}]},{"title":"åœ¨çº¿å­¦ä¹ ç®—æ³•FTRLåˆæ¢","date":"2018-11-18T07:36:28.000Z","path":"2018/11/18/åœ¨çº¿å­¦ä¹ ç®—æ³•FTRLåˆæ¢/","text":"","tags":[]},{"title":"Wide_and_Deepæ¨¡å‹åˆæ¢","date":"2018-11-18T06:01:03.000Z","path":"2018/11/18/Wide-and-Deepæ¨¡å‹åˆæ¢/","text":"1. BackgroundWide and Deepæ˜¯Googleåœ¨16å¹´å·¦å³æå‡ºçš„æ¨¡å‹ï¼Œä¸»è¦åº”ç”¨åœ¨äº†Google Playçš„åº”ç”¨æ¨èç³»ç»Ÿä¸­ï¼ŒåŒæ—¶å›½å†…çš„ä¸€äº›å¤§å‚ä¹Ÿåœ¨å„è‡ªçš„ä¸šåŠ¡ä¸­æœ‰ä½¿ç”¨åˆ°è¿™ä¸€æ¨¡å‹ï¼Œæ¯”å¦‚ç¾å›¢ã€‚è€Œè¯¥ç®—æ³•çš„æ ¸å¿ƒä¸»è¦åœ¨äºç»“åˆäº†çº¿æ€§æ¨¡å‹çš„è®°å¿†èƒ½åŠ›Memorization å’Œæ·±åº¦ç¥ç»ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›Generaizationï¼Œä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–è¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œå³Jointly Trainingã€‚ 2. Overview å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¯¥æ¨¡å‹çš„ä¸»è¦ç»“æ„æ˜¯ä¸­é—´Wide&amp;Deep Modelsé‚£ä¸ªéƒ¨åˆ†ï¼Œå…¶ä¸»è¦ç”±å·¦è¾¹çš„Wide Models å’Œå³è¾¹çš„Deep Modelsç»„æˆã€‚ 3. Wide Partæœ€å·¦ç«¯çš„å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œä¸åšè¿‡å¤šçš„è§£é‡Šã€‚ è¯¥éƒ¨åˆ†çš„æ¨¡å‹ï¼Œè¾“å…¥çš„ç‰¹å¾å¯ä»¥æ˜¯è¿ç»­ç‰¹å¾ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¨€ç–çš„ç¦»æ•£ç‰¹å¾ã€‚ä½†è¯¥æ¨¡å‹å¯ä»¥å¯¹ç¦»æ•£ç‰¹å¾ä¹‹é—´åšç‰¹å¾äº¤å‰ï¼Œå³Cross-Productï¼Œä¸ºäº†å¯ä»¥ä»å†å²æ•°æ®ä¸­å‘ç°ç‰¹å¾ä¹‹é—´çš„ç›¸å…³ä¿¡æ¯ï¼Œç„¶åå¯¹äº¤å‰è¿‡åçš„ç‰¹å¾åšOne-Hotå¤„ç†ã€‚ 4. Deep Partæœ€å³éƒ¨åˆ†æ˜¯ä¸€ä¸ªDNNæ¨¡å‹ï¼Œä¸€ä¸ªå‰é¦ˆçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚ è¯¥éƒ¨åˆ†çš„æ¨¡å‹ï¼Œè¾“å…¥çš„ç‰¹å¾åŒæ ·å¯ä»¥æ˜¯è¿ç»­ç‰¹å¾ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¨€ç–çš„ç¦»æ•£ç‰¹å¾ã€‚ä½†å’Œä¸Šé¢éƒ¨åˆ†çš„ç¦»æ•£ç‰¹å¾å¤„ç†æ–¹å¼ä¸åŒï¼Œè¿™é‡Œå¯¹ç¦»æ•£ç‰¹å¾åšäº†Embeddingï¼Œå³å°†é«˜ç»´ç¨€ç–çš„ç¦»æ•£ç‰¹å¾è½¬åŒ–ä¸ºä½çº¬çš„ç¨ å¯†ç‰¹å¾ã€‚ Embeddingçš„åˆå§‹æƒé‡æ˜¯éšæœºèµ‹äºˆçš„ï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸æ–­å‚ä¸ä¼˜åŒ–ã€‚ æ‰€ä»¥è®­ç»ƒçš„è¿‡ç¨‹å¤§è‡´æ˜¯éœ€è¦åšEmbeddingçš„å‘é‡å…ˆè½¬åŒ–æˆDense Vectorï¼Œç„¶åå’Œå…¶ä»–çš„è¿ç»­å‹æˆ–è€…æ•°å€¼å‹çš„ç‰¹å¾åˆå¹¶ä¸€èµ·ï¼Œå–‚ç»™æ¥ä¸‹å»çš„å‡ å±‚DNNã€‚ å…¶ä¸­éšå«å±‚çš„è®¡ç®—æ–¹æ³•ä¸º$$a^{l + 1} = f(W^{(l)}a^{(l)} + b^{(l)})$$è¿™é‡Œ læ˜¯éšå«å±‚æ•°ï¼Œ fæ˜¯æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ä½¿ç”¨ReLUsä½œä¸ºæ¿€æ´»å‡½æ•° 5. Wide&amp;Deep Partå¦‚ä¸Šå›¾ä¸­é—´éƒ¨åˆ†æ‰€ç¤ºï¼Œä¸¤ä¸ªæ¨¡å‹æœ€åæ˜¯åˆå¹¶åˆ°ä¸€èµ·è®­ç»ƒçš„ï¼Œå¹¶å°†ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœçš„åŠ æƒå’Œä½œä¸ºæœ€ç»ˆçš„é¢„æµ‹ç»“æœï¼ˆæœ€ç»ˆå–‚ç»™äº†åŒä¸€ä¸ªLRæ¨¡å‹å»è®­ç»ƒï¼‰ï¼Œå…·ä½“å¦‚ä¸‹å…¬å¼æ‰€ç¤ºã€‚$$P(Y=1 | x) = \\sigma(w^T_{wide}[x, \\phi(x)] + w^T_{deep}a^{(l_f)} + b)$$è¿™é‡Œ$\\sigma(Â·)$æ˜¯sigmoidæ¿€æ´»å‡½æ•°ã€‚ åœ¨è®ºæ–‡ä¸­ä½œè€…ä¹Ÿç‰¹åˆ«çš„æåˆ°ï¼Œè¿™é‡Œçš„Joinly Training å’ŒEnsembleæ¨¡å‹æ˜¯ä¸ä¸€æ ·çš„ã€‚å¯¹äºEnsembleæ¨¡å‹ï¼Œå„ä¸ªå­æ¨¡å‹æ˜¯å•ç‹¬è®­ç»ƒçš„ï¼Œå„è‡ªçš„é¢„æµ‹ç»“æœä¸å½±å“å…¶ä»–çš„å­æ¨¡å‹ï¼Œåªæ˜¯æœ€åå¤§å®¶çš„é¢„æµ‹ç»“æœé€šè¿‡æŸä¸€ç§æ–¹å¼ï¼Œæ¯”å¦‚æŠ•ç¥¨ï¼ŒåŠ æƒç­‰ï¼Œæœ€ç»ˆå¾—å‡ºä¸€ä¸ªé¢„æµ‹ç»“æœã€‚ Jointly Trainingçš„é¢„æµ‹ç»“æœï¼Œæœ€ç»ˆä¼šé€šè¿‡Back-Propagating the Gradientsï¼Œä¼ æ’­åˆ°Wide Partå’ŒDeep Partã€‚è¿™é‡Œä¸¤éƒ¨åˆ†æ¨¡å‹é‡‡ç”¨äº†ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼š Wide Modelsé‡‡ç”¨äº†FTRL + L1æ­£åˆ™ Deep Modelsé‡‡ç”¨äº†AdaGrad 6. Conclusionåœ¨æœ€ç»ˆçš„æµ‹è¯•éƒ¨åˆ†ï¼Œæˆ‘è‡ªå·±å‡†å¤‡äº†ä¸€ä»½æ•°æ®ç”¨äºæµ‹è¯•è¯¥æ¨¡å‹ã€‚ Wide Part Deep Part Wide &amp; Deep Part ä»ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹å‡ºï¼Œä»Wide Modelsåˆ°Deep Modelsï¼Œè®­ç»ƒæ•ˆæœæå‡è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ï¼Œä½†ä»Deep Modelsåˆ°Wide&amp;Deep Modelsè®­ç»ƒç»“æœéä½†æ²¡æœ‰æå‡ï¼Œè¿˜æœ‰äº†ä¸€ä¸ä¸çš„ä¸‹é™ï¼Œä¸”è¿™ä¸ªæˆ‘ä¹Ÿå·²ç»æµ‹è¯•äº†å¾ˆå¤šæµ‹ï¼Œä¸»è¦ä¿®æ”¹äº†Embeddingä¸­å’ŒWide Modelsä¸­çš„ä¸€äº›å‚æ•°ï¼Œåè€…å§‹ç»ˆæ²¡æœ‰è¶…è¿‡å‰è€…çš„æ•ˆæœã€‚ä½†åœ¨ç½‘ä¸Šæ‰¾çš„ä¸€äº›æµ‹è¯•æ•°æ®ä¸­ï¼ŒWide&amp;Deep Modelsè¿˜æ˜¯æœ‰ä¸€ä¸ä¸çš„æå‡ã€‚åŸå› æˆ‘åˆ†æäº†ä¸€äº›å¯èƒ½ç”±å¦‚ä¸‹é€ æˆ æ•°æ®é›†æœ¬èº«å¯èƒ½ä¸å¤ªé€‚åˆè¿™ä¸ªç®—æ³• æ•°æ®å¤„ç†æ–¹å¼ï¼Œå› ä¸ºæœ‰éƒ¨åˆ†æ•°æ®æœ‰ç©ºå€¼ï¼Œæˆ‘åŸºæœ¬åªæ˜¯ç®€å•çš„å¡«å……äº†ä¸€ä¸‹ Wide Modelsè®­ç»ƒæ•°æ® è®­ç»ƒå‚æ•° 7. References æ·±åº¦å­¦ä¹ åœ¨ç¾å›¢ç‚¹è¯„æ¨èå¹³å°æ’åºä¸­çš„è¿ç”¨ Wide &amp; Deep Learning for Recommender Systems ç®€å•æ˜“å­¦çš„æ·±åº¦å­¦ä¹ ç®—æ³•â€”â€”Wide &amp; Deep Learning","tags":[{"name":"Wide&Deep","slug":"Wide-Deep","permalink":"http://chenson.com/tags/Wide-Deep/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://chenson.com/tags/Pytorch/"}]},{"title":"å›¾æ•°æ®åº“-åæ¬ºè¯ˆç½‘ç»œè®¾è®¡","date":"2018-08-18T06:45:01.000Z","path":"2018/08/18/å›¾æ•°æ®åº“-åæ¬ºè¯ˆç½‘ç»œè®¾è®¡/","text":"","tags":[]},{"title":"å›¾æ•°æ®åº“-Neo4j-å¸¸ç”¨ç®—æ³•","date":"2018-08-18T06:03:23.000Z","path":"2018/08/18/å›¾æ•°æ®åº“-Neo4j-å¸¸ç”¨ç®—æ³•/","text":"æœ¬æ¬¡ä¸»è¦å­¦ä¹ å›¾æ•°æ®åº“ä¸­å¸¸ç”¨åˆ°çš„ä¸€äº›ç®—æ³•ï¼Œä»¥åŠå¦‚ä½•åœ¨Neo4jä¸­è°ƒç”¨ï¼Œæ‰€ä»¥è¿™ä¸€ç¯‡åå®æˆ˜ï¼Œæ¯ä¸ªç®—æ³•çš„åŸç†å°±ç®€å•çš„æä¸€ä¸‹ã€‚ 1. å›¾æ•°æ®åº“ä¸­å¸¸ç”¨çš„ç®—æ³• PathFinding &amp; Search ä¸€èˆ¬ç”¨æ¥å‘ç°Nodesä¹‹é—´çš„æœ€çŸ­è·¯å¾„ï¼Œå¸¸ç”¨ç®—æ³•æœ‰å¦‚ä¸‹å‡ ç§ Google Search Results Dijkstra - è¾¹ä¸èƒ½ä¸ºè´Ÿå€¼ Folyd - è¾¹å¯ä»¥ä¸ºè´Ÿå€¼ï¼Œæœ‰å‘å›¾ã€æ— å‘å›¾ Bellman-Ford SPFA Centrality ä¸€èˆ¬ç”¨æ¥è®¡ç®—è¿™ä¸ªå›¾ä¸­èŠ‚ç‚¹çš„ä¸­å¿ƒæ€§ï¼Œç”¨æ¥å‘ç°æ¯”è¾ƒé‡è¦çš„é‚£äº›Nodesã€‚è¿™äº›ä¸­å¿ƒæ€§å¯ä»¥æœ‰å¾ˆå¤šç§ï¼Œæ¯”å¦‚ Degree Centrality - åº¦ä¸­å¿ƒæ€§ Weighted Degree Centrality - åŠ æƒåº¦ä¸­å¿ƒæ€§ Betweenness Centrality - ä»‹æ•°ä¸­å¿ƒæ€§ Closeness Centrality - ç´§åº¦ä¸­å¿ƒæ€§ Community Detection åŸºäºç¤¾åŒºå‘ç°ç®—æ³•å’Œå›¾åˆ†æNeo4jè§£è¯»ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹ ç”¨äºå‘ç°è¿™ä¸ªå›¾ä¸­å±€éƒ¨è”ç³»æ¯”è¾ƒç´§å¯†çš„Nodesï¼Œç±»ä¼¼æˆ‘ä»¬å­¦è¿‡çš„èšç±»ç®—æ³•ã€‚ Strongly Connected Components Weakly Connected Components (Union Find) Label Propagation Lovain Modularity Triangle Count and Average Clustering Coefficient 2. è·¯å¾„æœç´¢ç®—æ³• Shortest Path 1234567MATCH (start:Loc&#123;name:&quot;A&quot;&#125;), (end:Loc&#123;name:&quot;F&quot;&#125;)CALL algo.shortestPath.stream(start, end, &quot;cost&quot;)YIELD nodeId, costMATCH (other:Loc) WHERE id(other) = nodeIdRETURN other.name AS name, cost Single Source Shortest Path 123456MATCH (n:Loc &#123;name:&quot;A&quot;&#125;)CALL algo.shortestPath.deltaStepping.stream(n, &quot;cost&quot;, 3.0YIELD nodeId, distanceMATCH (destination) WHERE id(destination) = nodeIdRETURN destination.name AS destination, distance All Pairs Shortest Path 1234567891011CALL algo.allShortestPaths.stream(&quot;cost&quot;,&#123;nodeQuery:&quot;Loc&quot;,defaultValue:1.0&#125;)YIELD sourceNodeId, targetNodeId, distanceWITH sourceNodeId, targetNodeId, distanceWHERE algo.isFinite(distance) = trueMATCH (source:Loc) WHERE id(source) = sourceNodeIdMATCH (target:Loc) WHERE id(target) = targetNodeIdWITH source, target, distance WHERE source &lt;&gt; targetRETURN source.name AS source, target.name AS target, distanceORDER BY distance DESCLIMIT 10 Minimum Weight Spanning Tree 12345MATCH (n:Place &#123;id:&quot;D&quot;&#125;)CALL algo.spanningTree.minimum(&quot;Place&quot;, &quot;LINK&quot;, &quot;cost&quot;, id(n), &#123;write:true, writeProperty:&quot;MINST&quot;&#125;)YIELD loadMillis, computeMillis, writeMillis, effectiveNodeCountRETURN loadMillis, computeMillis, writeMillis, effectiveNodeCount; CASE 123456789101112131415MERGE (a:Loc &#123;name:&quot;A&quot;&#125;)MERGE (b:Loc &#123;name:&quot;B&quot;&#125;)MERGE (c:Loc &#123;name:&quot;C&quot;&#125;)MERGE (d:Loc &#123;name:&quot;D&quot;&#125;)MERGE (e:Loc &#123;name:&quot;E&quot;&#125;)MERGE (f:Loc &#123;name:&quot;F&quot;&#125;)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(b)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(c)MERGE (a)-[:ROAD &#123;cost:100&#125;]-&gt;(d)MERGE (b)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:80&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:30&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:80&#125;]-&gt;(f)MERGE (e)-[:ROAD &#123;cost:40&#125;]-&gt;(f); 3. ä¸­å¿ƒæ€§ç®—æ³• PageRank 123456CALL algo.pageRank.stream(&quot;Page&quot;, &quot;LINKS&quot;,&#123;iterations:20&#125;)YIELD nodeId, scoreMATCH (node) WHERE id(node) = nodeIdRETURN node.name AS page,scoreORDER BY score DESC Degree Centrality Betweenness Centrality 12345CALL algo.betweenness.stream(&quot;User&quot;, &quot;MANAGES&quot;, &#123;direction:&quot;out&quot;&#125;)YIELD nodeId, centralityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user,centralityORDER BY centrality DESC; Closeness Centrality 123456CALL algo.closeness.stream(&quot;Node&quot;, &quot;LINK&quot;)YIELD nodeId, centralityMATCH (n:Node) WHERE id(n) = nodeIdRETURN n.id AS node, centralityORDER BY centrality DESCLIMIT 20; CASE 12345678910111213141516171819202122MERGE (home:Page &#123;name:&quot;Home&quot;&#125;)MERGE (about:Page &#123;name:&quot;About&quot;&#125;)MERGE (product:Page &#123;name:&quot;Product&quot;&#125;)MERGE (links:Page &#123;name:&quot;Links&quot;&#125;)MERGE (a:Page &#123;name:&quot;Site A&quot;&#125;)MERGE (b:Page &#123;name:&quot;Site B&quot;&#125;)MERGE (c:Page &#123;name:&quot;Site C&quot;&#125;)MERGE (d:Page &#123;name:&quot;Site D&quot;&#125;)MERGE (home)-[:LINKS]-&gt;(about)MERGE (about)-[:LINKS]-&gt;(home)MERGE (product)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(product)MERGE (links)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(links)MERGE (links)-[:LINKS]-&gt;(a)MERGE (a)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(b)MERGE (b)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(c)MERGE (c)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(d)MERGE (d)-[:LINKS]-&gt;(home) 4. ç¤¾åŒºå‘ç°ç®—æ³• Strongly Connected Components 1234CALL algo.scc.stream(&quot;User&quot;,&quot;FOLLOWS&quot;)YIELD nodeId, partitionMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS name, partition Weakly Connected Components (Union Find) 1234CALL algo.unionFind.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId,setIdMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS user, setId Label Propagation 12CALL algo.labelPropagation.stream(&quot;User&quot;, &quot;FOLLOWS&quot;, &#123;direction: &quot;OUTGOING&quot;, iterations: 10&#125;) Lovain Modularity 12345CALL algo.louvain.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId, communityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user, communityORDER BY community; Triangle Count and Average Clustering Coefficient 123456CALL algo.triangle.stream(&quot;Person&quot;,&quot;KNOWS&quot;)YIELD nodeA,nodeB,nodeCMATCH (a:Person) WHERE id(a) = nodeAMATCH (b:Person) WHERE id(b) = nodeBMATCH (c:Person) WHERE id(c) = nodeCRETURN a.id AS nodeA, b.id AS nodeB, c.id AS node 5. References Neo4j in deep å®˜æ–¹æ–‡æ¡£ï¼šComprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.com/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.com/tags/Neo4j/"}]},{"title":"å›¾æ•°æ®åº“-Neo4j-åˆæ¢","date":"2018-08-17T05:15:46.000Z","path":"2018/08/17/å›¾æ•°æ®åº“-Neo4j-åˆæ¢/","text":"æœ¬æ¬¡åˆæ¢ä¸»è¦å­¦ä¹ å¦‚ä½•å®‰è£…Neo4jï¼Œä»¥åŠCypherçš„åŸºæœ¬è¯­æ³•ã€‚ 1. å®‰è£…Neo4j Desktopç‰ˆæœ¬ neo4j-desktop Serverç‰ˆæœ¬ï¼ˆCommunityç‰ˆ) æ¯”è¾ƒå»ºè®®å®‰è£…è¿™ä¸ªç‰ˆæœ¬ï¼Œå› ä¸ºDesktopç‰ˆæœ¬çš„è€æ˜¯é—ªé€€ï¼Œä¸”è¦æ¿€æ´»ä¹‹ç±»çš„ã€‚ ä¸‹è½½Neo4jæ•°æ®åº“ neo4j-server-community ä¸‹è½½å¸¸ç”¨ç®—æ³•çš„æ’ä»¶ graph-algorithms neo4j-graph-algorithms apoc-procedures neo4j-apoc-procedures å°†ä¸‹è½½ä¸‹æ¥çš„ç®—æ³•æ’ä»¶æ”¾å…¥åˆ°$NEO4J_HOME/pluginsæ–‡ä»¶å¤¹ä¸‹ Serviceç‰ˆä¿®æ”¹é…ç½®æ–‡ä»¶$NEO4J_HOME/conf/neo4j.conf 1234567891011# è§£å†³ç™»å…¥çš„æ—¶å€™æŠ¥æ²¡æœ‰æˆæƒçš„é”™è¯¯dbms.security.auth_enabled=false# æ·»åŠ ä¸‹è½½çš„ç®—æ³•æ’ä»¶dbms.security.procedures.unrestricted=apoc.*,algo.*apoc.import.file.enabled=true#å¢åŠ é¡µç¼“å­˜åˆ°è‡³å°‘4Gï¼Œæ¨è20G:dbms.memory.pagecache.size=4g#JVMå †ä¿å­˜ç•™å†…å­˜ä»1Gèµ·ï¼Œæœ€å¤§4G:dbms.memory.heap.initial_size=1gdbms.memory.heap.max_size=4g å¯åŠ¨/åœæ­¢ (æŠŠserveræ‰€åœ¨çš„è·¯å¾„æ·»åŠ åˆ°ç³»ç»Ÿçš„PATH) 1234567# å»ºè®®å°†neo4jæ‰€åœ¨çš„è·¯å¾„æ¡ä»¶åˆ°ç³»ç»Ÿ$PATHå½“ä¸­ï¼Œ# export NEO4J_HOME=\"path-to-neo4j\"$NEO4J_HOME/bin/neo4j start$NEO4J_HOME/bin/neo4j console$NEO4J_HOME/bin/neo4j stop$NEO4J_HOME/bin/neo4j start -u neo4j -p neo4j$NEO4J_HOME/bin/cypher-shell 1CALL dbms.procedures() // æŸ¥çœ‹neo4jå¯ç”¨çš„è¿›ç¨‹ï¼ŒåŒ…æ‹¬åˆšåˆšå®‰è£…çš„æ’ä»¶ 2. CypheråŸºæœ¬è¯­æ³• NodesåŸºæœ¬è¯­æ³• åœ¨Cypheré‡Œé¢é€šè¿‡ä¸€å¯¹å°æ‹¬å·ä»£è¡¨ä¸€ä¸ªèŠ‚ç‚¹ () ä»£è¡¨åŒ¹é…ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ (node1) ä»£è¡¨åŒ¹é…ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¹¶ç»™å®ƒèµ·äº†ä¸€ä¸ªåˆ«å (:Lable) ä»£è¡¨æŸ¥è¯¢ä¸€ä¸ªç±»å‹çš„æ•°æ® (person:Lable) ä»£è¡¨æŸ¥è¯¢ä¸€ä¸ªç±»å‹çš„æ•°æ®ï¼Œå¹¶ç»™å®ƒèµ·äº†ä¸€ä¸ªåˆ«å (person:Lable {name:â€å°ç‹â€}) æŸ¥è¯¢æŸä¸ªç±»å‹ä¸‹ï¼ŒèŠ‚ç‚¹å±æ€§æ»¡è¶³æŸä¸ªå€¼çš„æ•°æ® (person:Lable {name:â€å°ç‹â€,age:23}) èŠ‚ç‚¹çš„å±æ€§å¯ä»¥åŒæ—¶å­˜åœ¨å¤šä¸ªï¼Œæ˜¯ä¸€ä¸ªANDçš„å…³ç³» RelationshipåŸºæœ¬è¯­æ³• ç³»ç”¨ä¸€å¯¹-ç»„æˆï¼Œå…³ç³»åˆ†æœ‰æ–¹å‘çš„è¿›å’Œå‡ºï¼Œå¦‚æœæ˜¯æ— æ–¹å‘å°±æ˜¯è¿›å’Œå‡ºéƒ½æŸ¥è¯¢ â€“&gt; æŒ‡å‘ä¸€ä¸ªèŠ‚ç‚¹ -[role]-&gt; ç»™å…³ç³»åŠ ä¸ªåˆ«å -[:acted_in]-&gt; è®¿é—®æŸä¸€ç±»å…³ç³» -[role:acted_in]-&gt; è®¿é—®æŸä¸€ç±»å…³ç³»ï¼Œå¹¶åŠ äº†åˆ«å -[role:acted_in {roles:[â€œneoâ€,â€Hadoopâ€œ]}]-&gt; åˆ›å»º/åˆ é™¤èŠ‚ç‚¹ 1234567891011121314151617181920212223// æ’å…¥ä¸€ä¸ªArtistç±»åˆ«çš„èŠ‚ç‚¹ï¼Œè€Œä¸”è¿™ä¸ªèŠ‚ç‚¹æœ‰ä¸€ä¸ªå±æ€§ä¸ºNameï¼Œå€¼ä¸ºLady GagaCREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;&#125;)// åˆ›å»ºå¹¶è¿”å›CREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;, Gemder:&quot;Femal&quot;&#125;) return a// ä¸€æ¬¡æ€§åˆ›å»ºå¤šä¸ªCREATE (a:Album &#123; Name: &quot;Killers&quot;&#125;), (b:Album &#123; Name: &quot;Fear of the Dark&quot;&#125;) RETURN a, bCREATE (a:Album &#123; Name: &quot;Piece of Mind&quot;&#125;) CREATE (b:Album &#123; Name: &quot;Somewhere in Time&quot;&#125;) RETURN a, b// åˆ é™¤èŠ‚ç‚¹ï¼Œå¦‚æœè¿™ä¸ªèŠ‚ç‚¹å’Œå…¶ä»–èŠ‚ç‚¹æœ‰è¿æ¥çš„è¯ï¼Œä¸èƒ½å•å•åˆ é™¤è¿™ä¸ªèŠ‚ç‚¹MATCH (a:Album &#123;Name: &quot;Killers&quot;&#125;) DELETE a// ä¸€æ¬¡æ€§åˆ é™¤å¤šä¸ªèŠ‚ç‚¹MATCH (a:Artist &#123;Name: &quot;Iron Maiden&quot;&#125;), (b:Album &#123;Name: &quot;Powerslave&quot;&#125;) DELETE a, b // åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹MATCH (n) DELETE n åˆ›å»º/åˆ é™¤å…³ç³» 123456789101112131415161718192021222324252627282930// å¯¹Lady Gagaå’Œä¸“è¾‘PieceOfMindä¹‹é—´åˆ›å»ºä¸€ä¸ªreleasedçš„å…³ç³»MATCH (a:Artist), (b:Album)WHERE a.Name = &quot;Lady Gaga&quot; AND b.Name = &quot;Piece of Mind&quot;CREATE (a)-[r:RELEASED]-&gt;(b)RETURN rMATCH (a:Artist), (b:Album), (p:Person)WHERE a.Name = &quot;Strapping Young Lad&quot; AND b.Name = &quot;Heavy as a Really Heavy Thing&quot; AND p.Name = &quot;Devin Townsend&quot; CREATE (p)-[pr:PRODUCED]-&gt;(b), (p)-[pf:PERFORMED_ON]-&gt;(b), (p)-[pl:PLAYS_IN]-&gt;(a)RETURN a, b, p // åˆ é™¤æŒ‡å®šçš„å…³ç³»MATCH (:Artist)-[r:RELEASED]-(:Album) DELETE r MATCH (:Artist &#123;Name: &quot;Strapping Young Lad&quot;&#125;)-[r:RELEASED]-(:Album &#123;Name: &quot;Heavy as a Really Heavy Thing&quot;&#125;) DELETE r // åˆ é™¤æ‰€æœ‰çš„å…³ç³»MATCH ()-[r:RELEASED]-() DELETE r // æ¸…é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³» MATCH (n)OPTIONAL MATCH(n)-[r]-()DELETE n,r // åˆ é™¤æ•´ä¸ªæ•°æ®åº“MATCH (n) DETACH DELETE n åˆ›å»º/åˆ é™¤çº¦æŸ åŒSQLä¸€æ ·ï¼ŒNeo4jæ•°æ®åº“æ”¯æŒå¯¹Nodeæˆ–relationshipçš„å±æ€§çš„UNIQUEçº¦æŸ 123CREATE CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUEDROP CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUE åˆ›å»º/åˆ é™¤ç´¢å¼• 123456CREATE INDEX ON :Album(Name) // View the schema:schemaDROP INDEX ON :Album(Name) æ›´æ–°ä¸€ä¸ªèŠ‚ç‚¹/è¾¹ 12MATCH (n:Person &#123; name: &quot;Andres&quot; &#125;)SET n.name = &quot;Taylor&quot;; ç­›é€‰è¿‡æ»¤ 123456789// WHEREMATCH (p1: Person)-[r:friend]-&gt;(p2: Person) WHERE p1.name=~&quot;K.+&quot; or p2.age=24 or &quot;neo&quot; in r.rels RETURN p1, r, p2 // NOT MATCH (p:Person)-[:ACTED_IN]-&gt;(m)WHERE NOT (p)-[:DIRECTED]-&gt;()RETURN p, m ç»“æœé›†è¿”å› 12345MATCH (p:Person)RETURN p, p.name AS name, upper(p.name), coalesce(p.nickname,&quot;n/a&quot;) AS nickname, &#123; name: p.name, label:head(labels(p))&#125; AS person MATCH (n) RETURN DISTINCT n.name; èšåˆå‡½æ•° Cypheræ”¯æŒcount, sum, avg, min, max èšåˆçš„æ—¶å€™nullä¼šè¢«è·³è¿‡ count è¯­æ³• æ”¯æŒ count( distinct role ) 123456MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations// æ”¶é›†èšåˆç»“æœMATCH (m:Movie)&lt;-[:ACTED_IN]-(a:Person)RETURN m.title AS movie, collect(a.name) AS cast, count(*) AS actors æ’åºå’Œåˆ†é¡µ 123MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a, count(*) AS appearancesORDER BY appearances DESC SKIP 3 LIMIT 10; Union è”åˆ 12345MATCH (actor:Person)-[r:ACTED_IN]-&gt;(movie:Movie)RETURN actor.name AS name, type(r) AS acted_in, movie.title AS titleUNION ï¼ˆALLï¼‰MATCH (director:Person)-[r:DIRECTED]-&gt;(movie:Movie)RETURN director.name AS name, type(r) AS acted_in, movie.title AS title Withè¯­å¥ withè¯­å¥ç»™Cypheræä¾›äº†å¼ºå¤§çš„pipelineèƒ½åŠ›ï¼Œå¯ä»¥ä¸€ä¸ªæˆ–è€…queryçš„è¾“å‡ºï¼Œæˆ–è€…ä¸‹ä¸€ä¸ªqueryçš„è¾“å…¥ å’Œreturnè¯­å¥éå¸¸ç±»ä¼¼ï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œwithçš„æ¯ä¸€ä¸ªç»“æœï¼Œå¿…é¡»ä½¿ç”¨åˆ«åæ ‡è¯†ã€‚ ä½¿ç”¨withæˆ‘ä»¬å¯ä»¥åœ¨æŸ¥è¯¢ç»“æœé‡Œé¢åœ¨ç»§ç»­åµŒå¥—æŸ¥è¯¢ã€‚ 1234MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)WITH p, count(*) AS appearances, COLLECT(m.Title) AS moviesWHERE appearances &gt; 1RETURN p.name, appearances, movies æœ‰ç‚¹ç±»ä¼¼SQLä¸­çš„havingï¼Œè¿™é‡Œæ˜¯with + whereä¸¤ä¸ªä¸€èµ·æ¥å®ç°çš„ã€‚ æŸ¥è¯¢æœ€çŸ­è·¯å¾„ 12MATCH (ms:Person &#123; name: &quot;Node A&quot; &#125;),(cs:Person &#123; name:&quot;Node B&quot; &#125;), p = shortestPath((ms)-[r:Follow]-(cs)) RETURN p; åŠ è½½æ•°æ® Cypher Neo4j Couldnâ€™t load the external resource neo4jåˆæ¢ åŠ è½½å­˜åœ¨æœ¬åœ°serverä¸Šçš„æ•°æ®ï¼Œä¼šåœ¨è·¯å¾„å‰é¢è‡ªåŠ¨åŠ ä¸ªå‰ç¼€ /path-to-neo4j/neo4j-community-3.4.5/importï¼Œå³Serverå¯¹åº”æ‰€åœ¨çš„è·¯å¾„ä¸‹çš„import 12345678910111213141516// åŠ è½½addressLOAD CSV WITH HEADERS FROM &quot;file:///data/addresses.csv&quot; AS csvLineCREATE (p:Person &#123;id: toInt(csvLine.id), email: csvLine.address &#125;)// åŠ è½½emailLOAD CSV WITH HEADERS FROM &quot;file:///data/emails.csv&quot; AS csvLineCREATE (e:Email &#123;id: toInt(csvLine.id), time: csvLine.time, content: csvLine.content &#125;) // åˆ›å»ºæ”¶å‘å…³ç³»USING PERIODIC COMMIT 500 // åˆ†æ®µåŠ è½½LOAD CSV WITH HEADERS FROM &quot;file:///data/relations.csv&quot; AS csvLineMATCH (p1:Person &#123;id: toInt(csvLine.fromId)&#125;),(e:Email &#123; id: toInt(csvLine.emailId)&#125;),(p2:Person&#123; id: toInt(csvLine.toId)&#125;)CREATE UNIQUE (p1)-[:FROM]-&gt;(e)CREATE(e)-[:TO]-&gt;(p2) å¦‚æœéœ€è¦å¯¼å…¥å…¶ä»–åœ°æ–¹çš„ï¼Œå¯ä»¥ä½¿ç”¨ 123456789LOAD CSV FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine[0], Name: csvLine[1]&#125;)// ä½¿ç”¨csvä¸­çš„header LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine.Id, Name: csvLine.Track, Length: csvLine.Length&#125;) // è‡ªå®šä¹‰csvæ–‡ä»¶ä¸­çš„åˆ†éš”ç¬¦LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLine FIELDTERMINATOR &quot;;&quot; ä½¿ç”¨ neo4j-import å¯¼å…¥æ•°æ® ä½¿ç”¨neo4j-importå¯¼å…¥æ•°æ® ä½¿ç”¨æ¡ä»¶ éœ€è¦å…ˆå…³é—­neo4j æ— æ³•å†åŸæœ‰çš„æ•°æ®åº“æ·»åŠ ï¼Œåªèƒ½é‡æ–°ç”Ÿæˆä¸€ä¸ªæ•°æ®åº“ å¯¼å…¥æ–‡ä»¶æ ¼å¼ä¸ºcsv å‚æ•° â€“intoï¼šæ•°æ®åº“åç§° â€“bad-toleranceï¼šèƒ½å®¹å¿çš„é”™è¯¯æ•°æ®æ¡æ•°ï¼ˆå³è¶…è¿‡æŒ‡å®šæ¡æ•°ç¨‹åºç›´æ¥æŒ‚æ‰ï¼‰ï¼Œé»˜è®¤1000 â€“multiline-fieldsï¼šæ˜¯å¦å…è®¸å¤šè¡Œæ’å…¥ï¼ˆå³æœ‰äº›æ¢è¡Œçš„æ•°æ®ä¹Ÿå¯è¯»å–ï¼‰ â€“nodesï¼šæ’å…¥èŠ‚ç‚¹ â€“relationshipsï¼šæ’å…¥å…³ç³» æ›´å¤šå‚æ•°å¯å…è®¸å‘½ä»¤bin/neo4j-import 1bin/neo4j-import --multiline-fields=true --bad-tolerance=1000000 --into graph.db --id-type string --nodes:person node.csv --relationships:related relation_header.csv,relation.csv è¿è¡Œå®Œåï¼Œå°†ç”Ÿæˆçš„graph.dbæ”¾å…¥data/databasesï¼Œè¦†ç›–åŸæœ‰æ•°æ®åº“ï¼Œå¯åŠ¨è¿è¡Œå³å¯ 3. References Neo4jçš„ç®€å•æ­å»ºä¸ä½¿ç”¨ Neo4j Tutorial Neo4jçš„æŸ¥è¯¢è¯­æ³•ç¬”è®° å®˜æ–¹æ–‡æ¡£ï¼šComprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.com/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.com/tags/Neo4j/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-RNNå¸¸è§ç½‘ç»œ","date":"2018-05-25T11:10:06.000Z","path":"2018/05/25/æ·±åº¦å­¦ä¹ ç¬”è®°-RNNå¸¸è§ç½‘ç»œ copy/","text":"è¯´å®è¯ï¼Œåªæ˜¯å¤§æ¦‚äº†è§£ä¸€ä¸‹è¿™äº›å¸¸è§çš„RNNæ¨¡å‹ï¼Œä½†æ˜¯è¿™äº›æ¨¡å‹ä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ï¼Œä¸ºä»€ä¹ˆèƒ½å¤ŸæŠ“å–ç½‘ç»œä¸­å¯èƒ½æœ‰ç”¨çš„é•¿çŸ­æœŸä¿¡æ¯å…¶å®æˆ‘è¿˜æ˜¯æ¯”è¾ƒæ‡µçš„ã€‚è¿™ä¸€å—ä»¥åä¹Ÿæ˜¯éœ€è¦å»æ·±å…¥ç†è§£çš„ä¸€éƒ¨åˆ†ï¼Œå…ˆmarkä¸€ä¸‹ã€‚ 1. å¾ªç¯ç¥ç»ç½‘ç»œ - RNN (Recurrent Neural Network)2. é—¨æ§å¾ªç¯å•å…ƒ - GRN (Gated Recurrent Neural Networks)æŒ‰æ—¶é—´é¡ºåºä¸Šæ¥çœ‹ï¼ŒGRNæ˜¯åœ¨LSTMä¹‹åæå‡ºæ¥çš„ã€‚ä½†æ˜¯è¿™ä¸ªç›¸å¯¹æ¥è¯´å¯èƒ½æ²¡é‚£ä¹ˆå¤æ‚ï¼Œæ‰€ä»¥å…ˆæ€»ç»“è¿™ä¸ªï¼Œç„¶åå†æ€»ç»“LSTMã€‚ é—¨æ§å¾ªç¯å•å…ƒï¼Œé‡ç‚¹æ˜¯åœ¨äºé—¨çš„ï¼Œå³Gatedã€‚è¿™é‡ŒæŒ‡çš„æ˜¯ $\\sigma$ = sigmod func. é‚£ä¹ˆä¸ºä»€ä¹ˆè¦å‡ºç°è¿™äº›é—¨å‘¢ï¼Œç›¸æ¯”åŸå…ˆçš„RNNæ¨¡å‹ï¼Œè¿™äº›é—¨çš„å‡ºç°èƒ½å¤Ÿè§£å†³ä»€ä¹ˆæ ·çš„é—®é¢˜ï¼Ÿ ä¹‹å‰è®¨è®ºè¿‡ï¼ŒRNNåœ¨(BPTT)è®¡ç®—çš„æ—¶å€™ï¼Œæ¢¯åº¦å®¹æ˜“å‡ºç°è¡°å‡æˆ–çˆ†ç‚¸ï¼Œç‰¹åˆ«æ˜¯æ¢¯åº¦è¡°å‡çš„é—®é¢˜ã€‚ç”±äºå‡ºç°äº†æ¢¯åº¦è¡°å‡ï¼Œé‚£ä¹ˆå¯¹äºä¸€äº›æ—¶é—´åºåˆ—ä¸Šï¼Œè·ç¦»è¾ƒé•¿çš„æ•°æ®ï¼Œæ¨¡å‹å°±æ¯”è¾ƒéš¾æ•æ‰åˆ°è¿™ä¸¤ä¸ªæ—¶é—´ç‚¹ä¸Šçš„å…³ç³»ã€‚ æ¨¡å‹è®¾è®¡å¦‚ä¸‹ï¼š é‡ç½®é—¨ - R (reset) æœ‰åŠ©äºæ•æ‰æ—¶åºæ•°æ®ä¸­çŸ­æœŸçš„ä¾èµ–å…³ç³»ã€‚ æ ¹æ®$sigmod$å‡½æ•°çš„æ€§è´¨ï¼Œå€¼ä»‹äº0-1ä¹‹é—´ï¼Œå¯èƒ½ä¸¢å¼ƒè¿‡å»çš„ä¸€äº›ä¸ç›¸å…³çš„ä¿¡æ¯ï¼š R == 0ï¼Œä¸¢å¼ƒäº†ä¸Šä¸€è½®çš„éšå«å±‚çš„ä¿¡æ¯ï¼› R == 1ï¼Œåˆ™ä¿ç•™ç›¸å…³çš„ä¿¡æ¯ã€‚ æ ¹æ®ä¸Šé¢çš„æ¨¡å‹å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é‡ç½®é—¨å…¶å®æ˜¯æœ‰ä¸¤ä¸ªè¾“å…¥çš„ï¼Œåˆ†åˆ«æ˜¯ï¼š $X_t$ $H_{t-1}$ å³å½“å‰å’Œä¸Šä¸€è½®çš„ä¿¡æ¯ï¼Œåˆ™æ˜¯çŸ­æœŸå†…çš„ä¿¡æ¯ã€‚$$R_t = \\sigma(X_t W_{xr} + H_{t-1}W_{hr} + b_r)$$ æ›´æ–°é—¨ æœ‰åŠ©äºæ•æ‰æ—¶åºæ•°æ®ä¸­é•¿æœŸçš„ä¾èµ–çŠ¶æ€ã€‚ ç”¨äºæ›´æ–°å½“å‰éšå«å±‚çš„çŠ¶æ€ï¼Œä¸ºäº†æ•æ‰åˆ°ä¹‹å‰æ¯”è¾ƒä¹…ã€é•¿æœŸä¹‹å‰çš„çŠ¶æ€$$Z_t = \\sigma(X_t W_{xz} + H_{t-1}W_{hz} + b_z)$$ $Z_t$ == 0ï¼Œåˆ™ä¸ä¿ç•™ä¹‹å‰çš„ä¿¡æ¯ï¼› $Z_t$ == 1ï¼Œä¿ç•™ä¹‹å‰çš„ä¿¡æ¯ã€‚ å€™é€‰éšå«çŠ¶æ€$$\\tilde H= tanh(X_tW_{xh} + R_t \\bigodot H_{t-1}W_{hh} + b_h$$ å½“å‰éšå«çŠ¶æ€$$H_t = Z_t\\bigodot H_{t-1} + (1 - Z_t) \\bigodot \\tilde H_t$$ 3. é•¿çŸ­æœŸè®°å¿† - LSTM (Long and Short-Term Memory)ç»“æ„æ¯”ä¹‹å‰çš„GRUæ›´ä¸ºå¤æ‚ä¸€äº›ï¼ŒLSTMçš„éšå«å±‚åŒ…å«éšå«å±‚å˜é‡Hå’Œè®°å¿†ç»†èƒCï¼Œå®ƒä»¬çš„è®¾è®¡å’Œå½¢çŠ¶ç›¸åŒï¼ˆå¯ä»¥çœ‹åˆ°åé¢çš„å‡ ä¸ªfuncè®¾è®¡ä¸Šå…¶å®æ˜¯ä¸€æ ·çš„ï¼Œjiã€‚ è¾“å…¥é—¨ çŸ­æœŸè®°å¿†$$I_t= \\sigma(X_t W_{xi} + H_{t-1}W_{hi} + b_i)$$ é—å¿˜é—¨ é•¿æœŸè®°å¿†$$F_t= \\sigma(X_t W_{xf} + H_{t-1}W_{hf} + b_f)$$ è¾“å‡ºé—¨$$O_t= \\sigma(X_t W_{xo} + H_{t-1}W_{ho} + b_o)$$ å€™é€‰è®°å¿†ç»†èƒ Memory Cell$$\\tilde C_t = \\tanh(X_t W_{xc} + H_{t-1}W_{hc} + b_c)$$ è®°å¿†ç»†èƒ$$C_t = F_t \\bigodot C_{t-1} + I_t \\bigodot \\tilde C_t$$ å½“å‰éšå«çŠ¶æ€ 4. æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ5. åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ6. References åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ -å¾ªç¯ç¥ç»ç½‘ç»œ RNN LSTMä¸GRUæ·±åº¦å­¦ä¹ æ¨¡å‹å­¦ä¹ ç¬”è®°","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.com/tags/Deep-Learning/"},{"name":"RNN","slug":"RNN","permalink":"http://chenson.com/tags/RNN/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-CNNå¸¸è§ç½‘ç»œ","date":"2018-04-27T02:32:41.000Z","path":"2018/04/27/æ·±åº¦å­¦ä¹ ç¬”è®°-CNNå¸¸è§ç½‘ç»œ/","text":"ä¸Šæ¬¡ç¬”è®°ä¸»è¦è®°å½•äº†å·ç§¯ç¥ç»ç½‘ç»œæ€»å·ç§¯Blockä¸­çš„åŸºæœ¬æ¦‚å¿µï¼Œæœ¬æ¬¡ç¬”è®°æ‰“ç®—æ•´ç†ä¸€ä¸‹å¸¸è§çš„CNNï¼Œä»å¼€å±±é¼»ç¥–çš„LeNetåˆ°ç›®å‰æ¯”è¾ƒæµè¡Œçš„ResNetå’ŒDenseNetç­‰ã€‚ 1. LeNetLeNetæ˜¯æ—©èµ·ç”¨æ¥è¯†åˆ«æ‰‹å†™æ•°å­—å›¾åƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œç®—æ˜¯å·ç§¯ç¥ç»ç½‘ç»œçš„å¼€å±±é¼»ç¥–ã€‚ LeNetä¸»è¦åˆ†ä¸ºå·ç§¯å±‚å—å’Œå…¨è¿æ¥å±‚å—ä¸¤å¤§éƒ¨åˆ†ã€‚ 1.1 å·ç§¯å±‚å—åŸºæœ¬å•ä½æ˜¯å·ç§¯å±‚åæ¥æœ€å¤§æ± åŒ–å±‚ï¼ˆå®é™…ä¸Šé‡Œé¢è¿˜å¯ä»¥é¢å¤–çš„æ·»åŠ ä¸€äº›å…¶ä»–çš„æ¨¡å—ï¼‰ï¼Œç„¶åå·ç§¯å±‚å—å°±ç”±è¿™ä¿©åŸºæœ¬å•ä½é‡å¤çš„å †å æ„æˆã€‚ å·ç§¯å±‚ ç”¨æ¥è¯†åˆ«å›¾åƒé‡Œçš„ç©ºé—´æ¨¡å¼ï¼Œè¯†åˆ«ä¸€äº›å±€éƒ¨çš„locallyç‰¹å¾ï¼Œæ¯”å¦‚çº¿æ¡å’Œç‰©ä½“çš„å±€éƒ¨ã€‚ å·ç§¯å±‚çš„è¾“å…¥ä¸º4Dæ•°æ®ï¼Œå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°ï¼Œé€šé“ï¼Œé«˜ï¼Œå®½)ï¼Œè¾“å‡ºä½¿ç”¨äº†Sigmoidæ¿€æ´»å‡½æ•°ã€‚ æœ€å¤§æ± åŒ–å±‚ ç”¨æ¥é™ä½å·ç§¯å±‚å¯¹ä½ç½®çš„æ•æ„Ÿæ€§ï¼Œæ± åŒ–å±‚æ³¨æ„è®¾ç½®å¥½pool_sizeå’Œstrideså¤§å°ã€‚ 1.2 å…¨è¿æ¥å±‚å—åœ¨Convå’ŒDenseäº¤ç•Œçš„éƒ¨åˆ†ï¼Œéœ€è¦å°†è¾“å‡ºçš„æ•°æ®flattenæ‰å¯ä»¥ç»™Denseç”¨ã€‚æ¿€æ´»å‡½æ•°ä½¿ç”¨Sigmod 1.3 MxNetå®ç°ä»£ç 12345678910111213141516171819202122232425import d2lzh as d2limport mxnet as mxfrom mxnet import autograd, gluon, init, ndfrom mxnet.gluon import loss as gloss, nnimport time# å®šä¹‰ç½‘ç»œnet = nn.Sequential()net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'), nn.MaxPool2D(pool_size=2, strides=2), nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'), nn.MaxPool2D(pool_size=2, strides=2), # Denseä¼šé»˜è®¤å°†(æ‰¹é‡å¤§å°,é€šé“,é«˜,å®½)å½¢çŠ¶çš„è¾“å…¥è½¬æ¢æˆ(æ‰¹é‡å¤§å°,é€šé“ * é«˜ * å®½)å½¢ # çŠ¶çš„è¾“å…¥ nn.Dense(120, activation='sigmoid'), nn.Dense(84, activation='sigmoid'), nn.Dense(10))# è®­ç»ƒéƒ¨åˆ†(æœ‰äº›ä»£ç ä¸å®Œæ•´ï¼Œè¯¦æƒ…è§æ²ç¥Githubä»£ç )lr, num_epochs = 0.9, 5# å…¶å®è¿™é‡Œæƒ³å±•ç¤ºçš„æ˜¯netæƒé‡çš„åˆå§‹åŒ–æ—¶ï¼Œä½¿ç”¨çš„æ˜¯Xavieréšæœºåˆå§‹åŒ–net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())trainer = gluon.Trainer(net.collect_params(), 'sgd', &#123;'learning_rate': lr&#125;)train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs) è¿è¡Œç»“æœ ä¸‹å›¾æœ‰ä¿©ç»“æœï¼Œä¸€ä¸ªæ˜¯æ²ç¥ç¬”è®°é‡Œé¢ç”¨K80è·‘çš„ç»“æœï¼Œä¸€ä¸ªæ˜¯æˆ‘ç¬”è®°æœ¬è·‘çš„ç»“æœï¼Œé…ç½®æ˜¯2015å¹´çš„ä¸­é…RMBPï¼Œcpuæ˜¯i5 8Gï¼Œå¤§æ¦‚é€Ÿåº¦å·®äº†10å€å·¦å³ã€‚ è°ƒå‚ç»ƒä¹  è¿™éƒ¨åˆ†å‘¢ï¼Œå…¶å®å°±æ˜¯æ‰¾ä¸€ä¸‹åœ¨æ·±åº¦å­¦ä¹ ä¸­è°ƒå‚çš„ä¸€äº›æ„Ÿè§‰ï¼Œæ¯•ç«Ÿæˆ‘åªæ˜¯ä¸€æšç‚¼ä¸¹å·¥ç¨‹å¸ˆã€‚ æ”¹ä¸ºæ™®é€šçš„éšæœºåˆå§‹åŒ–ç½‘ç»œæƒé‡å¯ä»¥çœ‹åˆ°ä¸‹å›¾åŸºæœ¬ä¸æ”¶æ•›ã€‚ æ¥ç€å°è¯•ä¸€ä¸‹æŠŠlrç»™è°ƒå°ä¸€äº›ï¼Œè°ƒåˆ°0.05ï¼Œä¼¼ä¹ä¹Ÿæ²¡å•¥æå‡ï¼Œç»§ç»­è°ƒå°åˆ°0.005ï¼Œä¸”å¢åŠ num_epochsåˆ°20ï¼ŒåŸºæœ¬ä¹Ÿæ²¡æå‡ã€‚ç„¶åæƒ³æƒ³æˆ‘æ˜¯æ”¹äº†ä¸€ä¸ªè¾ƒä¸ºåˆé€‚çš„æƒé‡ï¼Œæ‰€ä»¥åº”è¯¥æŠŠlrè°ƒé«˜ä¸€äº›ï¼Œæ–¹ä¾¿å¿«é€Ÿè°ƒæ•´åˆ°åˆé€‚çš„æƒé‡ï¼Œå°±æŠŠlræé«˜åˆ°1.5ï¼Œç”¨çš„éšæœºåˆå§‹åŒ–ï¼Œä¸€ç›´ç­‰åˆ°epocheåˆ°äº†10å·¦å³çš„æ—¶å€™accæ‰å¼€å§‹æå‡ï¼Œå¯ä»¥è¯´æ”¶æ•›å¾—éå¸¸æ…¢äº†ã€‚çœ‹æ¥è¿™ä¸ªæƒé‡çš„åˆå§‹åŒ–éå¸¸é‡è¦ï¼Œéœ€è¦å•ç‹¬è°ƒç ”ä¸€ä¸‹ï¼Œåˆ°æ—¶å€™çœ‹å•¥åŸå› ã€‚ å¾€å°è°ƒ å¾€å¤§è°ƒ å…¶å®lrè°ƒå¤ªå°äº†(0.05/0.005, åŸå…ˆæ˜¯0.9)ï¼Œè™½ç„¶æ”¹å›å»Xavieråˆå§‹åŒ–æƒé‡ä¹ŸåŸºæœ¬æ˜¯ä¸Šé¢æ•ˆæœï¼Œä¼°è®¡æ˜¯å¤ªå°äº†ä¸æ”¶æ•›ï¼Ÿè‹¥æ˜¯è°ƒåˆ°0.5çš„è¯ï¼Œå¯ä»¥æ”¶æ•›ï¼Œä½†ç›¸æ¯”é€Ÿåº¦ä¸Šä¼šæ”¶æ•›å¾—æ…¢ä¸€äº›ï¼Œæ‰€ä»¥é€‰å®šä¸€ä¸ªåˆé€‚çš„lrä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚ è°ƒæ•´å·ç§¯çª—å£çš„å¤§å° è°ƒæ•´è¾“å‡ºé€šé“æ•° è°ƒæ•´æ¿€æ´»å‡½æ•° è°ƒæ•´å…¨è¿æ¥å±‚è¾“å‡ºä¸ªæ•° 2. AlexNetï¼ˆ2013ï¼‰ æ€»å…±å…­ä¸ªé˜¶æ®µ kernelï¼Œstridesç›¸å¯¹å¤§å¾ˆå¤š Xavieræ˜¯æ€ä¹ˆåˆå§‹åŒ–çš„ï¼Ÿä¸é»˜è®¤çš„åŒºåˆ«ï¼Ÿ LeNet V.S AlexNet 3. VGGä½¿ç”¨é‡å¤å…ƒç´ çš„éå¸¸æ·±çš„ç½‘ç»œ ä½¿ç”¨å¾ˆå¤šç›¸å¯¹åŠ å¤šçš„kernelï¼Œæ¯”å¦‚3x3çš„å·ç§¯ï¼Œç„¶åæ¥ä¸Šä¸€ä¸ªæ± åŒ–å±‚ï¼Œä¹‹åå†å°†è¿™ä¸ªæ¨¡å—é‡å¤å¾ˆå¤šæ¬¡ã€‚ VGG Block ç›¸åŒå åŠ  VGG Stack ç›¸ä¼¼å åŠ  VGG11, VGG13, VGG16, VGG19 â€¦ â€¦ 4. Bath Normæ‰¹é‡å½’ä¸€åŒ–ï¼Œå°†æ•°æ®å‡å€¼å˜ä¸º0ï¼Œæ–¹å·®å˜ä¸º1ï¼Œä½¿æ•°å€¼æ›´åŠ ç¨³å®šã€‚ $\\lambda$ $\\beta$ å…¨è¿æ¥2Dæ•°æ® æ¯ä¸ªæ ·æœ¬ä¹‹é—´ï¼Œå‡å€¼å˜ä¸º0ï¼Œæ–¹å·®å˜ä¸º1 å¦‚æœæ˜¯å·ç§¯3Dæ•°æ® å¯¹æ¯ä¸ªchannelåšå˜æ¢ï¼Œä½¿å¾—å‡å€¼å˜ä¸º0ï¼Œæ–¹å·®å˜ä¸º1 Q&amp;A: batchNorm åŠ çš„ä½ç½®ï¼Ÿ ä¸ºä»€ä¹ˆæ˜¯Conv2Dçš„åé¢ï¼ŒActivationçš„å‰é¢ï¼Œè€Œä¸æ˜¯Conv2Dçš„å‰é¢ï¼Ÿ æ¯ä¸ªbatchä¹‹é—´çš„å½’ä¸€åŒ–ä¸ä¸€æ ·ï¼Ÿ è®­ç»ƒçš„æ—¶å€™okï¼Œæµ‹è¯•çš„æ—¶å€™å¦‚ä½•åšå½’ä¸€åŒ–ï¼Ÿ ä¿ç•™è®­ç»ƒæ—¶å€™ç®—å‡ºæ¥çš„ç»“æœåšå½’ä¸€åŒ– mean, variance, $\\lambda$ , $\\beta$ 5. Network in Networ (NiN) AlexNet å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚åˆ†åˆ«åŠ æ·±åŠ å®½ä»è€Œå¾—åˆ°æ·±åº¦ç½‘ç»œ NiN ä¸²è”æ•°ä¸ªå·ç§¯å±‚å—å’Œå…¨è¿æ¥å±‚å—æ¥æ„å»ºæ·±åº¦ç½‘ç»œ Q&amp;Aï¼š å¦‚ä½•è§£å†³Convå’ŒDenseä¹‹é—´è¾“å…¥ç»´åº¦çš„é—®é¢˜ï¼Œæ¯”å¦‚Convçš„è¾“å…¥ä¸º4Dï¼Œä½†æ˜¯Denseå¯èƒ½æ— æ³•è¾“å‡ºå¦‚æ­¤ç»´åº¦çš„æ•°æ® æŠŠDenseå±‚æ¢æˆkernelä¸º1x1çš„Conv 4Dæ•°æ®åˆ°2Dæ•°æ®é€šå¸¸ä¼šç¼–ç¨‹éå¸¸å¤§ 6. GoogLeNet(2014) Inception","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.com/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.com/tags/CNN/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-CNN","date":"2018-04-20T06:45:37.000Z","path":"2018/04/20/æ·±åº¦å­¦ä¹ ç¬”è®°-CNN/","text":"æœ€è¿‘æ­£åœ¨å­¦ä¹ ææ²æ²ç¥çš„åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ï¼Œç›®å‰å¬åˆ°äº†ç¬¬äº”è¯¾ï¼ŒçœŸçš„æ˜¯éå¸¸å¥½çš„è¯¾ç¨‹ã€‚å…¶å®åœ¨å­¦ä¹ ä¹‹å‰ä¹Ÿæœ‰æ–­æ–­ç»­ç»­çš„å­¦è¿‡ä¸€äº›æ·±åº¦å­¦ä¹ ç›¸å…³çš„ä¸œè¥¿ï¼Œä¹Ÿä¼šç”¨PyTorchè·‘ä¸€äº›ç®€å•çš„æ¨¡å‹ï¼Œä½†ä»…é™äºæ­¤ï¼Œå¯¹ç†è®ºçŸ¥è¯†ç†è§£è¿˜æ˜¯æ¯”è¾ƒæµ…è–„çš„ï¼Œåªæ˜¯ä¸€ä¸ªè°ƒåŒ…ä¾ ï¼Œæ‰€ä»¥è¿™æ¬¡å­¦ä¹ æ‰“ç®—è®¤çœŸå­¦ä¹ ä¸€ä¸‹ç›¸å…³çš„ç†è®ºçŸ¥è¯†ã€‚ åœ¨å­¦ä¹‹å‰ä¹Ÿæœ‰å¬äº†ä¸€ä¸‹NGåˆšå‘å¸ƒçš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹ï¼Œä½†å¯¹æ¯”äº†ä¸€ä¸‹ï¼Œè¿˜æ˜¯è§‰å¾—æ²ç¥çš„æ¯”è¾ƒé€‚åˆæˆ‘ï¼Œæ¯•ç«Ÿæ•™å­¦ç”¨çš„æ˜¯ä¸­æ–‡ï¼Œæ–¹ä¾¿ç†è§£ï¼Œä¸”ç†è®ºçŸ¥è¯†æ·±åº¦åˆé€‚ï¼Œæ‰€ä»¥æ‰“ç®—å…ˆå¬å®Œè¿™é—¨è¯¾ï¼Œä»¥åç†è®ºæ–¹é¢æœ‰éœ€è¦çš„ï¼Œå†å¬NGç›¸å…³çš„è¯¾ç¨‹ã€‚ 1. å·ç§¯ç¥ç»ç½‘ç»œ CNN - (Convolutional Neural Network)1.1 ç¥ç»ç½‘ç»œ - Neural Network å•ä¸ªç¥ç»å…ƒ ç¥ç»ç½‘ç»œ 1. 2 å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„åŸºæœ¬ç»“æ„å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªblock è¾“å…¥å±‚ å·ç§¯å±‚+æ¿€æ´»å‡½æ•°+æ± åŒ–å±‚ è¾“å‡ºå±‚ è¾“å…¥å±‚ â€”&gt; å·ç§¯å±‚ â€”&gt; æ¿€æ´»å‡½æ•° â€”&gt; æ± åŒ–å±‚ â€”&gt; å…¨è¿æ¥å±‚ ä¸­é—´ä¸‰ä¸ªéƒ¨åˆ†å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªblockï¼Œå¤æ‚çš„æ¨¡å‹å¯ä»¥é‡å¤å¤šä¸ªè¿™æ ·çš„blockï¼Œé‡Œé¢å‚æ•°å¯ä»¥æ˜¯ä¸€æ ·æˆ–è€…ç›¸ä¼¼çš„ã€‚ 1. 3 å·ç§¯ - Convolutionalç›¸å¯¹æ™®é€šçš„ç¥ç»ç½‘ç»œï¼Œå·ç§¯ç¥ç»ç½‘ç»œåœ¨ä¸­çš„æ ¸å¿ƒå°±æ˜¯åœ¨äºå·ç§¯è®¡ç®—å±‚äº†ï¼Œå·ç§¯ç¥ç»ç½‘ç»œåŒ…å«äº†ä¸€ä¸ªç”±å·ç§¯å±‚å’Œå­é‡‡æ ·å±‚æ„æˆçš„ç‰¹å¾æŠ½å–å™¨ã€‚å¯¹äºå¦‚ä½•ç†è§£å·ç§¯ï¼Œè£‚å¢™æ¨èçŸ¥ä¹ä¸Šçš„è¿™ç¯‡æ–‡ç« é€šä¿—ç†è§£ã€å·ç§¯ã€â€”â€”ä»å‚…é‡Œå¶å˜æ¢åˆ°æ»¤æ³¢å™¨ï¼Œä»ä¿¡å·å¤„ç†è¿™ä¸ªæœ¬è´¨çš„è§’åº¦æ¥ç†è§£å·ç§¯ï¼Œéå¸¸æ£’ğŸ‘ã€‚ å·ç§¯çš„æ„æ€å°±æ˜¯ï¼Œç¥ç»ç½‘ç»œæ˜¯å¯¹å›¾ç‰‡ä¸Šçš„ä¸€å°å—åŒºåŸŸè¿›è¡Œå¤„ç†ï¼Œè¿™ç§åšæ³•åŠ å¼ºäº†å›¾ç‰‡ä¿¡æ¯çš„è¿ç»­æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥çœ‹åˆ°å›¾ç‰‡ä¸Šçš„å›¾å½¢ï¼Œè€Œéæ˜¯ä¸€ä¸ªä¸ªç¦»æ•£çš„ç‚¹ã€‚ä¸‹å›¾æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„çš„å·ç§¯è¿ç®—ï¼Œä¸­é—´çš„æ ¸æ˜¯å·ç§¯ä¸­çš„kernelã€‚ æ ¹æ®å›¾ç‰‡ç¤ºæ„ï¼Œæˆ‘ä»¬å°†è¾“å…¥å±‚è“è‰²çš„æ¡†æ¡†éƒ¨åˆ†ä¸kernelç›¸ä¹˜ï¼Œæœ‰ï¼š$$0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19$$å½“ç„¶æ­¤æ—¶åªè¿ç®—äº†ä¸€å°éƒ¨åˆ†ï¼Œå·¦è¾¹çš„è¾“å…¥æ•°ç»„ä¸­ï¼Œè“è‰²çš„å››æ ¼å°æ¡†æ¡†å¯ä»¥é€æ­¥çš„å¾€å³è¾¹ä¸€æ ¼ä¸€æ ¼çš„æŒªåŠ¨ï¼Œç„¶ååˆ°è¾¾æœ€å³è¾¹åï¼Œåˆå¾€ä¸‹ç§»åŠ¨ä¸€ä¸ªæ ¼å­ï¼Œä»å·¦å¾€å³é‡å¤ä¸Šé¢çš„è¿ç®—ï¼Œå³æœ‰æœ€å³è¾¹çš„è¾“å‡ºçŸ©é˜µ$$0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19 \\1 \\times 0 + 2 \\times 1 + 4 \\times 2 + 5 \\times 3 = 25 \\3 \\times 0 + 4 \\times 1 + 6 \\times 2 + 7 \\times 3 = 37 \\4 \\times 0 + 5 \\times 1 + 7 \\times 2 + 8 \\times 3 = 43$$ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ–¹ä¾¿ç†è§£çš„åŠ¨å›¾ ä»¥ä¸Šå°±æ˜¯å·ç§¯æœ€åŸºæœ¬çš„è¿ç®—è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸‹å‡ ä¸ªå‡è®¾ kernel size çš„å¤§å° åœ¨ä¸Šè¿°æˆ‘ä»¬å‡è®¾äº†kernelçš„sizeæ˜¯ $2 \\times 2$ çš„ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥æ˜¯å…¶ä»–ç»´åº¦çš„kernelï¼Œæ¯”å¦‚è¯´ $3 \\times 3$ ä¹‹ç±»çš„ï¼Œæˆ–è€…æ˜¯ $4 \\times 5$ è¿™æ ·çš„éæ­£æ–¹å½¢çŸ©é˜µ å‘¨å›´çš„å¡«å……å€¼ padding å…¶å®è¿™ä¸€éƒ¨åˆ†ä¸Šå›¾ä¸­å¹¶æ²¡æœ‰ä½“ç°å‡ºæ¥è¿™å‡è®¾ï¼Œpaddingçš„ä¸­æ–‡æ„æ€æ˜¯å¡«å……ï¼Œé¡¾æ˜æ€è®®å°±æ˜¯åœ¨è¾“å…¥çš„äºŒç»´çŸ©é˜µçš„é«˜å’Œå®½ä¸¤ä¾§å¡«å……å…ƒç´ ï¼ˆé€šå¸¸æ˜¯0ï¼‰ã€‚æ¯”å¦‚ä¸‹å›¾ è¿™é‡Œå‡è®¾paddingä¸º0ï¼Œå³åœ¨ç»™å®šè¾“å…¥çš„äºŒç»´çŸ©é˜µçš„é«˜å’Œå®½ä¸¤ä¾§åˆ†åˆ«å¡«å……äº†ä¸€åœˆ0å…ƒç´ ï¼ˆæ˜¯å¦å¯ä»¥æ˜¯ä¸¤åœˆæˆ–è€…å¤šåœˆï¼Ÿï¼‰ã€‚ ç›¸æ¯”ä¸Šä¸€å¼ å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¾“å‡ºçš„äºŒç»´çŸ©é˜µä¹Ÿå˜å¤§äº†ä¸€åœˆã€‚ å…¶å®å¯¹äºç»™å®šçš„è¾“å…¥çš„äºŒç»´çŸ©é˜µã€kernelå¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºè¾“å‡ºçŸ©é˜µçš„å¤§å°çš„ æ²¡æœ‰è®¾å®špadding$$(n_h - k_h + 1)/1 \\times (n_w - k_w + 1)/1$$è¿™é‡Œhä»£è¡¨çš„æ˜¯é«˜åº¦ã€wä»£è¡¨äº†å®½åº¦ï¼š è¾“å‡ºå®½åº¦ = è¾“å…¥å®½åº¦ - kernelå®½åº¦ + 1 è¾“å…¥é«˜åº¦ = è¾“å…¥é«˜åº¦ - kernelé«˜åº¦ + 1 è®¾å®šäº†padding $$(n_h - k_h + p_h + 1)/1 \\times (n_w - k_w + p_w + 1)/1$$ è¾“å‡ºå®½åº¦ = è¾“å…¥å®½åº¦ - kernelå®½åº¦ + å¡«å……çš„å®½åº¦ + 1 è¾“å…¥é«˜åº¦ = è¾“å…¥é«˜åº¦ - kernelé«˜åº¦ + å¡«å……çš„é«˜åº¦ + 1 è¿™é‡Œå›ç­”ä¸€ä¸‹ä¸Šé¢æˆ‘æ‹¬å·å†…åŠ ç²—çš„é—®é¢˜ï¼Œå½“ç»™å‘¨å›´å¡«å……ä¸€åœˆ0çš„æ—¶å€™ï¼Œpaddingçš„å€¼ä¸º $p_w \\&amp; p_h = 1$ï¼Œå½“ä¸¤åœˆ0çš„æ—¶å€™ä¸º $ p_w \\&amp; p_h=2 $ã€‚é€šå¸¸å¯¹äºpaddingçš„å–å€¼ä¸ºï¼š$$p_w = k_w - 1 \\p_h = k_h - 1$$è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†è¾“å‡ºçš„çŸ©é˜µå˜å¤§ï¼Œå°½é‡ä¿æŒå’Œå’Œè¾“å…¥å¤§å°æ¯”è¾ƒç›¸è¿‘ï¼Œé¿å…å¤šå±‚å·ç§¯çš„æ—¶å€™ï¼ŒçŸ©é˜µè¶Šæ¥è¶Šå°ï¼Œç›¸å½“äºä¿¡æ¯è¶Šæ¥è¶Šå°‘ã€‚ å¦‚æœéœ€è¦ä¿æŒè¾“å…¥å’Œè¾“å‡ºç›¸åŒçš„å¤§å°ï¼Œåˆ™å¯ä»¥é‡‡ç”¨å¦‚ä¸‹è®¾å®šï¼š kernelå¤§å°ä¸ºå¥‡æ•°ï¼Œåœ¨çŸ©é˜µçš„é«˜å®½ä¸¤ä¾§éƒ½å¡«å……på€¼ kernelå¤§å°ä¸ºå¶æ•°ï¼Œåœ¨çŸ©é˜µçš„é«˜å®½ä¸€ä¾§ä¸­å¡«å……på€¼ æ»‘åŠ¨çš„æ­¥é•¿ stride è¿™é‡Œæˆ‘ä»¬å‡è®¾è¾“å…¥æ¡†æ¡†ä»å·¦å¾€å³å’Œä»ä¸Šåˆ°ä¸‹çš„ç§»åŠ¨æ­¥ä¼æ˜¯1ä¸ªå°æ ¼å­ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥æ˜¯2ä¸ªå°æ ¼å­ï¼Œ3ä¸ªå°æ ¼å­ç­‰ï¼Œè¿™ä¸ªå‚æ•°åœ¨CNNä¸­å«åšstrideã€‚ä¸‹å›¾strideçš„å¤§å°ä¸º(2, 3)ï¼Œå³å‘å³æ»‘åŠ¨çš„æ—¶å€™ï¼Œæ¯æ¬¡2ä¸ªæ ¼å­ï¼Œå‘ä¸‹æ»‘åŠ¨çš„æ—¶å€™ï¼Œæ¯æ¬¡3ä¸ªæ ¼å­ã€‚ æ³¨æ„åˆ°äº†åœ¨paddingéƒ¨åˆ†æœ‰ä¸€ä¸ªå¸¸æ•°é¡¹ä¸º1äº†å—ï¼Œå…¶å®é‚£ä¸ªå°±æ˜¯æˆ‘ä»¬è®¾å®šçš„strideçš„æ»‘åŠ¨å¤§å°ï¼Œå¦‚æœstrideä¸ä¸º(1, 1)çš„æ—¶å€™ï¼Œæ­¤æ—¶è¾“å…¥å¤§å°è®¡ç®—å…¬å¼ä¸ºï¼š$$(n_h - k_h + p_h + s_h)/s_h \\times (n_w - k_w + p_w + s_w)/s_w$$ 1.4 æ¿€æ´»å‡½æ•°å…ˆè¯´ç»“è®ºï¼Œæ¿€æ´»å‡½æ•°ä¸»è¦çš„ä½œç”¨æ˜¯åŠ å…¥éçº¿æ€§å› ç´ ï¼Œå› ä¸ºçº¿æ€§çš„è¡¨è¾¾èƒ½åŠ›ä¸å¤Ÿï¼ˆå¦‚ä½•ä½“ç°ï¼Ÿï¼‰ã€‚ å…¶å®ä»ä¸Šé¢å·ç§¯çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œè¿‡ç¨‹å¤§è‡´å¯ä»¥ç†è§£ä¸ºï¼š ä»ä¸€ä¸ªè¾“å…¥çš„çŸ©é˜µä¸­æŠ½å–å‡ºä¸€ä¸ªå±€éƒ¨çŸ©é˜µï¼Œç„¶åè·Ÿkernelçº¿æ€§ç›¸ä¹˜ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªç»“æœå€¼ã€‚é€šè¿‡ä»å·¦å‘å³ï¼Œä»ä¸Šè‡³ä¸‹çš„æ»‘åŠ¨ï¼Œéå†äº†æ•´ä¸ªè¾“å…¥çŸ©é˜µçš„ã€‚ ä¸Šè¿°è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªå¾ˆé‡è¦çš„æ¦‚å¿µæ˜¯çº¿æ€§ç›¸ä¹˜ï¼Œå·ç§¯çš„æ“ä½œå…¶å®æ˜¯çº¿æ€§çš„ã€‚æ­¤æ—¶åŠ å…¥æ¿€æ´»å‡½æ•°å¯ä»¥ç»™æ¨¡å‹åŠ å…¥éçº¿æ€§å› ç´ ï¼Œä¸”æ¿€æ´»å‡½æ•°éœ€è¦å…·å¤‡ä»¥ä¸‹å‡ ä¸ªæ€§è´¨ éçº¿æ€§ çº¿æ€§æ¿€æ´»å±‚å¯¹äºæ·±å±‚ç¥ç»ç½‘ç»œæ²¡æœ‰ä½œç”¨ï¼Œå› ä¸ºå…¶ä½œç”¨ä»¥åä»ç„¶æ˜¯è¾“å…¥çš„å„ç§çº¿æ€§å˜æ¢ è¿ç»­å¯å¾® å› ä¸ºç¥ç»ç½‘ç»œä½¿ç”¨çš„æ¢¯åº¦ä¸‹é™æ³• èŒƒå›´æœ€å¥½ä¸é¥±å’Œï¼ˆä¸æ˜¯å¾ˆç†è§£ï¼‰ å½“æœ‰é¥±å’Œçš„åŒºé—´æ®µæ—¶ï¼Œè‹¥ç³»ç»Ÿä¼˜åŒ–è¿›å…¥åˆ°è¯¥æ®µï¼Œæ¢¯åº¦è¿‘ä¼¼ä¸º0ï¼Œç½‘ç»œçš„å­¦ä¹ å°±ä¼šåœæ­¢ã€‚ å•è°ƒæ€§ å½“æ¿€æ´»å‡½æ•°æ˜¯å•è°ƒæ—¶ï¼Œå•å±‚ç¥ç»ç½‘ç»œçš„è¯¯å·®å‡½æ•°æ˜¯å‡¸çš„ï¼Œå¥½ä¼˜åŒ– åœ¨åŸç‚¹å¤„è¿‘ä¼¼çº¿æ€§ æ ·å½“æƒå€¼åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„éšæœºå€¼æ—¶ï¼Œç½‘ç»œå¯ä»¥å­¦ä¹ çš„è¾ƒå¿«ï¼Œä¸ç”¨å¯ä»¥è°ƒèŠ‚ç½‘ç»œçš„åˆå§‹å€¼ å¸¸ç”¨çš„å‡ ä¸ªæ¿€æ´»å‡½æ•°æœ‰ï¼š Sigmoid åœ¨CNNå·ç§¯éƒ¨åˆ†ä¸­åŸºæœ¬è¢«æ·˜æ±°ï¼Œå› ä¸ºé¥±å’Œæ—¶æ¢¯åº¦å€¼éå¸¸å°ï¼Œå½“å±‚æ•°æ¯”è¾ƒå¤šçš„æ—¶å€™ï¼Œç”¨BPç®—æ³•æ–¹å‘ä¼ æ’­çš„æ—¶å€™ï¼Œé è¿‘å‰é¢å±‚éƒ¨åˆ†åŸºæœ¬å¾—ä¸åˆ°æ›´æ–°ï¼Œå³æ¢¯åº¦è€—æ•£ã€‚ä¸”è¾“å‡ºçš„å€¼ä¸æ˜¯ä»¥0ä¸ºä¸­å¿ƒã€‚ Tanh å¦‚ä¸‹å›¾ï¼ŒåŸºæœ¬ç¼ºç‚¹åŒSigmoidå‡½æ•° ReLu ï¼ˆæœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼‰ Alexåœ¨2012æå‡ºAlexNetæ—¶å€™æå‡ºçš„ å½“x&gt;0çš„æ—¶å€™ï¼Œæ¢¯åº¦æ’ä¸º1ï¼Œä¸å­˜åœ¨æ¢¯åº¦è€—æ•£çš„é—®é¢˜ï¼Œæ”¶æ•›å¿«ã€‚ å½“x&lt;0çš„æ—¶å€™ï¼Œè¾“å‡ºä¸º0ï¼Œå¢åŠ ç½‘ç»œçš„ç¨€ç–æ€§ï¼Œè¿™æ ·æå–å‡ºçš„ç‰¹å¾å…·æœ‰ä»£è¡¨æ€§ï¼Œæ³›åŒ–æ€§å¼ºã€‚ï¼ˆå¦‚ä½•ä¿è¯x&lt;0çš„éƒ¨åˆ†å€¼æ˜¯æ— æ„ä¹‰çš„ï¼Ÿï¼‰ ç¼ºç‚¹å°±æ˜¯å…·æœ‰æ­»äº¡ç‰¹æ€§ï¼ˆæœ‰ç‚¹æ„Ÿè§‰ï¼Œä½†ä¸æ˜¯å¾ˆç†è§£ï¼‰ï¼Œéœ€è¦æ³¨æ„è°ƒæ•´Learning Rateæ¥é¿å…æ­»äº¡èŠ‚ç‚¹è¿‡å¤šã€‚ Leaky ReLu æ”¹æ•£äº†ReLUçš„æ­»äº¡ç‰¹æ€§ï¼ŒåŒæ—¶ä¹ŸæŸå¤±äº†ä¸€éƒ¨åˆ†ç¨€ç–æ€§ï¼Œå¢åŠ ä¸€ä¸ªè¶…å‚ã€‚ 1.5 æ± åŒ–å±‚ - Poolingå…ˆè¯´ç»“è®ºï¼šæ± åŒ–å±‚çš„å‡ ä¸ªä¸»è¦ä½œç”¨ï¼š ç¼“è§£äº†å·ç§¯å±‚å¯¹ä½ç½®çš„è¿‡åº¦æ•æ„Ÿæ€§ èƒ½å¤Ÿå‹ç¼©å›¾ç‰‡ï¼Œä½¿ç‰¹å¾å›¾å˜å°ï¼ŒåŒæ—¶ä¿ç•™äº†æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæå–ä¸»è¦ç‰¹å¾ æ± åŒ–å¤§è‡´å°±æ˜¯ä¸€ä¸ªç­›é€‰è¿‡æ»¤çš„è¿‡ç¨‹ï¼Œèƒ½è¿‡å°†å·¦è¾¹çš„è¾“å‡ºå±‚ä¸­æœ‰ç”¨çš„ä¿¡æ¯ç­›é€‰å‡ºç»™åˆ°è¾“å‡ºå±‚ã€‚è®¡ç®—æ–¹å¼å’Œkernelçš„è®¡ç®—æ–¹å¼æ¯”è¾ƒç›¸ä¼¼ï¼Œä¹Ÿæ˜¯ç»™å®šä¸€ä¸ª(w, h)å¤§å°çš„poolingçŸ©é˜µï¼Œä»å·¦å‘å³ï¼Œä»ä¸Šè‡³ä¸‹ç¼“æ…¢æ»‘åŠ¨è®¡ç®—è¾“å‡ºä¸€ä¸ªå€¼ç»™åˆ°è¾“å‡ºå±‚ã€‚ä¸kernelä¸åŒçš„æ—¶å€™ï¼Œè¿™ä¸åšçŸ©é˜µè¿ç®—ï¼Œåªæ˜¯è®¡ç®—è¾“å…¥çŸ©é˜µä¸­å½“å‰å±€éƒ¨çŸ©é˜µ(w, h)ä¸­çš„æœ€å¤§å€¼æˆ–æ˜¯å‡å€¼ï¼Œå¯¹åº”ä½¿ç”¨çš„å°±æ˜¯æœ€å¤§æ± åŒ–æˆ–æ˜¯å‡å€¼æ± åŒ–ã€‚ï¼ˆæ»‘åŠ¨çš„å¤§å°æ—¶å€™å¦‚ä½•è®¾å®šï¼Ÿï¼‰ å›ç­”ä¸€ä¸‹ä¸Šè¿°æ‹¬å·ä¸­çš„é—®é¢˜ï¼Œæ± åŒ–å±‚åŒæ ·ä¹Ÿæœ‰paddingå’Œstrideçš„æ¦‚å¿µï¼Œæ–¹å¼å’Œå·ç§¯ä¸­æ¯”è¾ƒç›¸ä¼¼ï¼Œå°±ä¸ä¸€ä¸€è§£é‡Šäº†ã€‚ 1.6 å…¶ä»–ä¸€äº›æ¦‚å¿µ æ„Ÿå—é‡ - Receptive Field ä¸­æ–‡åçœŸçš„æ˜¯â€¦ â€¦ å¤ªå¤ªå¤ªéš¾ç†è§£äº†ï¼Œç›´æ¥çœ‹è‹±æ–‡åReceptive Fieldæ¯”è¾ƒç›´è§‚ä¸€äº›ã€‚ åŸºæœ¬å®šä¹‰å°±æ˜¯ å·ç§¯ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚è¾“å‡ºçš„ç‰¹å¾å›¾ï¼ˆFeature apï¼‰ä¸Šçš„åƒç´ ç‚¹åœ¨åŸå›¾åƒä¸Šæ˜ å°„çš„åŒºåŸŸå¤§å°ã€‚ ç¬¬ä¸€å±‚å·ç§¯ è¾“å…¥ä¸º10x10çš„å›¾ç‰‡ï¼Œç»è¿‡3x3çš„kernelï¼Œè¾“å‡ºä¸º8x8çš„å¤§å°ï¼ˆè¿™é‡Œåªè€ƒè™‘å•å±‚ï¼‰ ä¸”åœ¨output1ä¸­çš„æ¯ä¸€ä¸ªåƒç´ ç‚¹ï¼Œéƒ½å—åˆ°åŸå§‹å›¾åƒå¯¹åº”çš„3x3åŒºåŸŸå†…çš„å½±å“ï¼Œç¬¬ä¸€åœºçš„æ„Ÿå—é‡ä¸º3ï¼Œç”¨å­—æ¯è¡¨ç¤ºRF1=3 ç¬¬äºŒå±‚å·ç§¯ åœ¨ç¬¬ä¸€å±‚å·ç§¯çš„è¾“å‡ºoutput1ä¸‹ï¼Œç»è¿‡ç¬¬äºŒå±‚3x3çš„kernelå·ç§¯ï¼Œè¾“å‡ºçš„å¤§å°ä¸º6x6 å¦‚æœä»output2å¾€å›æ¨çš„è¯ï¼Œoutput2ä¸Šçš„ä¸€ä¸ªåƒç´ ç‚¹ï¼Œå—åˆ°output1ä¸Šä¸‰ä¸ªåƒç´ ç‚¹çš„å½±å“ï¼Œè€Œè¿™ä¸‰ä¸ªåƒç´ ç‚¹åˆæ€»å…±å—åˆ°è¾“å…¥å±‚äº”ä¸ªåƒç´ ç‚¹çš„å½±å“ï¼Œæ‰€ä»¥ç¬¬äºŒå±‚çš„æ„Ÿå—é‡RF2=5 ç¬¬ä¸‰å±‚å·ç§¯ æ­¤æ—¶kernel3ä¾æ—§ä¸º3x3ï¼Œæ ¹æ®ä¸Šé¢æ¨ï¼ŒRF3=7 ç¬¬å››æ¬¡å·ç§¯ ç¬¬äº”æ¬¡æ± åŒ–è¿ç®— é€šé“ - Channel å¤šè¾“å…¥é€šé“ ä¸Šé¢æˆ‘ä»¬è®¨è®ºçš„éƒ½æ˜¯äºŒç»´çš„æ•°æ®çŸ©é˜µï¼Œå…¶å®åœ¨å›¾åƒå¤„ç†è¿‡ç¨‹ä¸­ï¼Œå›¾ç‰‡é€šå¸¸éƒ½æ˜¯ä¸‰ç»´çš„ã€‚é™¤äº†å›¾ç‰‡çš„é«˜åº¦å’Œå®½åº¦å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä»é¢œè‰²çš„è§’åº¦ï¼Œåˆ†ä¸ºRGBä¸‰ä¸ªé¢œè‰²é€šé“ã€‚å³ä¸€ä¸ªä¸‰ç»´çš„çŸ©é˜µï¼Œè¿™é‡Œçš„ç¬¬ä¸‰ç»´ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºChannelï¼Œå¯ä»¥è¯´çš„ç›¸äº’ç‹¬ç«‹çš„(è®¡ç®—ç”¨åˆ°çš„kernelå¯ä»¥æ˜¯ç‹¬ç«‹çš„)ã€‚ ä¾æ—§æ˜¯ä»¥ä¸Šé¢çš„ä¾‹å­ä¸ºä¾‹ï¼Œä»Channelä¸€ç»´å‡çº§åˆ°äºŒç»´ï¼Œè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š $$channel_1:1 \\times 1 + 2 \\times 2 + 3 \\times 4 + 5 \\times 4 = 37 \\channel_2:0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 4 = 19 \\output = 37 + 19 = 56$$ å¤šè¾“å‡ºé€šé“ ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œè¾“å‡ºé€šé“ä¸º1ï¼Œå³æœ€ç»ˆå¯¹æ¯ä¸ªç‹¬ç«‹channelè¿ç®—çš„é€šé“è¾“å‡ºçš„ç»“æœåšç´¯åŠ è¿ç®—ã€‚ å¦‚æœæ˜¯éœ€è¦è¾“å‡ºå¤šé€šé“å‘¢ï¼Ÿç›¸ä¼¼çš„ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—ç‹¬ç«‹channelçš„æ—¶å€™ï¼Œå°±è¾“å‡ºä¸€ä¸ªå¤šé€šé“çš„çš„è¾“å‡ºï¼Œç„¶ååœ¨ä¸åŒchannelä¹‹é—´ï¼Œç»§ç»­åšç´¯åŠ è¿ç®—ã€‚ $1 \\times 1$ å·ç§¯å±‚ è¿™ä¸ªå·ç§¯å±‚å¾ˆæœ‰æ„æ€ï¼Œæ²ç¥åœ¨ä¸Šè¯¾çš„è¿‡ç¨‹ä¸­å¤šæ¬¡æåˆ°äº†è¯¥å·ç§¯ã€‚ å½“kernelçš„sizeä¸º(1, 1)çš„æ—¶å€™ï¼Œè¯´æ˜æ­¤æ—¶kernelä¸å¯¹è¾“å…¥å±‚åšé‡‡æ ·ç„¶åè®¡ç®—ï¼Œå³å¤±å»äº†å·ç§¯å±‚å¯ä»¥è¯†åˆ«é«˜å’Œå®½ç»´åº¦ä¸Šç›¸é‚»å…ƒç´ æ„æˆçš„æ¨¡å¼çš„åŠŸèƒ½ã€‚ å¯ä»¥è¯´æ­¤æ—¶ä¸€å¼€å§‹çš„äºŒç»´å±‚é¢ä¸Šï¼Œkernelæ˜¯æ²¡å•¥ç”¨çš„ï¼Œä½†æ˜¯åœ¨ç¬¬ä¸‰ç»´channelçš„è§’åº¦è¿˜æ˜¯æœ‰ç”¨çš„ï¼Œå³æˆ‘ä»¬å¯ä»¥ç”¨1x1çš„çŸ©é˜µåšç»´åº¦å˜æ¢ã€‚ ä¸Šå›¾ä¸­ï¼Œè¾“å…¥é€šé“çš„ç»´åº¦ä¸º(3, 3, 3)ï¼Œ kernelçš„ç»´åº¦ä¸º(2, 1, 1), æ­¤æ—¶çš„è¾“å‡ºé€šé“ä¸º(2, 3, 3)ã€‚ å…·ä½“è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ 1234567def corr2d_multi_in_out_1_times_1(Xï¼Œ K)ï¼šc_i, h, w = X.shape # 3, 3, 3 c_o = K.shape[0] # 2X = X.reshape((c_o, h * w)) # (2, 3 * 3)K = K.reshape((c_o, c_i)) # (2, 3)Y = nd.dot(K, X) # å…¨è¿æ¥å±‚çš„çŸ©é˜µä¹˜æ³•return Y.reshape((c_o, h, w)) # (2, 3, 3) 2. PyTorchä»£ç å®ç°å¯¹ä¸èµ·æ²ç¥ï¼Œæˆ‘è¿˜æ˜¯ç”¨çš„PyTorchï¼Œè€Œä¸æ˜¯MxNetã€‚ ğŸ¤¦â€â™‚ï¸ ğŸ¤¦â€â™‚ï¸ ğŸ¤¦â€â™‚ï¸ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class CNN(nn.Module): def __init__(self, in_dim, n_class): super(CNN, self).__init__() # è¡¨ç¤ºå°†ä¸€ä¸ªæœ‰åºçš„æ¨¡å—å†™åœ¨ä¸€èµ· # ç›¸å½“äºç¥ç»ç½‘ç»œçš„å±‚æŒ‰é¡ºåºæ”¾ä¸€èµ·æ–¹ä¾¿ç»“æ„æ˜¾ç¤º self.conv = nn.Sequential( # å·ç§¯å±‚ï¼Œæœ‰äº”ä¸ªå‚æ•°ï¼š # in_channels: è¡¨ç¤ºçš„æ˜¯è¾“å…¥å·ç§¯å±‚çš„å›¾ç‰‡åšåº¦ # out_channels: æ¯”å¶å¥¥æ•°çš„æ˜¯è¦è¾“å‡ºçš„åšåº¦ # kernel_size: è¡¨ç¤ºçš„æ˜¯å·ç§¯æ ¸çš„å¤§å°ï¼Œä¸€ä¸ªæ•°å­—çš„è¯è¡¨ç¤ºé•¿å®½ç›¸ç­‰çš„å·ç§¯æ ¸ # stride: è¡¨ç¤ºå·ç§¯æ ¸æ»‘åŠ¨çš„æ­¥é•¿ # padding: è¡¨ç¤ºåœ¨å›¾ç‰‡èµ°ä½å¡«å……0çš„å¤šå°‘ï¼Œpadding=0è¡¨ç¤ºä¸å¡«å……ï¼Œpadding=1å››å‘¨éƒ½å¡«å……1ç»´ nn.Conv2d(in_dim, 6, 3, stride=1, padding=1), # æ¿€æ´»å‡½æ•°ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªå‚æ•°inplace # Falseï¼Œè¡¨ç¤ºæ–°åˆ›å»ºä¸€ä¸ªå¯¹è±¡å¯¹å…¶ä¿®æ”¹ # Trueï¼Œè¡¨ç¤ºç›´æ¥å¯¹è¿™ä¸ªå¯¹è±¡è¿›è¡Œä¿®æ”¹ nn.ReLU(True), # æœ€å¤§æ± åŒ–å±‚ï¼Œä¹Ÿæœ‰å¹³å‡æ± åŒ–å±‚ç­‰ï¼Œé‡Œé¢çš„å‚æ•°æœ‰: # kernel_size: è¡¨ç¤ºæ± åŒ–çš„çª—å£çš„å¤§å°ï¼Œå’Œå·ç§¯é‡Œé¢çš„kernel_sizeæ˜¯ä¸€æ ·çš„ # stride: ä¹Ÿå’Œå·ç§¯å±‚é‡Œé¢ä¸€æ ·ï¼Œéœ€è¦è‡ªå·±è®¾ç½®æ»‘åŠ¨æ­¥é•¿ # padding å’Œå·ç§¯å±‚ä¸€æ ·ï¼Œé»˜è®¤æ˜¯0 nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5, stride=1, padding=0), nn.ReLU(True), nn.MaxPool2d(2, 2) ) # å…¨è¿æ¥å±‚ self.fc = nn.Sequential( nn.Linear(400, 120), nn.Linear(120, 84), nn.Linear(84, n_class) ) def forward(self, x): out = self.conv(x) # batch_size out = out.view(out.size(0), -1) out = self.fc(out) return out # å®šä¹‰losså’Œoptimizercriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 # å¼€å§‹è®­ç»ƒ for epoch in range(num_epoches): print('epoch &#123;&#125;'.format(epoch + 1)) print('*' * 10) running_loss = 0.0 running_acc = 0.0 for i, data in enumerate(train_loader, 1): img, label = data if use_gpu: img = img.cuda() label = label.cuda() img = Variable(img) label = Variable(label) # å‘å‰ä¼ æ’­ out = model(img) loss = criterion(out, label) running_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() accuracy = (pred == label).float().mean() running_acc += num_correct.data[0] # å‘åä¼ æ’­ optimizer.zero_grad() loss.backward() optimizer.step() # ========================= Log ====================== step = epoch * len(train_loader) + i # (1) Log the scalar values info = &#123;'loss' : loss.data[0], 'accuracy' : accuracy.data[0]&#125; for tag, value in info.items(): logger.scalar_summary(tag, value, step) # (2) Log values and gradients of the parameters (histogram) for tag, value in model.named_parameters(): tag = tag.replace('.', '/') logger.histo_summary(tag, to_np(value), step) logger.histo_summary(tag + '/grad', to_np(value.grad), step) # (3) Log the images info = &#123;'images': to_np(img.view(-1, 28, 28)[:10])&#125; for tag, images in info.items(): logger.image_summary(tag, images, step) if i % 300 == 0: print('[&#123;&#125;/&#123;&#125;] Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, num_epoches, running_loss / (batch_size * i), running_acc / (batch_size * i))) print('Finish &#123;&#125; epoch, Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, running_loss / (len(train_dataset)), running_acc / (len( train_dataset)))) model.eval() eval_loss = 0 eval_acc = 0 for data in test_loader: img, label = data if use_gpu: img = Variable(img, volatile=True).cuda() label = Variable(label, volatile=True).cuda() else: img = Variable(img, volatile=True) label = Variable(label, volatile=True) out = model(img) loss = criterion(out, label) eval_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() eval_acc += num_correct.data[0] print('Test Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format(eval_loss / (len( test_dataset)), eval_acc / (len(test_dataset)))) print()# ä¿å­˜æ¨¡å‹torch.save(model.state_dict(), './model/cnn.pth') 3. References è«çƒ¦ - ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œ CNN (Convolutional Neural Network) ä»€ä¹ˆæ˜¯æ„Ÿå—é‡ é€šä¿—ç†è§£ã€å·ç§¯ã€â€”â€”ä»å‚…é‡Œå¶å˜æ¢åˆ°æ»¤æ³¢å™¨ ä¸€æ–‡è¯»æ‡‚å·ç§¯ç¥ç»ç½‘ç»œCNN ä¸€å¥è¯CNNï¼šå¦‚ä½•ç†è§£paddingçš„ä½œç”¨å’Œç®—æ³• CNNå…¥é—¨è®²è§£ï¼šä»€ä¹ˆæ˜¯æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.com/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.com/tags/CNN/"}]},{"title":"Macç³»ç»Ÿä¸‹matplotlibæ˜¾ç¤ºä¸­æ–‡","date":"2018-04-10T07:08:20.000Z","path":"2018/04/10/Macç³»ç»Ÿä¸‹matplotlibæ˜¾ç¤ºä¸­æ–‡/","text":"1. Macç³»ç»Ÿä¸‹è®©matplotlibæ˜¾ç¤ºä¸­æ–‡ç°åœ¨çš„æ—¶é—´æ˜¯2018.04.10ï¼Œæ›´æ–°ä¸€ä¸‹Macç³»ç»Ÿä¸‹çš„è§£å†³æ–¹æ³• æˆ‘çš„ç¯å¢ƒï¼šanaconda3 + Python 3.6.3 æ·»åŠ å­—ä½“ æ·»åŠ  SimHei å­—ä½“ï¼ˆsimhei.ttfæ–‡ä»¶ï¼‰åˆ° ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf ä¸­ï¼› ä¸‹è½½åœ°å€ï¼šé»‘ä½“å­—ä½“simhei.ttf æˆ‘ç”¨çš„æ˜¯ anaconda3 ä¸‹çš„ python ç¯å¢ƒï¼Œè¿™ä¸ªåœ°å€å¯¹åº”ä½ æ­£åœ¨ä½¿ç”¨çš„ python å®‰è£…åœ°å€ â€‹ ä¿®æ”¹matplotlibé…ç½®æ–‡ä»¶ 12cd ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-datavi matplotlibrc # ç¼–è¾‘é…ç½®æ–‡ä»¶ æ‰¾åˆ° font.sans-serif æ·»åŠ  SimHei åˆ°å­—ä½“åˆ—è¡¨ ï¼ˆå¦‚å›¾ï¼Œå¤§çº¦åœ¨211è¡Œï¼‰ åŒæ—¶ä¿®æ”¹ axes.unicode_minusï¼Œå°† True æ”¹ä¸º Falseï¼Œä½œç”¨å°±æ˜¯è§£å†³è´Ÿå·â€™-â€˜æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜ï¼ˆå¤§çº¦åœ¨330è¡Œï¼‰ åˆ é™¤ç¼“å­˜æ–‡ä»¶ Mac ç³»ç»Ÿä¸‹åˆ é™¤ ~/.matplotlib/ ä¸‹çš„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ 12rm -rf ~/.matplotlib/*.cacherm -rf ~/.matplotlib/fontList.json Linuxã€CentOS åˆ é™¤ ~/.cache/matplotlibç›®å½•ä¸‹çš„ä¸¤ä¸ªç¼“å­˜æ–‡ä»¶ï¼ˆåŒä¸Šï¼‰ æ³¨æ„ rm -rf å‘½ä»¤ï¼Œç¡®è®¤è·¯å¾„æ²¡é”™åœ¨ç”¨ â€‹ ç”»å›¾æµ‹è¯• æœªä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œéœ€è¦æ·»åŠ å¦‚ä¸‹ä»£ç ï¼š 1234567891011121314#coding:utf-8 import matplotlib #æŒ‡å®šé»˜è®¤å­—ä½“ matplotlib.rcParams['font.sans-serif'] = ['SimHei'] matplotlib.rcParams['font.family']='sans-serif' #è§£å†³è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜ matplotlib.rcParams['axes.unicode_minus'] = False from matplotlib.font_manager import _rebuild_rebuild()plt.plot([-1,2,-5,3]) plt.title(u'ä¸­æ–‡',fontproperties=myfont) plt.show() â€‹ å¦‚æœå·²ç»ä¿®æ”¹äº† matplotlib é…ç½®æ–‡ä»¶ï¼Œåˆ™ä¸éœ€è¦ä¸Šè¿°ä»£ç ï¼Œç›´æ¥ç”»å›¾å³å¯ã€‚ â€‹ 2. References å½»åº•è§£å†³matplotlibä¸­æ–‡ä¹±ç é—®é¢˜ matplotlibå›¾ä¾‹ä¸­æ–‡ä¹±ç ?","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"http://chenson.com/tags/Jupyter/"}]},{"title":"PyTorchå­¦ä¹ ç¬”è®°","date":"2018-03-22T07:52:02.000Z","path":"2018/03/22/PyTorchå­¦ä¹ ç¬”è®°/","text":"è¶ç€å…¬å¸æœ€è¿‘ä¸æ˜¯å¾ˆå¿™ï¼Œè®¤çœŸå­¦ä¹ ä¸€ä¸‹è¿™ä¸¤å¹´æ¯”è¾ƒç«çš„PyTorchï¼Œè™½ç„¶æ²¡æœ‰å®æ‰“å®çš„ã€è®¤è®¤çœŸçœŸçš„å­¦è¿‡æ·±åº¦å­¦ä¹ ï¼Œä½†ä¹Ÿç¨å¾®æ‡‚ä¸€ç‚¹ã€‚æ‰“ç®—å…ˆç®€å•å­¦ä¹ ä¸€ä¸‹åŸºæœ¬æ“ä½œï¼Œæ­ä¸ªç®€å•çš„DNNã€CNNä¹‹ç±»çš„ã€‚çœ‹çœ‹åé¢æœ‰æ²¡æœ‰æ—¶é—´ï¼Œç„¶åå†ä¸Šä¸ªå´æ©è¾¾æˆ–è€…æé£é£çš„è¯¾ç¨‹ï¼Œè¡¥å……ä¸€ä¸‹ç†è®ºçŸ¥è¯†ã€‚æ‰€ä»¥è¿™ç¯‡æ–‡ç« å°±è®°å½•ä¸€äº›ç®€å•çš„æ¦‚å¿µå’Œä¸Šæ‰‹è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„é—®é¢˜ã€‚ 1. æŸå¤±å‡½æ•°1.1 äºŒåˆ†ç±»é—®é¢˜æŸå¤±å‡½æ•°ç”¨ criterion = nn.BCELoss() æ­¤æ—¶ï¼Œyçš„ç±»å‹åº”è¯¥ä¸ºfloatç±»å‹ï¼Œ ä¸”y_predç”¨çš„æ—¶å€™éœ€è¦åœ¨è¯¥å±‚å‰é¢åŠ ä¸Š Sigmoid å‡½æ•° 123456import torch.nn.functional as Floss_fn = torch.nn.BCELoss(reduce=False, size_average=False)y_true = Variable(torch.randn(3, 4)) # é™åˆ¶äº†targetç±»å‹ä¸ºfloaty_pred = Variable(torch.FloatTensor(3, 4).random_(2))loss = loss_fn(F.sigmoid(y_pred), y_true) 1.2 å¤šåˆ†ç±»çš„é—®é¢˜ (&gt;= 2ï¼ŒäºŒåˆ†ç±»ä¸ºç‰¹æ®Šçš„å¤šåˆ†ç±»)æŸå¤±å‡½æ•°ç”¨ nn.CrossEntropyLoss, å¤šåˆ†ç±»ç”¨çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨è¿™ä¸ª loss å‰é¢ä¸éœ€è¦åŠ  Softmax å±‚ 123456789# ç±»åˆ«ä¹‹é—´çš„æ ·æœ¬å‡è¡¡é—®é¢˜weight = torch.Tensor([1, 2, 1, 1, 10])loss_fn = torch.nn.CrossEntropyLoss(reduce=False, size_average=False, weight=weight) # è€ƒè™‘åˆ°æ ·æœ¬ä¸å‡è¡¡çš„é—®é¢˜y_pred = Variable(torch.randn(3, 5)) y_true = Variable(torch.FloatTensor(3).random_(5)) # é™åˆ¶äº†targetç±»å‹ä¸ºlongloss = loss_fn(y_pred, y_true) 1.3 å…¶ä»–å¸¸è§çš„æŸå¤±å‡½æ•°2. ä¼˜åŒ–å™¨ä¼˜åŒ–å™¨çš„é€‰æ‹© 3. é‡åˆ°çš„é—®é¢˜ Learnning Rate ä¹‹å‰å› ä¸ºæ¥è§¦æ·±åº¦å­¦ä¹ æ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥å¯¹æ·±åº¦å­¦ä¹ çš„è°ƒå‚ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Œæ‰€ä»¥é‡åˆ°äº†è¿™ä¸ªå‘ã€‚ä¹‹å‰åœ¨æ­ä¸€ä¸ªæ¯”è¾ƒç®€å•çš„DNNçš„æ—¶å€™ï¼Œå‘ç°å‡½æ•°ä¸€ç›´ä¸ä¼šæ”¶æ•›ï¼Œæ‰¾äº†å¥½ä¹…å‘ç°æ˜¯Learning rateè®¾ç½®å¤ªå¤§äº†ï¼Œæ”¹å°äº†å°±æ¯”è¾ƒå¥½äº†ï¼Œä»0.05æ”¹åˆ°0.001æ•ˆæœå°±æ¯”è¾ƒå¥½ã€‚ ç„¶åæ›´æ”¹ç½‘ç»œç»“æ„çš„æ—¶å€™ï¼Œä¹Ÿæ˜¯é‡åˆ°ç±»ä¼¼çš„é—®é¢˜ï¼Œè¾“å‡ºç»å¸¸éƒ½æ˜¯ä¸ºåŒä¸€ä¸ªå€¼ï¼Œå¯ä»¥å°è¯•ä¸€ä¸‹æŠŠå­¦ä¹ ç‡å¢å¤§æˆ–è€…è°ƒå°è¯•è¯•çœ‹ï¼Œå¤šè¯•å‡ ä¸ªå€¼ã€‚ ä¼˜åŒ–å‡½æ•° æŸå¤±å‡½æ•° 4. å¯è§†åŒ–123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import torch.nn.functional as f# å»ºé€ æ•°æ®é›†data = torch.ones((100, 2))x0 = torch.normal(2 * data, 1)y0 = torch.zeros(100) # y0æ˜¯æ ‡ç­¾ shape(100,),æ˜¯ä¸€ç»´x1 = torch.normal(-2 * data, 1)y1 = torch.ones(100) # y1ä¹Ÿæ˜¯æ ‡ç­¾ shape(100,)ï¼Œæ˜¯ä¸€ç»´x = torch.cat((x0, x1), 0).type(torch.FloatTensor)y = torch.cat((y0, y1), 0).type(torch.LongTensor)x, y = Variable(x), Variable(y) # è®­ç»ƒç¥ç»ç½‘ç»œåªèƒ½æ¥å—å˜é‡è¾“å…¥ï¼Œæ•…è¦æŠŠx, yè½¬åŒ–ä¸ºå˜é‡plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], # è¿™ä¸¤ä¸ªå‚æ•°åˆ†åˆ«ä»£è¡¨x,yè½´åæ ‡ c=y.data.numpy(), s=100, cmap='RdYlGn') # cä¸ºcolor,yæœ‰ä¸¤ç§æ ‡ç­¾ï¼Œä»£è¡¨ä¸¤ç§é¢œè‰²çš„ç‚¹ï¼Œ'RdYlGn'çº¢è‰²å’Œç»¿è‰²plt.show()# å»ºé€ ç¥ç»ç½‘ç»œæ¨¡å‹class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) self.out = torch.nn.Linear(n_hidden, n_output) def forward(self, x): x = f.relu(self.hidden(x)) y = self.out(x) return y# è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹å¹¶å°†è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–net = Net(n_feature=2, n_hidden=10, n_output=2)optimizer = torch.optim.SGD(net.parameters(), lr=0.02)loss_func = torch.nn.CrossEntropyLoss()plt.ion()for i in range(100): out = net(x) loss = loss_func(out, y) optimizer.zero_grad() loss.backward() optimizer.step() # ç»˜å›¾ if i % 10 == 0: plt.cla() # torch.max(a,1) è¿”å›æ¯ä¸€è¡Œä¸­æœ€å¤§å€¼çš„é‚£ä¸ªå…ƒç´ ï¼Œä¸”è¿”å›å…¶ç´¢å¼•ï¼ˆè¿”å›æœ€å¤§å…ƒç´ åœ¨è¿™ä¸€è¡Œçš„åˆ—ç´¢å¼• # f.softmax(outï¼‰æ˜¯å°†outçš„å†…å®¹ä»¥æ¦‚ç‡è¡¨ç¤ºã€‚ # torch.max()è¿”å›çš„æ˜¯ä¸¤ä¸ªVariableï¼Œç¬¬ä¸€ä¸ªVariableå­˜çš„æ˜¯æœ€å¤§å€¼ï¼Œç¬¬äºŒä¸ªå­˜çš„æ˜¯å…¶å¯¹åº”çš„ä½ç½®ç´¢å¼•indexã€‚è¿™é‡Œæˆ‘ä»¬æƒ³è¦å¾—åˆ°çš„æ˜¯ç´¢å¼•ï¼Œæ‰€ä»¥åé¢ç”¨[1]ã€‚ prediction = torch.max(f.softmax(out), 1)[1] pred_y = prediction.data.numpy().squeeze() target_y = y.data.numpy() plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[ :, 1], c=pred_y, s=100, cmap='RdYlGn') accuracy = sum(pred_y == target_y) / len(y_train) print('&gt;&gt;&gt; accuracy :', accuracy) plt.text(1.5, -4, 'accuracy=%.2f' % accuracy, fontdict=&#123;'size': 10, 'color': 'red'&#125;) plt.pause(0.1)plt.ioff()plt.show() 5. Utils in PyTorch12345678910111213141516171819202122232425262728293031323334def _reshape(df_meta, features, label='14d', shape=(6, 4, 32)): \"\"\"å°†ä¸€ç»´çš„æ•°æ®è½¬æ¢æˆå¤šç»´çš„\"\"\" df = df_meta.reset_index(drop=True) matrix, label = [], [] for i in range(df.shape[0]): x = df.iloc[i, :][features] y = df.iloc[i, :][features] m = np.array(x).reshape(shape) matrix += [m] label += [y] return np.array(matrix), np.array(label)def data2tensor(X, y=None, x_dtype='float32', y_dtype='float32'): \"\"\"å°†è®­ç»ƒç”¨çš„Xå’Œyåˆå¹¶æˆä¸€ä¸ªTonsorDataset\"\"\" X2 = torch.from_numpy(np.array(X).astype(dtype=x_dtype)) if y is None: return TensorDataset(X2) else: y2 = torch.from_numpy(np.array(y).astype(dtype=y_dtype)) return TensorDataset(X2, y2)def df2var(df, dtype='float32'): \"\"\"æŠŠdfè½¬æˆPyTorchä¸­çš„variable\"\"\" from torch.autograd import Variable return Variable(torch.from_numpy(np.array(df).astype(dtype=dtype)))def adjust_learning_rate(optimizer, epoch): \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\" lr = args.lr * (0.1 ** (epoch // 30)) for param_group in optimizer.param_groups: param_group['lr'] = lr 6. References Pytorch Loss Function æ€»ç»“ PyTorchä¸­çš„Loss Fucntion","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://chenson.com/tags/PyTorch/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.com/tags/Deep-Learning/"}]},{"title":"Dockerå¸¸ç”¨æŒ‡ä»¤","date":"2018-03-13T05:12:33.000Z","path":"2018/03/13/Dockerå¸¸ç”¨æŒ‡ä»¤/","text":"1. dockeré•œåƒæ„å»º12cd analyze-serverdocker build -t analyze-server . 2. åˆ›å»º12345docker create analyze-server # åˆ›å»ºå®¹å™¨ï¼Œä½†å¤„äºåœæ­¢çŠ¶æ€docker run analyze-server # åˆ›å»ºå¹¶å¯åŠ¨å®¹å™¨# å¯åŠ¨å®¹å™¨ï¼Œå¹¶æŒ‚è½½æ–‡ä»¶docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server 3. æŸ¥çœ‹12docker ps # æŸ¥çœ‹å½“å‰è¿è¡Œçš„å®¹å™¨docker ps -a # æŸ¥çœ‹æ‰€æœ‰çš„å®¹å™¨ï¼ŒåŒ…æ‹¬åœæ­¢çš„ 4. å¯åŠ¨1docker start [NAME]/[CONTAINER ID] # é€šè¿‡docker startæ¥å¯åŠ¨ä¹‹å‰å·²ç»åœæ­¢çš„docker_runé•œåƒ 12docker exec -it analyze /bin/bash # åœ¨è¿è¡Œçš„å®¹å™¨ä¸­æ‰§è¡Œåé¢çš„æŒ‡ä»¤docker exec -it -u root spark-notebook /bin/bash # ä»¥rootç”¨æˆ·è¿è¡ŒæŒ‡ä»¤ äº¤äº’å‹å®¹å™¨ï¼šè¿è¡Œåœ¨å‰å°ï¼Œå®¹å™¨ä¸­ä½¿ç”¨exitå‘½ä»¤æˆ–è€…è°ƒç”¨docker stopã€docker killå‘½ä»¤ï¼Œå®¹å™¨åœæ­¢ã€‚ docker -it --name analyze -i -t -i æ‰“å¼€å®¹å™¨çš„æ ‡å‡†è¾“å…¥ -t å‘Šè¯‰dockerä¸ºå®¹å™¨åˆ›å»ºä¸€ä¸ªå‘½ä»¤è¡Œç»ˆç«¯ --name analyze æŒ‡å®šå®¹å™¨åç§°ï¼Œå¯ä»¥ä¸å¡«(éšæœº) analyze-server å‘Šè¯‰æˆ‘ä»¬ä½¿ç”¨ä»€ä¹ˆé•œåƒæ¥å¯åŠ¨å®¹å™¨ /bin/bash å‘Šè¯‰dockerè¦åœ¨å®¹å™¨é‡Œé¢æ‰§è¡Œæ­¤å‘½ä»¤ åå°å‹å®¹å™¨ï¼šè¿è¡Œåœ¨åå°ï¼Œåˆ›å»ºåä¸ç»ˆç«¯æ— å…³ï¼Œåªæœ‰è°ƒç”¨docker stopã€docker killå‘½ä»¤æ‰èƒ½ä½¿å®¹å™¨åœæ­¢ã€‚ 1docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server -d ä½¿å®¹å™¨åœ¨åå°è¿è¡Œã€‚ -c å¯ä»¥è°ƒæ•´å®¹å™¨çš„CPUä¼˜å…ˆçº§ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„å®¹å™¨æ‹¥æœ‰ç›¸åŒçš„CPUä¼˜å…ˆçº§å’ŒCPUè°ƒåº¦å‘¨æœŸï¼Œä½†ä½ å¯ä»¥é€šè¿‡Dockeræ¥é€šçŸ¥å†…æ ¸ç»™äºˆæŸä¸ªæˆ–æŸå‡ ä¸ªå®¹å™¨æ›´å¤šçš„CPUè®¡ç®—å‘¨æœŸã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬ä½¿ç”¨-cæˆ–è€…â€“cpu-shares =0å¯åŠ¨äº†C0ã€C1ã€C2ä¸‰ä¸ªå®¹å™¨ï¼Œä½¿ç”¨-c/â€“cpu-shares=512å¯åŠ¨äº†C3å®¹å™¨ã€‚è¿™æ—¶ï¼ŒC0ã€C1ã€C2å¯ä»¥100%çš„ä½¿ç”¨CPUèµ„æºï¼ˆ1024ï¼‰ï¼Œä½†C3åªèƒ½ä½¿ç”¨50%çš„CPUèµ„æºï¼ˆ512ï¼‰ã€‚å¦‚æœè¿™ä¸ªä¸»æœºçš„æ“ä½œç³»ç»Ÿæ˜¯æ—¶åºè°ƒåº¦ç±»å‹çš„ï¼Œæ¯ä¸ªCPUæ—¶é—´ç‰‡æ˜¯100å¾®ç§’ï¼Œé‚£ä¹ˆC0ã€C1ã€C2å°†å®Œå…¨ä½¿ç”¨æ‰è¿™100å¾®ç§’ï¼Œè€ŒC3åªèƒ½ä½¿ç”¨50å¾®ç§’ã€‚ -c åçš„å‘½ä»¤æ˜¯å¾ªç¯ï¼Œä»è€Œä¿æŒå®¹å™¨çš„è¿è¡Œ 5. åœæ­¢12docker stop [NAME]/[CONTAINER ID] # é€€å‡ºå®¹å™¨docker kill [NAME]/[CONTAINER ID] # å¼ºåˆ¶åœæ­¢ä¸€ä¸ªå®¹å™¨ 6. åˆ é™¤12345docker rm [NAME]/[CONTAINER ID] # ä¸èƒ½å¤Ÿåˆ é™¤ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„å®¹å™¨ï¼Œä¼šæŠ¥é”™ã€‚éœ€è¦å…ˆåœæ­¢å®¹å™¨docker rm 'docker ps -a -q' # ä¸€æ¬¡æ€§åˆ é™¤æ‰€æœ‰å®¹å™¨ # -aæ ‡å¿—åˆ—å‡ºæ‰€æœ‰å®¹å™¨ï¼Œ # -qæ ‡å¿—åªåˆ—å‡ºå®¹å™¨çš„IDï¼Œç„¶åä¼ é€’ç»™rmå‘½ä»¤ï¼Œä¾æ¬¡åˆ é™¤å®¹å™¨ 7. References Docker ç®€æ˜“æ•™ç¨‹ Docker å‘½ä»¤å¤§å…¨","tags":[{"name":"docker","slug":"docker","permalink":"http://chenson.com/tags/docker/"}]},{"title":"Hadoop2.7.4å®Œå…¨åˆ†å¸ƒå¼é›†ç¾¤æ­å»ºå’Œæµ‹è¯•","date":"2017-10-10T05:24:58.000Z","path":"2017/10/10/Hadoop2-7-4å®Œå…¨åˆ†å¸ƒå¼é›†ç¾¤æ­å»ºå’Œæµ‹è¯•/","text":"1. ç¯å¢ƒé…ç½®1.1 ç¯å¢ƒè¯´æ˜ 1.2 ä¿®æ”¹æœºå™¨åç§°å’Œhostsç­‰ vi /etc/sysconfig/network 1234HOSTNAME=hadoop-masterHOSTNAME=hadoop-salve1HOSTNAME=hadoop-salve2HOSTNAME=hadoop-salve3 æ‰§è¡Œ reboot åç”Ÿæ•ˆ sudo vi /etc/hostname 1234# ç›¸åº”ä¿®æ”¹ä¸‰å°æœºå™¨hadoop-masterhadoop-slave1hadoop-slave2 sudo vi /etc/hosts 1234567127.0.0.1 localhost localhost.localdomain VM-0-6-ubuntu# Hadoop Cluster# ã€æ³¨æ„ã€‘ï¼šç”¨å†…ç½‘IPï¼Œè‹¥ç”¨å…¬ç½‘IPï¼Œåˆ™æ— æ³•å¯åŠ¨masterä¸Šçš„9000ç›‘å¬ç«¯å£172.17.6 hadoop-master172.17.11 hadoop-salve1 172.17.7 hadoop-salve2 1.5. SSHæ— å¯†ç éªŒè¯é…ç½® å®‰è£… ssh 123sudo apt-get install openssh-serverps -e | grep &quot;ssh&quot;ssh localhost ç”Ÿæˆå¯†é’¥ pair 12345678910# æŸ¥çœ‹æƒé™ls -aldrwxr-x--x 2 root root 4096 Dec 23 2015 .ssh# ç»™ç”¨æˆ·æƒé™# sudo chown ubuntu .sshchmod 700 .ssh # ç”Ÿæˆå¯†é’¥ssh-keygen -t rsa åœ¨ master ä¸Šå¯¼å…¥ authorized_keys 123456789# é‡è¦sudo chmod 700 .ssh sudo chmod 640 .ssh/authorized_keyssudo chown $USER .sshsudo chown $USER .ssh/authorized_keyscat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# åŒæ—¶ç»™å…¶ä»–çš„ salver åœ¨å„ä¸ªæœºå™¨éªŒè¯ä¸€ä¸‹ 2. æ‰€éœ€è½¯ä»¶2.1 JDKè½¯ä»¶tutorial 1 å®‰è£… JDK 123456sudo add-apt-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installersudo apt-get install openjdk-8-jdksudo apt-get install openjdk-8-jre é…ç½®ç¯å¢ƒå˜é‡ vi /etc/profile 123456789101112131415# JAVA# 1. AWS EC2 Ubuntu 16export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64# 2. è…¾è®¯äº‘ Ubuntu 14export JAVA_HOME=/usr/lib/jvm/java-8-oracle# 3. MAC OSexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Homeexport JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar æ‰§è¡Œå‘½ä»¤ä½¿ä¹‹ç”Ÿæ•ˆ source /etc/profile 2.2 Hadoop è½¯ä»¶ ä¸‹è½½è½¯ä»¶ 123wget http://apache.uberglobalmirror.com/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gztar xvf hadoop-2.7.4.tar.gz é…ç½® vi ~/.bashrc è®¾ç½® Hadoop çš„ç¯å¢ƒå˜é‡ 1234export HADOOP_HOMEexport PATHexport HADOOP_CONF_DIRexport YARN_CONF_DIR 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# ~/.bashrc: executed by bash(1) for non-login shells.# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)# for examples# If not running interactively, don't do anythingcase $- in *i*) ;; *) return;;esac# don't put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don't overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern \"**\" used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)[ -x /usr/bin/lesspipe ] &amp;&amp; eval \"$(SHELL=/bin/sh lesspipe)\"# set variable identifying the chroot you work in (used in the prompt below)if [ -z \"$&#123;debian_chroot:-&#125;\" ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we \"want\" color)case \"$TERM\" in xterm-color|*-256color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the prompt#force_color_prompt=yesif [ -n \"$force_color_prompt\" ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it's compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ \"$color_prompt\" = yes ]; then PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ 'else PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h:\\w\\$ 'fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase \"$TERM\" inxterm*|rxvt*) PS1=\"\\[\\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h: \\w\\a\\]$PS1\" ;;*) ;;esac# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' alias grep='grep --color=auto' alias fgrep='fgrep --color=auto' alias egrep='egrep --color=auto'fi# colored GCC warnings and errors#export GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'# some more ls aliasesalias ll='ls -alF'alias la='ls -A'alias l='ls -CF'# Add an \"alert\" alias for long running commands. Use like so:# sleep 10; alertalias alert='notify-send --urgency=low -i \"$([ $? = 0 ] &amp;&amp; echo terminal || echo error)\" \"$(history|tail -n1|sed -e '\\''s/^\\s*[0-9]\\+\\s*//;s/[;&amp;|]\\s*alert$//'\\'')\"'# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don't need to enable# this, if it's already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi# HADOOPexport HADOOP_HOME=/home/ubuntu/workdir/hadoop-2.7.4export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=$CLASSPATH:$HADOOP_HOME/bin# JAVAexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib# LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/home/ubuntu/workdir/hadoop-2.7.4/lib/native 1export CLASSPATH=$CLASSPATH:/home/ubuntu/workdir/hadoop-2.7.4/etc/hadoop:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/contrib/capacity-scheduler/*.jar åœ¨ $HADOOP_CONF_DIR ä¸­è®¾ç½® JAVA_HOME 123vi /home/hadoop-2.7.4/etc/hadoop/hadoop-env.sh è®¾ç½®JAVA_HOMEvi /home/hadoop-2.7.4/etc/hadoop/mapred-env.sh è®¾ç½®JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 vi slaves è¿™é‡Œé¢å¡«å†™çš„å…¨æ˜¯slaves 12hadoop-slave1hadoop-slave2 vi $HADOOP_CONF_DIR/core-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!-- æ³¨æ„è¿™é‡Œçš„åŒºåˆ« ä¸€ä¸ªé€‚ç”¨äºå•æœº ä¸€ä¸ªé€‚ç”¨äºé›†ç¾¤ --&gt; &lt;!-- å•æœº --&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;!-- é›†ç¾¤ master --&gt; &lt;value&gt;hdfs://hadoop-master:9000&lt;/value&gt; &lt;description&gt;è®¾å®šnamenodeçš„ä¸»æœºååŠç«¯å£(å»ºè®®ä¸è¦æ›´æ”¹ç«¯å£å·)&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;description&gt; è®¾ç½®ç¼“å­˜å¤§å° &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop-2.7.4/tmp&lt;/value&gt; &lt;description&gt; å­˜æ”¾ä¸´æ—¶æ–‡ä»¶çš„ç›®å½• &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/hdfs-site.xml 1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/name&lt;/value&gt; &lt;description&gt; namenode ç”¨æ¥æŒç»­å­˜æ”¾å‘½åç©ºé—´å’Œäº¤æ¢æ—¥å¿—çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/data&lt;/value&gt; &lt;description&gt; DataNode åœ¨æœ¬åœ°å­˜æ”¾å—æ–‡ä»¶çš„ç›®å½•åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš” &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt; è®¾å®š HDFS å­˜å‚¨æ–‡ä»¶çš„å‰¯æœ¬ä¸ªæ•°ï¼Œé»˜è®¤ä¸º3 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; vi $HADOOP_CONF_DIR/mapred-site.xml (å¤åˆ¶mapred-site.xml.template,å†ä¿®æ”¹æ–‡ä»¶å) 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;hadoop-master:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop-master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://hadoop-master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;!-- master node çš„åå­— --&gt; &lt;!-- å•æœºå’Œé›†ç¾¤çš„åŒºåˆ«ï¼Ÿï¼Ÿ --&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop-master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop-master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;hadoop-master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop-master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345dfs.nameservices â€”â€“ HDFS NNçš„é€»è¾‘åç§°ï¼Œä½¿ç”¨ä¸Šé¢è®¾ç½®çš„myhdfsdfs.ha.namenodes.myhdfs â€”â€“ ç»™å®šæœåŠ¡é€»è¾‘åç§°myhdfsçš„èŠ‚ç‚¹åˆ—è¡¨dfs.namenode.rpc-address.myhdfs.nn1 â€”â€“ myhdfsä¸­nn1èŠ‚ç‚¹å¯¹å¤–æœåŠ¡çš„RPCåœ°å€dfs.namenode.http-address.myhdfs.nn1 â€”â€“ myhdfsä¸­nn1èŠ‚ç‚¹å¯¹å¤–æœåŠ¡çš„httpåœ°å€dfs.namenode.shared.edits.dir â€”â€“ è®¾ç½®ä¸€ç»„ journalNode çš„ URI åœ°å€ï¼Œactive NN å°† edit log å†™å…¥è¿™äº› 12345åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•/home/hadoop-2.7.4/hdfs ç”¨æ¥å­˜æ”¾é›†ç¾¤æ•°æ®ã€‚åœ¨ä¸»èŠ‚ç‚¹nodeä¸Šåˆ›å»ºç›®å½•/home/hadoop-2.7.4/hdfs/name ç”¨æ¥å­˜æ”¾æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®ã€‚åœ¨æ¯ä¸ªä»èŠ‚ç‚¹ä¸Šåˆ›å»ºç›®å½•/home/hadoop-2.7.4/hdfs/data ç”¨æ¥å­˜æ”¾çœŸæ­£çš„æ•°æ®ã€‚æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„æ—¥å¿—ç›®å½•ä¸º/home/hadoop-2.7.4/logsæ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„ä¸´æ—¶ç›®å½•ä¸º/home/hadoop-2.7.4/tmp 1ä¸Šé¢çš„é…ç½®åªéœ€è¦åœ¨masterä¸­é…å¥½, ç„¶åå¤åˆ¶åˆ°å…¶ä»–çš„slavesèŠ‚ç‚¹ä¸­ä¸­å» æ ¼å¼åŒ– namenode å’Œ datanodeï¼ˆåœ¨ master ä¸Šæ‰§è¡Œå°±å¯ä»¥äº† ä¸éœ€è¦åœ¨ slaves ä¸Šæ‰§è¡Œï¼‰ 12hdfs namenode -formathdfs datanode -format åˆ†åˆ«åœ¨ master å’Œ slaves ä¸­ç”¨ jps æŸ¥çœ‹è¿›ç¨‹","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"}]},{"title":"åŒæ­¥å¼‚æ­¥IO","date":"2017-09-29T03:54:40.000Z","path":"2017/09/29/åŒæ­¥å¼‚æ­¥IO/","text":"1. å¤§è‡´åŒºåˆ«æ‰€è°“çš„åŒæ­¥å’Œå¼‚æ­¥çš„åŒºåˆ«ï¼ŒæŒ‡çš„æ˜¯Applicationå’Œkernelä¹‹é—´çš„äº¤äº’æ–¹å¼ã€‚ å¦‚æœApplicationä¸éœ€è¦ç­‰å¾…kernelçš„å›åº”ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯å¼‚æ­¥çš„ï¼ˆkernelä¼šå‘ä¿¡å·é€šçŸ¥ï¼‰ã€‚ å¦‚æœApplicationæäº¤å®ŒIOè¯·æ±‚åï¼Œéœ€è¦ç­‰å¾…â€œå›æ‰§â€ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯åŒæ­¥çš„ã€‚ è€Œé˜»å¡å’Œéé˜»å¡ï¼ŒæŒ‡çš„æ˜¯Applicationæ˜¯å¦ç­‰å¾…IOæ“ä½œçš„å®Œæˆã€‚ å¦‚æœApplicationå¿…é¡»ç­‰åˆ°IOæ“ä½œå®é™…å®Œæˆä»¥åå†æ‰§è¡Œæ¥ä¸‹é¢çš„æ“ä½œï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯é˜»å¡çš„ã€‚ï¼ˆä»å‘å‡ºä¿¡å·å¼€å§‹ï¼Œä¼šä¸€ç›´blockå¯¹åº”çš„è¿›ç¨‹çŸ¥é“æ“ä½œå®Œæˆï¼Œï¼‰ åä¹‹ï¼Œå¦‚æœä¸éœ€è¦ç­‰å¾…IOæ“ä½œçš„å®Œæˆå°±å¼€å§‹æ‰§è¡Œå…¶ä»–çš„æ“ä½œï¼Œé‚£ä¹ˆå°±æ˜¯éé˜»å¡çš„ã€‚ï¼ˆåœ¨kernelè¿˜å‡†å¤‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°±ç«‹åˆ»è¿”å›äº†ã€‚ï¼‰ å«è½¦ä¹‹åï¼Œå°±ä¸€ç›´åœ¨è·¯å£ç­‰ç€ï¼Œè½¦æ¥äº†è‡ªå·±ä¸Šå» â€”â€”åŒæ­¥ï¼Œé˜»å¡ å«è½¦ä¹‹åï¼Œä¸€è¾¹ç­‰ç€ä¸€è¾¹çœ‹ç¾å¥³ï¼Œè½¦æ¥äº†è‡ªå·±ä¸Šå» â€”â€”åŒæ­¥ï¼Œéé˜»å¡ å«è½¦ä¹‹åï¼Œå…‰é¡¾ç€çœ‹ç¾å¥³ï¼Œå¸æœºåˆ°äº†ä¹‹åæ‰“ç”µè¯ç»™ä½  â€”â€”å¼‚æ­¥ï¼Œéé˜»å¡ å³æ˜¯ï¼šåŒæ­¥å°±æ˜¯ä½ è¦è‡ªå·±æ£€æŸ¥è½¦æ¥äº†æ²¡æœ‰ï¼›å¼‚æ­¥å°±æ˜¯è½¦æ¥äº†å¸æœºè”ç³»ä½ ã€‚é˜»å¡å°±æ˜¯ç­‰è½¦çš„æ—¶å€™è€å®ç­‰ç€ï¼Œåˆ«å¹²åˆ«çš„ï¼ˆè¢«é˜»å¡ï¼‰ï¼›éé˜»å¡å°±æ˜¯ç­‰è½¦çš„æ—¶å€™ä½ å¯ä»¥åšå…¶ä»–äº‹æƒ…ã€‚ 2. Blocking IO 3. Nonblocking IO 4. IO multiplexing / Event driven IOä½¿ç”¨select å’Œepollï¼Œå•ä¸ªprocesså¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªç½‘ç»œè¿æ¥çš„IOã€‚åŸºæœ¬åŸç†å°±æ˜¯select/epolè¿™ä¸ªfunctionä¼šä¸æ–­çš„è½®è¯¢æ‰€è´Ÿè´£çš„æ‰€æœ‰çš„socektï¼Œå½“æŸä¸ªsocketæœ‰æ•°æ®åˆ°è¾¾äº†ï¼Œå°±é€šçŸ¥ç”¨æˆ·è¿›ç¨‹ã€‚å®ƒçš„æµç¨‹å¦‚ä¸‹å›¾ï¼š è¿™ä¸ªå›¾å’Œblocking IOçš„å›¾å…¶å®å¹¶æ²¡æœ‰å¤ªå¤§çš„ä¸åŒï¼Œäº‹å®ä¸Šï¼Œè¿˜æ›´å·®ä¸€äº›ã€‚å› ä¸ºè¿™é‡Œéœ€è¦ä½¿ç”¨ä¸¤ä¸ªsystem call (select å’Œ recvfrom)ï¼Œè€Œblocking IOåªè°ƒç”¨äº†ä¸€ä¸ªsystem call (recvfrom)ã€‚ä½†æ˜¯ï¼Œç”¨selectçš„ä¼˜åŠ¿åœ¨äºå®ƒå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªconnectionã€‚ï¼ˆå¤šè¯´ä¸€å¥ã€‚æ‰€ä»¥ï¼Œå¦‚æœå¤„ç†çš„è¿æ¥æ•°ä¸æ˜¯å¾ˆé«˜çš„è¯ï¼Œä½¿ç”¨select/epollçš„web serverä¸ä¸€å®šæ¯”ä½¿ç”¨multi-threading + blocking IOçš„web serveræ€§èƒ½æ›´å¥½ï¼Œå¯èƒ½å»¶è¿Ÿè¿˜æ›´å¤§ã€‚select/epollçš„ä¼˜åŠ¿å¹¶ä¸æ˜¯å¯¹äºå•ä¸ªè¿æ¥èƒ½å¤„ç†å¾—æ›´å¿«ï¼Œè€Œæ˜¯åœ¨äºèƒ½å¤„ç†æ›´å¤šçš„è¿æ¥ã€‚ï¼‰åœ¨IO multiplexing Modelä¸­ï¼Œå®é™…ä¸­ï¼Œå¯¹äºæ¯ä¸€ä¸ªsocketï¼Œä¸€èˆ¬éƒ½è®¾ç½®æˆä¸ºnon-blockingï¼Œä½†æ˜¯ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæ•´ä¸ªç”¨æˆ·çš„processå…¶å®æ˜¯ä¸€ç›´è¢«blockçš„ã€‚åªä¸è¿‡processæ˜¯è¢«selectè¿™ä¸ªå‡½æ•°blockï¼Œè€Œä¸æ˜¯è¢«socket IOç»™blockã€‚ 5. Asynchronous IO ç”¨æˆ·è¿›ç¨‹å‘èµ·readæ“ä½œä¹‹åï¼Œå°±å¯ä»¥ç«‹åˆ»åšå…¶ä»–çš„äº‹æƒ…ã€‚è€Œå¦ä¸€æ–¹é¢ï¼Œä»kernelçš„è§’åº¦ï¼Œå½“å®ƒæ”¶åˆ°ä¸€ä¸ªasynchronous readä¹‹åï¼Œé¦–å…ˆå®ƒå°±ä¼šç«‹åˆ»è¿”å›ä¸€ä¸ªä¿¡å·ï¼Œæ‰€ä»¥ä¸ä¼šå¯¹ç”¨æˆ·è¿›ç¨‹äº§ç”Ÿä»»ä½•çš„blockã€‚ ç„¶åï¼Œkernelä¼šç­‰å¾…æ•°æ®å‡†å¤‡æ™šé¤ï¼Œå°†æ•°æ®æ‹·è´åˆ°ç”¨æˆ·å†…å­˜ï¼Œå½“è¿™ä¸€åˆ‡éƒ½å®Œæˆåï¼Œkernelä¼šç»™ç”¨æˆ·è¿›ç¨‹å‘é€ä¸€ä¸ªsignalï¼Œå‘Šè¯‰å®ƒreadæ“ä½œå·²ç»å®Œæˆäº†ã€‚","tags":[]},{"title":"å¤šçº¿ç¨‹ä¸å¤šè¿›ç¨‹ç®€å•ç†è§£","date":"2017-09-22T06:28:08.000Z","path":"2017/09/22/å¤šçº¿ç¨‹ä¸å¤šè¿›ç¨‹ç®€å•ç†è§£/","text":"1. å¤šè¿›ç¨‹ç›´è§‚æ¥çœ‹ï¼Œå°±æ˜¯ä¸€ä¸ªä¸ªpidï¼Œè¿›ç¨‹æ˜¯ç¨‹åºåœ¨è®¡ç®—æœºä¸Šçš„ä¸€æ¬¡æ‰§è¡Œæ´»åŠ¨ã€‚ åˆ›å»ºå­è¿›ç¨‹çš„è°ƒç”¨æ˜¯ fork() fork() çš„åŠŸèƒ½å°±æ˜¯äº§ç”Ÿå­è¿›ç¨‹ï¼Œè°ƒç”¨ä¸€æ¬¡ï¼Œè¿”å›ä¸¤æ¬¡ã€‚ ä¸€æ¬¡è¿”å›0ï¼Œä¸€æ¬¡è¿”å›å­è¿›ç¨‹çš„pid å­è¿›ç¨‹æ°¸è¿œè¿”å› 0ï¼Œè€Œçˆ¶è¿›ç¨‹è¿”å›å­è¿›ç¨‹çš„IDã€‚è¿™æ ·åšçš„ç†ç”±æ˜¯ï¼Œä¸€ä¸ªçˆ¶è¿›ç¨‹å¯ä»¥forkå‡ºå¾ˆå¤šå­è¿›ç¨‹ï¼Œæ‰€ä»¥ï¼Œçˆ¶è¿›ç¨‹è¦è®°ä¸‹æ¯ä¸ªå­è¿›ç¨‹çš„IDï¼Œè€Œå­è¿›ç¨‹åªéœ€è¦è°ƒç”¨ getppid() å°±å¯ä»¥æ‹¿åˆ°çˆ¶è¿›ç¨‹çš„ID å¤±è´¥è¿”å›-1 12345678910111213141516171819202122232425#include &lt;unistd.h&gt; #include &lt;sys/types.h&gt; #include &lt;stdio.h&gt; void print_exit() &#123; printf(\"the exit pid:%d/n\",getpid() ); &#125; main()&#123; pit_t, pid; atexit(print_exit); //æ³¨å†Œè¯¥è¿›ç¨‹é€€å‡ºæ—¶çš„å›è°ƒå‡½æ•° pid = fork(); if (pid &lt; 0) printf(\"error in fork!\"); else if (pid == 0) // å­è¿›ç¨‹ printf(\"i am the child process, my process id is %d/n\", getpid()); else // çˆ¶è¿›ç¨‹ &#123; printf(\"i am the parent process, my process id is %d/n\", getpid()); sleep(2); wait(); &#125; &#125; 2. å¤šçº¿ç¨‹è¿›ç¨‹æ˜¯ç”±è‹¥å¹²çº¿ç¨‹ç»„æˆçš„ï¼Œä¸€ä¸ªè¿›ç¨‹è‡³å°‘æœ‰ä¸€ä¸ªçº¿ç¨‹ã€‚ çº¿ç¨‹å°±æ˜¯æŠŠä¸€ä¸ªè¿›ç¨‹åˆ†æˆå¾ˆå¤šç‰‡ï¼Œæ¯ä¸€ç‰‡éƒ½æ˜¯å¯ä»¥ç‹¬ç«‹çš„æµç¨‹ linuxæä¾›çš„å¤šçº¿ç¨‹çš„ç³»ç»Ÿè°ƒç”¨ï¼š 123456int pthread_create(pthread_t *restrict tidp, const pthread_attr_t *restrict attr, void *(*start_rtn)(void), void *restrict arg);Returns: 0 if OK, error number on failure ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæŒ‡å‘çº¿ç¨‹æ ‡è¯†ç¬¦çš„æŒ‡é’ˆã€‚ç¬¬äºŒä¸ªå‚æ•°ç”¨æ¥è®¾ç½®çº¿ç¨‹å±æ€§ã€‚ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯çº¿ç¨‹è¿è¡Œå‡½æ•°çš„èµ·å§‹åœ°å€ã€‚æœ€åä¸€ä¸ªå‚æ•°æ˜¯è¿è¡Œå‡½æ•°çš„å‚æ•°ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include&lt;stdio.h&gt;#include&lt;string.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;pthread.h&gt; void* task1(void*);void* task2(void*);void usr();int p1,p2;int main()&#123; usr(); getchar(); return 1;&#125; void usr()&#123; pthread_t pid1, pid2; pthread_attr_t attr; void *p; int ret=0; pthread_attr_init(&amp;attr); // åˆå§‹åŒ–çº¿ç¨‹å±æ€§ç»“æ„ pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_DETACHED); // è®¾ç½®attrç»“æ„ä¸ºåˆ†ç¦» pthread_create(&amp;pid1, &amp;attr, task1, NULL); // åˆ›å»ºçº¿ç¨‹ï¼Œè¿”å›çº¿ç¨‹å·ç»™pid1,çº¿ç¨‹å±æ€§è®¾ç½®ä¸ºattrçš„å±æ€§ï¼Œçº¿ç¨‹å‡½æ•°å…¥å£ä¸ºtask1ï¼Œå‚æ•°ä¸ºNULL pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE); pthread_create(&amp;pid2, &amp;attr, task2, NULL); // å‰å°å·¥ä½œ ret=pthread_join(pid2, &amp;p); // ç­‰å¾…pid2è¿”å›ï¼Œè¿”å›å€¼èµ‹ç»™p printf(\"after pthread2:ret=%d,p=%d/n\", ret,(int)p); &#125;void* task1(void *arg1)&#123; printf(\"task1/n\"); // è‰°è‹¦è€Œæ— æ³•é¢„æ–™çš„å·¥ä½œï¼Œè®¾ç½®ä¸ºåˆ†ç¦»çº¿ç¨‹ï¼Œä»»å…¶è‡ªç”Ÿè‡ªç­ pthread_exit( (void *)1);&#125;void* task2(void *arg2)&#123; int i=0; printf(\"thread2 begin./n\"); // ç»§ç»­é€å¤–å–çš„å·¥ä½œ pthread_exit((void *)2);&#125; 3. çº¿ç¨‹ VS è¿›ç¨‹ è¿›ç¨‹å¼€é”€å¤§ï¼Œçº¿ç¨‹å¼€é”€å°ã€‚ çº¿ç¨‹å®‰å…¨ï¼šæ¦‚å¿µæ¯”è¾ƒç›´è§‚ã€‚ä¸€èˆ¬è¯´æ¥ï¼Œä¸€ä¸ªå‡½æ•°è¢«ç§°ä¸ºçº¿ç¨‹å®‰å…¨çš„ï¼Œå½“ä¸”ä»…å½“è¢«å¤šä¸ªå¹¶å‘çº¿ç¨‹åå¤è°ƒç”¨æ—¶ï¼Œå®ƒä¼šä¸€ç›´äº§ç”Ÿæ­£ç¡®çš„ç»“æœã€‚ çº¿ç¨‹å®‰å…¨çš„æ¡ä»¶ï¼šä¸»è¦éœ€è¦è€ƒè™‘çš„æ˜¯çº¿ç¨‹ä¹‹é—´çš„å…±äº«å˜é‡ã€‚å±äºåŒä¸€è¿›ç¨‹çš„ä¸åŒçº¿ç¨‹ä¼šå…±äº«è¿›ç¨‹å†…å­˜ç©ºé—´ä¸­çš„å…¨å±€åŒºå’Œå †ï¼Œè€Œç§æœ‰çš„çº¿ç¨‹ç©ºé—´åˆ™ä¸»è¦åŒ…æ‹¬æ ˆå’Œå¯„å­˜å™¨ã€‚å› æ­¤ï¼Œå¯¹äºåŒä¸€è¿›ç¨‹çš„ä¸åŒçº¿ç¨‹æ¥è¯´ï¼Œæ¯ä¸ªçº¿ç¨‹çš„å±€éƒ¨å˜é‡éƒ½æ˜¯ç§æœ‰çš„ï¼Œè€Œå…¨å±€å˜é‡ã€å±€éƒ¨é™æ€å˜é‡ã€åˆ†é…äºå †çš„å˜é‡éƒ½æ˜¯å…±äº«çš„ã€‚åœ¨å¯¹è¿™äº›å…±äº«å˜é‡è¿›è¡Œè®¿é—®æ—¶ï¼Œå¦‚æœè¦ä¿è¯çº¿ç¨‹å®‰å…¨ï¼Œåˆ™å¿…é¡»é€šè¿‡åŠ é”çš„æ–¹å¼ å¤šè¿›ç¨‹æ¨¡å¼æœ€å¤§çš„ä¼˜ç‚¹å°±æ˜¯ç¨³å®šæ€§é«˜ï¼Œå› ä¸ºä¸€ä¸ªå­è¿›ç¨‹å´©æºƒäº†ï¼Œä¸ä¼šå½±å“ä¸»è¿›ç¨‹å’Œå…¶ä»–å­è¿›ç¨‹ã€‚ å¤šè¿›ç¨‹æ¨¡å¼çš„ç¼ºç‚¹æ˜¯åˆ›å»ºè¿›ç¨‹çš„ä»£ä»·å¤§ï¼Œåœ¨Unix/Linuxç³»ç»Ÿä¸‹ï¼Œç”¨forkè°ƒç”¨è¿˜è¡Œï¼Œåœ¨Windowsä¸‹åˆ›å»ºè¿›ç¨‹å¼€é”€å·¨å¤§ã€‚å¦å¤–ï¼Œæ“ä½œç³»ç»Ÿèƒ½åŒæ—¶è¿è¡Œçš„è¿›ç¨‹æ•°ä¹Ÿæ˜¯æœ‰é™çš„ï¼Œåœ¨å†…å­˜å’ŒCPUçš„é™åˆ¶ä¸‹ï¼Œå¦‚æœæœ‰å‡ åƒä¸ªè¿›ç¨‹åŒæ—¶è¿è¡Œï¼Œæ“ä½œç³»ç»Ÿè¿è°ƒåº¦éƒ½ä¼šæˆé—®é¢˜ å¤šçº¿ç¨‹æ¨¡å¼é€šå¸¸æ¯”å¤šè¿›ç¨‹å¿«ä¸€ç‚¹ï¼Œä½†æ˜¯ä¹Ÿå¿«ä¸åˆ°å“ªå»ï¼Œè€Œä¸”ï¼Œå¤šçº¿ç¨‹æ¨¡å¼è‡´å‘½çš„ç¼ºç‚¹å°±æ˜¯ä»»ä½•ä¸€ä¸ªçº¿ç¨‹æŒ‚æ‰éƒ½å¯èƒ½ç›´æ¥é€ æˆæ•´ä¸ªè¿›ç¨‹å´©æºƒï¼Œå› ä¸ºæ‰€æœ‰çº¿ç¨‹å…±äº«è¿›ç¨‹çš„å†…å­˜ã€‚åœ¨Windowsä¸Šï¼Œå¦‚æœä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œçš„ä»£ç å‡ºäº†é—®é¢˜ï¼Œä½ ç»å¸¸å¯ä»¥çœ‹åˆ°è¿™æ ·çš„æç¤ºï¼šâ€œè¯¥ç¨‹åºæ‰§è¡Œäº†éæ³•æ“ä½œï¼Œå³å°†å…³é—­â€ï¼Œå…¶å®å¾€å¾€æ˜¯æŸä¸ªçº¿ç¨‹å‡ºäº†é—®é¢˜ï¼Œä½†æ˜¯æ“ä½œç³»ç»Ÿä¼šå¼ºåˆ¶ç»“æŸæ•´ä¸ªè¿›ç¨‹ 4. è¿›ç¨‹é—´é€šä¿¡4.1 å•æœºå¤šè¿›ç¨‹é€šä¿¡IPCï¼ˆInter Process Communicationï¼‰åŒ…æ‹¬ï¼šç®¡é“ã€æ–‡ä»¶ã€å’Œæ¶ˆæ¯ä¼ é€’ ç®¡é“ï¼ˆPipeï¼‰åŠæœ‰åç®¡é“ï¼ˆnamed pipeï¼‰ï¼šç®¡é“å¯ç”¨äºå…·æœ‰äº²ç¼˜å…³ç³»è¿›ç¨‹é—´çš„é€šä¿¡ï¼Œæœ‰åç®¡é“å…‹æœäº†ç®¡é“æ²¡æœ‰åå­—çš„é™åˆ¶ï¼Œå› æ­¤ï¼Œé™¤å…·æœ‰ç®¡é“æ‰€å…·æœ‰çš„åŠŸèƒ½å¤–ï¼Œå®ƒè¿˜å…è®¸æ— äº²ç¼˜å…³ç³»è¿›ç¨‹é—´çš„é€šä¿¡ï¼› ä¿¡å·ï¼ˆSignalï¼‰ï¼šä¿¡å·æ˜¯æ¯”è¾ƒå¤æ‚çš„é€šä¿¡æ–¹å¼ï¼Œç”¨äºé€šçŸ¥æ¥å—è¿›ç¨‹æœ‰æŸç§äº‹ä»¶å‘ç”Ÿï¼Œé™¤äº†ç”¨äºè¿›ç¨‹é—´é€šä¿¡å¤–ï¼Œè¿›ç¨‹è¿˜å¯ä»¥å‘é€ä¿¡å·ç»™è¿›ç¨‹æœ¬èº«ï¼›linuxé™¤äº†æ”¯æŒUnixæ—©æœŸä¿¡å·è¯­ä¹‰å‡½æ•°sigalå¤–ï¼Œè¿˜æ”¯æŒè¯­ä¹‰ç¬¦åˆPosix.1æ ‡å‡†çš„ä¿¡å·å‡½æ•°sigactionï¼ˆå®é™…ä¸Šï¼Œè¯¥å‡½æ•°æ˜¯åŸºäºBSDçš„ï¼ŒBSDä¸ºäº†å®ç°å¯é ä¿¡å·æœºåˆ¶ï¼Œåˆèƒ½å¤Ÿç»Ÿä¸€å¯¹å¤–æ¥å£ï¼Œç”¨sigactionå‡½æ•°é‡æ–°å®ç°äº†signalå‡½æ•°ï¼‰ï¼› æŠ¥æ–‡ï¼ˆMessageï¼‰é˜Ÿåˆ—ï¼ˆæ¶ˆæ¯é˜Ÿåˆ—ï¼‰ï¼šæ¶ˆæ¯é˜Ÿåˆ—æ˜¯æ¶ˆæ¯çš„é“¾æ¥è¡¨ï¼ŒåŒ…æ‹¬Posixæ¶ˆæ¯é˜Ÿåˆ—system Væ¶ˆæ¯é˜Ÿåˆ—ã€‚æœ‰è¶³å¤Ÿæƒé™çš„è¿›ç¨‹å¯ä»¥å‘é˜Ÿåˆ—ä¸­æ·»åŠ æ¶ˆæ¯ï¼Œè¢«èµ‹äºˆè¯»æƒé™çš„è¿›ç¨‹åˆ™å¯ä»¥è¯»èµ°é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯ã€‚æ¶ˆæ¯é˜Ÿåˆ—å…‹æœäº†ä¿¡å·æ‰¿è½½ä¿¡æ¯é‡å°‘ï¼Œç®¡é“åªèƒ½æ‰¿è½½æ— æ ¼å¼å­—èŠ‚æµä»¥åŠç¼“å†²åŒºå¤§å°å—é™ç­‰ç¼ºç‚¹ã€‚ å…±äº«å†…å­˜ï¼šä½¿å¾—å¤šä¸ªè¿›ç¨‹å¯ä»¥è®¿é—®åŒä¸€å—å†…å­˜ç©ºé—´ï¼Œæ˜¯æœ€å¿«çš„å¯ç”¨IPCå½¢å¼ã€‚æ˜¯é’ˆå¯¹å…¶ä»–é€šä¿¡æœºåˆ¶è¿è¡Œæ•ˆç‡è¾ƒä½è€Œè®¾è®¡çš„ã€‚å¾€å¾€ä¸å…¶å®ƒé€šä¿¡æœºåˆ¶ï¼Œå¦‚ä¿¡å·é‡ç»“åˆä½¿ç”¨ï¼Œæ¥è¾¾åˆ°è¿›ç¨‹é—´çš„åŒæ­¥åŠäº’æ–¥ã€‚ ä¿¡å·é‡ï¼ˆsemaphoreï¼‰ï¼šä¸»è¦ä½œä¸ºè¿›ç¨‹é—´ä»¥åŠåŒä¸€è¿›ç¨‹ä¸åŒçº¿ç¨‹ä¹‹é—´çš„åŒæ­¥æ‰‹æ®µã€‚ å¥—æ¥å£ï¼ˆSocketï¼‰ï¼šæ›´ä¸ºä¸€èˆ¬çš„è¿›ç¨‹é—´é€šä¿¡æœºåˆ¶ï¼Œå¯ç”¨äºä¸åŒæœºå™¨ä¹‹é—´çš„è¿›ç¨‹é—´é€šä¿¡ã€‚èµ·åˆæ˜¯ç”±Unixç³»ç»Ÿçš„BSDåˆ†æ”¯å¼€å‘å‡ºæ¥çš„ï¼Œä½†ç°åœ¨ä¸€èˆ¬å¯ä»¥ç§»æ¤åˆ°å…¶å®ƒç±»Unixç³»ç»Ÿä¸Šï¼šLinuxå’ŒSystem Vçš„å˜ç§éƒ½æ”¯æŒå¥—æ¥å­—ã€‚ 4.2 åˆ†å¸ƒå¼è¿›ç¨‹é€šä¿¡5. çº¿ç¨‹é€šä¿¡å¤šæ•°çš„å¤šçº¿ç¨‹éƒ½æ˜¯åœ¨åŒä¸€ä¸ªè¿›ç¨‹ä¸‹çš„ï¼Œå®ƒä»¬å…±äº«è¯¥è¿›ç¨‹çš„å…¨å±€å˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å…¨å±€å˜é‡æ¥å®ç°çº¿ç¨‹é—´é€šä¿¡ã€‚å¦‚æœæ˜¯ä¸åŒçš„è¿›ç¨‹ä¸‹çš„2ä¸ªçº¿ç¨‹é—´é€šä¿¡ï¼Œç›´æ¥å‚è€ƒè¿›ç¨‹é—´é€šä¿¡ã€‚ 6. References http://blog.csdn.net/hairetz/article/details/4281931","tags":[]},{"title":"Machine Learning - PCA","date":"2017-06-17T12:59:32.000Z","path":"2017/06/17/Machine-Learning-PCA/","text":"1. PCAPCA(Principal Components Analysis)æ˜¯ä¸»æˆæˆåˆ†åˆ†æï¼Œä¹‹å‰ä¹Ÿå«åšPricipal Factor Analysisã€‚é¡¾åæ€ä¹‰å°±æ˜¯åˆ†ææ•°æ®é‡Œé¢çš„ä¸»è¦éƒ¨åˆ†ï¼Œæ˜¯æœ€å¸¸ç”¨çš„ä¸€ç§é™ç»´æ–¹æ³•ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆéœ€è¦é™ç»´å‘¢ï¼Ÿ å› ä¸ºåœ¨çœŸå®çš„è®­ç»ƒæ•°æ®ä¸­ï¼Œæ€»æ˜¯ä¼šå­˜åœ¨å„ç§å†—ä½™çš„æ•°æ® æ¯”å¦‚è¯´æ‹¿åˆ°ä¸€ä¸ªæ±½è½¦çš„æ ·æœ¬ï¼Œé‡Œé¢æ—¢æœ‰ä»¥â€œåƒç±³/æ¯å°æ—¶â€åº¦é‡çš„æœ€å¤§é€Ÿåº¦ç‰¹å¾ï¼Œä¹Ÿæœ‰â€œè‹±é‡Œ/å°æ—¶â€çš„æœ€å¤§é€Ÿåº¦ç‰¹å¾ï¼Œæ˜¾ç„¶è¿™ä¸¤ä¸ªç‰¹å¾æœ‰ä¸€ä¸ªå¤šä½™ æ‹¿åˆ°ä¸€ä¸ªæ•°å­¦ç³»çš„æœ¬ç§‘ç”ŸæœŸæœ«è€ƒè¯•æˆç»©å•ï¼Œé‡Œé¢æœ‰ä¸‰åˆ—ï¼Œä¸€åˆ—æ˜¯å¯¹æ•°å­¦çš„å…´è¶£ç¨‹åº¦ï¼Œä¸€åˆ—æ˜¯å¤ä¹ æ—¶é—´ï¼Œè¿˜æœ‰ä¸€åˆ—æ˜¯è€ƒè¯•æˆç»©ã€‚æˆ‘ä»¬çŸ¥é“è¦å­¦å¥½æ•°å­¦ï¼Œéœ€è¦æœ‰æµ“åšçš„å…´è¶£ï¼Œæ‰€ä»¥ç¬¬äºŒé¡¹ä¸ç¬¬ä¸€é¡¹å¼ºç›¸å…³ï¼Œç¬¬ä¸‰é¡¹å’Œç¬¬äºŒé¡¹ä¹Ÿæ˜¯å¼ºç›¸å…³ã€‚é‚£æ˜¯ä¸æ˜¯å¯ä»¥åˆå¹¶ç¬¬ä¸€é¡¹å’Œç¬¬äºŒé¡¹å‘¢ï¼Ÿ è¿™ä¸ªä¸ç¬¬äºŒä¸ªæœ‰ç‚¹ç±»ä¼¼ï¼Œå‡è®¾åœ¨IRä¸­æˆ‘ä»¬å»ºç«‹çš„æ–‡æ¡£-è¯é¡¹çŸ©é˜µä¸­ï¼Œæœ‰ä¸¤ä¸ªè¯é¡¹ä¸ºâ€œlearnâ€å’Œâ€œstudyâ€ï¼Œåœ¨ä¼ ç»Ÿçš„å‘é‡ç©ºé—´æ¨¡å‹ä¸­ï¼Œè®¤ä¸ºä¸¤è€…ç‹¬ç«‹ã€‚ç„¶è€Œä»è¯­ä¹‰çš„è§’åº¦æ¥è®²ï¼Œä¸¤è€…æ˜¯ç›¸ä¼¼çš„ï¼Œè€Œä¸”ä¸¤è€…å‡ºç°é¢‘ç‡ä¹Ÿç±»ä¼¼ï¼Œæ˜¯ä¸æ˜¯å¯ä»¥åˆæˆä¸ºä¸€ä¸ªç‰¹å¾å‘¢ï¼Ÿ å› ä¸ºè¿™äº›å†—ä½™çš„æ•°æ®ï¼Œå¸¸å¸¸ä¼šå¯¼è‡´æˆ‘ä»¬çš„æ¨¡å‹è¿‡åº¦æ‹Ÿåˆã€‚è¿™ä¸ªæ—¶å€™ï¼Œå°±éœ€è¦ä¸€ç§ç‰¹å¾é™ç»´çš„æ–¹æ³•æ¥å‡å°‘ç‰¹å¾æ•°ï¼Œå‡å°‘å™ªéŸ³å’Œå†—ä½™ï¼Œå‡å°‘è¿‡åº¦æ‹Ÿåˆçš„å¯èƒ½æ€§ã€‚è€ŒPCAå°±æ˜¯é™ç»´çš„ç®—æ³•ä¹‹ä¸€ï¼Œå°†åŸå…ˆçš„æ•°æ®ä»nç»´æ˜ å°„åˆ°kç»´ä¸Š(k&lt;n)ã€‚è¿™é‡Œkç»´æ˜¯å…¨æ–°çš„æ­£äº¤ç‰¹å¾ï¼Œè¿™kç»´ç‰¹å¾æˆä¸ºä¸»å…ƒï¼Œæ˜¯é‡æ–°æ„é€ å‡ºå‡ºæ¥çš„kç»´ç‰¹å¾ï¼Œè€Œä¸æ˜¯ç®€å•çš„ä»nç»´å½“ä¸­ç§»å»äº†(n-k)ç»´ï¼Œç„¶åå‰©ä¸‹ä¸»è¦çš„kç»´ç‰¹å¾ã€‚ é‚£ä¹ˆè¯·æ€è€ƒä¸€ä¸ªé—®é¢˜ï¼šå¯¹äºæ­£äº¤å±æ€§ç©ºé—´ä¸­çš„æ ·æœ¬ç‚¹ï¼Œå¦‚ä½•ç”¨ä¸€ä¸ªè¶…å¹³é¢(ç›´çº¿çš„é«˜ç»´æ¨å¹¿)ï¼Œå¯¹æ‰€æœ‰çš„æ ·æœ¬è¿›è¡Œæ°å½“çš„è¡¨è¾¾ï¼Ÿé‚£ä¹ˆéœ€è¦æ‰¾åˆ°æ€ä¹ˆæ ·çš„ä¸€ä¸ªè¶…å¹³é¢æ¥åˆ†å‰²å‘¢ï¼Ÿä¸€èˆ¬éœ€è¦å…·å¤‡å¦‚ä¸‹ç‰¹å¾ï¼š æœ€è¿‘é‡æ„æ€§ï¼šæ ·æœ¬ç‚¹åˆ°è¿™ä¸ªè¶…å¹³é¢çš„è·ç¦»éƒ½è¶³å¤Ÿè¿‘ æœ€å¤§å¯åˆ†æ€§ï¼šæ ·æœ¬ç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½çš„è¦åˆ†å¼€ï¼Œè€Œä¸æ˜¯é‡å ä¸€èµ· 2. è®¡ç®—è¿‡ç¨‹é‚£ä¹ˆå¦‚ä½•æ¥æ„å»ºå‘¢ï¼Ÿä¸¾ä¸ªæ —å­ï¼š ç°åœ¨æœ‰è¿™ä¹ˆä¸€ç»„æ•°æ®ï¼Œxå’Œyæ˜¯ä¸¤ä¸ªç‰¹å¾ ç¬¬ä¸€æ­¥ï¼šæ±‚å‡ºæ‰€æœ‰ç»´åº¦çš„å¹³å‡å€¼ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œxçš„å¹³å‡å€¼æ˜¯1.81ï¼Œyçš„å¹³å‡å€¼æ˜¯1.91 ç¬¬äºŒæ­¥ï¼šä¸­å¿ƒåŒ–ï¼Œå³$\\sum x_i=0$.å°†æ‰€æœ‰çš„æ ·æœ¬éƒ½å‡å»è¿™ä¿©ä¸ªå¹³å‡å€¼ã€‚æ¯”å¦‚ç¬¬ä¸€ä¸ªæ ·æœ¬(2.5, 2.4) - (1.81, 1.91) = (0.69, 0.49) ç¬¬ä¸‰æ­¥ï¼šæ±‚ç‰¹å¾çš„åæ–¹å·®çŸ©é˜µå¦‚æœæœ‰x, y, zä¸‰ä¸ªç‰¹å¾ï¼Œåˆ†åˆ«éœ€è¦æ±‚cov(x, x), cov(x, y), cov(x, z), cov(y, y), cov(y, z), cov(z, z)è¿™å‡ ä¸ªã€‚ å½“åæ–¹å·®å¤§äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºxå’Œyè‹¥æœ‰ä¸€ä¸ªå¢åŠ ï¼Œå¦ä¸€ä¸ªä¹Ÿä¼šå¢åŠ ã€‚ å½“åæ–¹å·®ä¸‹äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºä¸€ä¸ªå¢åŠ ï¼Œå¦ä¸€ä¸ªä¼šå‡å°‘ å½“åæ–¹å·®ç­‰äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºæ¥è€…ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ã€‚ åæ–¹å·®çš„ç»å¯¹å€¼è¶Šå¤§ï¼Œä¸¤è€…å¯¹å½¼æ­¤çš„å½±å“ä¹Ÿå°±è¶Šå¤§ å…·ä½“å¦‚ä¸‹å›¾ è€Œæˆ‘ä»¬ä¸Šé¢çš„ä¾‹å­ä¸­åªæœ‰xå’Œyä¸¤ä¸ªå˜é‡ï¼Œå³ ç¬¬å››æ­¥ï¼šæ±‚åæ–¹å·®çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå¾—åˆ° ä¸Šé¢æ˜¯ä¸¤ä¸ªç‰¹å¾å€¼ï¼Œä¸‹é¢æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ æ¯”å¦‚ç‰¹å¾å€¼0.0490833989å¯¹åº”çš„ç‰¹å¾å‘é‡ä¸º(-0.735178656, 0.677873399)Tï¼Œè¿™é‡Œçš„ç‰¹å¾å‘é‡éƒ½å½’ä¸€åŒ–ä¸ºå•ä½å•ä½å‘é‡ æ±‚åæ–¹å·®çš„æ­¥éª¤ï¼š ç¬¬äº”æ­¥ï¼šå°†ç‰¹å¾å€¼æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ï¼Œé€‰æ‹©å…¶ä¸­ä¸ªæœ€å¤§çš„kä¸ªï¼Œç„¶åå°†å…¶å¯¹åº”çš„kä¸ªç‰¹å¾å‘é‡åˆ†åˆ«ä½œä¸ºåˆ—å‘é‡ç»„æˆç‰¹å¾å‘é‡çŸ©é˜µã€‚ ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç‰¹å¾å€¼åªæœ‰ä¸¤ä¸ªï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©å…¶ä¸­æœ€å¤§çš„é‚£ä¸ªã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬é€‰æ‹©1.28402771ï¼Œå¯¹åº”çš„ç‰¹å¾å‘é‡æ˜¯ (-0.677873399, -0.735178656)T. ç¬¬å…­æ­¥ï¼šå°†æ ·æœ¬æŠ•å½±åˆ°é€‰å–çš„ç‰¹å¾å‘é‡ä¸Šå»ã€‚ å‡è®¾æ ·æœ¬æ•°é‡ä¸ºmï¼Œç‰¹å¾æ•°é‡ä¸ºnï¼Œå‡å»å‡å€¼åçš„æ ·æœ¬çŸ©é˜µä¸ºDataAdjust(m*n)ï¼Œåæ–¹å·®çŸ©é˜µæ˜¯n*mï¼Œé€‰å–çš„kä¸ªç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µä¸ºEigenVectors(n*k)ã€‚é‚£ä¹ˆæŠ•å½±åçš„æ•°æ®FinalDataä¸º$$FinalData(m\\ast k) = DataAdjust(m\\ast n) \\times EigenVectors(n \\ast k)$$æ‰€ä»¥ä¸Šé¢çš„ä¾‹å­ä¸­$$FinalData(10\\ast 1) = DataAdjust(10\\ast 2) \\times EigenVectors(-0.677873399, -0.735178656)^T$$å¾—åˆ°çš„ç»“æœæ˜¯ è¿™æ ·å°±å°†åŸå§‹çš„æ ·æœ¬æ•°æ®ä»nç»´ç‰¹å¾å˜æˆäº†kç»´ç‰¹å¾ï¼Œè¿™kç»´å°±æ˜¯åŸå§‹ç‰¹å¾åœ¨kç»´ä¸Šçš„æŠ•å½±ã€‚ ä¸‹é¢çš„å›¾æè¿°äº†ä¸Šé¢çš„è¿‡ç¨‹ï¼š åŸå…ˆæ‰€æœ‰æ•°æ®æ˜¯åˆ†å¸ƒåœ¨x-yè¿™ä¸ªåæ ‡ç³»ä¸Šï¼Œ+è¡¨ç¤ºçš„æ˜¯æ ·æœ¬æ•°æ®ã€‚å·®ä¸å¤šæ˜¯å¯¹è§’çº¿ä¸Šçš„ä¸¤æ¡çº¿åˆ†åˆ«ä»£è¡¨äº†ä¸¤ä¸ªæ­£äº¤çš„ç‰¹å¾å‘é‡ã€‚ç”±äºåæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œå› æ­¤å…¶ç‰¹å¾å‘é‡æ­£äº¤ã€‚ç„¶åæˆ‘ä»¬å°†åŸæœ‰çš„æ ·æœ¬æ•°æ®æŠ•å½±åˆ°è¿™ä¸ªæ–°çš„åæ ‡ç³»ä¸­å»ï¼Œå¾—åˆ°è½¬æ¢åçš„æ•°æ®ï¼š ä»ä¸Šé¢çš„å›¾æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œåœ¨æ–°çš„åæ ‡ç³»ä¸­ï¼Œxè½´åŸºæœ¬å°±å¯ä»¥è¡¨ç¤ºåŸæœ‰çš„æ ·æœ¬æ•°æ®ç‰¹å¾ã€‚è€Œæ•´ä¸ªè¿‡ç¨‹ï¼Œçœ‹èµ·æ¥æœ‰ç‚¹åƒæ˜¯å°†åæ ‡ç³»åšäº†æ—‹è½¬ã€‚å¦‚æœå½“k=1çš„æ—¶å€™ï¼Œä¸Šé¢æ•°æ®å°±åªä¼šå˜æˆä¸€ç»´çš„æ•°æ®ã€‚è€Œå½“åŸæœ‰çš„æ•°æ®ç»´åº¦å¾ˆé«˜çš„æ—¶å€™ï¼Œæœ‰æ—¶å€™ä¸ºäº†å¯è§†åŒ–æ“ä½œï¼Œæˆ‘ä»¬å°±ä¼šå¯¹å…¶è¡ŒPCAé™ç»´ã€‚ 3. PCAç†è®ºåŸºç¡€ä¹‹å‰æˆ‘ä»¬æåˆ°äº†ï¼Œæˆ‘ä»¬å¸Œæœ›æ–°çš„è¶…å¹³é¢å…·å¤‡æœ€å¤§å¯åˆ†æ€§å’Œæœ€è¿‘é‡æ„æ€§è¿™ä¸¤ä¸ªç‰¹å¾ã€‚å®é™…ä¸Šæ ¹æ®è¿™ä¸¤ä¸ªç‰¹å¾ï¼Œèƒ½å¤Ÿåˆ†åˆ«å¾—åˆ°ä¸»æˆåˆ†åˆ†æçš„ä¸¤ç§ç­‰ä»·æ¨å¯¼ã€‚ æœ€å¤§å¯åˆ†æ€§ï¼šæœ€å¤§æ–¹å·®ç†è®º æ ·æœ¬ç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½å°½é‡åˆ†å¼€ï¼Œå³æŠ•å½±åçš„æ ·æœ¬ä¹‹é—´çš„æ–¹å·®è¦æœ€å¤§åŒ–ã€‚å‡è®¾ç°åœ¨æœ‰äº”ä¸ªæ ·æœ¬åˆ†å¸ƒåœ¨x-yåæ ‡ç³»ä¸‹ï¼Œå¦‚ä¸‹å›¾ æŠ•å½±çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š çº¢è‰²ç‚¹è¡¨ç¤ºæŸä¸€ä¸ªæ ·æœ¬ç‚¹$x^{(i)}$ï¼Œè“è‰²ç‚¹è¡¨ç¤º $x^{(i)}$åœ¨è¶…å¹³é¢uä¸Šçš„æŠ•å½±ï¼Œåœ¨è¿™é‡Œuæ˜¯ç›´çº¿çš„æ–œç‡ï¼Œä¹Ÿæ˜¯ç›´çº¿çš„æ–¹å‘å‘é‡ï¼Œä¸”æ˜¯å•ä½å‘é‡ã€‚è“è‰²çš„ç‚¹$x^{(i)}$åœ¨uä¸Šçš„æŠ•å½±å¸¦ä½ ï¼Œç¦»åŸç‚¹çš„è·ç¦»æ˜¯($x^{(i)}$, u)ï¼Œå³$(x^{(i)})^Tu$æˆ–è€…$u^Tx^{(i)}$ï¼Œè€Œè“è‰²ç‚¹åˆ°åŸç‚¹çš„è·ç¦»ï¼Œå°±æ˜¯åœ¨è“è‰²åæ ‡è½´ä¸Šçš„åæ ‡ã€‚ç”±äºè¿™äº›æ ·æœ¬ç‚¹çš„æ¯ä¸€ç»´çš„ç‰¹å¾å‡å€¼éƒ½æ˜¯0ï¼ˆä¹‹å‰å½’ä¸€åŒ–è¿‡ï¼‰ï¼Œå› æ­¤æŠ•å½±åˆ°uä¸Šçš„æ ·æœ¬ç‚¹çš„å‡å€¼ä»ç„¶æ˜¯0. å¦‚æœæˆ‘ä»¬å°†è¿™è¿™äº”ä¸ªæ ·æœ¬æŠ•å½±åˆ°æŸä¸€ä¸ªç»´åº¦ä¸Šï¼Œå³ä»äºŒç»´æŠ•å½±åˆ°ä¸€ç»´ä¸Šã€‚è¿™é‡Œä¸ºäº†æ¯”è¾ƒä¸åŒæ•ˆæœï¼Œåˆ†åˆ«é€‰äº†ä¸€æ¡è¿‡åŸç‚¹çš„ç›´çº¿è¡¨ç¤ºã€‚å¦‚ä¸‹å›¾ ä»ä¸Šé¢ä¸¤å¹…å›¾å¯ä»¥çœ‹å‡ºï¼Œå·¦è¾¹çš„å›¾ä¸Šçš„æ ·æœ¬é—´çš„è·ç¦»æ¯”å³è¾¹çš„è¦å¤§ï¼Œå³å·¦è¾¹æŠ•å½±åçš„æ ·æœ¬ç‚¹ä¹‹é—´çš„æ–¹å·®æœ€å¤§ï¼Œä¸ºï¼š æœ€åçš„ä¸€ä¸ªç­‰å¼ä¸­ï¼Œä¸­é—´é‚£éƒ¨åˆ†æ°å¥½æ˜¯æ±‚åæ–¹å·®çš„å…¬å¼(è¿™é‡Œç”¨mï¼Œè€Œä¸æ˜¯m-1)ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨$\\lambda$è¡¨ç¤ºå·¦è¾¹éƒ¨åˆ†ï¼Œå³æ ·æœ¬ç‚¹ä¹‹é—´çš„æ–¹å·®:$$\\lambda = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2$$ç”¨$\\sum$è¡¨ç¤ºä¸­é—´éƒ¨åˆ†ï¼š$$\\sum = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2$$é‚£ä¹ˆä¸Šé¢çš„å¼å­å¯ä»¥æ”¹å†™ä¸ºï¼š$$\\lambda = u^T\\sum u$$ä¹‹å‰æåˆ°äº†uæ˜¯è¶…å¹³é¢çš„å•ä½å‘é‡ï¼Œå³æœ‰$u^Tu = 1$ï¼Œå°†ä¸Šé¢å·¦å³ä¸¤è¾¹çš„å¼å­éƒ½ä¹˜ä»¥uï¼Œå¯å¾—ï¼š$$u\\lambda = \\lambda u= uu^T\\sum u = \\sum u$$å³$\\sum u = \\lambda u$ï¼Œæ‰€ä»¥$\\lambda$å°±æ˜¯$\\sum$çš„ç‰¹å¾å€¼ï¼Œuæ˜¯ç‰¹å¾å‘é‡ã€‚æœ€ä½³çš„æŠ•å½±ç›´çº¿æ˜¯ç‰¹å¾å€¼$\\lambda$æœ€å¤§æ—¶å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œå…¶æ¬¡æ˜¯$\\lambda$ç¬¬äºŒå¤§å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œä¾æ¬¡ç±»æ¨ã€‚ å› æ­¤åªéœ€è¦å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå¾—åˆ°çš„å‰kå¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å°±æ˜¯æœ€ä½³çš„kç»´æ–°ç‰¹å¾ï¼Œè€Œä¸”è¿™kç»´æ–°ç‰¹å¾æ˜¯æ­£äº¤çš„ã€‚å¾—åˆ°å‰kä¸ªuä»¥åï¼Œå°±å¯å°†æ ·ä¾‹$x^{(i)}$è¡¨ç¤ºä¸ºä¸‹é¢çš„æ–°æ ·æœ¬ï¼š å…¶ä¸­çš„ç¬¬jç»´å°±æ˜¯$x^{(i)}$åœ¨$u_j$ä¸Šçš„æŠ•å½±ã€‚é€šè¿‡é€‰å–æœ€å¤§çš„kä¸ªuï¼Œä½¿å¾—æ–¹å·®è¾ƒå°çš„ç‰¹å¾ï¼ˆå¦‚å™ªå£°ï¼‰è¢«ä¸¢å¼ƒã€‚ æœ€è¿‘é‡æ„æ€§ï¼šæœ€å°å¹³æ–¹è¯¯å·®ç†è®º å‡è®¾ç°åœ¨é€‰çš„è¶…å¹³é¢æ˜¯L(è¿™ä¸ªä¾‹å­ä¸­æ˜¯ç›´çº¿)ï¼Œé‚£ä¹ˆæŸä¸€æ ·æœ¬$x_k$åˆ°Lçš„å‚ç›´è·ç¦»ä¸ºdâ€™ï¼Œé‚£ä¹ˆæœ‰æ‰€æœ‰ç‚¹åˆ°è¯¥ç›´çº¿çš„è·ç¦»ä¸ºï¼š$$\\sum_{k=1}^n||(x_kâ€™ - x_k)||^2$$ä¸Šé¢è¿™ä¸ªå…¬å¼ç§°ä½œæœ€å°å¹³æ–¹è¯¯å·®(Least Squared Erroe)","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"PCA","slug":"PCA","permalink":"http://chenson.com/tags/PCA/"}]},{"title":"Machine Learning - æ”¯æŒå‘é‡æœº","date":"2017-06-16T09:21:11.000Z","path":"2017/06/16/Machine-Learning-æ”¯æŒå‘é‡æœº/","text":"1. Margins Logistic Regression â€‹åœ¨logistic regressionä¸­, å¦‚æœæˆ‘ä»¬è¦åˆ¤æ–­ä¸€ä¸ªç‚¹æ˜¯å±äº0è¿˜æ˜¯1æ¦‚ç‡æ˜¯Î¸ $$p(y=1 | x; \\theta) \\h_\\theta(x) = g(\\theta^T x)$$ logistic regressionå’ŒSVMçš„åŒºåˆ« ç”±ä¸Šå›¾å¯çŸ¥, Cæ˜¯éå¸¸é è¿‘Boundaryçš„, è€ŒAæ˜¯ç¦»Boundaryæœ€è¿œçš„ç‚¹. æ‰€ä»¥æˆ‘ä»¬éå¸¸æœ‰ä¿¡å¿ƒçš„è¯´Aæ˜¯å±äº+, è€ŒCæ¯”è¾ƒæœ‰å¯èƒ½å±äº+. æ‰€ä»¥å¯¹äºé‚£äº›è·ç¦»è¾¹ç•Œæ¯”è¾ƒè¿‘çš„ç‚¹æ‰æ˜¯æˆ‘ä»¬éœ€è¦é‡ç‚¹è€ƒè™‘çš„. è€Œè¿™ä¹Ÿæ­£æ˜¯logistic regressionå’ŒSVMä¹‹é—´çš„åŒºåˆ«. logistic regressionè€ƒè™‘å…¨å±€(å¦‚ä½•è€ƒè™‘å…¨å±€ï¼Ÿansï¼šRMSEæœ€å°å€¼)çº¢çº¿ SVMè€ƒè™‘å±€éƒ¨(æœ€å°é—´éš”è¦å¤§äºå¤šå°‘)ç»¿çº¿ 2. SVM ç¬¦å·è¯´æ˜ åœ¨Logsticä¸­, å¯¹äºäºŒåˆ†ç±»é—®é¢˜, y âˆˆ {0, 1} åœ¨SVMä¸­, å¯¹äºäºŒåˆ†ç±»é—®é¢˜, y âˆˆ {-1, +1}, ä¸”é‡æ–°å®šä¹‰å…¬å¼ $$h_{w,b} (x) = g(w^Tx + b) \\g(z) = 1 \\space (z\\geq 0) \\g(z) = -1 \\space (z &lt; 0)$$ å‡½æ•°é—´éš” Functional Margins ç¬¬ä¸€ä¸ªå…¬å¼è¡¨ç¤ºçš„æ˜¯æŸä¸ªæ ·æœ¬çš„å‡½æ•°é—´éš”. ç¬¬äºŒä¸ªå…¬å¼è¡¨ç¤ºçš„æ˜¯å…¨å±€çš„å‡½æ•°é—´éš”, ä¹Ÿå°±æ˜¯å‡½æ•°é—´éš”ä¸ºæ‰€æœ‰æ ·æœ¬ä¸­å‡½æ•°é—´éš”æœ€å°çš„é‚£ä¸ªå‡½æ•°é—´éš”å†³å®š. ä¹‹å‰ä½¿ç”¨æ­£è´Ÿ1æ¥è¡¨ç¤º y, æ‰€ä»¥è®¡ç®—å‡ºæ¥çš„è·ç¦»éƒ½æ˜¯ä¸€ä¸ªéè´Ÿæ•°, ä¸”è¿™ä¸ªæ•°å€¼çš„å¤§å°è¡¨ç¤ºäº†å¯¹äºé¢„æµ‹ç»“æœçš„confidence. y å€¼è¶Šæ¥è¿‘1, è¡¨ç¤ºå¯¹è¿™ä¸ªåˆ¤æ–­è¶Šè‚¯å®š.$$\\hat {\\gamma}^{(i)} = y^{(i)} (w^Tx^{(i)} + b) \\\\hat \\gamma = \\min_{i=1,â€¦,m} \\hat \\gamma^{(i)}$$å‡½æ•°é—´éš”è¶Šå¤§, ä»£è¡¨äº†æˆ‘ä»¬å¯¹äºåˆ†ç±»çš„ç»“æœéå¸¸çš„è‚¯å®š, æ‰€ä»¥å¸Œæœ›å‡½æ•°é—´éš”è¶Šå¤§è¶Šå¥½, ä½†éœ€è¦å¯¹è¿™ä¸ªé—´éš”åŠ ä¸Šä¸€äº›é™åˆ¶æ¡ä»¶(åé¢å…·ä½“è®²)æ‰è¡Œ. å› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ä¸æ”¹å˜è¿™ä¸ªè¶…å¹³é¢çš„æƒ…å†µä¸‹, åªè¦æˆæ¯”ä¾‹å¢åŠ wå’Œb, å°±èƒ½è®©å‡½æ•°é—´éš”ä»»æ„çš„å¤§. å‡ ä½•è·ç¦» Geometrix Margins ä¸‹å›¾ä¸­, å¦‚æœæˆ‘ä»¬çŸ¥é“Bç‚¹æ‰€åœ¨çš„è¶…å¹³é¢(separating hyperplane)çš„è§£æå¼, ä»»ä½•å…¶ä»–ç‚¹åˆ°è¯¥é¢çš„è·ç¦»éƒ½å¯ä»¥ç”¨ä¸Šé¢å®šä¹‰è¿‡çš„å‡½æ•°é—´éš”æ¥è¡¨ç¤º. $$å†³ç­–è¾¹ç•Œï¼šw^Tx + b = 0$$wæ˜¯è¶…å¹³é¢çš„æ³•å‘é‡, å‚ç›´äºå†³ç­–è¾¹ç•Œ, ä¹Ÿå°±æ˜¯è¿™ä¸ªè¶…å¹³é¢. è‹¥Bæ˜¯Aåœ¨åˆ†å‰²é¢ä¸Šçš„æŠ•å½± (ABå‚ç›´äºè¶…å¹³é¢), é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è®¡ç®—Aåˆ°è¶…å¹³é¢çš„è·ç¦» Î³ . å‡è®¾Aç‚¹æ˜¯$x_i$, é‚£ä¹ˆBç‚¹ä¸ºï¼š$$\\overrightarrow {OB} = \\overrightarrow {OA} - \\overrightarrow {BA} \\x^{(i)} - \\gamma^{(i)} Â· \\frac w {||w||}$$å› ä¸ºBç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Š, æ‰€ä»¥æˆ‘ä»¬å°†è¿™ä¸ªç‚¹å¸¦å›è¶…å¹³é¢å¾—åˆ°ï¼š$$w^T(x^{(i)} - \\gamma^{(i)} Â· \\frac w {||w||}) + b = 0$$é€šè¿‡ä¸Šé¢çš„å¼å­, å¯ä»¥è§£å‡ºÎ³ï¼š$$w^Tx^{(i)} - \\gamma^{(i)} Â· \\frac {w^Tw} {||w||} + b = 0 \\w^Tx^{(i)} + b = \\gamma^{(i)} ||w|| \\\\gamma^{(i)} = (\\frac w {||w||})^Tx^{(i)} + \\frac b {||w||}$$åŠ ä¸Šå‰é¢çš„$y^{(i)}$, äºæ˜¯æˆ‘ä»¬å°±èƒ½å¾—åˆ°äº†å‡ ä½•é—´éš”ï¼š$$\\gamma^{(i)} = y^{(i)}(\\frac {w^T} {||w||}x^{(i)} + \\frac b {||w||})$$é€šè¿‡ä¸Šé¢çš„å¼å­, å‘ç°å½“||w|| = 1æ—¶, å‡ ä½•é—´éš”å°±æ˜¯å‡½æ•°é—´éš”. è¿™ä¸ªæ—¶å€™, å¦‚æœä»»æ„æ”¾å¤§||w||, å‡ ä½•é—´éš”æ˜¯ä¸ä¼šæ”¹å˜çš„. å› ä¸º||w||ä¹Ÿä¼šéšç€è¢«æ”¾å¤§. å‡ ä½•é—´éš”ä¸å‡½æ•°é—´éš”çš„å…³ç³»ä¸ºï¼š$$\\gamma^{(i)} = \\frac {\\hat \\gamma^{(i)}} {||w||}$$å¯¹äºæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬, å‡ ä½•é—´éš”ä¸ºï¼š$$\\gamma = \\min_{i=1,â€¦,m} \\gamma^{(i)}$$ 2. é—´éš”æœ€å¤§åŒ–æ ¹æ®ä¸Šä¸€èŠ‚, æˆ‘ä»¬å¯ä»¥æ±‚å‡ºå‡ ä½•é—´éš”Î³. å¦‚æœç°åœ¨éœ€è¦æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢S, ä½¿å¾—ç¦»è¶…å¹³é¢æœ€è¿‘çš„ç‚¹çš„å‡ ä½•é—´éš”è¶Šå¤§è¶Šå¥½, å¯ä»¥ç”¨ä¸‹åˆ—ä¼˜åŒ–é—®é¢˜è¡¨ç¤ºï¼š$$\\max_{w,b} \\gamma \\s.t. \\space y_i(\\frac w {||w||}Â·x_i + \\frac b {||w||}) \\geq \\gamma, \\space i = 1, 2, â€¦, N$$å³æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¶…å¹³é¢(w, b)å…³äºè®­ç»ƒæ•°æ®é›†çš„å‡ ä½•é—´éš”Î³, çº¦æŸæ¡ä»¶è¡¨ç¤ºè¶…å¹³é¢(w,b)å…³äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ç‚¹çš„å‡ ä½•é—´éš”è‡³å°‘æ˜¯Î³. è€ƒè™‘åˆ°å‡ ä½•é—´éš”ä¸å‡½æ•°é—´éš”çš„å…³ç³»å¼, å¯ä»¥å°†è¿™ä¸ªé—®é¢˜æ”¹å†™æˆå‡½æ•°é—´éš”æ¥è¡¨ç¤º, å³ï¼š$$\\max_{w,b} \\frac {\\hat \\gamma} {||w||} \\s.t. \\space y_i(wÂ·x_i + b) \\geq \\hat \\gamma, \\space i = 1, 2, â€¦, N$$ä¸Šé¢å¼å­ä¸­, å‡½æ•°é—´éš”çš„å–å€¼å¹¶ä¸ä¼šå½±å“åˆ°æœ€ä¼˜åŒ–é—®é¢˜çš„è§£. äº‹å®ä¸Š, å‡è®¾å°†wå’ŒbæŒ‰æ¯”ä¾‹æ”¹å˜ä¸ºÎ»wå’ŒÎ»b, è¿™æ—¶å‡½æ•°é—´éš”ä¹Ÿä¼šè¢«å½“å¤§Î»å€, æ‰€ä»¥è¿™ä¸ªå¯¹ä¸Šé¢çš„æœ€ä¼˜åŒ–é—®é¢˜çš„ä¸ç­‰å¼çº¦æŸæ˜¯æ²¡æœ‰å½±å“çš„, å¯¹ç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–ä¹Ÿæ²¡æœ‰å½±å“. è¿™æ ·, å¯ä»¥å»å‡½æ•°é—´éš”Î³=1, ä»£å…¥åˆ°ä¸Šé¢çš„æœ€ä¼˜åŒ–é—®é¢˜, æ³¨æ„åˆ°æœ€å¤§åŒ– $\\frac 1 {||w||}$å’Œæœ€å°åŒ– $\\frac 1 2$ $||w||^2$æ˜¯ç­‰ä»·çš„, ä¹Ÿæ˜¯å°±å¾—åˆ°å¦é—¨çš„çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœºå­¦ä¹ çš„æœ€ä¼˜åŒ–é—®é¢˜ï¼š$$\\min_{w,b} \\frac 1 2 ||w||^2 \\s.t. \\space y_i(wÂ·x_i + b) - 1 \\geq 0$$è¿™ä¸ªæ—¶å€™æˆ‘ä»¬çš„é—®é¢˜å°±è½¬åŒ–æˆäº†åœ¨çº¿æ€§çº¦æŸä¸‹çš„äºŒæ¬¡è§„åˆ’, å¯ä»¥ä½¿ç”¨äºŒæ¬¡è§„åˆ’çš„è½¯ä»¶æ¥è§£å†³è¿™ä¸ªä¼˜åŒ–é—®é¢˜,, ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„æœ€ä¼˜é—´éš”åˆ†ç±»å™¨.å®é™…ä¸Š, æˆ‘ä»¬æœ‰æ›´å¥½çš„åŠæ³•å»è§£è¿™ä¸ªä¼˜åŒ–é—®é¢˜. ä½†åœ¨è¿™ä¹‹å‰, æˆ‘ä»¬éœ€è¦è¡¥å……ä¸€ä¸‹å…¶ä»–çš„ç›¸å…³çŸ¥è¯†. 3. æ‹‰æ ¼æœ—æ—¥å¯¹å¶åœ¨çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ä¸­, å¸¸å¸¸åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§(Lagrange dulity)è®²åŸå§‹é—®é¢˜è½¬æ¢ä¸ºå¯¹å¶é—®é¢˜, é€šè¿‡è§£åº¦å¶é—®é¢˜è€Œå¾—åˆ°çš„åŸå§‹é—®é¢˜çš„è§£. è¯¥æ–¹æ³•åº”ç”¨åœ¨è®¸å¤šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸­, ä¾‹å¦‚æœ€å¤§ç†µæ¨¡å‹å’Œæ”¯æŒå‘é‡æœº. åŸå§‹é—®é¢˜$$\\min_w f(w) \\s.t. \\space h_i(w) = 0, \\space i = 1,2,â€¦,l$$ä¸‹é¢æ˜¯çº¦æŸæ¡ä»¶, ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•, å°†é—®é¢˜è½¬æ¢ä¸ºï¼š$$L(w, b) = f(w) + \\sum_{i=1}^l\\beta_ih_i(w)$$è¿™é‡Œé¢, Î²iä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜å­(Lagrange Multipliers). ç„¶åä»¤åå¯¼ä¸º0æ¥è§£å¾—wå’ŒÎ²ï¼š$$\\frac {âˆ‚L} {âˆ‚w_i} = 0 \\\\frac {âˆ‚L} {âˆ‚\\beta_i} = 0$$æ›´åŠ å¹¿æ³›çš„çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ï¼š$$\\min_w f(w) \\s.t. \\space g_i(x) â‰¤ 0, \\space i = 1, 2, â€¦,k \\\\space \\space \\space \\space \\space \\space \\space h_i(w) = 0, \\space i = 1,2,â€¦,l$$æ‰€ä»¥æˆ‘ä»¬å®šä¹‰å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å…¬å¼(Generalized Lagrangian)ä¸ºï¼š$$L(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^k \\alpha_ig_i(w) + \\sum_{i=1}^l \\beta_ih_i(w)$$å…¶ä¸­, Î±i,Î²iä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜å­(Lagrange Multipliers). ç°åœ¨æˆ‘ä»¬å®šä¹‰ï¼š$$\\theta_p(w) = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} L (w, \\alpha, \\beta)$$ â€‹ å…¶ä¸­ä¸‹è¡¨Pä»£è¡¨äº†â€œprimalâ€. è‹¥ä¸¤ä¸ªçº¦æŸæ¡ä»¶å½“ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå¾—ä¸åˆ°æ»¡è¶³çš„æ—¶å€™, åˆ™å¯ä»¤Î±iä¸ºæ— ç©·å¤§æˆ–Î²iä¸ºæ— ç©·å¤§ä½¿å¾—ï¼š$$\\theta_P = f(w) \\space \\space \\space (wæ»¡è¶³åŸå§‹é—®é¢˜çš„çº¦æŸ)\\\\theta_p = \\infty \\space \\space \\space (å…¶ä»–)\\$$â€‹ å¯¹äºæ»¡è¶³åŸå§‹çº¦æŸçš„wæ¥è¯´, Î¸pä¸åŸå§‹é—®é¢˜ä¸­çš„ç›®æ ‡å‡½æ•°ç›¸åŒ. å¯¹äºè¿ååŸå§‹çº¦æŸé—®é¢˜çš„wæ¥è¯´Î¸p = âˆ â€‹ å¦‚æœè€ƒè™‘æœ€å°åŒ–ï¼š â€‹$$\\min_w\\theta_p(w) = \\min_w \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0 } L(w, \\alpha, \\beta)$$â€‹ å®ƒæ˜¯ä¸åŸå§‹æœ€ä¼˜åŒ–é—®é¢˜ç›¸ç­‰ä»·çš„, å³å®ƒä»¬æœ‰ç›¸åŒçš„è§£. å¯¹å¶é—®é¢˜ ç°åœ¨çœ‹å¦å¤–ä¸€ä¸ªé—®é¢˜ï¼š$$\\theta_D(\\alpha, \\beta) = \\min_w L(w, \\alpha, \\beta)$$å…¶ä¸­ä¸‹è¡¨Dä»£è¡¨äº†å¯¹å¶(dual). åœ¨åŸå§‹é—®é¢˜ä¸­, æˆ‘ä»¬å…ˆæœ€å¤§åŒ–å…³äºÎ±å’ŒÎ²çš„å‡½æ•°, å†æœ€å°åŒ–å…³äºwçš„å‡½æ•°ï¼› è€Œå¯¹å¶é—®é¢˜ä¸­, æˆ‘ä»¬å…ˆæœ€å°åŒ–å…³äºwçš„å‡½æ•°, åœ¨æœ€å¤§åŒ–å…³äºÎ±å’ŒÎ²çš„å‡½æ•°ï¼š$$\\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\theta_D(\\alpha, \\beta) = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\min_w L(w, \\alpha, \\beta)$$å®ƒä»¬å”¯ä¸€çš„åŒºåˆ«å°±åœ¨äºminå’Œmaxçš„é¡ºåºä¸åŒ. æˆ‘ä»¬ä»¤:$$d^{*} = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\min_w L(w, \\alpha, \\beta) \\leq \\min_w \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} L(w, \\alpha, \\beta) = p^{*}$$ä¹Ÿå°±æ˜¯è¯´, åœ¨æŸç§æƒ…å†µä¸‹, ä¼šæœ‰d = p. è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±å¯æŠŠæ±‚åŸå§‹é—®é¢˜è½¬åŒ–æˆæ±‚å¯¹å¶é—®é¢˜. å‡è®¾få’Œgéƒ½æ˜¯å‡¸å‡½æ•°(convex function), hæ˜¯ä»¿å°„çš„, å¹¶ä¸”å­˜åœ¨wæ˜¯å¯¹æ‰€æœ‰çš„i, èƒ½å¤Ÿä½¿å¾—:$$g_i(w) &lt; 0$$ä¸Šè¿°å‡è®¾æ¡ä»¶ä¸‹, ä¸€å®šå­˜åœ¨w*, Î±*å’ŒÎ²*ä½¿å¾—w*æ˜¯åŸå§‹é—®é¢˜çš„è§£. Î±*å’ŒÎ²*æ˜¯å¯¹å¶é—®é¢˜çš„è§£. å¹¶ä¸”è¿˜æœ‰ p = d = L(w, Î±\\, Î²*). w*, Î±*å’ŒÎ²*æ»¡è¶³KKTæ¡ä»¶(Karush-Kuhn-Tucker conditions)ï¼š å¦‚æœå­˜åœ¨æ»¡è¶³KKTæ¡ä»¶çš„w*,Î±*,Î²*, åˆ™åŸå§‹æ¡ä»¶ä¸å¯¹å¶é—®é¢˜ä¸€å®šæœ‰è§£. å…¬å¼(5)åˆç§°ä¹‹ä¸ºKKTå¯¹å¶äº’è¡¥æ¡ä»¶, è¿™ä¸ªæ¡ä»¶è¡¨æ˜å¦‚æœ Î±* &gt; 0, é‚£ä¹ˆå°±æœ‰g(w*) = 0. å³çº¦æŸæ¡ä»¶ g(w*) &lt;= 0 æ¿€æ´», wå¤„äºå¯è¡ŒåŸŸçš„è¾¹ç•Œä¸Š. è€Œå…¶ä»–è°“è¯­å¯è¡ŒåŸŸå†…éƒ¨g(w*) &lt; 0çš„ç‚¹éƒ½ä¸èµ·çº¦æŸä½œç”¨, å¯¹åº”çš„Î±* = 0. ä¸ºä»€ä¹ˆè¦å¼•å…¥å¯¹å¶ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ ç”¨æ™®é€šçš„åŸºäºæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦å’Œçš„å‡å€¼çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆBGDï¼‰æ˜¯è¡Œä¸é€šçš„ï¼ŒåŸå› åœ¨äºæˆ‘ä»¬çš„æŸå¤±å‡½æ•°é‡Œé¢æœ‰é™å®šï¼Œåªæœ‰è¯¯åˆ†ç±»çš„Mé›†åˆé‡Œé¢çš„æ ·æœ¬æ‰èƒ½å‚ä¸æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç”¨æœ€æ™®é€šçš„æ‰¹é‡æ¢¯åº¦ä¸‹é™(ä¸ºä»€ä¹ˆï¼Ÿï¼Ÿ), åªèƒ½é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æˆ–è€…å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMBGDï¼‰ã€‚ å¯¹å¶çš„ä¼˜åŠ¿ å¯¹å¶å½¢å¼å°†æƒé‡å‘é‡wè½¬åŒ–æˆå®ä¾‹$x_i$å’Œ$y_i$çš„çº¿æ€§ç»„åˆå½¢å¼ã€‚ä¸”å¯¹å¶æ˜¯ä»¥å†…ç§¯çš„å½¢å¼å‡ºç°çš„ï¼Œå¯ä»¥é¢„å…ˆä½¿ç”¨GramçŸ©é˜µå‚¨å­˜ï¼Œç”¨ç©ºé—´æ¢æ—¶é—´çš„æ–¹æ³•æé«˜è®¡ç®—æ•ˆç‡ã€‚åŸå§‹å½¢å¼æ¯æ¬¡åˆ¤æ–­è¯¯åˆ†ç±»ç‚¹æ—¶éƒ½éœ€è¦è¿›è¡Œå‘é‡ç‚¹ä¹˜è¿ç®—ã€‚ è¿™é‡ŒåŒæ—¶ä¹Ÿä¸ºåé¢å¼•å…¥æ ¸å‡½æ•°åšä¼ç¬”ï¼Œå› ä¸ºæ„ŸçŸ¥æœºæ˜¯ç¥ç»ç½‘ç»œå’Œæ”¯æŒå‘é‡æœºçš„åŸºç¡€ã€‚ 4. æœ€ä¼˜é—´éš”åˆ†ç±»å™¨æ ¹æ®ä¸Šé¢çš„å†…å®¹, å›é¡¾SVMçš„é—®é¢˜$$\\min_{\\gamma,w,b} \\frac 1 2 ||w||^2 \\s.t. \\space y^{(i)}(w^Tx^{(i)} + b) \\geq 1 \\$$æ ¹æ®æ‹‰æ ¼æœ—æ—¥å¯¹å¶é—®é¢˜, ä¿®æ”¹çº¦æŸæ¡ä»¶ä¸º$$g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0$$é‚£ä¹ˆå°±å’Œæ‹‰æ ¼æœ—æ—¥å…¬å¼æ˜¯ä¸€æ ·çš„äº†. æ ¹æ®ä¸Šé¢çš„KTTæ¡ä»¶å¯çŸ¥åªæœ‰å‡½æ•°é—´éš”æ˜¯1, çº¿æ€§çº¦æŸå¼å‰é¢çš„ç³»æ•°å¤§äº0, g(w)=0, å…¶ä»–çš„ä¸åœ¨çº¿ä¸Šçš„ç‚¹(g(w)&lt;0), æå€¼ä¸ä¼šåœ¨ä»–ä»¬æ‰€åœ¨çš„èŒƒå›´å†…å–å¾—, å› æ­¤å‰é¢çš„ç³»æ•°ç­‰äº0. è€ƒè™‘ä¸‹å›¾, æœ€å¤§é—´éš”åˆ†ç±»è¶…å¹³é¢ä¸ºå®çº¿ï¼š å…¶ä¸­ä¸€ä¸ªæ­£æ ·æœ¬å’Œä¸¤ä¸ªè´Ÿæ ·æœ¬æ­£å¥½åœ¨å¹³è¡Œäºåˆ†ç±»è¶…å¹³é¢çš„è™šçº¿ä¸Š, åªæœ‰è¿™ä¸‰ä¸ªæ ·æœ¬å¯¹åº”çš„ Î±i&lt;0, å…¶ä»–æ ·æœ¬å¯¹åº”çš„ Î±i=0. è¿™ä¸‰ä¸ªæ ·æœ¬å°±å«åšæ”¯æŒå‘é‡æœº. ä»è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹å‡º, æ”¯æŒå‘é‡çš„ä¸ªæ•°è¿œè¿œå°äºé›†è®­é›†çš„å¤§å°. ç°åœ¨æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š$$L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]$$æ‰€ä»¥æ¥ä¸‹å»çš„ä»»åŠ¡å°±æ˜¯æ±‚è§£å¯¹å¶é—®é¢˜, æ ¹æ®ä¸Šé¢çš„çŸ¥è¯†, æœ‰ï¼š$$d* = \\max_{\\alpha:\\alpha_i \\geq 0} \\theta_D(\\alpha) = \\max_{\\alpha:\\alpha_i \\geq 0} \\min_{w, b} L(w, b,\\alpha)$$é¦–å…ˆ, æ±‚L(w,b,Î±)å…³äºw, b çš„æœ€å°å€¼. ä»¤åå¯¼æ•°ä¸º0ï¼š$$\\frac {âˆ‚L} {âˆ‚w} = w - \\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)} = 0 \\\\frac {âˆ‚L} {âˆ‚b} = 0 - \\sum_{i=1}^m\\alpha_iy^{(i)}= 0$$å¯å¾—ï¼š$$w = \\sum_{i=1}^m\\alpha_i y^{(i)} x^{(i)} \\\\frac {âˆ‚} {âˆ‚b} L(w, b, \\alpha) = \\sum_{i=1}^m\\alpha_i y^{(i)} = 0$$å°†æ±‚å¾—çš„wå¸¦å›æ‹‰æ ¼æœ—æ—¥å‡½æ•°L(w,b,Î±), å¯å¾—åˆ°ï¼š$$L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]$$ è®¡ç®—å‡ºäº†min L(w,b,Î±), ä¾¿å¯ä»¥ç»§ç»­è¿›è¡Œmaxæ“ä½œ, å³ï¼š å¯ä»¥è¯æ˜è¯¥ä¼˜åŒ–é—®é¢˜æ»¡è¶³KKTæ¡ä»¶, æ±‚å¾—$Î±^{*}i$ä¹‹å(åé¢å°†å¦‚ä½•æ±‚è§£), å¯é€šè¿‡:$$w = \\sum{i=1}^m \\alpha_i y^{(i)}x^$$æ±‚å¾—w*, æœ€åé€šè¿‡ä¸‹é¢å¼å­æ±‚å¾—b*$$b^{*} = -\\frac {\\max_{i:y^{(i)}=-1}w^{*T}x^{(i)} + \\min_{i:y^{(i)} = 1}w^{*T}x^{(i)}}{2}$$å½“æ±‚å‡ºæ‰€æœ‰çš„å‚æ•°, å°±å¯ä»¥é€šè¿‡$w^T$x + b æ¥è¿›è¡Œåˆ†ç±»äº†ï¼š$$w^T + b = (\\sum_i^m \\alpha_iy_ix_i)^T x + b= \\sum_i^m \\alpha_iy_iâŸ¨x_i, xâŸ© + b$$é€šè¿‡ä¸Šé¢å¼å­å‘ç°, ç°åœ¨æ–°æ¥ä¸€ä¸ªæ–°æ•°æ®, åªéœ€è¦è®¡ç®—å®ƒä¸è®­ç»ƒæ ·æœ¬çš„å†…ç§¯å³å¯. å¹¶é€šè¿‡å‰é¢çš„KKTæ¡ä»¶æˆ‘ä»¬çŸ¥é“, åªæœ‰é™¤äº†æ”¯æŒå‘é‡çš„é‚£äº›åŸæœ¬, éƒ½æœ‰$Î±_i$ = 0. æ‰€ä»¥, æˆ‘ä»¬åªéœ€è¦å°†æ–°æ ·æœ¬ä¸æ”¯æŒå‘é‡æœºåšå†…ç§¯è¿ç®—, å³å¯æ±‚å‡º$w^T$x + b Example å‡è®¾è¿™é‡Œæœ‰ä¸‰ä¸ªæ ·æœ¬ç‚¹ï¼Œæ­£æ ·æœ¬ç‚¹x1=(3,3)^T, x2=(4,3)^T, è´Ÿæ ·æœ¬ç‚¹æ˜¯x3=(1,1)^Tï¼Œè¯•ç”¨æ„ŸçŸ¥æœºå­¦ä¹ ç®—æ³•å¯¹å¶å½¢å¼æ±‚æ„ŸçŸ¥æœºæ¨¡å‹ã€‚ ANSWER å–Î±i = 0ï¼Œè¿™é‡Œi=1ï¼Œ2ï¼Œ3ï¼Œb=0ï¼Œn=1 è®¡ç®—GramçŸ©é˜µ$$G= \\begin {bmatrix} ||x_1||^2 &amp; x_1Â·x2 &amp; x_1Â·x3 \\ x2Â· x1 &amp; ||x2||^2 &amp; x_2Â·x3 \\\\ ||x_3Â· x1 &amp; x_3Â·x2 &amp; ||x_3||^2 \\end{bmatrix} = \\begin {bmatrix} 18 &amp; 21 &amp; 6 \\ 21 &amp; 25 &amp; 7 \\\\ 6 &amp; 7 &amp; 2 \\end{bmatrix} $$ è¯¯åˆ†æ¡ä»¶$$y_i(\\sum_{j=1}^N\\alpha_jy_jx_j Â·x_i + b) \\leq 0$$å‚æ•°æ›´æ–°$$\\alpha_i \\leftarrow \\alpha_i + 1 \\b \\leftarrow b + y_i$$â€‹ è¿­ä»£è¿‡ç¨‹å¦‚ä¸‹ï¼Œç»“æœåˆ—äºä¸‹è¡¨ | k | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 || :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€”-: || | | x1 | x3 | x3 | x3 | x1 | x3 | x3 || Î±1 | 0 | 1 | 1 | 1 | 2 | 2 | 2 | 2 || Î±2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 || Î±3 | 0 | 0 | 1 | 2 | 2 | 3 | 4 | 5 || b | 0 | 1 | 0 | -1 | 0 | -1 | -2 | -3 | w, båˆ†åˆ«ä¸ºï¼š $$w = 2x_1 + 0x_2 - 5x_3 = (1, 1)^T \\b = -3$$ åˆ†ç¦»è¶…å¹³é¢ï¼š$$x^{(1)} + x^{(2)} - 3 = 0$$ 5. Kernelsåœ¨ä¹‹å‰çš„çº¿æ€§å›å½’çš„ç« èŠ‚ä¸­ï¼Œæœ‰æåˆ°è¿‡polynomial regressionã€‚å‡è®¾xæ˜¯æˆ¿å­çš„é¢ç§¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªç‰¹å¾$x$, $x^2$, $x^3$æ¥æ„é€ ä¸€ä¸ªä¸‰æ¬¡å¤šé¡¹å¼ã€‚ è¿™é‡Œé¢æœ‰ä¸¤ä¸ªæ¦‚å¿µè¦åŒºåˆ†ä¸€ä¸‹ã€‚xä¸ºåŸå…ˆæˆ¿å­çš„é¢ç§¯ï¼Œæ˜¯å±æ€§(attributes)ã€‚é€šè¿‡è¿™ä¸ªå±æ€§xæ˜ å°„å‡ºæ¥çš„$x$, $x^2$, $x^3$å«åšç‰¹å¾(features)ã€‚åœ¨è¿™é‡Œä½¿ç”¨Ï•æ¥è¡¨ç¤ºä»å±æ€§åˆ°ç‰¹å¾çš„ç‰¹å¾æ˜ å°„(featuer mapping)ã€‚æ¯”å¦‚ï¼š$$Ï•(x) = \\begin{bmatrix} x \\ x^2 \\ x^3 \\end{bmatrix}$$é‚£ä¹ˆåœ¨SVMä¸­ï¼Œå¦‚ä½•ä½¿ç”¨è¿™ç§ç‰¹å¾æ˜ å°„å‘¢ï¼Ÿ é€šè¿‡ä¸Šé¢çŸ¥è¯†ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ‰€æœ‰å‡ºç°$âŸ¨x^{(i)}, x^{(j)}âŸ©$ æ›¿æ¢ä¸º $âŸ¨Ï•^{(i)}, Ï•^{(j)}âŸ©$ çœ‹ä¸Šå»å¥½åƒæˆ‘ä»¬æ—¢åœ¨SVMä¸­ä½¿ç”¨äº†ç‰¹å¾æ˜ å°„, åˆè§£å†³äº†æ•°æ®åœ¨ä½ç»´ç©ºé—´ä¸­çº¿æ€§ä¸å¯åˆ†çš„æƒ…å†µ. ä½†æ˜¯, è¿™é‡Œæœ‰ä¸ªé—®é¢˜. å¦‚æœæˆ‘ä»¬é€šè¿‡ç‰¹å¾æ˜ å°„å¾—åˆ°çš„$Ï•(x)$æ˜¯ä¸€ä¸ªå¾ˆé«˜ç»´ç”šè‡³æ˜¯æ— ç©·ç»´çš„, é‚£ä¹ˆè®¡ç®—$âŸ¨Ï•(x^{(i)}),Ï•(x^{(j)})âŸ©$å°±ä¸æ˜¯é‚£ä¹ˆç°å®äº†, è®¡ç®—æ—¶é—´ä¼šå¾ˆä¹…. è¿™é‡Œæˆ‘ä»¬å°±è¦å¼•å‡ºä¸€ä¸ªå«kernelsçš„æ¦‚å¿µ, å‡è®¾ Q: zåœ¨è¿™é‡Œä»£è¡¨çš„ä»€ä¹ˆï¼Ÿ $$K(x, z) = (x^Tz)^2 \\space \\space \\space \\space \\space \\space \\space \\space x, z \\in R^b$$ å±•å¼€K(x, z):$$K(x, z) = (\\sum_{i=1}^n x_iz_i) (\\sum_{j=1}^n x_iz_i) \\= \\sum_{i=1}^n \\sum_{j=1}^n x_ix_j z_i z_j \\= \\sum_{i,j=1}^n (x_ix_j) (z_iz_j)$$å±•å¼€åæˆ‘ä»¬å‘ç°ï¼ŒK(x, z)è¿˜å¯ä»¥å†™æˆ$K(x, z) = Ï•(x)^T Ï•(z)$ï¼Œå…¶ä¸­ï¼š$$Ï•(x) = \\begin{bmatrix} x _1x_1 \\ x _1x_2 \\x _1x_3 \\ x _2x_1 \\ x _2x_2 \\ x _2x_3 \\ x _3x_1 \\ x _3x_2 \\ x _3x_3 \\ \\end{bmatrix}$$åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ˜ å°„åç‰¹å¾çš„å†…ç§¯å’ŒåŸå§‹ç‰¹å¾çš„å†…ç§¯çš„å¹³æ–¹æ˜¯ç­‰ä»·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬åªéœ€è¦è®¡ç®—åŸå§‹ç‰¹å¾çš„å†…ç§¯å†è¿›è¡Œå¹³æ–¹å°±å¯ä»¥äº†ï¼Œå¹¶ä¸éœ€è¦å…ˆå¾—åˆ°æ˜ å°„åå†è®¡ç®—æ˜ å°„åç‰¹å¾çš„å†…ç§¯ã€‚è®¡ç®—åŸå§‹ç‰¹å¾å†…ç§¯çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n)ï¼Œè€Œè®¡ç®—æ˜ å°„ç‰¹å¾çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n^2)ã€‚ å†çœ‹å¦å¤–ä¸€ä¸ªkernels$$K(x, z) = (x^T z + c)^2 \\= \\sum_{i,j=1}^n (x_ix_j) (z_iz_j) + \\sum_{i=1}^n (\\sqrt {scx_i})( \\sqrt {scx_j} ) + c^2$$åŒæ ·æˆ‘ä»¬ä¹Ÿå¯ä»¥æ”¹å†™ä¸Šé¢çš„å¼å­ æ‰€ä»¥å¹¿æ³›çš„æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰$$K(x, z) = (x^T z + c)^d$$è¿™ä¸ªkernelå°†nç»´çš„ç‰¹å¾æ˜ å°„ä¸º(d, n+d)ç»´ï¼Œå³è¿™é‡Œé¢å¯¹åº”çš„å¤šé¡¹å¼$x_{i1}$, $x_{i2}$, â€¦, $x_{ik}$æœ€å¤šåˆ°dç»´ã€‚å°½ç®¡ç©ºé—´ç»´åº¦ä¸ºO(n^d)ï¼Œä½†è®¡ç®—æ—¶é—´ä»ç„¶åªæ˜¯O(n)ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸éœ€è¦å°†æ˜ å°„åçš„ç‰¹å¾å…¨éƒ¨è®¡ç®—å‡ºæ¥å†è®¡ç®—å†…ç§¯ã€‚ ä½†å› ä¸ºè®¡ç®—çš„æ˜¯å†…ç§¯ï¼Œæœ‰IRä¸­çš„ä½™å¼¦ç›¸ä¼¼åº¦å¯å­©å­ï¼Œå¦‚æœxå’Œzçš„å‘é‡å¤¹è§’è¶Šå°ï¼Œé‚£ä¹ˆæ ¸å‡½æ•°çš„å€¼å°±è¶Šå¤§ã€‚åä¹‹å°±è¶Šå°ã€‚å› æ­¤æ ¸å‡½æ•°å€¼æ˜¯Ï•(x)å’ŒÏ•(z)ç›¸ä¼¼åº¦ã€‚ å†çœ‹å¦å¤–ä¸€ä¸ªå¾ˆå‡½æ•°$$K(x, z) = exp(\\frac {||x - z||^2} {2\\sigma^2})$$åœ¨è¿™ä¸ªæ ¸å‡½æ•°ä¸­ï¼Œå¦‚æœxå’Œzå¾ˆç›¸è¿‘ï¼Œåˆ™(||x-z||~= 0)ï¼Œé‚£ä¹ˆæ ¸å‡½æ•°å€¼ä¸º1. å¦‚æœç›¸å·®å¾ˆå¤§ï¼Œåˆ™(||x-z||&gt;&gt; 0), é‚£ä¹ˆå¾ˆå‡½æ•°çš„å€¼çº¦ç­‰äº0. ç”±äºè¿™ä¸ªæ ¸å‡½æ•°ç±»ä¼¼äºé«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤æˆä¸ºé«˜æ–¯æ ¸å‡½æ•°ï¼Œä¹Ÿå«åšå¾„å‘åŸºå‡½æ•°(Radial Basis Function ç®€ç§°RBF)ï¼Œå®ƒèƒ½å¤ŸæŠŠåŸå§‹ç‰¹å¾æ˜ å°„åˆ°æ— ç©·ç»´ã€‚ ç±»ä¼¼äºé«˜æ–¯å¾ˆå‡½æ•°ï¼Œæ¯”è¾ƒxå’Œzçš„ç›¸ä¼¼åº¦ï¼Œå¹¶æ˜ å°„åˆ°0~1ä¹‹é—´ã€‚logistic Regressionä¸­ï¼Œ sigmoidå‡½æ•°ä¹Ÿå¯ä»¥ï¼Œæ‰€ä»¥è¿˜æœ‰sigmoidæ ¸å‡½æ•°ã€‚$$K(x, z) = tanh(\\beta Â· xz + b)$$ä¸‹é¢æœ‰å¼ å›¾è¯´æ˜åœ¨ä½ç»´çº¿æ€§ä¸å¯åˆ†æ—¶ï¼Œæ˜ å°„åˆ°é«˜ç»´åå°±å¯åˆ†äº†ï¼Œä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°ã€‚ åœ¨SVMä¸­ï¼Œå¯¹äºè®­ç»ƒæ ·æœ¬å­¦ä¹ å‡ºwå’Œbå‚æ•°åï¼Œå¯¹äºæ–°æ¥çš„æ ·æœ¬æˆ‘ä»¬åªéœ€è¦è®¡ç®—$w^Tx + b$æ¥åˆ¤æ–­ã€‚é‚£ä¹ˆåœ¨ä½¿ç”¨äº†æ ¸å‡½æ•°ä¹‹åï¼Œåˆ™éœ€è¦ç›¸åº”çš„æ”¹ä¸º$w^TÏ•(x) + b$ é‚£ä¹ˆæ˜¯éœ€è¦å…ˆè®¡ç®—å¥½Ï•(x)å†è¿›è¡Œé¢„æµ‹å‘¢ï¼Ÿå®é™…ä¸Šä¸éœ€è¦çš„, ä¹‹å‰è®¡ç®—è¿‡$$w^Tx + b = (\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)})^T x + b \\= \\sum_{i=1}^m \\alpha_i y^{(i)} âŸ¨x^{(i)}, xâŸ© + b$$æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦å°†$$âŸ¨x^{(i)}, xâŸ© \\æ›¿æ¢ä¸º \\K(x^{(i)}, x)$$ 6. æ ¸å‡½æ•°çš„æœ‰æ•ˆæ€§åˆ¤æ–­æ ¸å‡½æ•°çš„æœ‰æ•ˆæ€§ï¼Œå³åˆ¤æ–­æ˜¯å¦å­˜åœ¨Ï•, ä½¿å¾—ä¸‹é¢å¼å­æˆç«‹$$K(x,z)=âŸ¨Ï•(x)Ï•(z)âŸ©$$å‡è®¾æˆ‘ä»¬æœ‰æ ¸Kå’Œmä¸ªè®­ç»ƒæ ·æœ¬{x(1),x(2),â€¦,x(m)}, å®šä¹‰ä¸€ä¸ª (mÃ—m) çš„çŸ©é˜µK$$K_{ij} = K(x^{(i)}, x^{(j)})$$å¦‚æœæ­¤æ—¶Kæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„kerneï¼Œé‚£ä¹ˆåˆ™æœ‰ï¼š$$K_{ij} = k(x^{(i)}, x^{(j)}) = Ï•(x^{(i)})Ï•(x^{(i)}) \\= Ï•(x^{(j)})^T Ï•(x^{(i)}) = K(x^{(j)}, x^{(i)}) = K_{ji}$$å³Kæ˜¯å¯¹ç§°çŸ©é˜µã€‚ç°åœ¨æˆ‘ä»¬ç”¨Ï•k(x)ä¸æ¾³æ˜¯å‘é‡Ï•(x)çš„ç¬¬kä¸ªå…ƒç´ ï¼Œå¯¹ä»»æ„çš„å‘é‡zéƒ½æœ‰ï¼š ä»ä¸Šé¢çš„è¯æ˜æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼Œå¦‚æœKæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„kernelï¼Œé‚£ä¹ˆå¯¹äºåœ¨è®­ç»ƒé›†ä¸Šçš„æ ¸çŸ©é˜µKä¸€ç‚¹æ˜¯åŠæ­£å®šçš„ã€‚äº‹å®ä¸Šï¼Œè¿™ä¸ä»…ä»…æ˜¯ä¸ªå¿…è¦æ¡ä»¶ï¼Œä¹Ÿæ˜¯å……åˆ†æ¡ä»¶ã€‚æœ‰æ•ˆæ ¸ä¹Ÿå«åšMercer Kernel 7. Reference http://zhihaozhang.github.io/2014/05/11/svm3/ http://www.cnblogs.com/bourneli/p/4199990.html http://www.cnblogs.com/90zeng/p/Lagrange_duality.html","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Machine Learning - Ensemble Learning","date":"2017-06-13T04:27:45.000Z","path":"2017/06/13/Machine-Learning-Ensemble-Learning/","text":"1. é›†æˆå­¦ä¹  (Ensemble Learning)åœ¨å¯¹æ–°çš„æ•°æ®å®ä¾‹è¿›è¡Œåˆ†ç±»çš„æ—¶å€™ï¼Œé›†æˆå­¦ä¹ é€šè¿‡è®­ç»ƒå¥½å¤šä¸ªå­¦ä¹ å™¨ï¼ŒæŠŠè¿™äº›åˆ†ç±»å™¨çš„çš„åˆ†ç±»ç»“æœè¿›è¡ŒæŸç§ç»„åˆ (æ¯”å¦‚æŠ•ç¥¨) å†³å®šåˆ†ç±»ç»“æœï¼Œä»¥å–å¾—æ›´å¥½çš„ç»“æœï¼Œå°±æ˜¯æˆ‘ä»¬ç”Ÿæ´»ä¸­é‚£å¥è¯â€œä¸‰ä¸ªè‡­çš®åŒ é¡¶ä¸ªè¯¸è‘›äº®â€ï¼Œé€šè¿‡ä½¿ç”¨å¤šä¸ªå†³ç­–è€…å…±åŒå†³ç­–ä¸€ä¸ªå®ä¾‹çš„åˆ†ç±»ä»è€Œæé«˜åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ åŒè´¨é›†æˆ (Homogeneous) éœ€è¦æ˜¯åŒç§ç±»å‹ï¼Œæ¯”å¦‚å…¨éƒ¨æ˜¯å†³ç­–æ ‘æˆ–ç¥ç»ç½‘ç»œç­‰ï¼Œæ¯ä¸ªä¸ªä½“å­¦ä¹ å™¨ç§°ä¹‹ä¸ºåŸºå­¦ä¹ å™¨ (base learner)ï¼Œç›¸åº”çš„å­¦ä¹ ç®—æ³•ç§°ä¸ºåŸºå­¦ä¹ ç®—æ³• (base learning algorithm) å¼‚è´¨é›†æˆ (Heterogenous) å¯ä»¥ç”±ä¸åŒçš„å­¦æ ¡ç®—æ³•ç”Ÿæˆï¼Œè¿™æ—¶å€™å°±ä¸å†æœ‰åŸºå­¦ä¹ ç®—æ³•ï¼Œæ¯ä¸ªä¸ªä½“å­¦ä¹ å™¨ç§°ä¸ºç»„ä»¶å­¦ä¹ å™¨ (component learner) ä»¥ä¸‹æˆ‘ä»¬é‡ç‚¹è®¨è®ºåŒè´¨é›†æˆã€‚ 2. åˆ†ç±»å™¨çš„é€‰æ‹© å·®å¼‚æ€§ é—®é¢˜ï¼šå¦‚ä½•é€‰æ‹©/æ„å»ºå·®å¼‚æ€§çš„åŸºåˆ†ç±»å™¨ï¼Ÿ(ans: section 3) ç²¾åº¦ &gt; 0.5 ç²¾åº¦ç•¥é«˜äº50%çš„åˆ†ç±»å™¨ç§°ä¹‹ä¸ºå¼±å­¦ä¹ å™¨ (weak learner) é—®é¢˜ï¼šå¦‚ä½•æŠ•ç¥¨é€‰æ‹©å‡ºæœ€ä½³çš„é¢„æµ‹ï¼Ÿ(ans: section 4) å¦‚ä½•ç»„åˆ å‡è®¾åœ¨äºŒåˆ†ç±»å™¨ä¸­ï¼Œè¿™é‡Œæœ‰ä¸‰ä¸ªåˆ†ç±»å™¨åœ¨æµ‹è¯•ä¸‰ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªåˆ†ç±»å™¨çš„æ­£ç¡®ç‡éƒ½æ˜¯66%ï¼Œé‚£ä¹ˆç»„åˆå‡ºçš„ç»“æœå¤§è‡´å¯ä»¥åˆ†æˆä»¥ä¸‹å‡ ç§ é›†æˆæ€§èƒ½æå‡ | | test1 | test2 | test3 || :â€“: | :â€”: | :â€”: | :â€”: || h1 | right | right | wrong || h2 | wrong | right | right || h3 | right | wrong | right | å¦‚æœåªæ˜¯ç®€å•æŠ•ç¥¨æ³•çš„è¯ï¼Œåœ¨æ¯ä¸ªtestä¸­ï¼Œæœ‰ä¸¤ä¸ªåˆ†ç±»å™¨æ˜¯å¯¹çš„ï¼Œä¸€ä¸ªæ˜¯é”™çš„ï¼Œé‚£ä¹ˆæŠ•ç¥¨å‡ºæ¥çš„ç»“æœæ˜¯å¯¹çš„ï¼Œæœ€ç»ˆç²¾åº¦å¯ä»¥è¾¾åˆ°100%ã€‚ é›†æˆä¸èµ·ä½œç”¨ | | test1 | test2 | test3 || :â€“: | :â€”: | :â€”: | :â€”: || h1 | right | right | wrong || h2 | right | right | wrong || h3 | right | right | wrong | ä¸‰ä¸ªåˆ†ç±»å™¨å¯¹ä¸‰ä¸ªtestè¿›è¡Œé¢„æµ‹ï¼Œæ°å¥½ä¸‰ä¸ªåˆ†ç±»å™¨å¯¹test3çš„æƒ…å†µåˆ†ç±»éƒ½æ˜¯é”™çš„ï¼Œè€Œå¯¹test1å’Œtest2çš„ç»“æœéƒ½é¢„æµ‹æ­£ç¡®ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„æµ‹è¯•ç»“æœæ²¡æœ‰å½±å“ï¼Œéƒ½æ˜¯66%ã€‚ é›†æˆèµ·è´Ÿä½œç”¨ | | test1 | test2 | test3 || â€”- | â€”â€“ | â€”â€“ | â€”â€“ || h1 | right | wrong | wrong || h2 | wrong | right | wrong || h3 | wrong | wrong | right | å’Œç¬¬ä¸€ç§æƒ…å†µç›¸åï¼Œæœ€ç»ˆçš„æµ‹è¯•ç»“æœæ˜¯33% é€šè¿‡ä¸Šé¢çš„ä¾‹å­ï¼Œå¯ä»¥åæ˜ å‡ºä¸€ä¸ªé—®é¢˜ï¼Œåœ¨åŒè´¨é›†æˆä¸­ï¼Œå¦‚ä½•æ„å»ºå¤šä¸ªè¯¯å·®æ˜¯ç›¸äº’ç‹¬ç«‹çš„åŸºå­¦ä¹ å™¨ã€‚å› ä¸ºè¿™äº›åŸºå­¦ä¹ å™¨æ˜¯ç”¨çš„åŒä¸€ç§ç®—æ³•ï¼ŒåŸºæœ¬ä¹Ÿæ˜¯ä½¿ç”¨åŒä¸€ç»„æ•°æ®ï¼Œè§£å†³çš„ä¹Ÿæ˜¯åŒä¸€ä¸ªé—®é¢˜ã€‚ æ ¹æ®ä¸ªä½“å­¦æœŸçš„ç”Ÿæˆæ–¹å¼ï¼Œé›†æˆå­¦ä¹ æ–¹æ³•å¤§è‡´å¯ä»¥åˆ†æˆä¸¤å¤§ç±»ï¼š Boosting ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œå¿…é¡»ä¸²è¡Œç”Ÿæˆçš„åºåˆ—åŒ–æ–¹æ³• Bagging å’Œ Random Forest ä¸ªä½“å­¦ä¹ å™¨ä¹‹å‰ä¸å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œå¯åŒæ—¶ç”Ÿæˆçš„å¹¶è¡ŒåŒ–æ–¹æ³• 3. æ„å»ºå·®å¼‚æ€§åŸºåˆ†ç±»å™¨åœ¨åŒä¸€ä¸ªæ•°æ®é›†ä¸Šï¼Œæ„å»ºä¸åŒçš„ï¼Œå…·æœ‰å·®å¼‚æ€§çš„åˆ†ç±»å™¨ (å¥½è€Œä¸åŒ)ï¼Œå°±æ˜¯é€šè¿‡æŠ½æ ·æŠ€æœ¯è·å–å¤šä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œç”Ÿæˆå¤šä¸ªå·®å¼‚æ€§åˆ†ç±»å™¨ã€‚ç›®å‰ä¸»è¦çš„æ–¹æ³•æœ‰ï¼šBagging å’Œ Boostingã€‚ 3.1 Boostingæå‡æ–¹æ³•æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œé€šè¿‡æ”¹å˜æ ·æœ¬åˆ†å¸ƒï¼Œä½¿å¾—åˆ†ç±»å™¨èšé›†åœ¨é‚£äº›å¾ˆéš¾åˆ†çš„æ ·æœ¬ä¸Šï¼Œå¯¹é‚£äº›å®¹æ˜“é”™åˆ†çš„æ•°æ®åŠ å¼ºå­¦ä¹ ï¼Œå¢åŠ é”™åˆ†æ•°æ®çš„æƒé‡ï¼Œè¿™æ ·é”™åˆ†çš„æ•°æ®å†ä¸‹ä¸€è½®çš„è¿­ä»£å°±æœ‰æ›´å¤§çš„ä½œç”¨ (å¯¹é”™åˆ†æ•°æ®è¿›è¡Œæƒ©ç½š)ã€‚ å…·ä½“æ¥è¯´ï¼Œå…ˆä»åˆå§‹è®­ç»ƒé›†è®­ç»ƒå‡ºä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼Œå†æ ¹æ®è¿™ä¸ªåŸºå­¦ä¹ å™¨çš„è¡¨ç°å¯¹è®­ç»ƒæ ·æœ¬åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—å…ˆå‰åŸºå­¦ä¹ å™¨åšé”™çš„è®­ç»ƒæ ·æœ¬åœ¨åç»­å—åˆ°æ›´å¤šçš„å…³æ³¨ï¼Œç„¶ååŸºäºè°ƒæ•´åçš„æ ·æœ¬åˆ†å¸ƒæ¥è®­ç»ƒä¸‹ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›å¦‚æ­¤é‡å¤è¿›è¡Œï¼Œä¸€ç›´åˆ°åŸºå­¦ä¹ å™¨çš„æ•°ç›®åˆ°è¾¾äº‹å…ˆæŒ‡å®šçš„Tå€¼ï¼Œç„¶åå°†æ‰€æœ‰çš„åŸºå­¦ä¹ å™¨ç›¸ç»“åˆã€‚(é—®é¢˜ï¼šå¦‚ä½•è°ƒæ•´åˆ†å¸ƒï¼Ÿ) æ•°æ®çš„æƒé‡æœ‰ä¸¤ä¸ªä½œç”¨ï¼Œä¸€æ–¹é¢æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æƒå€¼ä½œä¸ºæŠ½æ ·åˆ†å¸ƒï¼Œè¿›è¡Œå¯¹æ•°æ®çš„æŠ½æ ·ï¼Œå¦ä¸€æ–¹é¢åˆ†ç±»å™¨å¯ä»¥ä½¿ç”¨æƒå€¼å­¦ä¹ æœ‰åˆ©äºé«˜æƒé‡æ ·æœ¬çš„åˆ†ç±»å™¨ã€‚æŠŠä¸€ä¸ªå¼±åˆ†ç±»å™¨æå‡ä¸ºä¸€ä¸ªå¼ºåˆ†ç±»å™¨ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒAdaBoostç®—æ³• (è¥¿ç“œä¹¦P173) Example å‡è®¾ç°åœ¨æœ‰ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨å¦‚ä¸‹è¡¨ | | é¢„æµ‹ + | é¢„æµ‹ - | count || :â€”â€”-: | :â€“: | :â€“: | :â€”: || å®é™… + | 24 | 16 | 40 || å®é™… - | 9 | 51 | 60 || count | 33 | 67 | 100 | é”™è¯¯ç‡è®¡ç®—$$\\epsilon = \\frac {(9 + 16)} {100} = 0.25$$ é”™åˆ†æ ·æœ¬çš„æƒå€¼æ›´æ–°$$w_e = \\frac 1 {2*\\epsilon} = 2$$ æ­£ç¡®æ ·æœ¬çš„æƒå€¼æ›´æ–°$$w_r = \\frac 1 {2*(1 - \\epsilon)} = \\frac 2 3$$ æ‰€ä»¥è®²æƒé‡ä¹˜ä»¥ç›¸åº”çš„æ•°æ®é‡ï¼Œå¾—åˆ°æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œå¦‚ä¸‹ é¢„æµ‹ + é¢„æµ‹ - count å®é™… + 24 * 2/3 = 16 16 * 2 = 32 16+32=48 å®é™… - 9 * 2 = 18 51 * 2/3 = 34 18 + 34 = 52 count 16 + 18 = 34 32 + 34 = 66 100 3.2 Baggingé€šè¿‡å¯¹åŸæ•°æ®é›†è¿›è¡Œæœ‰æ”¾å›çš„é‡‡æ · (bootstrap sampling) æ„å»ºå‡ºå¤§å°å’ŒåŸæ•°æ®é›†å¤§å°ä¸€æ ·çš„æ–°æ•°æ®é›†D1ï¼ŒD2ï¼ŒD3â€¦..ï¼Œç„¶åç”¨è¿™äº›æ–°çš„æ•°æ®é›†è®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨H1ï¼ŒH2ï¼ŒH3â€¦.ã€‚å› ä¸ºæ˜¯æœ‰æ”¾å›çš„é‡‡æ ·æ‰€ä»¥ä¸€äº›æ ·æœ¬å¯èƒ½ä¼šå‡ºç°å¤šæ¬¡ï¼Œè€Œå…¶ä»–æ ·æœ¬ä¼šè¢«å¿½ç•¥ï¼Œç†è®ºä¸Šåˆå§‹è®­ç»ƒé›†ä¸­çº¦æœ‰63.2%çš„æ ·æœ¬ä¼šå‡ºç°åœ¨é‡‡æ ·é›†ä¸­ã€‚(è¥¿ç“œä¹¦P27) è‡ªåŠ©é‡‡æ ·æ³• (Bootstrap sampling) å‡è®¾ç»™å®šä¸€ä¸ªåŒ…å«äº†mä¸ªæ ·æœ¬çš„æ•°æ®é›†Dï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæ–°çš„æ•°æ®D1ï¼Œå¤§å°å’ŒDä¸€æ ·çš„ã€‚é‚£ä¹ˆæ¯æ¬¡æˆ‘ä»¬ä»Dä¸­å–å‡ºä¸€ä¸ªæ ·æœ¬å…¥åˆ°D1ä¸­ï¼Œä¹‹åæŠŠæ ·æœ¬æ”¾å›Dä¸­ï¼Œé‡æ–°é‡‡æ ·ï¼Œæ€»å…±é‡‡æ ·mæ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸ºmçš„D1ã€‚å› ä¸ºæˆ‘ä»¬æŠŠæ ·æœ¬åˆé‡æ–°æ”¾å…¥Dä¸­ï¼Œä¸‹æ¬¡è¿˜æœ‰å¯èƒ½æŠ½å–åˆ°ï¼Œå¯¼è‡´åœ¨D1ä¸­ï¼Œæœ‰äº›æ ·æœ¬ä¼šå‡ºç°å¤šæ¬¡ï¼Œè€Œæœ‰äº›æ ·æœ¬åˆ™ä¸€æ¬¡ä¹Ÿä¸ä¼šå‡ºç°ã€‚æ‰€ä»¥æ ¹æ®å¦‚ä¸‹å…¬å¼ï¼š$$lim_{m -&gt; \\infty} (1 - \\frac 1 m)^m -&gt; \\frac 1 e \\approx 0.368$$æœ€ç»ˆåœ¨åˆå§‹æ ·æœ¬æ•°æ®é›†Dä¸­ï¼Œæœ‰36.8%çš„æ ·æœ¬æ˜¯ä¸€æ¬¡ä¹Ÿæ²¡æœ‰å‡ºç°çš„ï¼Œè€Œ63.2%é‡å¤å‡ºç°äº†ï¼Œè¿™æ ·æˆ‘ä»¬å°±æ”¹å˜äº†åˆå§‹æ ·æœ¬æ•°æ®é›†çš„åˆ†å¸ƒã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†D1ä½œä¸ºè®­ç»ƒé›†ï¼ŒD\\D1ï¼Œå³å‰©ä¸‹çš„36.8%æœªå‡ºç°åœ¨D1ä¸­çš„ä½œä¸ºæµ‹è¯•é›†ã€‚è¿™æ ·çš„æµ‹è¯•ç»“æœç§°ä¹‹ä¸ºåŒ…å¤–ä¼°è®¡ (out-of-bag-estimate) Baggingé€šè¿‡é™ä½åŸºåˆ†ç±»å™¨æ–¹å·®æ”¹å–„äº†æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤Baggingçš„æ€§èƒ½ä¾èµ–äºåŸºåˆ†ç±»å™¨çš„ç¨³å®šæ€§ï¼Œå¦‚æœåŸºåˆ†ç±»å™¨æ˜¯ä¸ç¨³å®šçš„ï¼ŒBaggingæœ‰åŠ©äºå‡ä½è®­ç»ƒæ•°æ®çš„éšæœºæ‰°åŠ¨å¯¼è‡´çš„è¯¯å·®ï¼Œä½†æ˜¯å¦‚æœåŸºåˆ†ç±»å™¨æ˜¯ç¨³å®šçš„ï¼Œå³å¯¹æ•°æ®å˜åŒ–ä¸æ•æ„Ÿï¼Œé‚£ä¹ˆBaggingæ–¹æ³•å°±å¾—ä¸åˆ°æ€§èƒ½çš„æå‡ï¼Œç”šè‡³ä¼šå‡ä½ï¼Œå› ä¸ºæ–°æ•°æ®é›†åªæœ‰63%ã€‚ 3.3 Random Forestéšæœºæ£®æ—æ˜¯Baggingçš„ä¸€ä¸ªæ‹“å±•å˜ä½“ï¼ŒåŸºäºBaggingæ¡†æ¶è¿›ä¸€æ­¥é™ä½äº†äº†æ¨¡å‹çš„æ–¹å·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨çš„å†³ç­–æ ‘ä¸ä¼ ç»Ÿçš„å†³ç­–æ ‘ç•¥æœ‰ä¸åŒã€‚ ä¼ ç»Ÿçš„å†³ç­–æ ‘ï¼Œæ¯”å¦‚C45æˆ–è€…CARTï¼Œæ¯å½“éœ€è¦åˆ’åˆ†ä¸€ä¸ªå±æ€§çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šå°†æ‰€æœ‰çš„å±æ€§çš„éƒ½è¿›è¡Œåˆ’åˆ†ä¸€éï¼Œç„¶åè®¡ç®—å…¶åˆ’åˆ†ä¹‹åä¸åˆ’åˆ†ä¹‹å‰ç†µçš„åå·®ï¼Œæˆ–è€…è®¡ç®—GiniæŒ‡æ•°ç­‰ï¼Œç„¶åä»ä¸­é€‰å–å‡ºæœ€ä½³å±æ€§è¿›è¡Œåˆ’åˆ†ã€‚ è€Œéšæœºæ£®æ—ï¼Œå¯¹åŸºå†³ç­–æ ‘çš„æ¯ä¸ªç»“ç‚¹ï¼Œå…ˆä»è¯¥èŠ‚ç‚¹çš„å±æ€§é›†åˆä¸­éšæœºé€‰æ‹©ä¸€ä¸ªåŒ…å«kä¸ªå±æ€§çš„å­é›†ï¼Œç„¶åå†ä»è¿™ä¸ªå­é›†ä¸­é€‰å–ä¸€ä¸ªæœ€ä¼˜å±æ€§ç”¨äºåˆ’åˆ† (ä»ç‰¹å¾çš„ä¸åŒå­é›†æ¥æ„å»ºæ ‘)ã€‚è¿™é‡Œçš„å‚æ•°kæ§åˆ¶äº†éšæœºæ€§çš„å¼•å…¥ç¨‹åº¦ï¼š è‹¥k=dï¼Œåˆ™åŸºå†³ç­–æ ‘å’Œä¼ ç»Ÿå†³ç­–æ ‘ç›¸åŒã€‚ è‹¥k=1ï¼Œåˆ™æ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªå±æ€§ç”¨äºåˆ’åˆ† æ¨èå€¼k=log_2 d éšæœºæ£®æ—é™¤äº†åŸºäºBaggingä¸­å¯¹äºæ ·æœ¬çš„æ‰°åŠ¨ï¼Œè¿˜åŒæ—¶å¯¹æ•°æ®é›†ä¸­çš„å±æ€§åˆ’åˆ†è¿›è¡Œäº†æ‰°åŠ¨ï¼Œè¿™æ ·å¯ä»¥å¢åŠ åŸºå­¦ä¹ å™¨çš„å¤šæ ·æ€§å’Œå·®å¼‚æ€§ï¼Œä½¿å¾—æœ€ç»ˆé›†æˆçš„æ³›åŒ–æ€§å¾—åˆ°æå‡ã€‚ 3.4 å¢å¼ºå¤šæ ·æ€§ æ•°æ®æ ·æœ¬æ‰°åŠ¨ è¾“å…¥å±æ€§æ‰°åŠ¨ è¾“å‡ºè¡¨ç¤ºæ‰°åŠ¨ ç®—æ³•å‚æ•°æ‰°åŠ¨ 4. ç»„åˆç­–ç•¥4.1 å½’å›é¢„æµ‹ (æ•°å€¼é¢„æµ‹) ç®€å•å¹³å‡å€¼æ³• å°±æ˜¯å–å„ä¸ªåˆ†ç±»å™¨ç»“æœçš„å¹³å‡å€¼ï¼ˆä¼šä¸ä¼šé™ä½è¿™é‡Œé¢æœ€å¥½çš„é‚£ä¸ªåˆ†ç±»å™¨çš„ç²¾åº¦ï¼‰$$H(x) = \\frac 1 T \\sum_{i=1}^T h_i(x)$$ åŠ æƒå¹³å‡æ³• ç»™ä¸åŒçš„åˆ†ç¦»å™¨èµ‹äºˆä¸ä¸€æ ·çš„æƒé‡å€¼ï¼Œç„¶åæ±‚å’Œ$$H(x) = \\sum_{i=1}^Tw_i Â· h_i(x)$$ 4.2 åˆ†ç±»é¢„æµ‹ (ç±»åˆ«é¢„æµ‹) ç®€å•æŠ•ç¥¨æ³• æ¯ä¸ªåˆ†ç±»å™¨çš„æƒé‡å¤§å°éƒ½ä¸€æ ·ï¼Œå°‘æ•°æœä»å¤šæ•° ç»å¯¹å¤šæ•°æŠ•ç¥¨æ³•ç¥¨æ•°è¿‡åŠï¼Œå¦åˆ™æ‹’ç»é¢„æµ‹ ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•ç¥¨æ•°æœ€å¤šçš„é‚£ä¸ª åŠ æƒæŠ•ç¥¨æ³• ç»™æ¯ä¸ªåˆ†ç±»å™¨èµ‹äºˆä¸€ä¸ªæƒé‡ï¼Œç„¶åæ ¹æ®æƒé‡æ¥æŠ•ç¥¨ï¼Œå¾—åˆ°ç¥¨æ•°é«˜çš„æœ€ä¸ºè¾“å‡ºç»“æœ æ¦‚ç‡æŠ•ç¥¨æ³•ï¼ˆå’Œç®€å•åˆ†ç±»æœ‰ä»€ä¹ˆä¸åŒï¼Ÿï¼‰ æœ‰çš„åˆ†ç±»å™¨çš„è¾“å‡ºæ˜¯æœ‰æ¦‚ç‡ä¿¡æ¯çš„ï¼Œæ¯”å¦‚ åˆ†ç±»å™¨Aè¾“å‡ºç»“æœ1çš„æ¦‚ç‡ä¸º75% åˆ†ç±»å™¨Bè¾“å‡ºç»“æœ0çš„æ¦‚ç‡ä¸º80% åˆ†ç±»å™¨Cè¾“å‡ºç»“æœ1çš„æ¦‚ç‡ä¸º52% æœ€ç»ˆè¾“å‡ºçš„ç»“æœä¸ºï¼Ÿï¼Ÿï¼Ÿï¼ˆæ¦‚ç‡ç›¸åŠ è¿˜æ˜¯æ¦‚ç‡çš„å¹³å‡å€¼ï¼Œç›¸åŠ å¯èƒ½æ€§é«˜ï¼‰ 4.3 å­¦ä¹ æ³•5. References è¥¿ç“œä¹¦ç¬¬å…«ç«  - å‘¨å¿—å éå‚è€ƒï¼Œæ¨èé˜…è¯» å°èœé¸Ÿå¯¹å‘¨å¿—åå¤§ç¥gcForestçš„ç†è§£ gcForestç®—æ³•ç†è§£","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"Ensemble","slug":"Ensemble","permalink":"http://chenson.com/tags/Ensemble/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸‰ï¼‰","date":"2017-06-09T06:08:52.000Z","path":"2017/06/09/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸‰ï¼‰/","text":"1. ä¸ºä»€ä¹ˆè¦ä½¿ç”¨Secondary Sortåœ¨Hadoopä¸­ï¼Œä»Mapåˆ°Reduceçš„è¿‡ç¨‹ä¸­ï¼Œkeyæ˜¯ä¸æ–­è¢«sortçš„ã€‚æ‰€ä»¥ä»mapå‡ºæ¥çš„æ—¶å€™å†™å…¥åˆ°ä¸€ä¸ªintermediate output fileçš„æ—¶å€™ï¼Œkeyæ˜¯æœ‰åºçš„ã€‚ç„¶åreduceä¸æ–­çš„ä»ä¸åŒçš„clusteré‡Œé¢fetché‡Œé¢çš„key-value pairsçš„æ—¶å€™ï¼Œä»ç„¶å¤šæ¬¡sortè¿™äº›pairsã€‚æ‰€ä»¥æœ€ç»ˆè¿›å…¥åˆ°reducerçš„keyæ˜¯æœ‰åºçš„ï¼Œä½†æ˜¯valueæ˜¯æ— åºçš„ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦å¯¹valueä¹Ÿè¿›è¡Œæ’åºå‘¢ï¼ŸGoogleçš„MRå†…ç½®äº†å‡½æ•°å¯¹valueä¹Ÿå¯ä»¥æ’åºï¼Œä½†æ˜¯Hadoopä¸è¡Œï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±å»å®šåˆ¶partitionerç­‰å»å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚ ä¸¾ä¸ªæ —å­ï¼š è¾“å…¥æ–‡ä»¶æ ¼å¼å¦‚ä¸‹ 12345678910112015,1,242015,3,542015,1,32015,2,-432015,4,52015,3,462014,2,642015,1,42015,1,212015,2,352015,2,20 â€‹ æœŸæœ›çš„è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼ˆvalueæ˜¯æœ‰åºçš„ï¼‰ 123452014-2 642015-1 3ï¼Œ4ï¼Œ21ï¼Œ242015-2 -43ï¼Œ0ï¼Œ352015-3 46ï¼Œ562015-4 5 â€‹ Hadoopé»˜è®¤çš„è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼ˆvalueæ˜¯æ— åºçš„ï¼‰ 123452014-2 642015-1 21ï¼Œ4ï¼Œ3ï¼Œ242015-2 0ï¼Œ35ï¼Œ-432015-3 56ï¼Œ462015-4 5 2. è§£å†³æ–¹æ¡ˆ ä¼ ç»Ÿçš„è§£å†³æ–¹æ³• å°±æ˜¯è¿›å…¥åˆ°åŒä¸€ä¸ªreducerçš„æ—¶å€™ï¼Œè¿™äº›åŒä¸€ä¸ªå¯ä»¥çš„valuesæ˜¯åœ¨ä¸€ä¸ªlisté‡Œé¢çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å…ˆæŠŠè¿™ä¸ªlisté‡Œé¢çš„valueå­˜åˆ°å†…å­˜ä¸­å»ï¼Œç„¶ååœ¨å†…å­˜ä¸­å°†è¿™äº›valueæ’åºã€‚è¿™ä¸ªæ–¹æ³•åªé€‚ç”¨äºæ•°æ®é‡è¾ƒå°çš„æ—¶å€™ï¼Œå½“æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™ï¼Œå†…å­˜å¹¶ä¸èƒ½åŒæ—¶å­˜å…¥è¿™äº›valuesï¼Œç¨‹åºå°±æŠ¥é”™æ— æ³•æ­£å¸¸è¿è¡Œã€‚ åˆ©ç”¨Hadoopç‰¹ç‚¹çš„æ–¹æ³• æ—¢ç„¶Hadoopå¯ä»¥å¯¹Keyè¿›è¡Œæ’åºï¼Œé‚£ä¹ˆå¯ä»¥åˆ©ç”¨è¿™ç‚¹ï¼Œå°†ä¹‹å‰çš„key-value pairs ç»„æˆä¸€ä¸ªæ–°çš„keyï¼Œè®©Hadoopå¯¹è¿™äº›Keyè¿›è¡Œæ’åºï¼Œæ —å­å¦‚ä¸‹ 12345// åŸå…ˆçš„key-value pair(2015,1) 21// æ–°çš„composite-key-value pair((2015,1),21) 21 è¿™é‡Œçš„(2015,1)æ˜¯æˆ‘ä»¬åŸå…ˆå°±æœ‰çš„keyï¼Œä¸ºäº†åŒºåˆ†æ–°çš„keyï¼Œå°†è¿™ä¸ªåŸæœ‰çš„key (k, v1) ,ç§°ä¸ºnatural keyï¼Œå°†æ–°çš„key ((k, v1), v2)ç§°ä¸ºcomposite keyã€‚å°†v2ç§°ä¸ºnatural valueã€‚å…·ä½“å¦‚ä¸‹å›¾ â€‹ ä¸ºäº†å®ç°Hadoopå¯¹æ–°çš„composite keyè¿›è¡Œæ’åºï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰partitionerå’Œgroupingæ¥ç¡®ä¿è¿™äº›composite keyä¸­natural keyç›¸åŒçš„ä¼šè¢«åˆ†é…åˆ°åŒä¸€ä¸ªreducerã€‚å› ä¸ºåœ¨composite keyä¸­ï¼Œå³ä½¿(k, v1)ç›¸åŒï¼Œåªè¦v2ä¸åŒï¼Œé»˜è®¤çš„partitionerå°±ä¼šè®¤ä¸ºè¿™æ˜¯ä¿©ä¸ªä¸åŒçš„keyï¼Œå°±å¾ˆæœ‰å¯èƒ½è®²è¿™ä¸ªcomposite keyåˆ†é…åˆ°ä¸åŒçš„reduceré‡Œå»ã€‚ â€‹ 3.å®ç°è¿‡ç¨‹ partitioner: å°†natural keyç›¸åŒçš„å‘é€åˆ°åŒä¸€ä¸ªreduceré‡Œå» åœ¨åŒä¸€ä¸ª[]é‡Œé¢è¯´æ˜æ˜¯ä¸€ä¸ªreducerï¼Œvalueä»»ç„¶æ˜¯æ— åºçš„ 123456789[((2014-2,64),64)][((2015-1,24),24), ((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,56),56), ((2015-3,46),46)][((2015-4,5),5)] â€‹ grouping comparator: å°†natural keyç›¸åŒçš„ä½œä¸ºä¸€ä¸ªgroupæ’åº 123456789[((2014-2,64),64)][((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21), ((2015-1,24),24)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,46),46), ((2015-3,56),56)][((2015-4,5),5)] â€‹ è¿›å…¥åˆ°Reducerçš„æ ¼å¼ (æœ‰ç‚¹é‡Œä¸ç†è§£æ­¤æ—¶composite keyé‡Œé¢çš„natural valueæ˜¯å¦‚ä½•ç¡®å®šçš„) 123456789((2014-2,64), (64))((2015-1,24), (2ï¼Œ4ï¼Œ21ï¼Œ24))((2015-2,35), (-43,0,35))((2015-3,46), (46,56))((2015-4,5), (5)) â€‹ æœ€ç»ˆçš„è¾“å‡º 12342014-2 642015-1 3,4,21,242015-2 -43,0,352015-4 5 æ•´ä¸ªæµç¨‹å›¾å¦‚ä¸‹ï¼ˆå›¾å†…æ•°æ®ä¸ä¸Šé¢æ•°æ®ä¸ç¬¦åˆï¼‰ 4. ä»£ç  composite key å°†æ—§çš„Keyï¼ˆnatural keyï¼‰å’ŒValueç»„åˆæˆæ–°çš„Keyï¼ˆcomposite keyï¼‰çš„ä»£ç  12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.secondarySort;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class Entry implements WritableComparable&lt;Entry&gt; &#123; private String yearMonth; private int count; public Entry() &#123; &#125; @Override public int compareTo(Entry entry) &#123; int result = this.yearMonth.compareTo(entry.getYearMonth()); if (result == 0) &#123; result = compare(count, entry.getCount()); &#125; return result; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(yearMonth); dataOutput.writeInt(count); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.yearMonth = dataInput.readUTF(); this.count = dataInput.readInt(); &#125; public String getYearMonth() &#123; return yearMonth; &#125; public void setYearMonth(String yearMonth) &#123; this.yearMonth = yearMonth; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public static int compare(int a, int b) &#123; return a &lt; b ? -1 : (a &gt; b ? 1 : 0); &#125; @Override public String toString() &#123; return yearMonth; &#125;&#125; â€‹ Partitioner 1234567891011package com.secondarySort; import org.apache.hadoop.mapreduce.Partitioner; public class EntryPartitioner extends Partitioner&lt;Entry, Integer&gt; &#123; @Override public int getPartition(Entry entry, Integer integer, int numberPartitions) &#123; return Math.abs((entry.getYearMonth().hashCode() % numberPartitions)); &#125;&#125; â€‹ Grouping Compartor 1234567891011121314151617package com.secondarySort; import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class EntryGroupingComparator extends WritableComparator &#123; public EntryGroupingComparator() &#123; super(Entry.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; Entry a1 = (Entry) a; Entry b1 = (Entry) b; return a1.getYearMonth().compareTo(b1.getYearMonth()); &#125;&#125; â€‹ Mapper 1234567891011121314151617181920212223public class SecondarySortMapper extends Mapper&lt;LongWritable, Text, Entry, Text&gt; &#123; private Entry entry = new Entry(); private Text value = new Text(); @Override protected void map(LongWritable key, Text lines, Context context) throws IOException, InterruptedException &#123; String line = lines.toString(); String[] tokens = line.split(\",\"); // YYYY = tokens[0] // MM = tokens[1] // count = tokens[2] String yearMonth = tokens[0] + \"-\" + tokens[1]; int count = Integer.parseInt(tokens[2]); entry.setYearMonth(yearMonth); entry.setCount(count); value.set(tokens[2]); context.write(entry, value); &#125;&#125; â€‹ Reducer 123456789101112public class SecondarySortReducer extends Reducer&lt;Entry, Text, Entry, Text&gt; &#123; @Override protected void reduce(Entry key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder builder = new StringBuilder(); for (Text value : values) &#123; builder.append(value.toString()); builder.append(\",\"); &#125; context.write(key, new Text(builder.toString())); &#125;&#125; â€‹ Deriver 123456789101112131415Configuration conf = new Configuration();Job job = Job.getInstance(conf);job.setJarByClass(Iteblog.class);job.setJobName(\"SecondarySort\"); FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setOutputKeyClass(Entry.class);job.setOutputValueClass(Text.class); job.setMapperClass(SecondarySortMapper.class);job.setReducerClass(SecondarySortReducer.class);job.setPartitionerClass(EntryPartitioner.class);job.setGroupingComparatorClass(EntryGroupingComparator.class); 5. å¸¸ç”¨çš„Secondary Sortä»£ç  IntPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; int first, second; // public IntPair() &#123;&#125; // // public IntPair(int left, int right) &#123; // set(left, right); // &#125; public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; public String toString()&#123; return \"(\" + first + \",\" + second + \")\"; &#125; @Override public void readFields(DataInput arg0) throws IOException &#123; // TODO Auto-generated method stub first = arg0.readInt(); second = arg0.readInt(); &#125; @Override public void write(DataOutput arg0) throws IOException &#123; // TODO Auto-generated method stub arg0.writeInt(first); arg0.writeInt(second); &#125; // å…³é”®ï¼šè‡ªå®šä¹‰ç±»å‹çš„æ¯”è¾ƒæ–¹æ³• @Override public int compareTo(IntPair arg0) &#123; // TODO Auto-generated method stub if (first != arg0.first) &#123; return first &lt; arg0.first ? -1 : 1; &#125; else if (second != arg0.second) &#123; return second &lt; arg0.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; public int hashCode() &#123; return first * 157 + second; &#125; public boolean equals(Object right) &#123; if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125;&#125;public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; &#123; // ç±»å‹è¦å’ŒMapperè¾“å‡ºçš„ä¸€æ · @Override public int getPartition(IntPair arg0, IntWritable arg1, int arg2) &#123; // TODO Auto-generated method stub return Math.abs((arg0.getFirst() * 127) % arg2); &#125;&#125;/* * ç¬¬ä¸€ç§æ–¹æ³•ï¼Œå®ç°æ¥å£RawComparator æ•°æ®ç±»å‹çš„æ¯”è¾ƒåœ¨MapReduceä¸­å¼åŠå…¶é‡è¦çš„, * Mapreduceä¸­æœ‰ä¸€ä¸ªæ’åºé˜¶æ®µï¼Œkeyå’Œå…¶ä»–çš„keyç›¸æ¯”è¾ƒã€‚ é’ˆå¯¹æ­¤ï¼ŒHadoop æä¾›çš„ä¸€ä¸ªä¼˜åŒ–æ˜¯ RawComparator * * public static class GroupingComparator implements RawComparator&lt;IntPair&gt;&#123; * * @Override public int compare(IntPair arg0, IntPair arg1) &#123; // TODO * Auto-generated method stub int l = arg0.getFirst(); int r = * arg0.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; * * @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int * s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, * Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; &#125; */// æ–¹æ³•äºŒpublic static class GroupingComparator extends WritableComparator &#123; protected GroupingComparator() &#123; super(IntPair.class, true);// è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•° &#125; public int compare(WritableComparable w1, WritableComparable w2) &#123; IntPair i1 = (IntPair) w1; IntPair i2 = (IntPair) w2; int l = i1.getFirst(); int r = i2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125;&#125; â€‹ TextPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package TextPair;// cc TextPair A Writable implementation that stores a pair of Text objects// cc TextPairComparator A RawComparator for comparing TextPair byte representations// cc TextPairFirstComparator A custom RawComparator for comparing the first field of TextPair byte representations// vv TextPairimport java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.io.WritableUtils;public class TextPair implements WritableComparable&lt;TextPair&gt; &#123; private Text first; private Text second; public TextPair() &#123; set(new Text(), new Text()); &#125; public TextPair(String first, String second) &#123; set(new Text(first), new Text(second)); &#125; public TextPair(Text first, Text second) &#123; set(first, second); &#125; public void set(Text first, Text second) &#123; this.first = first; this.second = second; &#125; public Text getFirst() &#123; return first; &#125; public Text getSecond() &#123; return second; &#125; @Override public void write(DataOutput out) throws IOException &#123; first.write(out); second.write(out); &#125; @Override public void readFields(DataInput in) throws IOException &#123; first.readFields(in); second.readFields(in); &#125; @Override public int hashCode() &#123; return first.hashCode() * 163 + second.hashCode(); &#125; @Override public boolean equals(Object o) &#123; if (o instanceof TextPair) &#123; TextPair tp = (TextPair) o; return first.equals(tp.first) &amp;&amp; second.equals(tp.second); &#125; return false; &#125; @Override public String toString() &#123; return first + \"\\t\" + second; &#125; @Override public int compareTo(TextPair tp) &#123; int cmp = first.compareTo(tp.first); if (cmp != 0) &#123; return cmp; &#125; return second.compareTo(tp.second); &#125; // ^^ TextPair // vv TextPairComparator public static class Comparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public Comparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); int cmp = TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); if (cmp != 0) &#123; return cmp; &#125; return TEXT_COMPARATOR.compare(b1, s1 + firstL1, l1 - firstL1, b2, s2 + firstL2, l2 - firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; &#125; static &#123; WritableComparator.define(TextPair.class, new Comparator()); &#125; // ^^ TextPairComparator // vv TextPairFirstComparator public static class FirstComparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public FirstComparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); return TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; if (a instanceof TextPair &amp;&amp; b instanceof TextPair) &#123; return ((TextPair) a).first.compareTo(((TextPair) b).first); &#125; return super.compare(a, b); &#125; &#125; // ^^ TextPairFirstComparator // vv TextPair&#125;// ^^ TextPair","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems(2)","date":"2017-05-15T08:43:49.000Z","path":"2017/05/15/Machine-Learning-Recommender-Systems-2/","text":"1. ååŒè¿‡æ»¤ï¼Œç»™ç”¨æˆ·æ¨èç‰©å“1.1 åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤ç®—æ³•UserCF ä¸»è¦é€šè¿‡åˆ†æç”¨æˆ·çš„è¡Œä¸ºè®°å½•ï¼Œè®¡ç®—ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦ è¯¥ç®—æ³•è®¤ä¸ºç‰©å“Aå’Œç‰©å“Bå…·æœ‰å¾ˆå¤§çš„ç›¸ä¼¼åº¦æ˜¯å› ä¸ºå–œæ¬¢ç‰©å“Açš„ç”¨æˆ·å¤§éƒ¨åˆ†ä¹Ÿå–œæ¬¢ç‰©å“B 1.1.1 æ­¥éª¤ æ‰¾åˆ°å’Œç›®æ ‡ç”¨æˆ·ç›¸ä¼¼çš„ç”¨æˆ·é›†åˆ æ‰¾åˆ°è¿™ä¸ªé›†åˆä¸­çš„ç”¨æˆ·å–œæ¬¢ï¼Œä¸”å’Œç›®æ ‡ç”¨æˆ·æ²¡æœ‰å¬è¯´è¿‡çš„çš„ç‰©å“æ¨èç»™ç›®æ ‡ç”¨æˆ· 1.1.2 è®¡ç®—ç›¸ä¼¼åº¦ å¾—åˆ°ç”¨æˆ·ä¹‹é—´çš„å…´è¶£ç›¸ä¼¼åº¦ä¹‹åï¼Œå¯»æ‰¾æœ€ç›¸è¿‘çš„Kä¸ªç”¨æˆ· 1.2åŸºäºç‰©å“çš„ååŒè¿‡æ»¤ç®—æ³•1.2.1 è®¡ç®—æ­¥éª¤ è®¡ç®—ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦ æ ¹æ®ç‰©å“çš„ç›¸ä¼¼åº¦å’Œç”¨æˆ·çš„å†å²è¡Œä¸ºç»™ç”¨æˆ·ç”Ÿæˆæ¨èåˆ—è¡¨ 1.2.2 è®¡ç®—ç›¸ä¼¼åº¦ 1.3 éšè¯­ä¹‰æ¨¡å‹ UserCF æ‰¾åˆ°å’Œä»–ä»¬çœ‹äº†åŒæ ·ä¹¦çš„å…¶ä»–ç”¨æˆ·ï¼Œå³æ˜¯å…´è¶£ç›¸ä¼¼ç”¨æˆ·ã€‚ç„¶åç»™ç”¨æˆ·æ¨èè¿™äº›ç”¨æˆ·ç›¸ä¼¼ç”¨æˆ·æ‰€å–œæ¬¢çš„å…¶ä»–ä¹¦ç± ItemCF åœ¨å·²ç»çœ‹è¿‡çš„ä¹¦ä¸­ï¼Œå¯»æ‰¾ç›¸ä¼¼çš„ä¹¦ï¼Œå³å’Œè¿™äº›çœ‹è¿‡çš„ä¹¦ï¼ŒåŒæ—¶å‡ºç°åœ¨å…¶ä»–ç”¨æˆ·çš„çœ‹è¿‡çš„ä¹¦ä¸­ï¼Œç„¶åæ¨èè¿™äº›å…¶ä»–çš„ä¹¦ å…¶ä»–æ–¹æ³• å¯ä»¥å¯¹ç‰©å“çš„å…´è¶£è¿›è¡Œåˆ†ç±»ï¼Œå¯¹äºæŸä¸ªç”¨æˆ·ï¼Œé¦–å…ˆå¾—åˆ°ä»–çš„å…´è¶£åˆ†ç±»ï¼Œç„¶åä»è¿™äº›åˆ†ç±»ä¸­æŒ‘å‡ºä»–å¯èƒ½å–œæ¬¢çš„ç‰©å“ å¦‚ä½•ç»™ç‰©å“è¿›è¡Œåˆ†ç±» éšå«è¯­ä¹‰åˆ†ææŠ€æœ¯(latent variable analysis) å¦‚ä½•ç¡®å®šç”¨æˆ·çš„å…´è¶£ï¼Œå³å¯¹å“ªäº›ç±»çš„ç‰©å“æ„Ÿå…´è¶£ï¼Œä»¥åŠæ„Ÿå…´è¶£çš„ç¨‹åº¦ å¯¹äºä¸€ä¸ªç»™å®šçš„ç±»ï¼Œé€‰æ‹©å“ªäº›å±äºè¿™ä¸ªç±»çš„ç‰©å“æ¨èç»™ç”¨æˆ·ï¼Ÿä»¥åŠå¦‚ä½•ç¡®å®šè¿™äº›ç‰©å“åœ¨è¿™ä¸ªä¸­çš„æƒé‡ã€‚å³å¦‚ä½•åœ¨è¿™ä¸ªç±»ä¸­ï¼ŒæŒ‘é€‰å‡ºåˆé€‚çš„ç‰©å“æ¨èç»™ç”¨æˆ· 1.3.1 LFM (latent factor model)1.4 ç”¨æˆ·æ ‡ç­¾æ•°æ® User Generated Content2. éœ€è¦è§£å†³çš„é—®é¢˜ - è¯„åˆ†é¢„æµ‹ï¼ˆç”¨æˆ·Aå¯¹ç”µå½±xçš„è¯„åˆ†é¢„æµ‹ï¼‰ 2.1 å®éªŒæ–¹æ³•2.1.1 åˆ’åˆ†è®­ç»ƒé›† ä¸æ—¶é—´æ— å…³ï¼Œå¯ä»¥å‡åŒ€åˆ†å¸ƒéšæœºæ¢åˆ†æ•°æ®é›†ã€‚å³å¯¹æ¯ä¸€ä¸ªç”¨æˆ·ï¼Œéšæœºé€‰å–ä¸€äº›è¯„åˆ†è®°å½•ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä¸‹çš„ä½œä¸ºæµ‹è¯•é›† ä¸æ—¶é—´ç›¸å…³ï¼Œé‚£ä¹ˆéœ€è¦å°†ç”¨æˆ·çš„æ—§è¡Œä¸ºä½œä¸ºè®­ç»ƒé›†ï¼Œè®²ç”¨æˆ·çš„æ–°è¡Œä¸ºä½œä¸ºæµ‹è¯•é›† ä¾‹å­ Netflixçš„è¯„åˆ†é¢„æµ‹ç³»ç»Ÿä¸­ï¼Œå°†æ¯ä¸ªç”¨æˆ·çš„è¯„åˆ†è®°å½•æŒ‰ç…§ä»æ—©åˆ°æ™šè¿›è¡Œæ’åºï¼Œç„¶åå°†ç”¨æˆ·æœ€åçš„10%çš„è¯„åˆ†è®°å½•ä½œä¸ºæµ‹è¯•é›†ï¼Œ90%çš„è¯„åˆ†è®°å½•ä½œä¸ºè®­ç»ƒé›†ã€‚ 2.1.2 è¯„åˆ†æ ‡å‡†$$RMSE = \\frac {\\sqrt {\\sum_{(u, i) \\in T} (r_{ui} - \\hat r_{ui})^2 }} {|Test|}$$ 2.1.3 è¯„åˆ†é¢„æµ‹ç®—æ³• å¹³å‡å€¼ æœ€ç®€å•çš„æ–¹æ³•ï¼šåˆ©ç”¨å¹³å‡å€¼é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ† å…¨å±€å¹³å‡å€¼ è®¡ç®—åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸­ï¼Œæ‰€æœ‰è¯„åˆ†è®°å½•çš„è¯„åˆ†å¹³å‡å€¼ ç”¨æˆ·è¯„åˆ†å¹³å‡å€¼ è®¡ç®—ç”¨æˆ·uåœ¨è®­ç»ƒé›†ä¸­æ‰€ç»™å‡ºçš„è¯„åˆ†çš„å¹³å‡å€¼ ç‰©å“è¯„åˆ†å¹³å‡å€¼è®¡ç®—è¯¥ç‰©å“wåœ¨è®­ç»ƒé›†ä¸­è¢«è¯„ä»·äº†çš„è¯„åˆ†çš„å¹³å‡å€¼ ç”¨æˆ·å¯¹ç‰©å“åˆ†ç±»çš„å¹³å‡å€¼ å‡è®¾è¿™é‡Œæœ‰ä¸¤ä¸ªåˆ†ç±»ï¼Œä¸€ä¸ªæ˜¯ç”¨æˆ·åˆ†ç±»å‡½æ•°Uï¼Œä¸€ä¸ªæ˜¯ç‰©å“åˆ†ç±»Wï¼ŒU(u)å®šä¹‰äº†ç”¨æˆ·uæ‰€å±çš„åˆ†ç±»ï¼ŒW(w)å®šä¹‰äº†ç‰©å“wæ‰€å±çš„åˆ†ç±»ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è®­ç»ƒé›†ä¸­åŒç±»ç”¨æˆ·å¯¹åŒç±»ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ† ä¹‹å‰æ˜¯ä¸‰ç§å¹³å‡å€¼å…¶å®æ˜¯ç”¨æˆ·å¯¹ç‰©å“åˆ†ç±»çš„å¹³å‡å€¼çš„ä¸€ç§ç‰¹ä¾‹ U(u) = 0ï¼ŒW(w) = 0ï¼Œé‚£ä¹ˆå°±æ˜¯å…¨å±€å¹³å‡å€¼ U(u) = uï¼ŒW(w) = 0ï¼Œé‚£ä¹ˆå°±æ˜¯ç”¨æˆ·è¯„åˆ†å¹³å‡å€¼ U(u) = 0ï¼ŒW(w) = wï¼Œé‚£ä¹ˆå°±æ˜¯ç‰©å“è¯„åˆ†å¹³å‡å€¼ åœ¨ä»¥ä¸Šçš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è€ƒè™‘åˆ°ç”¨æˆ·çš„æ´»è·ƒåº¦å’Œç‰©å“çš„æµè¡Œç¨‹åº¦ã€‚å®é™…ä¸Šå¯ä»¥å°†è¿™ä¸¤ç‚¹è€ƒè™‘è¿›å»ï¼Œå¯¹æ´»è·ƒç”¨æˆ·å’Œæµè¡Œçš„ç‰©å“ç»™å®šä¸€ç‚¹penalty 2.1.4 åŸºäºé¢†åŸŸçš„æ–¹æ³•ï¼ˆåŸºäºç”¨æˆ·çš„é¢†åŸŸå’ŒåŸºäºç‰©å“çš„é¢†åŸŸç®—æ³•ï¼‰ åŸºäºç”¨æˆ·çš„é¢†åŸŸç®—æ³•$$\\hat r_{ui} = \\overline r_u +\\frac {\\sum_{v \\in S(u, K) \\bigcap N(i)} w_{uv}*(r_{vi} - \\overline r_v)}{\\sum_{v \\in S(u, K) \\bigcap N(i)} |w_{uv}|}$$E(r_u) æ˜¯ç”¨æˆ·uå¯¹ä»–ç‚¹è¯„è¿‡çš„æ‰€æœ‰ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼ S(u, K) æ˜¯å’Œç”¨æˆ·uå…´è¶£æœ€ç›¸ä¼¼çš„Kä¸ªç”¨æˆ·çš„é›†åˆ N(i) æ˜¯å¯¹ç‰©å“iç‚¹è¯„è¿‡åˆ†æ•°çš„ç”¨æˆ·é›†åˆ r_vi æ˜¯ç”¨æˆ·vå¯¹ç‰©å“içš„è¯„åˆ† E(r_v) æ˜¯ç”¨æˆ·vå¯¹ä»–è¯„åˆ†è¿‡çš„æ‰€æœ‰ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼ w_uv æ˜¯ç”¨æˆ·uå’Œvä¹‹é—´çš„ç›¸ä¼¼åº¦ â€‹ åŸºäºç‰©å“çš„é¢†åŸŸç®—æ³•$$\\hat r_{ui} = \\overline r_i +\\frac {\\sum_{j \\in S(i, K) \\bigcap N(u)} w_{ij}*(r_{uj} - \\overline r_i)}{\\sum_{j \\in S(i, K) \\bigcap N(u)} |w_{ij}|}$$E(r_i) æ˜¯ç‰©å“içš„å¹³å‡åˆ†ï¼Œæ˜¯æ‰€æœ‰ç”¨æˆ·å¯¹ç‰©å“iç‚¹è¯„è¿‡çš„åˆ†æ•°çš„å¹³å‡å€¼ S(i, K) æ˜¯å’Œç‰©å“iæœ€ç›¸ä¼¼çš„Kä¸ªç‰©å“çš„é›†åˆ N(u) æ˜¯ç”¨æˆ·uç‚¹è¯„å¤šåˆ†æ•°çš„ç‰©å“é›†åˆ r_uj æ˜¯ç”¨æˆ·uå¯¹ç‰©å“jçš„è¯„åˆ† w_ij æ˜¯ç‰©å“iå’Œjä¹‹é—´çš„ç›¸ä¼¼åº¦ 2.1.5 è®¡ç®—ç›¸ä¼¼åº¦ ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆcosine similarity)$$w_{ij} = \\frac {\\sum_{u \\in U} r_{ui} r_{uj}} {\\sqrt{\\sum_{u \\in U}r_{ui}^2 \\sum_{u \\in U}r_{uj}^2}}$$ çš®å°”é€Šç³»æ•°ï¼ˆpearson correlationï¼‰$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_i) (r_{uj} - \\overline r_j)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_i)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_j)^2}}$$ ä¿®æ­£ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆadjust cosine similarityï¼‰ï¼ˆåœ¨MovieLensæ•°æ®é›†ä¸Šæ•ˆæœæœ€å¥½ï¼‰$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_u) (r_{uj} - \\overline r_u)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_u)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_u)^2}}$$ 2.1.6 éšè¯­ä¹‰æ¨¡å‹çš„çŸ©é˜µåˆ†è§£æ¨¡å‹ ï¼ˆLatent Factor Model)éšå«ç±»åˆ«æ¨¡å‹ã€éšè¯­ä¹‰æ¨¡å‹ç­‰ï¼Œåœ¨æœ¬è´¨ä¸Šéƒ½æ˜¯ä¸ºäº†æ‰¾å‡ºæŸä¸€ä¸œè¥¿çš„æ½œåœ¨çš„ä¸»é¢˜æˆ–è€…åˆ†ç±»ã€‚åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå¯ä»¥åŸºäºç”¨æˆ·çš„è¡Œä¸ºåˆ©ç”¨éšè¯­ä¹‰æ¨¡å‹ï¼Œå¯¹itemè¿›è¡Œè‡ªåŠ¨èšç±»ï¼Œè¿™æ ·å¯ä»¥é¿å…äº†äººä¸ºåˆ†ç±»çš„åå·®ã€‚ ä¸¾ä¾‹è¯´æ˜ï¼š â€‹ ç”¨æˆ·Aå–œæ¬¢çœ‹æ•°å­¦ï¼Œå†å²å’Œè®¡ç®—æœºçš„ä¹¦ç± â€‹ ç”¨æˆ·Bå–œæ¬¢çœ‹æœºå™¨å­¦ä¹ ï¼Œç¼–ç¨‹è¯­è¨€å’Œç¦»æ•£æ•°å­¦æ–¹é¢çš„ä¹¦ç± â€‹ ç”¨æˆ·Cå–œæ¬¢çœ‹å¤§å¸ˆçš„ä½œå“ï¼Œæ¯”å¦‚ä¸“é—¨çœ‹Knuthæˆ–è€…Jiawei Hançš„ä¹¦ç± é‚£ä¹ˆç³»ç»Ÿåœ¨å¯¹ç”¨æˆ·çš„å–œå¥½è¿›è¡Œæ¨èçš„æ—¶å€™ï¼Œéœ€è¦æ‰¾å‡ºåŒå±äºç”¨æˆ·å…´è¶£åœˆå­çš„ä¹¦ç±ã€‚å¯¹äºä¹‹å‰æåˆ°è¿‡çš„ä¸‰ä¸ªç”¨æˆ·æ¥è¯´ â€‹ ç”¨æˆ·Açš„åœˆå­ï¼šæ•°å­¦ã€è®¡ç®—æœºã€å†å² â€‹ ç”¨æˆ·Bçš„åœˆå­ï¼šè¿™ä¸‰æœ¬ä¹¦å¯ä»¥åŒæ—¶åˆ†åˆ°è®¡ç®—æœºçš„åœˆå­ï¼Œä½†æ˜¯ç¦»æ•£æ•°å­¦å´åˆå¯ä»¥åˆ†åˆ°æ•°å­¦åœˆå­å» â€‹ ç”¨æˆ·Cçš„åœˆå­ï¼šæ ¹æ®ä½œè€…ä¸åŒæ¥åˆ’åˆ†åœˆå­ï¼Œé‚£ä¹ˆè¿™ä¸ªåœˆå­å°±å’Œä¹‹å‰ç”¨æˆ·Aã€Bçš„è§’åº¦æ˜¯ä¸åŒçš„ã€‚ å‡è®¾è®©äººå·¥æ¥å®Œæˆä¹‹å‰ä¹¦ç±çš„åˆ†ç±»ï¼Œé‚£ä¹ˆç»å¸¸ä¼šç¢°åˆ°åˆ’åˆ†çš„ç²’åº¦ä¸åŒï¼Œè§’åº¦ä¸åŒç­‰æƒ…å†µã€‚ åŒæ—¶ï¼Œéœ€è¦æ³¨æ„ä¸€ä¸‹ä¸¤ç‚¹ï¼š ç”¨æˆ·Aå¯¹è¿™ä¸‰ä¸ªç±»åˆ«ä¹¦ç±æ„Ÿå…´è¶£ï¼Œä¸ä»£è¡¨ä¸å¯¹å…¶ä»–ç±»åˆ«çš„æ•°æ®æ„Ÿå…´è¶£ åŒä¸€æœ¬ä¹¦å¯ä»¥å±äºå¤šä¸ªç±»åˆ«ï¼Œæ‰€ä»¥æ¯æœ¬ä¹¦åœ¨æ¯ä¸ªç±»åˆ«é‡Œé¢éƒ½æœ‰ä¸€ä¸ªæƒé‡ï¼Œæƒé‡å€¼è¶Šå¤§ï¼Œè¯´æ˜å±äºè¿™ä¸ªç±»åˆ«çš„å¯èƒ½æ€§è¶Šé«˜ é‚£ä¹ˆï¼ŒLFMæ˜¯å¦‚ä½•è§£å†³ä¸Šé¢çš„å‡ ä¸ªé—®é¢˜çš„å‘¢ï¼Ÿ å›ç­”ä¸Šé¢çš„é—®é¢˜å‰ï¼Œæˆ‘ä»¬å¯ä»¥æ€è€ƒä¸€ä¸‹æˆ‘ä»¬éœ€è¦åšçš„å“ªäº›å·¥ä½œã€‚ å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰çš„Userçœ‹åšåˆ—ï¼ŒæŠŠæ‰€æœ‰çš„Itemçœ‹åšè¡Œï¼Œæ„å»ºä¸€ä¸ªmxnçš„äºŒç»´çŸ©é˜µï¼Œå¦‚ä¸‹å›¾ æœ€å·¦è¾¹çš„RçŸ©é˜µæ˜¯ä¸€ä¸ªuser-itemçŸ©é˜µï¼ŒçŸ©é˜µå€¼Rijè¡¨ç¤ºçš„æ˜¯ç”¨æˆ·iå¯¹item jçš„å…´è¶£åº¦ï¼Œæˆ–è€…æ˜¯è¯„åˆ†ã€‚é‚£ä¹ˆæˆ‘ä»¬çš„è¯„åˆ†é¢„æµ‹ï¼Œå°±å¯ä»¥è½¬æ¢æˆå¯¹è¿™ä¸ªçŸ©é˜µä¸­çš„æŸäº›å€¼ï¼ˆç¼ºå¤±å€¼ï¼‰çš„é¢„æµ‹ï¼ŒåŒæ—¶éœ€è¦ä¿è¯æˆ‘ä»¬çš„é¢„æµ‹å€¼å¯¹äºè¿™ä¸ªçŸ©é˜µçš„æ‰°åŠ¨çš„æœ€å°çš„ã€‚ï¼ˆå³è¡¥å…¨ä¹‹åçŸ©é˜µçš„ç‰¹å¾å€¼å’Œè¡¥å…¨ä¹‹å‰çš„ç‰¹å¾å€¼ç›¸å·®ä¸å¤§ï¼Œå…·ä½“è§SVDåˆ†è§£ï¼‰ è€Œå³è¾¹çš„ä¸¤ä¸ªPå’ŒQçŸ©é˜µå°±æ˜¯LFMæ‰€åšçš„ï¼ŒLFMç®—æ³•ä»æ•°æ®é›†æ±‡æ€»æŠ½å‡ºè‹¥å¹²ä¸ªclassï¼Œè®¡ç®—å‡ºæ‰€æœ‰userå¯¹è¿™äº›classçš„æ„Ÿå…´è¶£é•¿åº¦ï¼Œå³PçŸ©é˜µã€‚åŒæ—¶è®¡ç®—å‡ºæ‰€æœ‰itemåœ¨è¿™äº›classä¸­çš„æƒé‡å€¼ï¼Œå³QçŸ©é˜µã€‚PçŸ©é˜µä½œä¸ºuserå’Œitemä¹‹é—´è¿æ¥çš„æ¡¥æ¢ï¼Œæ‰€ä»¥Rå¯ä»¥è¡¨ç¤ºä¸ºPçŸ©é˜µå’ŒQçŸ©é˜µç›¸ä¹˜ã€‚$$R_{UI} = P_U Q_I = \\sum_{k=1}^K = P_{U,k}Q_{k,I}$$ä»¥ä¸‹æ˜¯LFMçš„ä¼˜ç‚¹ï¼š ä¸éœ€è¦å…³å¿ƒçŸ©é˜µPæ˜¯æ€ä¹ˆæ„å»ºçš„ï¼Œå³ä¸éœ€è¦å¦‚ä½•ç»™ç‰©å“è¿›è¡Œèšç±»ã€åˆ’åˆ†ç­‰ï¼ˆè§’åº¦ï¼Œç²’åº¦ç­‰ï¼‰ QçŸ©é˜µä¸­ï¼Œå¯¹äºä¸€ä¸ªitemå¹¶ä¸æ˜¯æ˜ç¡®ç»™åˆ’åˆ†åˆ°æŸä¸€ä¸ªåˆ†ç±»ï¼Œè€Œæ˜¯è®¡ç®—è¿™ä¸ªitemå±äºè¿™äº›æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡ï¼Œå€¼è¶Šå¤§å¯èƒ½æ€§è¶Šé«˜ åŒç†ï¼ŒPçŸ©é˜µä¸­ï¼Œå¯¹äºä¸€ä¸ªuserå¹¶æ²¡æœ‰é™å®šåœ¨æŸäº›classä¸­ï¼Œè€Œæ˜¯è®¡ç®—è¿™ä¸ªuserå¯¹äºè¿™äº›classesçš„æ„Ÿå…´è¶£ç¨‹åº¦ è™½ç„¶æˆ‘ä»¬çŸ¥é“äº†LFMä¸ºæˆ‘ä»¬åšäº†å“ªäº›å·¥ä½œï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯ä¸çŸ¥é“è¯¥å¦‚ä½•æ±‚è§£å‡ºçŸ©é˜µPå’ŒçŸ©é˜µQä¸­çš„å‚æ•°å€¼ï¼Œä¸€èˆ¬çš„åšæ³•æ˜¯æœ€ä¼˜æŸå¤±å‡½æ•°æ¥æ±‚å‚æ•°ã€‚ ä¼ ç»Ÿçš„SVDåˆ†è§£ ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼Œç»™å®šä¸€ä¸ªuser-itemçš„çŸ©é˜µRã€‚ é¦–å…ˆéœ€è¦å¯¹è¯„åˆ†çŸ©é˜µRä¸­çš„ç¼ºå¤±å€¼è¿›è¡Œç®€å•çš„è¡¥å…¨ï¼Œæ¯”å¦‚ç”¨å…¨å±€å¹³å‡å€¼ï¼Œæˆ–è€…ç”¨æˆ·/ç‰©å“çš„å¹³å‡å€¼è¡¥å…¨ï¼Œå¾—åˆ°è¡¥å…¨åçš„çŸ©é˜µRâ€™ å¾—åˆ°è¡¥å…¨åçš„çŸ©é˜µRâ€™ï¼Œæ¥ç€å¯ä»¥åˆ©ç”¨SVDåˆ†è§£ï¼Œå°†Râ€™åˆ†è§£æˆå¦‚ä¸‹å½¢å¼$$Râ€™ = U^TSV \\R \\in R^{m n} \\U \\in R^{k m} \\V \\in R^{k n} \\S \\in R^{k k}$$Uå’ŒVæ˜¯ä¸¤ä¸ªæ­£äº¤çŸ©é˜µï¼ŒSæ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šçš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯çŸ©é˜µçš„å¥‡å¼‚å€¼ã€‚ ä¸ºäº†å¯¹Râ€™è¿›è¡Œé™ç»´ï¼Œå¯ä»¥å–æœ€å¤§çš„fä¸ªå¥‡å¼‚å€¼ç»„æˆå¯¹ç„¦çŸ©é˜µSfï¼Œå¹¶ä¸”æ‰¾åˆ°è¿™ä¸ªfä¸ªå¥‡å¼‚å€¼æ±‡ä¸­æ¯ä¸ªå€¼åœ¨Uã€VçŸ©é˜µä¸­å¯¹åº”çš„è¡Œå’Œåˆ—ï¼Œå¾—åˆ°Ufã€Vfï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªé™ç»´åçš„è¯„åˆ†çŸ©é˜µï¼š$$R_fâ€™=U_f^TS_fV_f$$è¯¥æ–¹æ³•çš„ä¸€äº›ç¼ºç‚¹ï¼š åœ¨ç°å®ä¸­ï¼ŒRçŸ©é˜µåŸºæœ¬ä¸Šä¼šæ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œå³95%çš„æ•°æ®æ˜¯ç¼ºå¤±çš„ï¼ŒåŒæ—¶è¯¥çŸ©é˜µéå¸¸çš„å¤§ã€‚ä¸€ç»è¡¥å…¨ï¼Œè¯¥çŸ©é˜µå°±æ˜¯ä¸€ä¸ªç¨ å¯†çŸ©é˜µï¼Œå‚¨å­˜å¼€é”€éå¸¸çš„å¤§ è®¡ç®—å¤æ‚åº¦éå¸¸çš„é«˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¡¥å…¨ä¹‹åçš„ç¨ å¯†çŸ©é˜µ â€‹ Funk-SVDåˆ†è§£ï¼Œå³ Latent Factor Modelï¼ˆLFMï¼‰ http://sifter.org/~simon/journal/20061211.html ä»çŸ©é˜µçš„è§’åº¦ï¼Œå°†è¯„åˆ†çŸ©é˜µRåˆ†è§£æˆä¸¤ä¸ªä½çº¬åº¦ç›¸ä¹˜ï¼š$$\\hat R = P^TQ \\R \\in R^{mn} \\P \\in R^{fm} \\Q \\in R^{f*n}$$Pã€Qæ˜¯ä¸¤ä¸ªé™ç»´åçš„çŸ©é˜µï¼Œé‚£ä¹ˆå¯¹äºç”¨æˆ·uå¯¹äºç‰©å“içš„è¯„åˆ†çš„é¢„æµ‹å€¼å¯ä»¥^R(u, i) = ^r_uiï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—ï¼š$$\\hat r_{ui} =b_{ui} + \\sum_fp_{uf}q_{if} \\p_{uf} = P(u, f) \\p_{if} = Q(i, f)$$Simon Funk-SVDçš„æ€æƒ³æ˜¯ç›´æ¥é€šè¿‡è®­ç»ƒé›†ä¸­çš„è§‚å¯Ÿå€¼ï¼Œåˆ©ç”¨æœ€å°åŒ–RMSEå­¦ä¹ Pã€QçŸ©é˜µã€‚ æŸå¤±å‡½æ•°çš„è®¡ç®—ï¼š$$C(p, q) = \\sum_{(u, i) \\in Train} (r_{ui} - \\hat r_{ui})^2= \\sum_{(u, i) \\in Train}(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})^2 + \\lambda(||p_u||^2 + ||q_i||^2) \\\\hat r_{ui} = \\mu + b_u + b_i + p_u^Tq_i$$è¦æœ€å°åŒ–ä¸Šé¢çš„æŸå¤±å‡½æ•°ï¼Œå¯ä»¥åˆ©ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ã€‚ä»¥ä¸‹æ˜¯ç®€å•çš„æ¨å¯¼è¯¥å…¬å¼ ä¸Šé¢çš„cost functionä¸­æ¬§ä¸¤ä¸ªå‚æ•°på’Œqï¼Œé¦–å…ˆ å¯¹ä»–ä»¬åˆ†åˆ«æ±‚åå¯¼ï¼Œæ±‚å‡ºæœ€å¿«ä¸‹é™çš„æ–¹å‘$$\\frac {âˆ‚C} {âˆ‚p_{uf}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})q_{ik} + 2\\lambda p_{uk} \\\\frac {âˆ‚C} {âˆ‚q_{if}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})p_{uk} + 2\\lambda q_{ik}$$ ç„¶åæ ¹æ®éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œéœ€è¦å°†å‚æ•°æ²¿æœ€å¿«çš„ä¸‹é™æ–¹å‘å‰è¿›ï¼Œå³å¯å¾—åˆ°å¦‚ä¸‹çš„é€’æ¨å…¬å¼ï¼š$$p_{uf} = p_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) q_{ik} - \\lambda p_{uk}) \\q_{if} = q_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) p_{uk} - \\lambda q_{ik})$$ æ‰€ä»¥ï¼Œæ‰§è¡ŒLFMéœ€è¦ï¼š æ ¹æ®æ•°æ®é›†åˆå§‹åŒ–På’ŒQçŸ©é˜µï¼ˆå¦‚ä½•åˆå§‹åŒ–ï¼‰ ç¡®å®šå››ä¸ªå‚æ•°ï¼šåˆ†ç±»ä¹¦Fï¼Œè¿­ä»£æ¬¡æ•°Nï¼Œå­¦ä¹ é€Ÿç‡Î±ï¼ˆÎ± *= 0.9ï¼‰ å’Œæ­£åˆ™åŒ–å‚æ•°Î» â€‹ ä¼ªä»£ç  1234567891011121314151617181920def LFM(user_items, F, N, alpha, lambda): #åˆå§‹åŒ–P,QçŸ©é˜µ [P, Q] = InitModel(user_items, F) #å¼€å§‹è¿­ä»£ For step in range(0, N): #ä»æ•°æ®é›†ä¸­ä¾æ¬¡å–å‡ºuserä»¥åŠè¯¥userå–œæ¬¢çš„itermsé›† for user, items in user_item.iterms(): #éšæœºæŠ½æ ·ï¼Œä¸ºuseræŠ½å–ä¸itemsæ•°é‡ç›¸å½“çš„è´Ÿæ ·æœ¬ï¼Œå¹¶å°†æ­£è´Ÿæ ·æœ¬åˆå¹¶ï¼Œç”¨äºä¼˜åŒ–è®¡ç®— samples = RandSelectNegativeSamples(items) #ä¾æ¬¡è·å–itemå’Œuserå¯¹è¯¥itemçš„å…´è¶£åº¦ for item, rui in samples.items(): #æ ¹æ®å½“å‰å‚æ•°è®¡ç®—è¯¯å·® eui = eui - Predict(user, item) #ä¼˜åŒ–å‚æ•° for f in range(0, F): P[user][f] += alpha * (eui * Q[f][item] - lambda * P[user][f]) Q[f][item] += alpha * (eui * P[user][f] - lambda * Q[f][item]) #æ¯æ¬¡è¿­ä»£å®Œåï¼Œéƒ½è¦é™ä½å­¦ä¹ é€Ÿç‡ã€‚ä¸€å¼€å§‹çš„æ—¶å€™ç”±äºç¦»æœ€ä¼˜å€¼ç›¸å·®ç”šè¿œï¼Œå› æ­¤å¿«é€Ÿä¸‹é™ï¼› #å½“ä¼˜åŒ–åˆ°ä¸€å®šç¨‹åº¦åï¼Œå°±éœ€è¦æ”¾æ…¢å­¦ä¹ é€Ÿç‡ï¼Œæ…¢æ…¢çš„æ¥è¿‘æœ€ä¼˜å€¼ã€‚ alpha *= 0.9 Baseline Estimats å¯¹æ¯”åŸºçº¿è€ƒè™‘åˆ°é‡å£éš¾è°ƒï¼Œæœ‰äº›userä¼šç»™å‡ºæ¯”è¾ƒé«˜çš„åˆ†æ•°ï¼Œæœ‰äº›è¦å»ä¸¥æ ¼çš„usersä¼šç»™å‡ºæ¯”è¾ƒä½çš„åˆ†æ•°ï¼Œè€Œæœ‰äº›è´¨é‡å¥½çš„å•†å“ä¼šå¾—åˆ°æ¯”è¾ƒé«˜çš„åˆ†æ•°ï¼Œè´¨é‡å·®çš„åˆ†æ•°è¾ƒä½ã€‚æ‰€ä»¥ä¸ºäº†è°ƒæ•´è¿™äº›ï¼Œå¼•å…¥äº†baseline estimateã€‚æ¯”å¦‚ä¸ºäº†ä¼°è®¡æŸä¸ªç”¨æˆ·uä¼šç»™ç”µå½±iæ‰“çš„è¯„åˆ†ï¼š$$b_{ui} = u + b_u + b_i$$ $\\mu$ æ˜¯è¯¥ç‰©å“çš„æ•´ä½“å¹³å‡å€¼ $b_u$ æ˜¯ç”¨æˆ·æ‰“åˆ†ç›¸å¯¹æ•´ä½“ç”¨æˆ·æ‰“åˆ†å¹³å‡å€¼çš„åå·® $b_i$ æ˜¯è¯¥ç‰©å“ç›¸å¯¹æ•´ä½“å¹³å‡å€¼çš„åå·® ä¸¾ä¸ªä¾‹å­ï¼šé¢„æµ‹è±†ç“£ç”¨æˆ·å°æ˜ç»™ç”µå½±æ³°å¦å°¼å…‹å·çš„è¯„åˆ† æ³°å¦å°¼å…‹å·åœ¨è±†ç“£ä¸Šçš„å¹³å‡åˆ†æ•°æ˜¯3.7åˆ†(u) æ³°å¦å°¼å…‹å·çš„å¹³å‡åˆ†æ•°åˆæ¯”æ‰€æœ‰ç”µå½±åœ¨è±†ç“£ä¸Šçš„å¹³å‡åˆ†æ•°é«˜0.5åˆ†($b_i$) ä½†æ˜¯å°æ˜æ˜¯ä¸ªç”µå½±çˆ±å¥½è€…ï¼Œæ¯”å¹³å‡ç”¨æˆ·æ‰“åˆ†åä½0.3åˆ†($b_u$) æ‰€ä»¥ä¸è€ƒè™‘regularizedçš„è¯ï¼Œé¢„æµ‹å°æ˜ç»™æ³°å¦å°¼å…‹å·çš„è¯„åˆ†åº”è¯¥æ˜¯ 3.7 - 0.3 + 0.5 = 3.9 æ‰€ä»¥è¿™é‡Œï¼Œå¾—åˆ° $b_u$ å’Œ $b_i$ çš„å€¼å¾ˆé‡è¦ è¿™é‡Œå¯¹$b_u$å’Œ$b_i$åŠ å…¥äº†penalityï¼Œä¸ºäº†é˜²æ­¢overfittingã€‚ è¿™é‡Œ$r_ui$æ˜¯æˆ‘ä»¬è®­ç»ƒæ•°æ®ä¸­çš„å·²çŸ¥ratingï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ $\\mu$æ˜¯æ•´ä½“å¹³å‡å€¼ï¼Œä¹Ÿå¯ä»¥æ ¹æ®è®­ç»ƒæ•°æ®è®¡ç®—å‡ºæ¥ $\\lambda_2$å’Œ$\\lambda_3$æ˜¯æˆ‘ä»¬æ‰‹åŠ¨è®¾ç½®çš„å‚æ•°ï¼ŒMovieLensæ•°æ®ä¸Šï¼Œ20æ¯”è¾ƒåˆé€‚ R(u)å’ŒR(i)ä¸ºç”¨æˆ·uratingè¿‡çš„ç‰©å“çš„é›†åˆï¼Œå’Œç‰©å“iè¢«ratingè¿‡ç”¨æˆ·çš„é›†åˆ æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™äº›æ•°æ®ï¼Œè®¡ç®—å‡º$b_u$å’Œ$b_i$b_i$ï¼ˆæ³¨æ„ç»´åº¦ï¼‰$$b_i = \\frac {\\sum_{u \\in R(u)}(r_{ui} - \\mu)} {\\lambda_2 + |R(i)|} \\b_u = \\frac {\\sum_{i \\in R(u)}(r_{ui} - \\mu - b_i)} {\\lambda_3 + |R(u)|}$$é™¤äº†ä¸Šé¢çš„æ–¹æ³•ï¼Œè¿˜æœ‰ä¸€ç§æ›´ä¸ºç®€ä¾¿çš„æ–¹æ³•è®¡ç®—$b_u$å’Œ$b_i$ï¼Œå°±æ˜¯ç›´æ¥ä½¿ç”¨userï¼Œitemçš„ratingçš„å¹³å‡å€¼ä¼°è®¡$$b_u = \\frac {\\sum R(u)} {len(R(u))} \\b_i = \\frac {\\sum R(i)} {len(R(i))}$$ Neighborhood Models item-oriented algorithm: a rating is estimated using known rating made by the same user on similarity items. user-oriented algorithm: estimate unknown ratings based on recorded ratings of like minded users. Similarity measure between items Pearson correlation è®¡ç®—ç‰©å“iå’Œjçš„ç›¸ä¼¼åº¦$$s_{ij} = \\frac {n_{ij}} {n_{ij} + \\lambda_2}Ï_{ij}$$ n_ij è¡¨ç¤ºéƒ½å¯¹ç‰©å“iå’Œjè¯„åˆ†è¿‡çš„ç”¨æˆ·çš„æ•°é‡ Ïijæ˜¯çš®å°”é€Šç³»æ•°ï¼Œé€šå¸¸å–ï¼Ÿï¼Ÿï¼Ÿ Î»2 é€šå¸¸å–100 é¢„æµ‹è¯„åˆ†$$\\hat r_{ui} = b_{ui} +\\frac{\\sum_{j \\in S^k(i; u)} s_{ij}(r_{ui} - b_{ui})}{\\sum_{j \\in S^k(i; u)}s_{ij}}$$ åœ¨ç”¨æˆ·uæ‰€æœ‰è¯„åˆ†è¿‡çš„ç‰©å“ä¸­ï¼Œæ‰¾åˆ°ç›¸ä¼¼åº¦å’Œiæœ€é«˜çš„kä¸ªç‰©å“ï¼ˆk-neighborsï¼‰ï¼Œç”¨$S^k(i; u)$è¡¨ç¤º ä½†æ˜¯è¿™ç§ç®—æ³•è¿˜æ˜¯æœ‰ä¸€äº›å±€é™æ€§ï¼Œæ¯”å¦‚å¯¹äºä¸¤ä¸ªå®Œå…¨æ²¡æœ‰å…³ç³»çš„ç‰©å“ä¹‹é—´çš„é¢„æµ‹ã€‚æˆ–è€…æ˜¯å¯¹äºæŸäº›ç‰©å“ï¼Œæœ€ä¸ºç›¸ä¼¼çš„kä¸ªç‰©å“ç¼ºå¤±ï¼Œæ‰€ä»¥å¯ä»¥ä¿®æ­£ä»¥ä¸Šçš„å…¬å¼ï¼ˆä¸æ˜¯å¾ˆæ˜ç™½è¿™é‡Œï¼‰$$\\hat r_{ui} = b_{ui} + \\sum_{j \\in S^k(i; u)} \\theta^u_{ij}(r_{ui} - b_{ui}) \\{\\theta^u_{ij} | j \\in S^k(i; u)}$$","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆäºŒï¼‰","date":"2017-05-04T14:13:45.000Z","path":"2017/05/04/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆäºŒï¼‰/","text":"1. Mapper 2. Reducer 3. MapReduceæ•°æ®éƒ½æ˜¯ä»¥ key-values pairs çš„å½¢å¼åœ¨Mapperå’ŒReduceä¹‹é—´ä¼ é€’çš„ Mapperè¾“å‡ºçš„ key-value pairs åº”è¯¥å’Œ Reducerè¾“å…¥çš„ key-value pairs ç±»å‹æ˜¯ä¸€æ ·çš„ åœ¨Reducerä¸­ï¼Œæ˜¯ key-valueLists pairs çš„å½¢å¼ 4. Deriverä¹Ÿå°±æ˜¯åˆå§‹åŒ–é…ç½®MRç„¶åè°ƒç”¨æ‰§è¡Œï¼Œä¸€èˆ¬å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ï¼š 123456789101112131415161718192021222324252627282930313233// è€APIpublic void run(String inputPath, String outputPath) throws Exception &#123; JobConf conf = new JobConf(WordCount.class); conf.setJobName(\"wordcount\"); // the keys are words (strings) conf.setOutputKeyClass(Text.class); // the values are counts (ints) conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(MapClass.class); conf.setReducerClass(Reduce.class); FileInputFormat.addInputPath(conf, new Path(inputPath)); FileOutputFormat.setOutputPath(conf, new Path(outputPath)); JobClient.runJob(conf);&#125;// æ–°APIpublic void run(String IN, String OUT) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1);&#125; 5. Data Flowåœ¨hadoopä¸­æ‰€æœ‰çš„Mapperå’ŒReduceréƒ½æ˜¯ç‹¬ç«‹å·¥ä½œçš„ï¼Œè¿™ä¹Ÿæ˜¯hadoopåˆ†å¸ƒå¼èƒ½å¤Ÿç¨³å®šè¿è¡Œçš„åŸå› ä¹‹ä¸€(æœ‰åˆ©äºå®¹é”™å¤„ç†)ã€‚åœ¨MapReduceæ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œåªæœ‰ä¸€æ¬¡æ•°æ®ç›¸äº’äº¤äº’ï¼Œå°±æ˜¯Mapperåˆ°Reducerè¿™ä¸ªè¿‡ç¨‹ã€‚ä»Mapperè¾“å‡ºçš„æ‰€æœ‰çš„intermediate dataä¼šè¢«ç»Ÿä¸€shuffle(å¿…é¡»è¦ç­‰æ‰€æœ‰MRæ‰§è¡Œå®Œæ¯•å—ï¼Ÿ)ï¼Œç„¶ååŒä¸€ä¸ªkeyçš„key-value pairs ä¼šè¢«åˆ†é…åˆ°åŒä¸€ä¸ªreducerä¸­å»ã€‚ 6. A Closer Lookç¬¬äº”éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„æ˜¯MRçš„å®è§‚æµç¨‹ï¼Œå…·ä½“çš„æµç¨‹å…·ä½“å¯ä»¥åˆ†ä¸º Map -&gt; Combiner -&gt; Partitioner -&gt; Sort -&gt; Shuffle -&gt; Sort -&gt; Reduce(è¿™é‡Œå¯¹shuffleçš„å®šä¹‰æœ‰ç‚¹ä¸åŒï¼Œä¸ªäººè®¤ä¸ºä»mapçš„è¾“å‡ºåˆ°reduceçš„è¾“å…¥è¿™æ®µè¿‡ç¨‹å¯ä»¥ç§°ä¹‹ä¸ºshuffleã€‚åŒæ—¶æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œcombineræ˜¯åœ¨mapperæœ€ç»ˆè¾“å‡ºå‰å¤šæ¬¡è°ƒç”¨çš„ï¼Œä»¥åŠåœ¨reduceré‡Œä¹Ÿæœ‰è°ƒç”¨) ä¸‹å›¾ä¸­ï¼Œinput filesåœ¨è¿›å…¥åˆ°Mapperä¹‹å‰æ—¶ï¼Œä¼šå¯¹è¿™äº›æ–‡ä»¶splitï¼Œå› ä¸ºä¸€ä¸ªmapperä¸€èˆ¬æ˜¯64MBæˆ–è€…128MBï¼Œå½“å¤§äºçš„æ—¶å€™éœ€è¦å¯¹è¾“å…¥æ–‡ä»¶å¤„ç†ï¼Œç„¶åä¼ ç»™RecordReadersï¼Œä»¥key-value pairsçš„å½¢å¼ä¼ ç»™mapperã€‚ å¯¹äºæ‰€æœ‰Mapperçš„outputï¼Œå…ˆä¼šå¯¹å…¶è¿›è¡Œparitionæ“ä½œï¼Œä¹Ÿå°±æ˜¯å†³å®šå»å“ªä¸€ä¸ªReducerã€‚å½“ç¡®å®šå¥½å“ªäº›Reducerï¼Œè¿™äº›key-value paris å°±ä¼šä¼ å…¥åˆ°è¯¥Reducerç›¸åº”çš„åˆ†åŒºï¼Œç„¶åå¯¹é½è¿›è¡Œæ’åºã€‚æœ€åå°†sortå¥½çš„key-valueLists ä¼ å…¥åˆ°Reducerè¿›è¡Œå¤„ç†ã€‚ 7. ShuffleHadoop The Definitive Guide P197 Mapper æ ¹æ®å®˜æ–¹å›¾ï¼ŒMapperçš„outputå‡ºæ¥çš„key-values pairsä¼šå…ˆè¿›å…¥åˆ°buffer memory(é»˜è®¤100MBå¤§å°)ï¼Œä½†bufferåˆ°80%å®¹é‡çš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹ä¼šspillåˆ°diskå»ï¼Œå¦‚æœæ­¤æ—¶bufferè¿˜æœªæ…¢çš„æƒ…å†µä¸‹ï¼Œmapperç»§ç»­è¾“å‡ºåˆ°bufferï¼Œå¦‚æœæ»¡äº†çš„è¯mapperä¼šè¢«blockï¼Œç›´åˆ°å¯ä»¥å†™å…¥ã€‚ åœ¨bufferä¸­çš„å†…å®¹spillåˆ°diskä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªpartitionerçš„æ­¥éª¤ã€‚å¯¹è¿™äº›å³å°†å†™å…¥åˆ°diskçš„å†…å®¹åˆ†ç»„ï¼ŒåŒä¸€ä¸ªkeyå’ŒåŒä¸€ä¸ªreducerçš„ker-value pairsä¼šåœ¨ä¸€èµ·ã€‚ç„¶åè¿™äº›key-value pairsæ’åº(sort)ã€‚å¦‚æœæ­¤æ—¶æˆ‘ä»¬å®šä¹‰äº†combiner functionï¼Œåœ¨è¾“å‡ºå‰ï¼Œè¿™äº›pairsä¼šå…ˆcombineï¼Œç„¶åè¾“å‡ºã€‚ä¹Ÿå°±æ˜¯è¯´combiner functionæ˜¯åœ¨partitionerä¹‹å? (Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer) æ¯å½“bufferè¾¾åˆ°é‚£ä¸ªthresholdçš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹å†™å…¥åˆ°diskä¸­(æ³¨æ„æ˜¯æ¯ä¸ªclusterè‡ªå·±çš„local diskï¼Œè€Œä¸æ˜¯HDFS)ï¼Œæ­¤æ—¶ä¼šæ–°å»ºä¸€ä¸ªä¸´æ—¶çš„spillæ–‡ä»¶ï¼Œæ¯æ¬¡å†™å…¥éƒ½ä¼šæ–°å»ºä¸€ä¸ªï¼Œç„¶åè¿™ä¸ªmapç»“æŸå‰è¿™äº›spill filesä¼šè¢«mergeåˆ°ä¸€ä¸ªpartitionedå’Œsortedçš„æ–‡ä»¶é‡Œå»ã€‚é™¤äº†ä¹‹å‰partitionçš„è¾“å‡ºåè°ƒç”¨äº†ä¸€æ¬¡combinerä¸€æ¬¡ä¹‹å¤–ï¼Œä½†åˆå¹¶è¿™äº›ä¸ªspill files( &gt;=3 )çš„æ—¶å€™ï¼Œä¼šç»§ç»­è°ƒç”¨combinerå»åˆå¹¶åŒä¸€ä¸ªreduceré‡Œçš„åŒä¸€ä¸ªkeyçš„valueï¼Œæ‰€ä»¥combineråœ¨output fileè¢«ç”Ÿæˆä¹‹å‰ï¼Œä¼šè¢«è°ƒç”¨è®¸å¤šæ¬¡ï¼Œä»¥å‡å°‘ä¹‹åioçš„æ¬¡æ•°ã€‚ä½†å½“spill filesåªæ˜¯1ä¸ªæˆ–åˆ™ä¸¤ä¸ªçš„æ—¶å€™ï¼Œå¹¶ä¸ä¼šè°ƒç”¨combinerã€‚æ³¨æ„æœ‰äº›æƒ…å†µä¸‹combinerå¹¶ä¸é€‚åˆä½¿ç”¨ï¼Œæ¯”å¦‚æ±‚å¹³å‡å€¼ã€‚ è€Œä¸åŒmapperç”Ÿæˆçš„spillæ–‡ä»¶æœ€ç»ˆä¼šè¢«mergeæˆ {key:[v1, v2, v3â€¦], â€¦}è¿™ç§å½¢å¼ã€‚(è¿™é‡Œæœ‰ç–‘é—®ï¼Œåšå®¢å›¾å’Œå®˜æ–¹å›¾æœ‰ç‚¹ä¸ä¸€æ ·ã€‚æŒ‰ç…§å®˜æ–¹å›¾çš„ç†è§£ï¼Œä¸€ä¸ªmapperå¯¹åº”çš„æ˜¯ä¸€ä¸ªspill fileï¼Œæ‰€ä»¥æœ€ç»ˆæ˜¯å¤šä¸ªspill filesï¼Ÿè¿˜æ˜¯è¿™äº›spill filesåœ¨ä¼ ç»™Reducerä¹‹å‰ä¼šè¢«mergeæˆä¸Šè¿°çš„listå½¢å¼ï¼Ÿä¸€ä¸ªclusterä¸€ä¸ªæœ€ç»ˆçš„output file)â€‹ Reducer ä¹‹å‰mapperç«¯çš„æ‰€æœ‰å·¥ä½œå·²ç»å®Œæˆäº†ï¼Œæ‰€æœ‰çš„mapperçš„outputéƒ½å·²ç»è¢«å†™å…¥åˆ°äº†ä¸€ä¸ªoutputæ–‡ä»¶é‡Œé¢å»äº†ã€‚é‚£ä¹ˆReducerå°±æ˜¯è¦æŠŠè¿™ä¸ªæ–‡ä»¶é‡Œé¢çš„key-valueLists pairs åˆ†ç»™ä¸åŒçš„reducersï¼Œè€Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¹‹ä¸ºFetchï¼Œå°±æ˜¯å°†ç›¸åº”çš„key-valueList pairs æ‹‰å–åˆ°ç›¸åº”çš„reducersé‡Œé¢å»ã€‚ åœ¨Reduceré˜¶æ®µï¼Œæ¯ä¸ªreducerä¼šè°ƒç”¨çº¿ç¨‹ä»å¤šä¸ªä¸åŒçš„clusterçš„output fileé‡Œé¢fetchæ•°æ®ï¼Œç„¶åå¯¹è¿™äº›æ•°æ®mergeã€‚è¿™é‡Œçš„è¿‡ç¨‹å’Œä¹‹å‰çš„mapperæœ‰ç‚¹åƒã€‚reducerä¹Ÿæœ‰ä¸€ä¸ªbuffer memory(é€šè¿‡JVMçš„heap sizeæ¥è®¾ç½®)ï¼Œfetchçš„æ•°æ®ä¼šè¢«ä¼ åˆ°é‡Œé¢(å¦‚æœæ”¾å¾—ä¸‹)ï¼Œå½“bufferè¾¾åˆ°thresholdçš„å€¼çš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹ä¼šè¢«mergeç„¶åspillåˆ°diské‡Œé¢å»(å®é™…ä¸Šæœ‰å¤šç§å½¢å¼å­˜æ”¾è¿™ä¸ªæ–‡ä»¶ï¼Œè¿™é‡Œä¸è®¨è®º)ï¼Œå¦‚æœä¹‹å‰æˆ‘ä»¬å·²ç»å®šä¹‰äº†combinerï¼Œè¿™é‡Œcombinerä¹Ÿä¼šè¢«è°ƒç”¨ã€‚ç›´åˆ°æ‰€æœ‰çš„mapperçš„output fileéƒ½è¢«fetchåˆ°ä¸€ä¸ªæ–‡ä»¶é‡Œå»ï¼Œreducerä¼šåœ¨è¾“å…¥å‰sorté‡Œé¢çš„å†…å®¹(å®é™…ä¸Šmergeçš„æ—¶å€™å·²ç»sortäº†)ï¼Œç„¶åè¿™ä¸ªå·²ç»æ’å¥½åºçš„fileå°±ä¼šè¢«ä¼ åˆ°reduceré‡Œé¢æ‰§è¡Œï¼Œæœ€ç»ˆè¾“å‡ºåˆ°HDFSä¸Šã€‚","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Sparkç¬”è®°","date":"2017-04-10T06:55:40.000Z","path":"2017/04/10/Sparkç¬”è®°/","text":"1. Resilient Distributed Dataset (RDD)RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators. RDDæ˜¯ä¸€ä¸ªå®¹é”™çš„ã€å¹¶è¡Œçš„æ•°æ®ç»“æ„ï¼Œå¯ä»¥è®©ç”¨æˆ·æ˜¾ç¤ºåœ°å°†æ•°æ®å­˜å‚¨åˆ°ç£ç›˜å’Œå†…å­˜ä¸­ï¼Œå¹¶èƒ½æ§åˆ¶æ•°æ®çš„åˆ†åŒºã€‚ Resilient RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Formally, an RDD is a read-only, partitioned collection of records. Rdds can be created throught deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel. There two ways to create RDDs. Parallelizing parallelizing an existing collection in your driver program Referencing reference a dataset in an external storage system, such as a shared file system, HDFS, HBase, or ant data source offering a Hadoop Input Format Sparkæä¾›äº†RDDä¸Šçš„ä¸¤ç±»æ“ä½œï¼štransformation å’Œ action Transformation: return a new RDD å½“å¯¹RDDè¿›è¡Œtransformationæ“ä½œçš„æ—¶å€™ï¼Œè¿™äº›æ“ä½œä¸ä¼šç«‹åˆ»å°±æ‰§è¡Œï¼Œè€Œæ˜¯å°†è¿™äº›æ“ä½œè®°å½•ä¸‹æ¥ã€‚å¦‚æœæœ‰å¤šä¸ªtransformationæ“ä½œæ—¶ï¼Œæ¯ä¸€æ¬¡å˜æ¢éƒ½æ˜¯ä¸€ä¸ªæ¥ç€ä¸€ä¸ªï¼Œæ­¤æ—¶å°±å½¢æˆäº†ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ã€‚è¿™ä¸ªå›¾å°±æ˜¯æ•°æ®å®¹é”™çš„å…³é”®æ‰€åœ¨ã€‚å¦‚æœå‡ºç°æ•°æ®ä¸¢å¤±çš„æ—¶å€™ï¼Œåªéœ€è¦æŸ¥æ‰¾è¿™ä¸ªå›¾å°±èƒ½æ ¹æ®ä¸¢å¤±çš„RDDè¿›è¡Œæ•°æ®æ¢å¤ Action: evaluates and returns a new value å½“å¯¹RDDåšactionæ“ä½œçš„æ—¶å€™ï¼Œè¿™ç±»actionä¸€èˆ¬ä½œç”¨æ˜¯è¿”å›ä¸€ä¸ªå€¼æˆ–è€…æ•°ç»„ç­‰ï¼Œæˆ–è€…æ˜¯å°†æ•°æ®æŒä¹…åŒ–åˆ°ç£ç›˜å½“ä¸­ã€‚æ­¤æ—¶æ‰§è¡Œactionï¼Œä¼šçœŸæ­£çš„æäº¤jobï¼Œæ‰§è¡Œä¹‹å‰çš„transformationè®°å½•çš„DAGã€‚æ‰§è¡ŒDAGç­–ç•¥ä¸­ï¼Œä¼šæœ‰å¤šä¸ªstageï¼Œæ¯ä¸€ä¸ªstageé’Ÿæœ‰å¤šä¸ªtaskï¼Œè¿™äº›taskå°±ä¼šè¢«åˆ†é…åˆ°å„ä¸ªnodesè¿›è¡Œæ‰§è¡Œ The difference between flatMap and Map flatMap Example map: å¯¹RDDæ¯ä¸ªå…ƒç´ è½¬æ¢ï¼Œå°†å‡½æ•°ç”¨äºRDDä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œå°†è¿”å›å€¼æ„æˆæ–°çš„RDDã€‚flatMap: å¯¹RDDæ¯ä¸ªå…ƒç´ è½¬æ¢, ç„¶åå†æ‰å¹³åŒ–ï¼ˆå³å°†æ‰€æœ‰å¯¹è±¡åˆå¹¶ä¸ºä¸€ä¸ªå¯¹è±¡ï¼‰ã€‚å°†å‡½æ•°åº”ç”¨äºrddä¹‹ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå°†è¿”å›çš„è¿­ä»£å™¨çš„æ‰€æœ‰å†…å®¹æ„æˆæ–°çš„rdd Example1ï¼š 123456// data æœ‰ä¸¤è¡Œæ•°æ®ï¼Œç¬¬ä¸€è¡Œ a b cï¼Œç¬¬äºŒè¡Œ 1 2 3scala&gt; data.map(line1 =&gt; line1.split(\",\")).collect()res11: Array[Array[String]] = Array(Array(a, b, c),Array(1, 2, 3))scala&gt; data.flatMap(line1 =&gt; line1.split(\",\")).collect()res13: Array[String] = Array(a, b, c, 1, 2, 3) Example2ï¼š 1234567891011scala&gt; val rdd = sc.parallelize(List(\"coffee panda\",\"happy panda\",\"happiest panda party\"))scala&gt; rdd.map(x=&gt;x).collectres0: Array[String] = Array(coffee panda, happy panda, happiest panda party) scala&gt; rdd.map(x=&gt;x.split(\" \")).collectres1: Array[Array[String]] = Array(Array(coffee, panda), Array(happy, panda), Array(happiest, panda, party))// ç›¸æ¯”ä¹‹å‰ï¼ŒflatMapå°†å‡ ä¸ªarrayåˆå¹¶æˆäº†ä¸€ä¸ªarrayscala&gt; rdd.flatMap(x=&gt;x.split(\" \")).collectres2: Array[String] = Array(coffee, panda, happy, panda, happiest, panda, party) 2. References ç†è§£Sparkçš„æ ¸å¿ƒRDD Spark RDD API Examples","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.com/tags/Spark/"}]},{"title":"å­—ç¬¦ä¸²æœç´¢ç®—æ³• - BM","date":"2017-04-02T04:28:44.000Z","path":"2017/04/02/å­—ç¬¦ä¸²æœç´¢ç®—æ³•-BM/","text":"å½“æˆ‘ä»¬è¦åœ¨æŸä¸€ä¸ªæ–‡æœ¬ä¸­è¦åŒ¹é…æŸä¸€ä¸ªå­—ç¬¦ä¸²çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ä¸€ä¸ªä¸€ä¸ªåŒ¹é…ï¼Œä¹Ÿå°±æ˜¯ä»å¤´å¼€å§‹åŒ¹é…æ–‡æœ¬å’Œå­—ç¬¦ä¸²ï¼Œä½†å‘ç°ä¸åŒçš„æ—¶å€™ï¼Œå°±æŠŠæ•´ä¸ªå­—ç¬¦ä¸²å³ç§»ä¸€ä½ï¼Œç„¶åä»å¤´é‡æ–°å’Œæ–‡æœ¬æ¯”è¾ƒã€‚å½“å…¨éƒ¨ç›¸åŒçš„æ—¶å€™ï¼Œåˆ™å³ç§»å­—ç¬¦ä¸²çš„é•¿åº¦ã€‚åœ¨ä¹‹å‰çš„ KMP ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬æ˜¯æ‰¾åˆ°å°½å¯èƒ½å³ç§»çš„æœ€å¤§ä½æ•°ï¼Œè€Œä¸æ˜¯ä¸€ä½ä¸€ä½çš„ç§»åŠ¨ï¼Œè¿™æ ·ç›¸æ¯”åŸå…ˆæ•ˆç‡å·²ç»æé«˜äº†å¾ˆå¤šäº†ã€‚ä½†åœ¨å¤§éƒ¨åˆ†çš„ç¼–è¾‘å™¨ä¸­ï¼Œâ€œæŸ¥æ‰¾â€åŠŸèƒ½ä½¿ç”¨çš„ä¸æ˜¯ KMP ï¼Œ è€Œæ˜¯ BM ç®—æ³•ã€‚ 1. Boyer-Moore Algorithmå’Œ KMP ä¸åŒçš„æ˜¯ï¼ŒBM æ˜¯ä» Pattern P å€’ç€åŒ¹é…ä¸Šæ¥çš„ é¦–å…ˆå®šä¹‰ä¸¤ä¸ªé¢„å¤„ç†çš„æ–¹æ³• åå­—ç¬¦ (Bad Character Heuristic) å½“æ–‡æœ¬ T ä¸­çš„æŸä¸ªå­—ç¬¦è·Ÿ Pattern P çš„æŸä¸ªå­—ç¬¦ä¸åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬ç§°æ–‡æœ¬ T ä¸­çš„è¿™ä¸ªä¸åŒ¹é…çš„å­—ç¬¦ä¸ºåå­—ç¬¦ã€‚ å¥½åç¼€ (Good Suffix Heuristic) å½“æ–‡æœ¬ T ä¸­çš„æŸä¸ªå­—ç¬¦è·Ÿ Pattern P çš„æŸä¸ªå­—ç¬¦ä¸åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬ç§°æ–‡æœ¬ T ä¸­çš„å·²ç»åŒ¹é…çš„å­—ç¬¦ä¸²ä¸ºå¥½åç¼€ã€‚(å› ä¸ºç®—æ³•ä»å°¾å·´åˆ°å¤´éƒ¨æ¯”è¾ƒ) å¥½åç¼€çš„ä½ç½®ä»¥æœ€åä¸€ä¸ªå­—ç¬¦ä¸ºå‡†ã€‚å‡å®šâ€ABCDEFâ€çš„â€EFâ€æ˜¯å¥½åç¼€ï¼Œåˆ™å®ƒçš„ä½ç½®ä»¥â€Fâ€ä¸ºå‡†ï¼Œå³5ï¼ˆä»0å¼€å§‹è®¡ç®—ï¼‰ã€‚ å¦‚æœâ€å¥½åç¼€â€åœ¨æœç´¢è¯ä¸­åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®ä¸º -1ã€‚æ¯”å¦‚ï¼Œâ€EFâ€åœ¨â€ABCDEFâ€ä¹‹ä¸­åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®ä¸º-1ï¼ˆå³æœªå‡ºç°ï¼‰ã€‚ å¦‚æœâ€å¥½åç¼€â€æœ‰å¤šä¸ªï¼Œåˆ™é™¤äº†æœ€é•¿çš„é‚£ä¸ªâ€å¥½åç¼€â€ï¼Œå…¶ä»–â€å¥½åç¼€â€çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®å¿…é¡»åœ¨å¤´éƒ¨ã€‚æ¯”å¦‚ï¼Œå‡å®šâ€BABCDABâ€çš„â€å¥½åç¼€â€æ˜¯â€DABâ€ã€â€ABâ€ã€â€Bâ€ï¼Œè¯·é—®è¿™æ—¶â€å¥½åç¼€â€çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®æ˜¯ä»€ä¹ˆï¼Ÿå›ç­”æ˜¯ï¼Œæ­¤æ—¶é‡‡ç”¨çš„å¥½åç¼€æ˜¯â€Bâ€ï¼Œå®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®æ˜¯å¤´éƒ¨ï¼Œå³ç¬¬0ä½ã€‚è¿™ä¸ªè§„åˆ™ä¹Ÿå¯ä»¥è¿™æ ·è¡¨è¾¾ï¼šå¦‚æœæœ€é•¿çš„é‚£ä¸ªâ€å¥½åç¼€â€åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å¯ä»¥æŠŠæœç´¢è¯æ”¹å†™æˆå¦‚ä¸‹å½¢å¼è¿›è¡Œä½ç½®è®¡ç®—â€(DA)BABCDABâ€ï¼Œå³è™šæ‹ŸåŠ å…¥æœ€å‰é¢çš„â€DAâ€ã€‚ æŒ‰ç…§æ­£å¸¸æ¯”è¾ƒæ–¹æ³•ï¼Œå½“å‘ç°ä¸€ä¸ªåå­—ç¬¦çš„æ—¶å€™ï¼Œä¼šå‡ºç°ä¸€ä¸‹ä¸‰ç§æƒ…å†µ åå­—ç¬¦å®Œå…¨ä¸å‡ºç°åœ¨ Pattern P ä¸­ï¼Œé‚£ä¹ˆå°±å¯ä»¥å®Œå…¨è·³è¿‡è¿™ä¸ªå­—ç¬¦ï¼Œå› ä¸ºæˆ‘ä»¬æ ¹æœ¬ä¸å¯èƒ½åŒ¹é…åˆ°è¿™ä¸ªå­—ç¬¦ã€‚ åå­—ç¬¦å‡ºç°åœ¨è¿˜æœªæ¯”è¾ƒçš„å‰ç¼€å½“ä¸­ (åªå‡ºç°åœ¨å‰ç¼€ï¼Œå‡ºç°åœ¨å‰ç¼€å’Œåç¼€) æ­¤æ—¶å°† Pattern P å³ç§»å¯¹é½è¿™ä¸¤ä¸ªå­—ç¬¦ï¼Œç„¶åä»å°¾éƒ¨ç»§ç»­æ¯”è¾ƒ åå­—ç¬¦åªå‡ºç°åœ¨ä¹‹å‰çš„å¥½åç¼€å½“ä¸­ï¼ˆä¹Ÿå°±æ˜¯å·²ç»æ¯”è¾ƒè¿‡äº†ï¼‰ å¯¹è¿™ç§æƒ…å†µä¸åšå¤„ç† æ ¹æ®ä¸Šé¢å‡ ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥æ€»ç»“å‡ºä»¥ä¸‹åå­—ç¬¦è§„åˆ™ï¼š åç§»ä½æ•° = åå­—ç¬¦çš„ä½ç½® - æœç´¢è¯ä¸­çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½® è¿™é‡Œæœ‰ä¸¤ç§ç‰¹æ®Šæƒ…å†µ å¦‚æœåå­—ç¬¦ä¸å­˜åœ¨äº Pattern P ä¸­ï¼Œåˆ™æœ€åä¸€æ¬¡å‡ºç°çš„ä½ç½®ä¸º -1ã€‚ å¦‚æœåå­—ç¬¦åœ¨ Pattern P ä¸­çš„ä½ç½®ä½äºå¤±é…ä½ç½®çš„å³ä¾§ï¼Œåˆ™æ­¤å¯å‘æ³•ä¸æä¾›ä»»ä½•å»ºè®®ã€‚ é™¤äº†åå­—ç¬¦ï¼Œæˆ‘ä»¬åŒæ ·å¯ä»¥åˆ©ç”¨å¥½åç¼€æ¥æé«˜æŸ¥æ‰¾æ•ˆç‡ æ¨¡å¼åç§»ä½æ•° = å¥½åç¼€åœ¨æ¨¡å¼ä¸­çš„å½“å‰ä½ç½® - å¥½åç¼€åœ¨æ¨¡å¼ä¸­æœ€å³å‡ºç°ä¸”å‰ç¼€å­—ç¬¦ä¸åŒçš„ä½ç½® ä¸¾ä¸ªæ —å­è®¡ç®—ï¼š 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 åå­—ç¬¦çš„ç§»åŠ¨ æ­¤æ—¶åå­—ç¬¦çš„ä½ç½®æ˜¯4ï¼Œåå­—ç¬¦ I ä¸åœ¨å­—ç¬¦ä¸²å½“ä¸­ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸ºï¼š 4 - (-1) = 5 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 å¥½åç¼€çš„ç§»åŠ¨ æ­¤æ—¶å¥½åç¼€æ˜¯ MPLEï¼Œå…¶ä¸­ PLE å‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³ä½ç½®ä¸º (01) 2ã€‚ æ­¤æ—¶å¥½åç¼€çš„å­—ç¬¦æ˜¯8(ä»¥æœ€åä¸€ä¸ªä¸ºå‡†)ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸ºï¼š8 - 2 = 6 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å¥½åç¼€çš„æŸ¥æ‰¾æ•ˆç‡æ›´é«˜ã€‚ å¦‚ä½•è®¡ç®—å¥½åç¼€æˆ‘ä»¬ä¹‹å‰å·²ç»æè¿‡äº†ï¼Œç»§ç»­ä»¥ä¸Šé¢æœ€åä¸€ä¸ªä¾‹å­è®²è§£ã€‚ MPLE : æœªå‡ºç°ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› PLE : æœªå‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› LE : æœªå‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› E : å‡ºç°åœ¨å¤´éƒ¨ï¼Œè¡¥å……è™šæ‹Ÿå­—ç¬¦ â€˜MPLâ€™Eï¼Œå‰ç¼€å­—ç¬¦ä¸ºç©ºï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º 0ï¼› æ­¤æ—¶ï¼Œæ‰€æœ‰çš„â€å¥½åç¼€â€ï¼ˆMPLEã€PLEã€LEã€Eï¼‰ä¹‹ä¸­ï¼Œåªæœ‰â€Eâ€åœ¨â€EXAMPLEâ€è¿˜å‡ºç°åœ¨å¤´éƒ¨ï¼Œæ‰€ä»¥åç§» 6 - 0 = 6ä½ã€‚ å¦‚æœä¹‹å‰æˆ‘ä»¬åªåˆ©ç”¨åå­—ç¬¦çš„è¯ï¼Œç§»åŠ¨çš„ä½æ•°åªæœ‰ 2 - (-1) = 3ä½ æ‰€ä»¥æˆ‘ä»¬æœ€ç»ˆçš„ç§»åŠ¨ä½ç½®ï¼Œæ˜¯åœ¨è¿™ä¸¤è€…ä¹‹é—´å–æœ€å¤§å€¼æ¥ç§»åŠ¨ã€‚ æ‰€ä»¥åœ¨æŸ¥æ‰¾ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é¢„è®¡ç®—ç”Ÿæˆåå­—ç¬¦è§„åˆ™è¡¨ å’Œ å¥½åç¼€è§„åˆ™è¡¨ï¼Œåˆ°æ—¶å€™åªéœ€è¦æŸ¥è¡¨å°±å¯ä»¥äº†ã€‚ç»§ç»­ä¸Šé¢çš„ä¾‹å­ã€‚ æ ¹æ®ä¸Šé¢è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬å°† Pattern P å³ç§»6ä½ã€‚ç„¶åé‡æ–°ä»å°¾éƒ¨å¼€å§‹æ¯”è¾ƒ æ–‡æœ¬ä¸­Pä¸åŒ¹é…ï¼Œæ­¤æ—¶åªèƒ½ä½¿ç”¨åå­—ç¬¦è§„åˆ™ï¼ŒPä¸Šä¸€æ¬¡å‡ºç°çš„ä½ç½®ä¸º4ï¼Œåˆ™ç§»åŠ¨6-4=2ä½ ç§»åŠ¨2ä½åï¼Œå‘ç°æ–‡æœ¬ä¸­çš„E ä¸ Pattern Pä¸­çš„E åŒ¹é…ï¼Œåˆ™ç»§ç»­å€’åºæ¯”è¾ƒï¼Œç›´åˆ°å‘ç°å…¨éƒ¨åŒ¹é…ï¼Œåˆ™åŒ¹é…åˆ°çš„ç¬¬ä¸€ä¸ªå®Œæ•´çš„æ¨¡å¼ P è¢«å‘ç°ã€‚ ç»§ç»­ä¸‹å»åˆ™æ˜¯ä¾æ®å¥½åç¼€è§„åˆ™è®¡ç®—å¥½åç¼€ â€œEâ€ çš„åç§»ä½ç½®ä¸º 6 - 0 = 6 ä½ï¼Œç„¶åç»§ç»­å€’åºæ¯”è¾ƒæ—¶å‘ç°å·²è¶…å‡ºæ–‡æœ¬ T çš„èŒƒå›´ï¼Œæœç´¢ç»“æŸã€‚ 2. References å­—ç¬¦ä¸²åŒ¹é…çš„Boyer-Mooreç®—æ³• Boyer-Moore å­—ç¬¦ä¸²åŒ¹é…ç®—æ³•","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"PostgreSQLå¤ä¹ ç¬”è®°","date":"2017-03-23T23:38:55.000Z","path":"2017/03/24/PostgreSQLå¤ä¹ ç¬”è®°/","text":"1 å¸¸è§é—®é¢˜1.1 åŒ¹é…ä¸€è‡´çš„ç¼–ç è§„åˆ™123SET character_set_client=&apos;utf8&apos;; -- gbkSET character_set_connection=&apos;utf8&apos;; -- gbkSET character_set_results=&apos;utf8&apos;; -- gbk 1234-- åˆ›å»ºæ•°æ®åº“æ—¶å€™è®¾ç½®CREATE DATABASE `test`CHARACTER SET &apos;utf8&apos;COLLATE &apos;utf8_general_ci&apos;; 1.2 å¦‚ä½•ç†è§£ç´¢å¼•ç´¢å¼•å­—é¢ä¸Šç†è§£å°±æ˜¯å¯¹æ•°æ®æ‰€å»ºç«‹ç›®å½•ï¼Œå®ƒå¯ä»¥åŠ å¿«æˆ‘ä»¬çš„æŸ¥è¯¢é€Ÿåº¦ï¼Œä½†æ˜¯åŒæ—¶ä¹Ÿé™ä½äº†å¢åˆ æ”¹çš„é€Ÿåº¦ã€‚ åˆ›å»ºåŸåˆ™ ä¸è¦è¿‡åº¦ä½¿ç”¨ç´¢å¼• æœ€å¥½åœ¨æŸ¥è¯¢é¢‘ç¹çš„åˆ—ä¸Šä½¿ç”¨ç´¢å¼• å¦‚æœæ„å»ºç´¢å¼•ï¼Œè¿™ä¸€åˆ—å°½é‡æ˜¯ç¦»æ•£å€¼ï¼Œè€Œä¸è¦è¿‡äºè¿ç»­çš„åŒºé—´ ç´¢å¼•çš„ç±»å‹ æ™®é€šçš„ç´¢å¼• index å”¯ä¸€çš„ç´¢å¼• unique index ä¸€å¼ è¡¨ä¸Šï¼Œåªèƒ½æœ‰ä¸€ä¸ªä¸»é”®ï¼Œä½†æ˜¯å¯ä»¥æœ‰ä¸€ä¸ªæˆ–æ˜¯å¤šä¸ªå”¯ä¸€ç´¢å¼• ä¸»é”®ç´¢å¼• primary key ä¸èƒ½é‡å¤ 12-- æŸ¥çœ‹ä¸€å¼ è¡¨ä¸Šçš„æ‰€æœ‰ç´¢å¼•show index from TABLE_NAMES; 1.3 æ¨¡ç³ŠæŸ¥è¯¢ % åŒ¹é…ä»»æ„å­—ç¬¦ _ åŒ¹é…å•ä¸ªå­—ç¬¦ ä¸¾ä¸ªæ —å­ 1234567891011SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC%&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC_&apos;; 1.4 ç†è§£ COUNT è§3.1 èšé›†å‡½æ•° 1.5 ç†è§£ UNION å’Œ UNION ALL UNION ç”¨äºåˆå¹¶ä¸¤ä¸ªæˆ–æ˜¯å¤šä¸ªSELECTè¯­å¥çš„ç»“æœé›† æ³¨æ„ï¼š SELECTè¯­å¥å¿…é¡»æ‹¥æœ‰ç›¸åŒçš„æ•°é‡çš„åˆ— åˆ—çš„éœ€è¦æ‹¥æœ‰ç›¸ä¼¼çš„æ•°æ®ç±»å‹ æ¯æ¡SELECTè¯­å¥ä¸­çš„åˆ—çš„é¡ºåºå¿…é¡»æ˜¯ä¸€è‡´çš„ ç»“æœä¸å…è®¸æœ‰é‡å¤ï¼Œå¦åˆ™ä½¿ç”¨ UNION ALL 123456SELECT mid, sex, ageFROM TABLE_1UNION -- UNIAON ALL -- å…è®¸æœ‰é‡å¤çš„å€¼å‡ºç°åœ¨ç»“æœé›†ä¸­SELECT mid, sex, ageFROM TABLE_2;-- ORDER BY mid; -- å¯ä»¥å¯¹å…¶ç»“æœè¿›è¡Œæ’åºï¼Œæ³¨æ„çš„æ˜¯æ’åºåªæ˜¯é’ˆå¯¹åˆå¹¶åçš„ç»“æœé›†æ’åº 1.6 ç†è§£ JOINï¼ˆå·¦é“¾æ¥ï¼Œå†…é“¾æ¥å’Œå¤–é“¾æ¥ï¼‰ ä¸åŒçš„å‡ ç§JOINç±»å‹ï¼Œä»¥åŠä¹‹é—´çš„å·®å¼‚ JOINï¼šå¦‚æœè¡¨ä¸­è‡³å°‘æœ‰ä¸€ä¸ªåŒ¹é…ï¼Œåˆ™è¿”å›è¡Œ LEFT JOINï¼šå³ä½¿å³è¡¨ä¸­æ²¡æœ‰åŒ¹é…ï¼Œä¹Ÿä»å·¦è¾¹è¿”å›æ‰€æœ‰çš„è¡Œ RIGHT JOINï¼šå³ä½¿å·¦è¡¨ä¸­æ²¡æœ‰åŒ¹é…ï¼Œä¹Ÿä»å³è¡¨ä¸­è¿”å›æ‰€æœ‰çš„è¡Œ FULL JOINï¼šåªè¦å…¶ä¸­ä¸€ä¸ªè¡¨å­˜åœ¨åŒ¹é…ï¼Œå°±è¿”å›è¡Œ INNER JOIN å¹³å¸¸æˆ‘ä»¬éœ€è¦é“¾æ¥ä¸¤ä¸ªè¡¨çš„æ—¶å€™ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹æ³• 123SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨JOINæ¥å®ç°ä¸Šé¢çš„è¯­å¥ 12345SELECT Person.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName LEFT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName æ³¨æ„åˆ°ä¸Šé¢çš„ï¼Œå·¦è¾¹æ‰€æœ‰çš„è¡Œéƒ½è¿”å›äº†ï¼Œå³ä½¿æ²¡æœ‰å‡ºç°åœ¨å³è¡¨å½“ä¸­ï¼Œæ²¡æœ‰çš„å€¼ä¸ºNULL RIGHT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName æ³¨æ„ï¼Œå³ä½¿å·¦è¾¹æ²¡æœ‰å…¨éƒ¨åŒ¹é…åˆ°å³è¾¹ï¼Œä¾ç„¶åœ¨æœ€åçš„OrderNoä¸­ï¼Œè¿”å›äº†å³è¡¨æ‰€æœ‰çš„è¡Œæ•°ï¼Œæ²¡æœ‰çš„å€¼ä¸ºNULL 1.7 ç†è§£ HAVING è§3.2 åœ¨SQLä¸­å¢åŠ HAVINGå­å¥çš„åŸå› æ˜¯ç”±äºWHEREä¸­æ— æ³•ä½¿ç”¨èšåˆå‡½æ•° 2 æ•°æ®åº“çš„åŸºæœ¬æ“ä½œ2.1 è¡¨çš„ä½¿ç”¨ ä½¿ç”¨åˆ—çº¦æŸå»ºè¡¨ 12345678CREATE [TEMPORARY] TABLE è¡¨å ( -- TEMPORARY å»ºç«‹ä¸€å¼ ä¸´æ—¶çš„è¡¨col_not_null INT NOT NULL, -- åˆ—å ç±»å‹ &#123;çº¦æŸ1 çº¦æŸ2 ...&#125;col_unique INT UNIQUE,col_prikey INT PRIMARY KEY, -- NOT NULL + UNIQUE ä¸»é”®col_default INT DEFAULT 42,col_check INT CHECK(col_check &lt; 42)col_ref INT REFERENCES -- çº¦æŸè¿™ä¸ªå€¼å¿…é¡»æ˜¯å¦ä¸€ç‹¬ç«‹çš„è¡¨çš„æŸä¸ªåˆ—ä¸­çš„æŸä¸ªå€¼); ä½¿ç”¨è¡¨çº§çº¦æŸå»ºè¡¨ 1234567CREATE TABLE è¡¨å ( myKey_1 INT, myKey_2 INT, myString varchar(15), CONSTRAINT cs1 CHECK (myString &lt;&gt; &apos;&apos;), -- ä¸èƒ½ä½ç©ºå­—ç¬¦ä¸² CONSTRAINT CS2 PRIMARY KEY (myKey_1, myKey_2)); -- ä¿®æ”¹è¡¨ç»“æ„ æ·»åŠ æ–°åˆ— 1ALTER TABLE è¡¨å ADD COLUMN åˆ—å ç±»å‹; é‡å‘½åæ–°æ·»åŠ çš„åˆ— 1ALTER TABLE è¡¨å RENAME COLUMN åˆ—å TO æ–°åˆ—å; æ”¹å˜ä¸€äº›çº¦æŸå’Œå…¶ä»–è§„åˆ™ 12ALTER TABLE è¡¨å DROP CONSTRAINT cs1; -- DROPçº¦æŸALTER TABLE è¡¨å ADD CONSTRAINT cs3 UNIQUE(åˆ—å); --æ·»åŠ æ–°çš„çº¦æŸ ä¿®æ”¹åˆ—çš„ç±»å‹ 1ALTER TABLE è¡¨å ALTER åˆ—å TYPE æ–°ç±»å‹; é‡å‘½åè¡¨å 1ALTER TABLE è¡¨å RENAME TO æ–°è¡¨å; ä½¿ç”¨ä¸´æ—¶è¡¨ ä¸´æ—¶è¡¨çš„åŠŸèƒ½åŸºæœ¬å’Œè¡¨æ˜¯å·®ä¸å¤šçš„ï¼ŒåŒºåˆ«åœ¨äºå½“ä½ çš„ä¼šè¯ç»“æŸæ—¶ï¼Œä½ ä¸æ•°æ®åº“è¿æ¥æ–­å¼€åï¼Œä¸´æ—¶è¡¨ä¼šè‡ªåŠ¨è¢«åˆ é™¤ã€‚ é”®çš„çº¦æŸ ä½œä¸ºä¸€ä¸ªåˆ—çš„çº¦æŸçš„å¤–é”®ï¼ˆåˆ—çº¦æŸï¼‰ 1234567CREATE TABLE è¡¨å ( ... ... customer_id INTEGER NOT NUll REFERENCES customer(customer_id), -- å…³è”åˆ°customerè¡¨ ... ...);-- REFERENCES å¤–è¡¨å(å¤–è¡¨åä¸­çš„åˆ—) è¡¨çº§çº¦æŸ 123456CREATE TABLE è¡¨å( ... ... customer_id INTEGER NOT NULL, ... ... CONSTRAINT è¡¨å_åˆ—å_fk FOREIGN KEY(åˆ—å) REFERENCES å¤–è¡¨å(å¤–é¢ä¸­çš„åˆ—)) æ³¨æ„ï¼šæ¯”è¾ƒæ¨èçš„æ˜¯ä½¿ç”¨è¡¨çº§çº¦æŸï¼Œè€Œä¸æ˜¯æ··å’Œç§ç”¨è¡¨çº§å’Œåˆ—çº§çº¦æŸ â€‹ çº¦æŸåè¡¨å_åˆ—å_fkå…è®¸å¤–é¢æ›´å®¹æ˜“å®šä½é”™è¯¯èµ„æº 2.2 è§†å›¾ å»ºç«‹è§†å›¾ 123CREATE VIEW è§†å›¾çš„åå­— AS selectç³»åˆ—è¯­å¥;-- ä¾‹å­CREATE VIEW item_price AS SELECT item_id, description, sell_price FROM item; å½“è§†å›¾å»ºç«‹å¥½çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨è¡¨ä¸€æ ·æ¥æŸ¥è¯¢è¿™ä¸ªè§†å›¾ï¼Œå¯ä»¥ä½¿ç”¨SELECTæˆ–WHEREè¯­å¥ç­‰ã€‚ æ¯æ¬¡åœ¨è§†å›¾ä¸­æ‰§è¡ŒSELECTæ—¶ï¼Œæ•°æ®éƒ½ä¼šè¢«é‡å»ºï¼Œæ‰€ä»¥æ•°æ®æ€»æ˜¯æœ€æ–°çš„ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåœ¨è§†å›¾è¢«å»ºç«‹çš„æ—¶å€™å†»ç»“çš„æ‹·è´ã€‚ä¹Ÿå°±æ˜¯å½“ä¸ä¹‹ç›¸å…³çš„è¡¨çš„æ•°æ®å‘ç”Ÿè¯¥è¡¨çš„æ—¶å€™ï¼ŒVIEWé‡Œé¢çš„æ•°æ®ä¹Ÿéšä¹‹æ”¹å˜ï¼Œè€Œä¸æ˜¯å‚¨å­˜äº†å»ºç«‹VIEWçš„æ—¶å€™çš„æ‹·è´å¯¹è±¡ã€‚æˆ–è€…ä¹Ÿå¯ä»¥ç†è§£ç±»ä¼¼æŒ‡é’ˆæŒ‡å‘åŸå…ˆçš„è¡¨ï¼Œå½“åŸå…ˆçš„è¡¨å‘ç”Ÿå˜åŒ–ï¼Œè¿™è¾¹çš„æ•°æ®è‡ªç„¶è€Œç„¶çš„å°±èƒ½å¤Ÿè¯»å–å‡ºæ¥ã€‚ å½“ç„¶ï¼ŒSELECTè¯­å¥æ˜¯å¯ä»¥åœ¨å¤šä¸ªä¸åŒçš„è¡¨ä¸­æå–æ•°æ®çš„ã€‚ åˆ é™¤å’Œæ›¿æ¢VIEW 12DROP VIEW åå­—; -- ä¸å½±å“æˆ‘ä»¬å·²æœ‰çš„æ•°æ®CREATE OR REPLACE VIEW åå­— AS æ–°çš„selectç³»åˆ—è¯­å¥; ä¸€äº›ä¸VIEWå¸¸ç”¨çš„æŒ‡ä»¤ 12\\dv -- æŸ¥çœ‹å½“å‰æ•°æ®åº“ä¸­çš„æ‰€æœ‰çš„VIEW\\d VIEWçš„åå­— -- æŸ¥çœ‹å…·ä½“çš„æŸä¸€ä¸ªVIEWçš„ç»“æ„ 2.3 INSERTè¯­å¥ åŸºæœ¬æ’å…¥è¯­å¥ 123INSERT INTO è¡¨å VALUES (æ¯åˆ—çš„å€¼çš„åˆ—è¡¨);INSERT INTO customer VALUES(18, &apos;Mr&apos;, &apos;Jeff&apos;, &apos;Baggott&apos;, &apos;Midtown Street A\\\\33&apos;, &apos;Milltown&apos;, &apos;MT9 8NQ&apos;, &apos;746 3956&apos;); PSï¼šè¿™ç§æ“ä½œå¾ˆå±é™©ï¼ŒSQLæ³¨å…¥æ”»å‡» æ¨èçš„å®‰å…¨æ–¹æ³• 12345INSERT INTO è¡¨å(åˆ—åçš„åˆ—è¡¨) VALUES(è·Ÿä¹‹å‰çš„åˆ—çš„åˆ—è¡¨å¯¹åº”åˆ—çš„æ•°å€¼);INSERT INTO customer(customer_id, title, fname, lname, addressline, ...)VALUES(19, &apos;Mrs&apos;, &apos;Sarah&apos;, &apos;Harvey&apos;, &apos;84 Willow Way&apos;, ...); PSï¼šé¿å…åœ¨æ’å…¥æ•°æ®çš„æ—¶å€™ä¸ºserialç±»å‹çš„æ•°æ®æä¾›æ•°å€¼ï¼Œå› ä¸ºè¿™ä¸ªæ˜¯ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ çš„ è®¿é—®åºåˆ—ç”Ÿæˆå™¨ åºåˆ—ç”Ÿæˆå™¨æ€» æ˜¯è¢«å‘½åä¸º&lt;è¡¨å&gt;_&lt;åˆ—å&gt;_seq 123currval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;);nextval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;);setval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;, æ–°çš„å€¼); æ’å…¥ç©ºå€¼ 123INSERT INTO customer VALUES(16, &apos;Mr&apos;, NULL, &apos;Smith&apos;,&apos;23 Harlestone&apos;, &apos;Milltown&apos;, &apos;MT7 7HI&apos;, &apos;746 3725&apos;); ä½¿ç”¨ \\copy å‘½ä»¤ æ­¥éª¤ å…ˆç”Ÿæˆå¦‚ä¸‹æ ¼å¼çš„æ•°æ® â€‹ å†ç”Ÿæˆå¦‚ä¸‹æ ¼å¼çš„æ•°æ®ï¼Œä¿å­˜æˆ.sqlæ‹“å±•åçš„æ–‡æœ¬æ–‡ä»¶ â€‹ ä½¿ç”¨\\copyå‘½ä»¤å¯¼å…¥æ•°æ® â€‹ PSï¼šSQL é‡Œå¤´çš„ COPY å‘½ä»¤æœ‰ä¸€ä¸ªä¼˜ç‚¹ï¼šå®ƒæ˜æ˜¾æ¯”\\copy å‘½ä»¤å¿«ï¼Œå› ä¸ºå®ƒç›´æ¥é€šè¿‡æœåŠ¡å™¨è¿›ç¨‹æ‰§è¡Œã€‚\\copy å‘½ä»¤æ˜¯åœ¨å®¢æˆ·è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œæœ‰å¯èƒ½éœ€è¦é€šè¿‡ç½‘ç»œä¼ è¾“æ‰€æœ‰æ•°æ®ã€‚è€Œä¸” COPY åœ¨å‘ç”Ÿé”™è¯¯çš„æ—¶å€™ä¼šæ›´å¯é ã€‚é™¤éä½ æœ‰å¤§é‡çš„æ•°æ®ï¼Œå¦åˆ™åŒºåˆ«ä¸ä¼šå¤ªæ˜æ˜¾ã€‚ 2.5 ä»æ•°æ®åº“ä¸­åˆ é™¤æ•°æ® DELETEè¯­å¥ è¯­æ³•ç±»ä¼¼äºUPDATEè¯­å¥ 1DELETE FROM è¡¨å WHERE æ¡ä»¶; TRUNCATEè¯­å¥ï¼ˆä¸æ¨èï¼Œå› ä¸ºä¸å®‰å…¨ï¼‰ TRUNCATEè¯­å¥æ˜¯æŠŠè¡¨ä¸­æ‰€æœ‰çš„æ•°æ®éƒ½åˆ é™¤ï¼Œä½†æ˜¯ä¿ç•™è¿™å¼ è¡¨çš„ç»“æ„ï¼Œä¹Ÿå°±æ˜¯è¯´æœ€åå‰©ä¸‹äº†ä¸€å¼ ç©ºè¡¨ï¼Œæ‰€æœ‰çš„è¡Œéƒ½è¢«åˆ é™¤äº†ã€‚ 1TRUNCATE TABLE è¡¨å; DROPè¯­å¥ DROPè¯­å¥å°±æ˜¯åˆ é™¤äº†æ•´å¼ è¡¨çš„å†…å®¹ï¼ŒåŒ…æ‹¬è¡¨çš„ç»“æ„ã€‚DROPå®Œä¹‹åï¼Œè¿™å¼ è¡¨å°±æ˜¯ä¸å­˜åœ¨çš„äº† 1DROP TABLE è¡¨å; 2.6 ä¿®æ”¹æ•°æ®åº“ä¸­çš„æ•°æ® UPDATEè¯­å¥ 1UPDATE è¡¨å SET åˆ—å = å€¼ WHERE æ¡ä»¶; 1UPDATE customer SET town = &apos;Leicester&apos;, zipcode = &apos;LE4 2WQ&apos; WHERE ä¸€äº›æ¡ä»¶; å¦‚æœæ²¡æœ‰WHEREå­å¥çš„è¯ï¼Œä¼šå¯¼è‡´è¡¨ä¸­çš„å¾ˆå¤šç”šè‡³æ˜¯æ‰€æœ‰çš„è¡Œéƒ½è¢«åŒæ—¶æ›´æ–°äº† é€šè¿‡å¦ä¸€ä¸ªè¡¨æ›´æ–° 1UPDATE è¡¨å FROM è¡¨å WHERE æ¡ä»¶; 3 é«˜çº§æ•°æ®é€‰æ‹©3.1 èšé›†å‡½æ•° Group By and count(*) é”™è¯¯ä½¿ç”¨ 1SELECT count(*), town FROM customer; æ­£ç¡®ä½¿ç”¨ 1SELECT count(*), town FROM customer GROUP BY town; ç»“æœæ˜¯è·å¾—ä¸€ä¸ªåŸé•‡çš„åˆ—è¡¨ä»¥åŠæ¯ä¸ªåŸé•‡çš„å®¢æˆ·æ•°é‡ï¼ˆcount(*)) åŒæ—¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨ä¸¤ä¸ªcolumns nameåœ¨GROUP BYä¸­ï¼Œç„¶åç”¨ORDER BYæŒ‡å®šæ’åˆ—é¡ºåºã€‚æ²¡æœ‰GROUP BYçš„è¯æŒ‰ç…§GROUP BYä¸­çš„townï¼Œlnameæ’åº Having Havingæ˜¯ä¸€ç§ç”¨äºèšé›†å‡½æ•°çš„WHEREä»å¥ï¼Œæˆ‘ä»¬ä½¿ç”¨HAVINGæ¥çº¦æŸè¿”å›çš„ç»“æœä¸ºé’ˆå¯¹ç‰¹å®šçš„èšé›†çš„æ¡ä»¶ä¸ºçœŸçš„è¡Œï¼Œæ¯”å¦‚count(*) &gt; 1 PSï¼šèšé›†å‡½æ•°æ— æ³•åœ¨WHEREä»å¥ä¸­ä½¿ç”¨ï¼Œåªèƒ½ç”¨åœ¨HAVINGä»å¥ä¸­ ä¸¾ä¸ªæ —å­ï¼š é€‰å‡ºæœ‰è¶…è¿‡ä¸€ä¸ªå®¢æˆ·çš„åŸé•‡ï¼Œåœ¨é‡Œä½¿ç”¨ä¸€ä¸ªHAVINGä»å¥æ¥çº¦æŸå¤§ä¸€çš„è¡Œ SELECTä¸­ä»å¥çš„ä¼˜å…ˆåº¦ 1234567SELECTFROM WHEREGROUP BYHAVINGORDER BY -- DESC ä¸‹é™LIMIT -- ç”¨äºé™åˆ¶rowsæ˜¯è¡Œæ•° mysql count(Column_Name) count(*) ç»Ÿè®¡æ‰€æœ‰çš„è¡Œ count(column_name) ç»Ÿè®¡æ‰€è¿™ä¸ªåˆ—ä¸­å€¼ä¸æ˜¯NULLçš„è¡Œ count(Distinct column) åªç»Ÿè®¡è¿™ä¸ªåˆ—ä¸­å”¯ä¸€çš„æƒ…å†µï¼Œä¸é‡å¤ç»Ÿè®¡ min min å‡½æ•°ä½¿ç”¨ä¸€ä¸ªåˆ—ååšå‚æ•°ä¸”è¿”å›è¿™ä¸ªåˆ—ä¸­æœ€å°çš„å€¼ã€‚å¯¹äº numeric ç±»å‹çš„åˆ—ï¼Œç»“æœåº”è¯¥å’Œé¢„æœŸä¸€ æ ·ã€‚å¯¹äºæ—¶æ€ç±»å‹ï¼Œä¾‹å¦‚ date çš„å€¼ï¼Œå®ƒè¿”å›æœ€å°çš„æ—¥æœŸï¼Œæ—¥æœŸæ—¢å¯ä»¥æ˜¯è¿‡å»ä¹Ÿå¯ä»¥æ˜¯æœªæ¥ã€‚å¯¹äºå˜é•¿çš„å­—ç¬¦ä¸²ï¼ˆvarchar ç±» å‹ï¼‰ï¼Œç»“æœå¯èƒ½å’Œé¢„æœŸæœ‰ç‚¹ä¸åŒï¼šå®ƒåœ¨å­—ç¬¦ä¸²å³è¾¹æ·»åŠ ç©ºç™½åå†è¿›è¡Œæ¯”è¾ƒã€‚ min å‡½æ•°å¿½ç•¥ NULL å€¼ã€‚å¿½ç•¥ NULL å€¼ æ˜¯æ‰€æœ‰çš„èšé›†å‡½æ•°çš„ä¸€ä¸ªç‰¹ç‚¹ï¼Œé™¤äº† count(*)ï¼ˆå½“ç„¶ï¼Œæ˜¯å¦ä¸€ä¸ªç”µè¯å·ç æ˜¯æœ€å°å€¼åˆæ˜¯å¦ä¸€ä¸ªé—®é¢˜äº† PSï¼šå°å¿ƒåœ¨ varchar ç±»å‹çš„åˆ—ä¸­ä½¿ç”¨ min æˆ–è€… maxï¼Œå› ä¸ºç»“æœå¯èƒ½ä¸æ˜¯ä½ é¢„æœŸçš„ã€‚ max sum Sum å‡½æ•°ä½¿ç”¨ä¸€ä¸ªåˆ—åä½œä¸ºå‚æ•°å¹¶æä¾›åˆ—çš„å†…å®¹çš„åˆè®¡ã€‚å’Œ min å’Œ max ä¸€æ ·ï¼ŒNULL å€¼è¢«å¿½ç•¥ã€‚ å’Œ count ä¸€æ ·ï¼Œsum å‡½æ•°æ”¯æŒ DISTINCT å˜ä½“ã€‚ä½ å¯ä»¥è®©å®ƒåªç»Ÿè®¡ä¸é‡å¤å€¼çš„å’Œï¼Œæ‰€ä»¥å¤šæ¡å€¼ç›¸åŒçš„è¡Œåªä¼šè¢«åŠ ä¸€ æ¬¡ avg æˆ‘ä»¬è¦çœ‹çš„æœ€åä¸€ä¸ªèšé›†å‡½æ•°æ˜¯ avgï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªåˆ—ååšå‚æ•°å¹¶è¿”å›è¿™ä¸ªåˆ—æ•°å€¼çš„å¹³å‡å€¼ã€‚å’Œ sum ä¸€æ ·ï¼Œå®ƒå¿½ç•¥ NULL å€¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹ â€‹ 3.2 å­æŸ¥è¯¢ é—®é¢˜ä¸€ æ‰¾åˆ°ä»·æ ¼æ¯”å¹³å‡ä»·æ ¼é«˜çš„å•†å“é¡¹ç›® æ–¹æ³•ä¸€ï¼ˆåœŸæ–¹æ³•ï¼‰ æ–¹æ³•äºŒï¼ˆç”¨åµŒå¥—WHEREä»å¥ï¼‰ é—®é¢˜äºŒ æ‰¾åˆ°é‚£äº›æˆæœ¬é«˜äºå¹³å‡æˆæœ¬ä½†å”®ä»·ä½äºå¹³å”®ä»·çš„äº§å“ æ–¹æ³•ä¸€ï¼ˆåœŸæ–¹æ³•ï¼‰ æ–¹æ³•äºŒï¼ˆç”¨åµŒå¥—WHEREä»å¥ï¼‰ é—®é¢˜ä¸‰ - è¿”å›å¤šè¡Œè®°å½•çš„å­æŸ¥è¯¢ ä¹‹å‰çš„ä¸¤ä¸ªé—®é¢˜ä¸­ï¼ŒWHEREä¸­çš„å­æŸ¥è¯¢ä¸­çš„SELECTå­—å¥è¿”å›çš„æœ€ååªæœ‰ä¸€ä¸ªå€¼â€”â€”å› ä¸ºç”¨äº†count()èšé›†å‡½æ•°ã€‚å¦‚æœWHEREä¸­çš„SELECTå­å¥è¿”å›å¤šä¸ªç»“æœå€¼å‘¢ï¼Ÿ ç­”æ¡ˆæ˜¯ç”¨ WHERE column_name IN (RESULTS) å½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨NOT IN æ¥æ’å‡ºé€‰é¡¹ 3.3 ç›¸å…³å­æŸ¥è¯¢â€‹ åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œè¿™é‡Œçš„ä¸¤ä¸ªSELECTå®é™…ä¸Šæ˜¯ä¸ç›¸å…³çš„ï¼Œä¹Ÿå°±æ˜¯åœ¨å†…éƒ¨çš„SELECTçš„ç»“æœåŸºç¡€ä¸Šï¼Œå¤–éƒ¨SELECTå†åšç»§ç»­æŸ¥è¯¢ â€‹ ä½†æ˜¯ç›¸å…³å­æŸ¥è¯¢åˆ™æ˜¯å†…å¤–çš„SELECTä¸­ï¼Œè¡¨ä¸è¡¨ä¹‹é—´æ˜¯æœ‰å…³ç³»çš„ æ ¼å¼ PSï¼šå»ºè®®åœ¨ç›¸å…³å­æŸ¥è¯¢ä¸­ä½¿ç”¨è¡¨çš„åˆ«å 3.4 UNIONé“¾æ¥ æ ¼å¼ 12345SELECT town FROM tcustUNIONSELECT town FROM customer;SELECT town, zipcode FROM tcust UNION SELECT town, zipcode FROM customer; PSï¼šUNION è¿æ¥çš„ä½¿ç”¨æœ‰ä¸€äº›é™åˆ¶ã€‚ä½ è¦è¿æ¥çš„ä¸¤ä¸ªä»ä¸¤ä¸ªè¡¨ä¸­æŸ¥æ‰¾åˆ—è¡¨çš„åˆ—å¿…é¡»æœ‰ç›¸åŒåˆ—æ•°ï¼Œè€Œä¸”é€‰æ‹©çš„æ¯ä¸ªåˆ—å¿…é¡»éƒ½æœ‰ç›¸å…¼å®¹çš„ç±»å‹ã€‚ è¿™ä¸ªæŸ¥è¯¢ï¼Œè™½ç„¶éå¸¸æ— æ„ä¹‰ï¼Œä½†æ˜¯æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸º PostgreSQL å¯ä»¥è¿æ¥è¿™ä¸¤ä¸ªåˆ—ï¼Œå³ä½¿ title æ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„åˆ—è€Œ town æ˜¯ä¸€ä¸ªå˜é•¿çš„åˆ—ï¼Œå› ä¸ºä»–ä»¬éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ã€‚ä¾‹å¦‚å¦‚æœæˆ‘ä»¬å°è¯•è¿æ¥ customer_id å’Œ townï¼ŒPostgreSQL ä¼šå‘Šè¯‰æˆ‘ä»¬ æ— æ³•åšåˆ°ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªåˆ—çš„ç±»å‹ä¸åŒã€‚ 3.5 è‡ªè¿æ¥ 3.6 å¤–é“¾æ¥4. è¡¨çš„ç®¡ç†â€‹ 5. äº‹åŠ¡å’Œé”6. References","tags":[{"name":"SQL","slug":"SQL","permalink":"http://chenson.com/tags/SQL/"}]},{"title":"Machine Learning - Linear Regression and Logistic Regression","date":"2017-03-17T01:06:30.000Z","path":"2017/03/17/Machine-Learning-Linear-Regression-and-Logistic-Regression/","text":"1. ä¸‰è¦ç´ å½“ä¸€å¼€å§‹æ¥è§¦Andrewåœ¨Courseraä¸Šçš„MLå…¬å¼€è¯¾çš„æ—¶å€™ï¼Œå¯¹çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’è¿™ä¸¤ç§æ¨¡å‹æœ‰ä¸ªå¤§ä½“çš„è®¤è¯†ã€‚ä½†æ˜¯åœ¨ä¸Šå®Œcs229çš„å‰ä¸‰èŠ‚è¯¾ï¼Œåˆæ­¥äº†è§£äº†è¿™ä¸¤ç§æ¨¡å‹èƒŒåçš„æ•°å­¦æ¨¡å‹ï¼ŒLinear Regressionå’ŒLogistic RegressionèƒŒåçš„æ¦‚ç‡åˆ†å¸ƒï¼Œäº†è§£åˆ°äº†è¿™ä¸¤ç§æ¦‚ç‡åˆ†å¸ƒå…¶å®åªæ˜¯exponential familyä¸­çš„ç‰¹ä¾‹ã€‚ä½†åŒæ—¶ä¹Ÿå¼€å§‹å¯¹ä¸€äº›æ¦‚å¿µæ€§çš„ä¸œè¥¿æ„Ÿè§‰å¾ˆæ¨¡ç³Šï¼Œæ‰€ä»¥è§‰å¾—æœ‰å¿…è¦å¥½å¥½æ•´ç†ä¸€ä¸‹è¿™éƒ¨åˆ†çš„å†…å®¹ã€‚ 1. 1 Hypothesisé¦–å…ˆå¯¹äºæ ·æœ¬æ•°æ®ï¼Œè¾“å…¥xå’Œè¾“å‡ºyä¹‹é—´æ˜¯é€šè¿‡Target functionåœ¨è½¬æ¢çš„ï¼Œä¹Ÿå°±æ˜¯ Target function f(x) = yã€‚ä½†æ˜¯æˆ‘ä»¬å¹¶ä¸çŸ¥é“è¿™ä¸ªf(x)éƒ½æ˜¯æ€æ ·çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å‡è®¾äº†è¿™ä¹ˆä¸€ä¸ªHypothesis functionå»æ¨¡æ‹Ÿè¿™ä¸ªTarget functionï¼Œä½¿å¾—æˆ‘ä»¬ç”¨åŒæ ·çš„è¾“å…¥xä¼šçš„ä¸€ä¸ªé¢„æµ‹å€¼yâ€™ï¼Œä½¿å¾—è¿™ä¸ªyâ€™ä¸æ–­é€¼è¿‘çœŸå®å€¼yã€‚ Linear Regression$$H(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx$$ Logistic Regression$$H(x) = g(\\theta^Tx) = \\frac 1 {1 + e^{-\\theta^Tx}}$$ç­‰ä»·äºï¼ˆå³log oddsï¼Œlogitï¼‰$$ln \\frac y {1 - y} = \\theta^T x = ln \\frac {p(y=1| x; \\theta)} {p(y=0| x; \\theta)}$$ 1. 2 Cost functionCost functionå‘¢ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥å«åšError functionï¼Œå°±æ˜¯ç”¨æˆ‘ä»¬ä¸Šé¢å‡è®¾çš„Hypothe functionæ‰€é¢„æµ‹å‡ºæ¥çš„å€¼yâ€™å’ŒçœŸå®å€¼yä¹‹é—´çš„è¯¯å·®ã€‚è€Œæˆ‘ä»¬éœ€è¦åšçš„æ˜¯æ ¹æ®å‡è®¾å‡ºçš„Hypothesis functionï¼Œå–ä¸€ä¸ªåˆé€‚çš„æƒé‡å€¼ï¼Œå³thetaçš„å€¼ï¼Œä½¿å…¶å–çš„ä¸€ä¸ªè¾ƒä½çš„costï¼Œä¹Ÿå°±æ˜¯è¿™é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®æœ€å°ã€‚ Ordinary Least Squares (Square Loss Function) å¸¸ç”¨çš„æ–¹æ³•æ˜¯æœ€å°äºŒä¹˜æ³• $$J(\\theta) = \\frac 1 2 \\sum_{i=i}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$ â€‹ å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»æ¦‚ç‡çš„è§’åº¦æ¥ç†è§£è¿™ä¸ªé—®é¢˜$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$â€‹ è¿™é‡Œçš„Ïµæ˜¯æˆ‘ä»¬é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œè¿™ä¸ªé—®é¢˜æˆ‘ä»¬ä¼šç•™åˆ°åé¢é‡ç‚¹è®²è§£ã€‚ 0-1 Loss Function Absolute Loss Function Log Loss Function 1.3 Algorithmè‡³äºæ€ä¹ˆä½¿å¾—ä¸Šé¢çš„cost functionæœ€å°å‘¢ï¼Œå› ä¸ºå¯¹äºæŸäº›æ•°æ®ï¼Œå…¶featuresæœ‰æˆåƒä¸Šç™¾ä¸ªï¼Œæˆ‘ä»¬å¾ˆéš¾å»æ‰¾åˆ°è¿™ä¸ªæœ€å°çš„æå€¼ç‚¹ï¼Œä½¿å¾—cost functionæœ€ä¸‹ï¼Œæ‰€ä»¥è¿™ä¸ªAlgorithmå°±æ˜¯ç”¨æ¥æ‰¾cost functionçš„æœ€å°å€¼çš„ã€‚å¸¸ç”¨çš„æ–¹æ³•æœ‰å¦‚ä¸‹ Gradient Descent åœ¨æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯ LMS update rules(Least Mean Squares)$$\\theta_j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}= \\alpha e^{(i)} x^{(i)}$$ å½“æˆ‘ä»¬çš„é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹å‰çš„è¯¯å·®Ïµå¾ˆå°æ—¶ï¼Œæˆ‘ä»¬å°±åªéœ€è¦å¯¹Î¸åšå‡ºå¾ˆå°çš„è°ƒæ•´ï¼Œåä¹‹ï¼Œè¯´æ˜å½“å‰çš„Î¸ä¸å¯¹ï¼Œéœ€è¦è°ƒæ•´çš„å¹…åº¦æ¯”è¾ƒå¤§ã€‚ç›´åˆ°æœ€åæ”¶æ•›ä¸ºæ­¢ã€‚ ä¸Šé¢Repeatä¸­çš„æ­¥éª¤å®é™…æ˜¯ç­‰åŒäºcost functionå¯¹Î¸æ±‚å¯¼çš„è¿‡ç¨‹ï¼Œæ‰€ä»¥ä¸ºäº†ä¿è¯æ”¶æ•›çš„æ•ˆæœï¼Œcost functionåº”è¯¥æ˜¯è¦ convex fuctionï¼Œå°±ä¸ä¼šå¯¼è‡´åœç•™åœ¨äº†local optimç‚¹ã€‚ æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼š â€‹ å¯è§†åŒ–åå¤§æ¦‚çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š æ ¹æ®å¯¹äºå“ªäº›Î¸æ±‚å¯¼ï¼ŒGradient Descentè¿˜å¯ä»¥ç»§ç»­åˆ†æˆä¸åŒçš„å‡ ç§æ–¹æ³• Batch Gradient Descent Stochastic Gradient Descent Mini Batch Gradient Descent SGD with mini-batch é—®é¢˜ï¼šä¸ºä»€ä¹ˆä¸‹é™æ˜¯ - ï¼ŒÎ¸å¤§Jä¸€å®šå¤§å—ï¼Ÿ Normal Equation (linear regression) æ¨å¯¼è¿‡ç¨‹æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦çš„æ•°å­¦çŸ¥è¯†æ¯”è¾ƒå¤šï¼Œè¿™é‡Œåªç»™å‡ºç»“è®ºã€‚æƒ³è¦çœ‹å…·ä½“æ¨å¯¼è¿‡ç¨‹çš„è¿˜è¯´çœ‹cs229çš„ç¬¬äºŒèŠ‚è¯¾å§ ï¼šï¼‰(cs229-notes-1, p11)$$\\theta = (X^TX)^{-1}X^T\\vec y$$ â€‹ Newton BGFS Simulated Annealing 2. Generalized Linear Model (GLM) å¹¿ä¹‰çº¿æ€§æ¨¡å‹ä¹‹å‰æˆ‘ä»¬åœ¨cost functionä¸­æåˆ°è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¦‚ç‡çš„è§’åº¦æ¥ç†è§£è¯¯å·®è¿™ä¸ªé—®é¢˜ã€‚å¯¹äºLinear Regressionå’ŒLogistic Regressionï¼Œæˆ‘ä»¬éƒ½å¯ä»¥å‡è®¾ï¼š$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$è¿™é‡Œè¯¯å·®Ïµå‡è®¾ä¸ºIID (independently and identically distributed) 2.1 Linear Regressionåœ¨Linear Regressionä¸­ï¼Œyæ˜¯è¿ç»­çš„å€¼ï¼Œæ‰€ä»¥è¯¯å·®Ïµä¹Ÿæ˜¯ä¸€ä¸ªè¿ç»­çš„å€¼ã€‚å‡è®¾è¯¯å·®Ïµæ˜¯ç¬¦åˆGaussian Distribution (Normal Distribution)ï¼Œæ‰€ä»¥æœ‰ Gaussian Distribution $$y | x; \\theta âˆ¼ N (Î¼, Ïƒ^2)$$ Probability of error â€‹ Ïƒå®é™…ä¸Šæ˜¯ä¸å½±å“æ¦‚ç‡çš„åˆ†å¸ƒçš„ï¼Œæ‰€ä»¥å‡è®¾Ïƒ = 1ï¼Œæ‰€ä»¥è¿™é‡Œå¯ä»¥å¿½ç•¥äº†ã€‚å› æ­¤ä¹Ÿå°±æ˜¯ç­‰åŒäºå¦‚ä¸‹ Likelihood$$L(\\theta) = L(\\theta; X, \\vec y) = p(\\vec y | X; \\theta)$$ â€‹ ä»¥ä¸Šæ˜¯åœ¨ç»™å®šè¾“å…¥xå’Œæƒé‡Î¸ä¸‹ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å€¼æ˜¯çœŸå®å€¼yçš„æ¦‚ç‡ï¼Œæ‰€ä»¥è¿™ä¸ªæ¦‚ç‡å‘¢ï¼Œå½“ç„¶æ˜¯è¶Šé«˜è¶Šå¥½å•¦ã€‚æˆ‘ä»¬å°±æ˜¯è¦æƒ³åŠæ³•å» maximum likelihoodã€‚ â€‹ å¯¹è¿™ä¸ªæ¦‚ç‡å–ä¸ªlogï¼ˆä¸å½±å“ç»“æœï¼‰ï¼Œæœ‰ â€‹ å¯ä»¥çœ‹åˆ°æœ€ç»ˆçš„å¼å­é‡Œé¢ï¼Œæˆ‘ä»¬å°±æ˜¯è¦æ±‚cost functionçš„æœ€å°å€¼ã€‚ 2.2 Logistic Regressionåœ¨Logistic Regressionä¸­ï¼Œ yæ˜¯ç¦»æ•£çš„å€¼ {0, 1}ï¼Œæ‰€ä»¥è¯¯å·®Ïµä¹Ÿæ˜¯ä¸€ä¸ªç¦»æ•£çš„å€¼ {0, 1}ã€‚å‡è®¾è¯¯å·®Ïµæ˜¯ç¬¦åˆBernoulli Distributionï¼Œæ‰€ä»¥æœ‰ Bernoulli distribution Probability of error æŠŠè¯¯å·®Ïµä»£å…¥åˆ°ä¸Šé¢çš„bernoulli functionï¼Œå¯å¾— Likelihood â€‹ åŒæ ·ï¼Œå¯¹ä¸Šé¢å»logä¹‹åæœ‰ â€‹ åŒæ ·ï¼Œæˆ‘ä»¬å°½é‡è¦maximize the likelihoodï¼Œå°±ç›¸å½“äºè¦æœ€å°åŒ–åé¢çš„é‚£éƒ¨åˆ†ï¼ˆcost functionï¼‰ã€‚è¿™é‡Œå¯ä»¥ç”¨Gradient Ascentç®—æ³•æ¥æ±‚æœ€å¤§å€¼ï¼Œä½†æ˜¯å’ŒGradient Descentä¸ä¸€æ ·ï¼ˆä¸ºä»€ä¹ˆï¼‰$$\\theta := \\theta + \\alphaâˆ‡_\\theta l(\\theta)$$â€‹ æ³¨æ„è¿™é‡Œæ˜¯åŠ å·ï¼Œåœ¨æ±‚cost functionçš„æœ€å°å€¼æ—¶ï¼Œç”¨çš„æ˜¯å‡å·ã€‚ â€‹ å¯¹å…¶æ±‚å¯¼å¯å¾— â€‹ æ‰€ä»¥æœ‰ â€‹ Digression Perception ï¼Ÿ ï¼Ÿ ï¼Ÿ 2.3 GLMä¼¼ä¹åˆ°ç°åœ¨ï¼Œè®²äº†åŠå¤©éƒ½ä¹Ÿæ²¡è®²ä»€ä¹ˆæ˜¯å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œå®é™…ä¸Šå‘¢ï¼Œä¸Šé¢æˆ‘ä»¬å·²ç»ä»è¯¯å·®æ¦‚ç‡çš„è§’åº¦ä¸Šæ¥åˆ†æäº†çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’ä¸¤ç§ç‰¹ä¾‹ï¼Œå› ä¸ºä»–ä»¬è¯¯å·®æœä»çš„æ¦‚ç‡åˆ†å¸ƒéƒ½æ˜¯å±äºExponential Familyä¸­çš„ä¸€ç§ã€‚ The Exponential Family Î· è¢«ç§°ä½œnatural parameterï¼Œå®ƒæ˜¯æŒ‡æ•°åˆ†å¸ƒæ—å”¯ä¸€çš„å‚æ•°T(y) è¢«ç§°ä½œsufficient statisticï¼Œå¾ˆå¤šæƒ…å†µä¸‹T(y)=y a(Î·) è¢«ç§°ä½œ log partition functionTå‡½æ•°ã€aå‡½æ•°ã€bå‡½æ•°å…±åŒç¡®å®šä¸€ç§åˆ†å¸ƒ é‚£ä¹ˆè¿™ä¸ªæ¨¡å‹å’Œä¸Šé¢æˆ‘ä»¬æåˆ°è¿‡çš„Gaussian Distribution å’ŒBernoulli Distributionæœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿå…¶å®ä¸Šé¢çš„è¿™å‡ ä¸ªå‚æ•°å–ä¸åŒçš„å€¼çš„æ—¶å€™ï¼Œå³å¯å¾—åˆ°ä¸åŒçš„åˆ†å¸ƒæ¨¡å‹ Gaussian Distribution Bernolli Distribution 2.4 å¦‚ä½•æ„å»ºä¸€ä¸ªGLMæ¨¡å‹åœ¨ä¸Šé¢æˆ‘ä»¬åªæ˜¯çœ‹åˆ°äº†ä¸€ä¸ªé€šç”¨çš„GLMæ¦‚ç‡æ¨¡å‹ å®é™…ä¸Šå¯¹äºæ„å»ºè¿™ä¹ˆä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œéœ€è¦ä½œå‡ºä¸‰ä¸ªå‡è®¾ä½œä¸ºå‰ææ¡ä»¶ï¼š p(y | x; Î¸) âˆ¼ ExponentialFamily(Î·). å¯¹äºç»™å®šçš„è¾“å…¥xï¼ŒÎ¸å’Œè¾“å‡ºyéœ€è¦æœä»æŸä¸€ç§æŒ‡æ•°åˆ†å¸ƒï¼Œè¿™ä¸ªæŒ‡æ•°åˆ†å¸ƒç”±Î· å†³å®šçš„ å¯¹äºç»™å®šçš„è¾“å…¥xï¼Œé¢„æµ‹T(y)çš„å€¼ï¼Œä¸”ç»å¸¸T(y) = yã€‚è€Œæˆ‘ä»¬æ˜¯é¢„æµ‹æ˜¯H(x) éœ€è¦æ»¡è¶³ H(x) = E[y|x] å¯¹äºè‡ªç„¶å‚æ•°Î·å’Œè¾“å…¥xä¹‹é—´ï¼Œéœ€è¦å­˜åœ¨ç›¸å…³æ€§å…³ç³»çš„ï¼Œå³ï¼šÎ· = Î¸T x","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"å­—ç¬¦ä¸²æœç´¢ç®—æ³• - KMP","date":"2017-03-15T06:48:47.000Z","path":"2017/03/15/å­—ç¬¦ä¸²æœç´¢ç®—æ³•-KMP/","text":"1. ç†è§£ä¸€ï¼šéƒ¨åˆ†åŒ¹é…è¡¨+å·²åŒ¹é…æ•°å­—1.1 Partial Match Table ä¸Šæ¥å…ˆä¸Šä¸ªç»“è®ºï¼Œè¿™ä¸ªå…ˆæš‚æ—¶ä¸ç®¡æ€ä¹ˆç”Ÿæˆï¼Œç”¨äºKMPè¡¨çš„ç§»åŠ¨ã€‚ ç§»åŠ¨ä½æ•° = å·²åŒ¹é…çš„å­—ç¬¦æ•° - å¯¹åº”çš„éƒ¨åˆ†åŒ¹é…å€¼ åŒ¹é…åˆ°äº†ç¬¬å…­ä¸ªå­—ç¬¦Bï¼ŒBåœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯2ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯6 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 6 - 2 = 4ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨4ä½ã€‚ åŒ¹é…åˆ°äº†ç¬¬ä¸‰ä¸ªå­—ç¬¦Cï¼ŒCçš„å‰ä¸€ä¸ªå­—ç¬¦Båœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯0ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯2 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 2 - 0 = 2ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨2ä½ å› ä¸ºç¬¬ä¸€ä¸ªå­—ç¬¦ä¸åŒ¹é…ï¼Œå°±å°†æ•´ä¸ªå­—ç¬¦ä¸²å‘åç§»ä¸€ä½ åŒ¹é…åˆ°äº†ç¬¬å…­ä¸ªå­—ç¬¦Bï¼ŒBåœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯2ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯6 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 6 - 2 = 4ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨4ä½ é€ä¸ªæ¯”è¾ƒï¼Œç›´åˆ°å®Œå…¨åŒ¹é… å¦‚æœè¿˜éœ€è¦ç»§ç»­æœç´¢çš„è¯ï¼ŒDåœ¨ä¸Šè¡¨ä¸­çš„å€¼ä¸º0ï¼ŒåŒ¹é…åˆ°çš„ä¸ªæ•°ä¸º7ï¼Œç§»åŠ¨çš„ä½æ•°= 7 - 0 = 7ï¼Œå°†æ•´ä¸ªå­—ç¬¦ä¸²å¾€åç§»åŠ¨7ä½ã€‚æ¥ç€å°±æ˜¯é‡å¤ä¹‹å‰çš„æ¯”è¾ƒæ­¥éª¤äº†ã€‚ 1.2 è®¡ç®— Partial Match Table â€‹ è¿™é‡Œéœ€è¦ç†è§£ä¸¤ä¸ªæ¦‚å¿µï¼šå‰ç¼€å’Œåç¼€ â€‹ â€œå‰ç¼€â€ æŒ‡é™¤äº†æœ€åä¸€ä¸ªå­—ç¬¦ä»¥å¤–ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²çš„å…¨éƒ¨å¤´éƒ¨ç»„åˆï¼› â€‹ â€œåç¼€â€ æŒ‡é™¤äº†ç¬¬ä¸€ä¸ªå­—ç¬¦ä»¥å¤–ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²çš„å…¨éƒ¨å°¾éƒ¨ç»„åˆã€‚ â€‹ è€Œæˆ‘ä»¬éœ€è¦çš„Partial Match Tableå°±æ˜¯å‰ç¼€å’Œåç¼€çš„æœ€é•¿å…±æœ‰å…ƒç´ çš„é•¿åº¦ â€‹ ç»§ç»­ä»¥ä¸Šé¢çš„ä¾‹å­è®²è§£ â€œAâ€çš„å‰ç¼€å’Œåç¼€éƒ½ä¸ºç©ºé›†ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABâ€çš„å‰ç¼€ä¸º[A]ï¼Œåç¼€ä¸º[B]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABCâ€çš„å‰ç¼€ä¸º[A, AB]ï¼Œåç¼€ä¸º[BC, C]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦0ï¼›â€œABCDâ€çš„å‰ç¼€ä¸º[A, AB, ABC]ï¼Œåç¼€ä¸º[BCD, CD, D]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABCDAâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD]ï¼Œåç¼€ä¸º[BCDA, CDA, DA, A]ï¼Œå…±æœ‰å…ƒç´ ä¸ºâ€Aâ€ï¼Œé•¿åº¦ä¸º1ï¼›â€œABCDABâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD, ABCDA]ï¼Œåç¼€ä¸º[BCDAB, CDAB, DAB, AB, B]ï¼Œå…±æœ‰å…ƒç´ ä¸ºâ€ABâ€ï¼Œé•¿åº¦ä¸º2ï¼›â€œABCDABDâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD, ABCDA, ABCDAB]ï¼Œåç¼€ä¸º[BCDABD, CDABD, DABD, ABD, BD, D]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ã€‚ äº†è§£äº†KMPçš„åŸç†ä¹‹åï¼Œæ¥çœ‹ä¸€ä¸‹ä»£ç è¯¥æ€ä¹ˆå†™ã€‚ ä¸¾ä¸ªæ —å­ï¼š Text = a b a c a a b a c c a b a c a b a a b b Pattern = a b a c a b æ ¹æ®å‰é¢çš„Partial Match Table, æˆ‘ä»¬å¯ä»¥ç®—å‡ºPatternçš„è¿™ä¸ªè¡¨ P a b a c a b steps 0 0 1 0 1 2 æ­¤æ—¶æˆ‘ä»¬ç”¨ä¸¤ä¸ªæŒ‡é’ˆ i å’Œ j æ¥è¡¨ç¤º Text å’Œ Pattern ä¸­çš„å­—ç¬¦ã€‚ å½“ T[ i : i + j ] == P[ 1 : j ] çš„æ—¶å€™ï¼Œå°±æ˜¯ Text ä¸­åŒ…å«äº†æˆ‘ä»¬éœ€è¦æŸ¥æ‰¾çš„ Pattern å…ˆè®© i å’Œ j éƒ½ä» 1 å¼€å§‹ï¼ˆpythonä»£ç ä¸­ä»0å¼€å§‹ï¼‰ å½“T[ i ] = P[ j ]çš„æ—¶å€™ï¼Œæ­¤æ—¶æŒ‡é’ˆåœ¨ Text å’Œ Pattern ä¸Šéƒ½å¾€å‰å„èµ°ä¸€æ­¥ï¼Œå³j+1ï¼Œi+1 å½“ i = 6ï¼Œj = 6 çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºT[ i ] != P[ j ]ï¼Œæ­¤æ—¶ j å°±ä¸èƒ½å†ç»§ç»­å¾€å‰èµ°äº†ï¼Œéœ€è¦é€€å›å»å‡ æ­¥ã€‚ é‚£ä¹ˆåˆ°åº•æ˜¯å‡ æ­¥å‘¢ï¼Œç»è¿‡ä¸Šé¢æŸ¥è¡¨ï¼Œæ­¤æ—¶åŒ¹é…åˆ°5ï¼Œé‡å¤çš„å­—ç¬¦ä¸²ä¸ªæ•°ä¸º1ï¼Œæ„æ€æ˜¯å¯¹äºè¿™ä¸ªå­—ç¬¦ä¸² abacaï¼Œabaca å’Œ abaca ä¸­æœ‰ä¸€ä¸ªé‡å¤äº†ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦å†æ¯”è¾ƒè¿™ä¸ªï¼Œè·³è¿‡è¿™ä¸ªå­—ç¬¦ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸º 6 - 1 = 5ï¼Œå°†å­—ç¬¦ä¸² Pattern å‘å‰æŒª5ä½ï¼Œæ–°çš„ j å°±ç­‰äº1äº†ï¼Œç„¶åé‡å¤ä¹‹å‰çš„æ­¥éª¤ã€‚ 1.3 Pythonä»£ç 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091DEBUG = TrueDEBUG = False# ç”Ÿæˆ partial match tabledef PMT(sent): length = len(sent) result = [0] for i in range(2, length + 1): sub_sent = sent[0 : i] sub_len = len(sub_sent) surfix = [] prefix = [] for j in range(sub_len): prefix.append(sub_sent[0 : j]) surfix.append(sub_sent[j + 1: sub_len ]) prefix.remove('') surfix.remove('') all_fix = list(set(prefix + surfix)) max_len = 0 com_str = '' for fix in all_fix: if fix in prefix and fix in surfix: if len(fix) &gt; max_len: max_len = len(fix) com_str = fix result.append(max_len) return result# KMPç®—æ³•def KMP(text, pattern): if (len(text) &lt; len(pattern)): return 'Not Match' pmt = PMT(pattern) if DEBUG: print('Current Pattern is = ', pattern) print('Partial Match Table = ', pmt) i = 0 j = 0 while(i &lt; len(text)): if (text[i] == pattern[j]): if (j == (len(pattern) - 1)): return 'Match, the position in text is: &#123;&#125; - &#123;&#125; = &#123;&#125;'.format(i - j, i, text[i-j: i]) i += 1 j += 1 continue else: if j &gt;= 1: j = pmt[j - 1] else: i += 1 return 'Not Match'# æµ‹è¯•å‡½æ•°def test(text, pattern): result = KMP(text, pattern) print('Resutl = ', result) print('Pattern =', pattern) if DEBUG: for i in range(len(text)): print('&#123;:3s&#125;'.format(str(i)), end='-') print('') for i in range(len(text)): print('&#123;:3s&#125;'.format(text[i]), end='-') print('') print('\\n')# æµ‹è¯•éƒ¨åˆ†text = 'BBC ABCDAB ABCDABCDABDE'pattern = 'ABCDABD'test(text, pattern)text = 'a b a c a a b a c c a b a c a b a a b b'pattern = 'a b a c a b'test(text, pattern)text = 'BBC ABCDAB ABCDABCDABE'pattern = 'ABCDABD'test(text, pattern)text = 'BBC AB'pattern = 'ABCDABD'test(text, pattern)text = 'ABCDABD'pattern = 'ABCDABD'test(text, pattern) 1.4 æµ‹è¯•ç»“æœ12345678910111213141516171819202122232425Text = BBC ABCDAB ABCDABCDABDEPattern = ABCDABDResutl = Match, the position in text is: 15 - 21 = ABCDABText = a b a c a a b a c c a b a c a b a a b bPattern = a b a c a bResutl = Match, the position in text is: 20 - 30 = a b a c a Text = BBC ABCDAB ABCDABCDABEPattern = ABCDABDResutl = Not MatchText = BBC ABPattern = ABCDABDResutl = Not MatchText = ABCDABDPattern = ABCDABDResutl = Match, the position in text is: 0 - 6 = ABCDAB[Finished in 0.1s] 2. ç†è§£äºŒï¼šéƒ¨åˆ†åŒ¹é…è¡¨+idx2.1 Partial Match Tableé¦–å…ˆç”Ÿæˆå’Œä¸Šé¢ä¸€æ ·çš„å‰ç¼€åç¼€è¡¨(0-base å’Œ 1-base) 2.2 Example 0-base æ‰¾åˆ°åŸæ–‡å’Œæœç´¢è¯ä¸åŒçš„é‚£ä¸ªå­—ç¬¦é‡Œé¢çš„åŒ¹é…å€¼ï¼Œç„¶åæŠŠæœç´¢å­—ç¬¦ä¸²å³ç§»åˆ°idx=åŒ¹é…å€¼çš„ä½ç½®ï¼Œè¿‡ç¨‹å¦‚ä¸‹å›¾ 1-base ä¸0-baseä¸åŒçš„æ˜¯ï¼Œæ‰¾çš„ä¸æ˜¯æœ€åä¸€ä¸ªä¸åŒçš„å­—ç¬¦ è€Œæ˜¯æœ€åä¸€ä¸ªç›¸åŒçš„å­—ç¬¦é‡Œé¢çš„åŒ¹é…å€¼ï¼Œç„¶åæŠŠæœç´¢å­—ç¬¦ä¸²å³ç§»åˆ°idx=åŒ¹é…å€¼çš„ä½ç½®ï¼Œè¿‡ç¨‹å¦‚ä¸‹å›¾ 3. References é˜®ä¸€å³° - å­—ç¬¦ä¸²åŒ¹é…çš„KMPç®—æ³•","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸€ï¼‰","date":"2017-03-06T13:44:33.000Z","path":"2017/03/06/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸€ï¼‰/","text":"1. Hadoop1.1 åˆè¯†Hadoopéå¸¸å¥½çš„Tutorial åœ¨å­¦ä¹ hadoopä¹‹å‰ï¼Œæˆ‘è§‰å¾—æœ‰å¿…è¦äº†è§£ä¸€ä¸‹hadoopçš„åŸºæœ¬æ„æˆä»¥åŠä¸€äº›æœ¯è¯­ã€‚ Block HDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks. By making a block large enough, the time to transfer the data from the diskcan be made to be significantly larger than the time to seek to the start of the block.Thus the time to transfer a large file made of multiple blocks operates at the disk transferrate.A quick calculation shows that if the seek time is around 10 ms, and the transfer rateis 100 MB/s, then to make the seek time 1% of the transfer time, we need to make theblock size around 100 MB. The default is actually 64 MB, although many HDFS installationsuse 128 MB blocks. This figure will continue to be revised upward as transferspeeds grow with new generations of disk drives.This argument shouldnâ€™t be taken too far, however. Map tasks in MapReduce normallyoperate on one block at a time, so if you have too few tasks (fewer than nodes in thecluster), your jobs will run slower than they could otherwise. Node ç®€å•çš„è¯´å°±æ˜¯ä¸€å°ä¸»æœºï¼Œä¸€å°ç”µè„‘ã€‚åœ¨hadoopä¸­ï¼Œæœ‰NameNode, DataNode, Secondary NameNode, JobTracker Node, TaskTracker Node, CheckpointNode å’Œ BackupNodeã€‚å¯¹ä¸€ä¸ªclusterï¼ŒNameNodeåªèƒ½æœ‰ä¸€ä¸ªï¼ŒDataNodeå¯ä»¥æœ‰å¤šä¸ª Rack ä¸­æ–‡æœºæŸœ/æœºæ¶ï¼Œå°±æ˜¯ç”¨æ¥å­˜æ”¾nodeçš„storageï¼Œé€šå¸¸ä¸€ä¸ªrackæœ‰å‡ åä¸ªnodesç»„æˆï¼Œè¿™äº›nodeså­˜æ”¾åœ¨åŒä¸€ä¸ªæœºæŸœï¼Œè¿æ¥ä¸€ä¸ªäº¤æ¢æœº A Node is simply a computer. This is typically non-enterprise, commodity hardware for nodes that contain data. Storage of Nodes is called as rack. A rack is a collection of 30 or 40 nodes that are physically stored close together and are all connected to the same network switch. Network bandwidth between any two nodes in rack is greater than bandwidth between two nodes on different racks.A Hadoop Cluster is a collection of racks. 1.2 Major Components Distributed Filesystem â€” hadoopä¸­æ˜¯HDFS MapReduce 1.5 Install configuration123456# å¾…æ•´ç†hadoop fshadoop dfs# fs refers to any file system, it oucld be local or HDFS# but dfs refers to only HDFS file system 2. MapReduce2.1 åˆè¯†MapReduce æ•´ä¸ªè¿‡ç¨‹å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼ŒInputï¼Œ MapReduce and Output åœ¨inputå’Œoutputé˜¶æ®µï¼Œæ•°æ®æ˜¯å­˜åœ¨HDFSæ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œå…¶ç³»ç»Ÿçš„block sizeå¤§å°é»˜è®¤æ˜¯64/128MBã€‚ åœ¨MapReduceä¸­ï¼Œåˆå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼ŒMap and Reduceï¼Œæ•°æ®ä»map functionåˆ°reduce functionæ˜¯å­˜åœ¨local diskä¸­ï¼Œ(soreing in HDFS with replication would be overkill)ï¼Œç„¶åé€šè¿‡networkä¼ è¾“æ•°æ®. åœ¨æ¯ä¸ªé˜¶æ®µä¸­ï¼Œinputå’Œoutputçš„æ•°æ®éƒ½æ˜¯ä»¥ (key, values) æ ¼å¼è¿›è¡Œå¤„ç†çš„ï¼Œç„¶åé€šè¿‡ map function å’Œ reduce function è¿›è¡Œå¤„ç†ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œinput dataçš„keyæ˜¯ä»æ•°æ®æ–‡ä»¶å¼€å§‹å¤„çš„è¡Œæ•°çš„åç§»é‡ï¼Œä½†æ˜¯map functionè¾“å‡ºçš„keyæ˜¯å¹´ä»½æ•°æ®ï¼Œä»¥åŠreduce functionè¾“å‡ºçš„keyæ˜¯ä¹Ÿä¸åŒçš„ã€‚æ‰€ä»¥è¿™ä¸‰ä¸ªkey-value pairsæ˜¯ä¸åŒçš„ã€‚ åŸå§‹æ•°æ® Key-Values ä»¥ä¸Šä¸ºåŸå§‹æ•°æ®ä¸­inputè¿›æ¥åçš„key-valuesçš„æ•°æ®ã€‚ç„¶åmap functioné˜¶æ®µï¼Œæå–å‡ºä¸Šé¢æ–‡ä»¶ä¸­çš„ 1950 å’Œ 0001 ä¹‹ç±»çš„æ•°æ®ï¼Œç»„æˆæ–°çš„key-valuesä½œä¸ºè¾“å‡ºç»™ä¸‹ä¸€é˜¶æ®µã€‚ Key-Values in Map Function åœ¨å°†Map Functionçš„è¾“å‡ºä¼ ç»™Reduce Functionä¹‹å‰ï¼Œå®é™…ä¸ŠMapReduce Frameworkè¿˜æ˜¯æœ‰å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªå¤„ç†æ­¥éª¤ã€‚ä»æœ€ä¸Šçš„å›¾ä¸€ä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥çœ‹åˆ°Mapå’ŒReduceä¹‹é—´æœ‰ä¸€ä¸ª Shuffle çš„è¿‡ç¨‹ã€‚å› ä¸ºä¹‹å‰æˆ‘ä»¬æåˆ°äº†ï¼ŒMapçš„è¿‡ç¨‹ä¸­ï¼Œåªæ˜¯å®ç°äº†ä¸€ä¸ªkey-valueåŒ¹é…çš„è¿‡ç¨‹ï¼Œæ‰€æœ‰å‡ºæ¥çš„æ•°æ®ä¹Ÿæ˜¯æ— åºçš„ï¼Œè€Œ Shuffle å°±æ˜¯å¯¹è¿™ä¸ªè¾“å‡º sort &amp; group çš„è¿‡ç¨‹ï¼Œç„¶åå°†è¾“å‡ºä¼ ç»™ Reduce Function è¿›è¡Œå¤„ç† å½“æ•°æ®ä»Reduce Functionä¸­å¤„ç†å®Œåå‡ºæ¥çš„å¤§æ¦‚å¦‚ä¸‹ï¼Œæ³¨æ„è¿™ä¸ªreduceåªæ˜¯é€‰æ‹©æœ€å¤§å€¼ï¼Œå…¶ä»–reduce functionå¯èƒ½åšçš„æ˜¯ç»Ÿè®¡æˆ–è€…å®ç°å…¶ä»–åŠŸèƒ½ã€‚ ç°åœ¨å†çœ‹å¦å¤–ä¸€ä¸ªç»å…¸çš„WordCountçš„ä¾‹å­ åœ¨Hadoopç³»ç»Ÿä¸­ï¼Œå¤„ç†ä¸€ä¸ªwordcountçš„ä»»åŠ¡å¯ä»¥å¤§è‡´åˆ†æˆå››ä¸ªä¸»è¦é˜¶æ®µï¼Œinputï¼Œmapï¼Œreduceï¼Œoutputã€‚å…¶ä¸­ Map å’Œ Reduce å¯ä»¥ç»§ç»­ç»†åˆ†ï¼Œå³åˆ†æˆå¤šä¸ª map tasks å’Œ reduce tasksã€‚ è¿™äº›tasksç„¶åè¢« YARN ç»™åˆ†é…é›†ç¾¤ä¸­å¤šå°ä¸åŒçš„æœºå™¨å¤„ç†ã€‚è¿™å…¶ä¸­çš„ç»†èŠ‚ç­‰åˆ°å¾€åå†è®¨è®ºã€‚ ä¸Šé¢æåˆ°çš„åˆ†æˆå¤šä¸ªtasksæ—¶ï¼Œåº”è¯¥æ˜¯input dataåˆ‡ç‰‡åˆ†ç»™å¤šä¸ªmapsï¼ˆè€Œä¸æ˜¯ä¸€ä¸ªå¤§çš„mapåˆ†æˆå¤šä¸ªå°çš„tasksï¼‰ï¼Œ æ¯ä¸ªMapReduceåˆ†åˆ°ä¸€ä¸ªfixed-sized çš„æ•°æ®ï¼Œé€šå¸¸æ˜¯64/128MBï¼Œè¿™ä¸ªè¿‡ç¨‹å«åš input splitsã€‚ç„¶åæ¯ä¸ªsplitåˆ†é…ä¸€ä¸ªmap taskï¼ŒåŒæ—¶è¿è¡Œåœ¨ä¸åŒçš„æœºå™¨ä¸Šå¤„ç†ã€‚è¿™æ ·åˆ’åˆ†çš„å¥½å¤„æ˜¯æœ‰åˆ©äºload-balancingï¼Œå¯¹äºæ€§èƒ½è¾ƒå¥½çš„æœºå™¨å¯ä»¥å¤„ç†æ›´è¿‡æ˜¯splitsã€‚ 2.2 Data Flow ä¸Šå›¾å¯ä»¥çœ‹å‡ºhadoopçš„æ•´ä¸ªæ•°æ®æµå‘ï¼Œå…¶ä¸­è™šçº¿ä»£è¡¨æ˜¯åœ¨ä¸€ä¸ªnodeï¼Œå®çº¿ä»£è¡¨çš„æ˜¯ä¸åŒnodeä¹‹é—´ã€‚åœ¨åŒä¸€ä¸ªnodeä¹‹é—´ï¼Œæ•°æ®çš„è¯»å–å­˜å‚¨å°±æœ‰é€Ÿåº¦ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸åŒnodeä¹‹é—´ï¼Œä¹Ÿå°±æ˜¯ä¸åŒä¸»æœºä¹‹é—´ï¼Œå°±å¿…é¡»é€šè¿‡networkè¿›è¡Œä¼ è¾“ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚ Partition å½“åªæœ‰ä¸€ä¸ªreduceçš„æ—¶å€™ï¼Œmap functionçš„outputå½“ç„¶å°±ç›´æ¥ä¼ ç»™è¿™ä¸ªreduceäº†ã€‚ä½†æ˜¯å½“æœ‰å¤šä¸ªreduceçš„æ—¶å€™ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿæ­¤æ—¶mapä¼šå°†å…¶è¾“å‡ºè¿›è¡Œpartition(åˆ†åŒº)ï¼Œæ¯ä¸€ä¸ªreduceçš„ä»»åŠ¡éƒ½ä¼šåˆ›å»ºä¸€ä¸ªåˆ†åŒºï¼Œä¸”æ¯ä¸€ä¸ªreduce taskéƒ½ä¼šæœ‰ä¸€ä¸ªpartition (There can be many keys (and their associated values)in each partition, but the records for any given key are all in a single partition)ï¼Œä¹Ÿå°±æ˜¯è¯´åŒä¸€ä¸ªkeyä¼šåœ¨åŒä¸€ä¸ªpartitionä¸­ã€‚ Shuffle and Sort åœ¨mapå’Œreduceä¹‹é—´çš„data flowæ˜¯Shuffleï¼Œä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªreduceå¯ä»¥æ¥å—æ¥è‡ªå¤šä¸ªä¸åŒçš„mapçš„outputï¼Œå…¶ä¸­åŒ…å«äº†sortï¼Œpartitionç­‰è¿‡ç¨‹ã€‚ Combiner Functions ä¹‹å‰æˆ‘ä»¬è®¨è®ºè¿‡ï¼Œdata flowåœ¨mapå’Œreduceä¹‹é—´æ˜¯é€šè¿‡networkè¿›è¡Œä¼ è¾“çš„ï¼Œä½†æˆ‘ä»¬çŸ¥é“map functionçš„outputæ˜¯ä¸€ä¸ªä¸ªkey-valueçš„é”®å€¼å¯¹çš„ï¼Œè¿™äº›key-value parisä¸­ï¼Œæœ‰äº›æ˜¯å¯ä»¥é€šè¿‡combiner functionè¿›è¡Œcombineçš„ï¼Œè¿™æ ·åšçš„ç›®çš„æ˜¯å‡å°mapå’Œreduceä¹‹é—´ä¼ è¾“çš„æ•°æ®å¤§å°ï¼ŒåŠ å¿«ä¼ è¾“æ•°æ®ã€‚ Combiner Functionåœ¨è®¸å¤šæƒ…å†µå’Œ Reduce Functionæ˜¯å¾ˆåƒçš„ï¼Œå› ä¸ºåšçš„å·¥ä½œå’Œreduceæ˜¯æ¯”è¾ƒç±»ä¼¼çš„ï¼Œåªæ˜¯å¤„ç†çš„æ˜¯å±€éƒ¨mapçš„output(å› æ­¤Combineræ˜¯è¿è¡Œåœ¨map outputç«¯)ï¼Œå‡å°‘data flowçš„sizeã€‚ä½†æ˜¯å¯¹äºæ˜¯å¦è°ƒç”¨combiner functionï¼Œè¿™ä¸ªæ˜¯ä¸ç¡®å®šçš„ã€‚å› ä¸ºæœ‰äº›æƒ…å†µä¸‹çš„outputæ˜¯ä¸é€‚åˆè¿›è¡Œcombineï¼Œæœ‰äº›åˆ™åˆæ˜¯è¦å¤šæ¬¡è°ƒç”¨è¿›è¡Œåˆå¹¶ã€‚å› ä¸ºè¿™ä¸ªï¼ŒCombineræ˜¯å¯é€‰çš„ï¼Œå³å¯ä»¥è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥ä¸è°ƒç”¨ï¼Œå½“ä¸è°ƒç”¨çš„æ—¶å€™ï¼Œå°±å¿…éœ€ä¸èƒ½å½±å“ç¨‹åºçš„æ­£å¸¸è¿è¡Œã€‚æ‰€ä»¥Combinerçš„inputå’Œoutputæ˜¯ä¸€æ ·çš„ï¼Œå’ŒMapperçš„outputã€Reducerçš„inputä¸€æ ·ã€‚ å¯¹äºæœ‰äº›ç‰¹æ®Šæƒ…å†µï¼Œç”šè‡³è¿reduce functionéƒ½ä¸éœ€è¦ã€‚ ä¸¾ä¸ªæ —å­ï¼š é€‚ç”¨æƒ…å†µï¼ˆCommutative &amp; Associativeï¼‰ â€‹ Reduce Function Combiner Function â€‹ Commutative: max(a, b) = max(b, a) â€‹ Associative: max(max(a, b), c) = max(a, max(b, c)) ä¸é€‚ç”¨æƒ…å†µï¼š ä¼ªä»£ç  In-Combiner Function Advantage ç›¸æ¯”Combinerï¼ŒIn-Combinerçš„æ•ˆç‡æ›´é«˜ã€‚ å¯ä»¥å‡å°‘ä¸€äº›Mapperå’ŒReducerä¹‹é—´çš„key-value pairsï¼Œå¯ä»¥å‡å°‘å¤„ç†è¿™éƒ¨åˆ†çš„å¼€é”€ã€‚å› ä¸ºCombineråªæ˜¯å‡å°‘äº†ä¸€äº›Mapperå’ŒReducerä¹‹é—´çš„intermediate dataï¼Œä½†æ˜¯å¹¶æ²¡å‡å°‘ä»Mapperçš„outputå‡ºæ¥çš„key-value pairsçš„æ•°é‡ã€‚ä½†æ˜¯In-Combineræ˜¯æ˜¯Mapper çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯è¯´key-value pairsåœ¨Mapper è¾“å‡ºå‰å°±å·²ç»å‡å°‘äº†ã€‚ å‡å°‘äº†key-value pairså¯ä»¥å‡å°‘ç³»ç»Ÿçš„object serialization and deserialization çš„å¼€é”€ï¼Œå³åƒåœ¾å›æ”¶æœºåˆ¶ Disadvantage å†…å­˜ä½¿ç”¨ï¼Œå› ä¸ºè¦ä¿å­˜ä¸€ä¸ªarrayåœ¨å†…å­˜ä¸­ï¼Œå½“æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™æœ‰å¯èƒ½ä¼šçˆ†äº†ã€‚è§£å†³æ–¹æ¡ˆ æœ‰ä¸¤ä¸ªï¼Œç¬¬ä¸€æ˜¯é™åˆ¶arrayçš„ä¸ªæ•°ï¼Œç¬¬äºŒæ˜¯é™åˆ¶å†…å­˜çš„ä½¿ç”¨ã€‚å½“è¿™ä¿©åˆ°è¾¾æŸä¸€ä¸ªé˜ˆå€¼çš„æ—¶å€™ï¼Œå°±å‘é€ç»™Reducerã€‚ ç¬¬äºŒæ˜¯è®²ä¸€ä¸ªMapçš„è¿‡ç¨‹åˆ†æˆå‡ ä¸ªéƒ¨åˆ†ï¼Œå¯¼è‡´debugä¸­å¯èƒ½å‡ºç°oedering-dependent bugsï¼Œè°ƒè¯•å¯èƒ½æ¯”è¾ƒå›°éš¾ã€‚ â€‹ â€‹ â€‹","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Recommender-Systems/","text":"1. What is Recommender Systemså¯¹äºæ¨èç³»ç»Ÿçš„å®šä¹‰ï¼Œæˆ‘ä»¬å…ˆä¸¾å‡ ä¸ªä¾‹å­æ¥ç†è§£ä¸€ä¸‹ã€‚ ç”µå½±ç½‘ç«™ç»™ç”¨æˆ·æ¨èç”µå½±ï¼Œå¯ä»¥æ ¹æ®è¯¥ç”¨æˆ·ä»¥å¾€çš„è¯„åˆ†ï¼Œæ¯”å¦‚ç»™æµªæ¼«çˆ±æƒ…ç”µå½±è¯„åˆ†é«˜ï¼Œç»™åŠ¨ä½œç‰‡è¯„åˆ†è¾ƒä½ï¼Œé‚£ä¹ˆç³»ç»Ÿå¯ä»¥æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œç»™ç”¨æˆ·æ¨èåå‘æµªæ¼«çˆ±æƒ…çš„ç”µå½± å¦‚æœæ˜¯æ–°ç”¨æˆ·å‘¢ï¼Ÿæˆ‘ä»¬æ²¡æœ‰è¯¥ç”¨æˆ·çš„è¯„åˆ†ä¿¡æ¯ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ ¹æ®æ•´ä¸ªç³»ç»Ÿä¸­ï¼ŒæŸäº›ç”µå½±è¯„åˆ†è¾ƒé«˜è¿›è¡Œæ¨è é‚£ä¹ˆå¦‚æœæ˜¯æ–°ç½‘ç«™ï¼Œæ–°ç”¨æˆ·å‘¢ï¼Ÿ ä»¥ä¸Šä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ¨èç³»ç»Ÿåˆ†æˆä¸¤ç±»ã€‚ Content-based systems Content-basedï¼Œå°±æ˜¯åŸºäºå·²æœ‰çš„ä¿¡æ¯è¿›è¡Œæ¨èã€‚å…·ä½“å“ªäº›ä¿¡æ¯å‘¢ï¼Ÿåœ¨ä¸Šé¢çš„ç”µå½±æ¨èç³»ç»Ÿä¸­ï¼Œæœ‰ä¸¤ç±»ä¿¡æ¯éœ€è¦åˆ†æã€‚ ç¬¬ä¸€ï¼Œæ˜¯Userçš„è¯„åˆ†ä¿¡æ¯ï¼Œæ¯”å¦‚ç»™çˆ±æƒ…ç‰‡è¯„åˆ†é«˜ï¼Œç»™åŠ¨ä½œç‰‡è¯„åˆ†ä½ã€‚ ç¬¬äºŒï¼Œæ˜¯Movieçš„ç‰¹å¾ä¿¡æ¯ï¼Œæ¯”å¦‚è¿™éƒ¨ç”µå½±åå‘çˆ±æƒ…ç‰‡å¤šä¸€äº›ï¼Œä½†ä¹Ÿæœ‰ä¸€éƒ¨åˆ†æç¬‘ã€‚æ‰€ä»¥åœ¨Aï¼ˆçˆ±æƒ…ç‰‡ï¼‰å’ŒBï¼ˆæç¬‘ç‰‡ï¼‰ä¸­ï¼Œ Açš„æƒé‡æ›´é«˜ï¼ŒBçš„è¾ƒä½ åŸºäºä»¥ä¸Šä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥ç»™ç”¨æˆ·æ¨èä»–æ‰€å–œæ¬¢çš„ç”µå½±ã€‚ Collaborative filterring systems ååŒè¿‡æ»¤å™¨ï¼Œåˆ™æ˜¯åŸºäºç”¨æˆ·/ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦è¿›è¡Œæ¨èçš„ã€‚å³ç”¨æˆ·Aå’Œç”¨æˆ·Béƒ½å–œæ¬¢çˆ±æƒ…ã€æµªæ¼«ç”µå½±ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠç”¨æˆ·Aè¯„åˆ†è¿‡çš„çˆ±æƒ…æµªæ¼«ç”µå½±ï¼Œæ¨èç»™ç”¨æˆ·Bã€‚ 2. Content-based systems2.1 Problem Analysisä»¥ç”µå½±æ¨èç³»ç»Ÿä¸ºä¾‹ï¼Œå‡è®¾æˆ‘ä»¬å·²ç»å¯¹ç³»ç»Ÿä¸­çš„ç”µå½±ç‰¹å¾æœ‰äº†è¾ƒä¸ºå®Œå–„ï¼Œå³æˆ‘ä»¬çŸ¥é“æŸéƒ¨ç”µå½±å±äºçˆ±æƒ…ç‰‡å¤šå°‘åˆ†ï¼Œå±äºåŠ¨ä½œç‰‡å¤šå°‘åˆ†ã€‚ é‚£ä¹ˆæˆ‘ä»¬ç°åœ¨ä»¥Aliceä¸ºä¾‹ï¼Œå¥¹å¯¹ä¸¤éƒ¨çˆ±æƒ…ç‰‡è¯„åˆ†æ¯”è¾ƒé«˜ï¼Œå¯¹äºä¸¤éƒ¨åŠ¨ä½œç‰‡è¯„åˆ†ä¸º0ã€‚é‚£ä¹ˆç³»ç»Ÿå°±å¯ä»¥ç»™Aliceæ¨èåå‘çˆ±æƒ…æµªæ¼«çš„ï¼Œä¸”ä¸æ€ä¹ˆå±äºåŠ¨ä½œç‰‡çš„ç”µå½±ã€‚ Movies Alice - Î¸(1) Bob - Î¸(2) Carol - Î¸(3) Dave - Î¸(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love ï¼Ÿ 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 2.2 Optimization Objectiveå®é™…ä¸Šæˆ‘ä»¬å·²ç»å‡è®¾ä¹‹å‰å¯¹æ‰€æœ‰ç”µå½±çš„ç‰¹å¾è¿›è¡Œäº†ç»Ÿè®¡ï¼Œæ‰€ä»¥æ­¤æ—¶æœ‰ç”µå½±ç‰¹å¾å‘é‡Xï¼Œä»¥åŠç”¨æˆ·å¯¹äºç”µå½±çš„è¯„åˆ†Yå‘é‡ã€‚æ ¹æ®æ­¤æ—¶å·²æœ‰çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡ºthetaçš„å€¼ã€‚æ‰€ä»¥èƒ½å¤Ÿå¯¹äºé‚£ä¹ˆæ²¡æœ‰è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œæ ¹æ®thetaå’Œxæ±‚å‡ºåˆ†æ•°yã€‚ å› ä¸ºä¸€å¼€å§‹theatçš„å€¼æ˜¯éšæœºçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨Linear Regressionçš„æ–¹æ³•ï¼Œä¸æ–­å‡å°‘cost functionçš„å€¼æ±‚å‡ºthetaã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºè¿™é‡Œæ˜¯å¤šä¸ªç”¨æˆ·ï¼Œæ¯ä¸€ä¸ªç”¨æˆ·æˆ‘ä»¬æ±‚å‡ºä¸€ä¸ªthetaå€¼ã€‚æœ€åå¯¹äºå¤šä¸ªç”¨æˆ·ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡ºå¤šä¸ªthetaå€¼ã€‚ Actually, we can assume that we have known all features about the all movies, that is x1, x2, â€¦, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 2.2 Gradient descent update 2.3 Gradient descent in Logistic Regression Question: Why we donâ€™t need $\\frac 1 m$? Anwser: As there are only one user However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to learn theta and features at the same time 3. Collaborative filtering3.1 Proble motivation Movies Alice - Î¸(1) Bob - Î¸(2) Carol - Î¸(3) Dave - Î¸(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love ? 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 3.2 How to do åœ¨ä¹‹å‰éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬äº†è§£åˆ°äº†content-basedï¼Œæ˜¯å·²çŸ¥ x å’Œ yï¼Œæ±‚ thetaã€‚ Assume:$$\\theta^{(1)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(2)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(3)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\space\\theta^{(4)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\spacex^{(1)} = \\begin{bmatrix} 1 \\ 1.0 \\ 0.0 \\end{bmatrix}$$For Movie 1, we can calculate the result of Movie1 rating by all users.$$\\theta^{(1)} * x^{(1)} \\approx 5 \\\\\\theta^{(2)} * x^{(1)} \\approx 5 \\\\\\theta^{(3)} * x^{(1)} \\approx 0 \\\\\\theta^{(4)} * x^{(1)} \\approx 0$$ ä½†æ˜¯å¯¹äºæœ‰äº›æƒ…å†µï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“xçš„ç‰¹å¾å€¼ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ é€†å‘æ€è€ƒï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ theat å’Œ yï¼Œæ¥æ±‚ x çš„å€¼ã€‚ é‚£ä¹ˆå¯¹äº thetaå’Œxçš„å€¼éƒ½ä¸çŸ¥é“çš„æƒ…å†µä¸‹å‘¢ï¼Ÿ å¯¹æ¯”ç‰¹å¾ Linear Regression Collaborative filtering ç‰¹æ€§å‘é‡X å·²çŸ¥æ•°æ® å¾…æ±‚è§£æ•°æ® æƒé‡ Î¸ å¾…æ±‚è§£æ•°æ® å¾…æ±‚è§£æ•°æ® yå€¼ å·²çŸ¥æ•°æ® å·²çŸ¥æ•°æ® 3.3 Optimization Algorithm For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. $$Î¸ -&gt; x -&gt; Î¸ -&gt; x -&gt; Î¸ -&gt; x -&gt; Î¸ -&gt; x -&gt; â€¦$$ Actually, these two steps are Linear Regression, we shoud do that simultaneously to update theta and x. 3.4 Collaborative filtering Optimization Algorithm å®é™…ä¸Šï¼Œä¸Šé¢æ˜¯ä¸¤ä¸ª LRçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢ä¸¤æ­¥åˆå¹¶åˆ°ä¸€èµ·ï¼Œè¿™ä¸ªå°±æ˜¯collaborative filterringï¼Œ æ­¤æ—¶çš„optimizatino object å°±ä» J(theta) å’Œ J(X) å˜ä¸ºäº† J(theta, X)ã€‚ å…·ä½“æ­¥éª¤å¦‚ä¸‹ 3.5 Vectorization: Low rank matrix factorizationé¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæŠŠè¯„åˆ†Yç”¨å‘é‡è¡¨ç¤ºå‡ºæ¥ï¼ŒåŒæ—¶è¡¨ç¤ºä¸ºThetaå’ŒXä¸¤ä¸ªçŸ©é˜µçš„ä¹˜ç§¯$$Y= \\begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \\ 5 &amp; ? &amp; ?&amp; 0 \\ ? &amp; 4 &amp; 0 &amp; ? \\ 0 &amp; 0 &amp; 5 &amp; 4 \\ 0 &amp; 0 &amp; 5 &amp; 0\\end{bmatrix} =\\begin{bmatrix}(\\theta^{(1)})^T(x^{(1)}) &amp; (\\theta^{(2)})^T(x^{(1)}) &amp; â€¦ &amp; (\\theta^{(n_u)})^T(x^{(1)}) \\(\\theta^{(1)})^T(x^{(2)}) &amp; (\\theta^{(2)})^T(x^{(2)}) &amp; â€¦ &amp; (\\theta^{(n_u)})^T(x^{(2)}) \\â€¦ &amp; â€¦ &amp; â€¦ &amp; â€¦ \\(\\theta^{(1)})^T(x^{(n_m)}) &amp; (\\theta^{(2)})^T(x^{(n_m)}) &amp; â€¦ &amp; (\\theta^{(n_u)})^T(x^{(n_m)})\\end{bmatrix} = X * \\Thetaâ€™, R \\in (n_m Ã— n_u)$$ $$X = \\begin{bmatrix}â€”(x^{(1)})^Tâ€” \\â€”(x^{(2)})^Tâ€” \\â€¦ \\â€”(x^{(n_m)})^Tâ€”\\end{bmatrix},x^{(n_m)} = \\begin{bmatrix}x^{(n_m)}_1 \\ x^{(n_m)}_2 \\ â€¦ \\ x^{(n_m)}_n\\end{bmatrix}, R \\in (n_m Ã— n)$$ $$\\Theta = \\begin{bmatrix}â€”(\\theta^{(1)})^Tâ€” \\â€”(\\theta^{(2)})^Tâ€” \\â€¦ \\â€”(\\theta^{(n_u)})^Tâ€”\\end{bmatrix},\\theta^{(n_u)} = \\begin{bmatrix}\\theta^{(n_u)}_1 \\ \\theta^{(n_u)}_2 \\ â€¦ \\ \\theta^{(n_u)}_n\\end{bmatrix}, R \\in (n_u Ã— n)$$ 3.6 Mean Normalizationå¯¹äºé‚£äº›æ–°æ³¨å†Œç”¨æˆ·ï¼Œç³»ç»Ÿä¸­æ²¡æœ‰è®°å½•ä»–ä»¬çš„åå¥½ï¼Œåˆ™é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚ å…ˆè®¡ç®—å‡ºæ¯éƒ¨ç”µå½±è¯„åˆ†çš„å¹³å‡å€¼muï¼Œç„¶åæŠŠæ‰€æœ‰çš„è¯„åˆ†éƒ½å‡å»å¹³å‡å€¼ï¼ˆæ­¤åå¤„ç†è¿‡çš„è¯„åˆ†å¹³å‡å€¼ä¸º0ï¼‰ã€‚è™½ç„¶è¿™æ ·åšå¯¹æœ‰è¯„åˆ†è®°å½•ç”¨æˆ·æ˜¯å¤šä½™çš„ï¼Œä½†å´å¯ä»¥å§æ²¡æœ‰è¯„åˆ†è®°å½•çš„ç”¨æˆ·ç»™ç»Ÿä¸€è¿›æ¥ï¼Œé¿å…å…¨æ˜¯0çš„æƒ…å†µã€‚ 4. Implement Algorithm4.1 Cost Function without Regularization Tipsï¼šè¿™é‡Œéœ€è¦è®¡ç®—çš„åªæ˜¯é’ˆå¯¹é‚£äº›å·²ç»è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œå¯¹äºç”¨æˆ·æ²¡æœ‰è¯„åˆ†è¿‡çš„ä¸éœ€è¦è®¡ç®—ã€‚ 4.2 Collaborative filtering gradient$$\\frac {\\partial J} {\\partial x_k^{(1)}} , \\frac {\\partial J} {\\partial x_k^{(2)}} , â€¦, \\frac {\\partial J} {\\partial x_k^{(n_m)}} \\space\\space for \\space each \\space movie\\\\frac {\\partial J} {\\partial \\theta_k^{(1)}} , \\frac {\\partial J} {\\partial \\theta_k^{(2)}} , â€¦, \\frac {\\partial J} {\\partial \\theta_k^{(n_u)}} \\space\\space for \\space each \\space user$$Tipsï¼š å¯¹äºä½¿ç”¨vectorizationæ–¹æ³•ï¼Œæœ€ç»ˆåªæœ‰ä¸¤ä¸ªfor-loopï¼Œä¸€ä¸ªè®¡ç®—$X_{grad}$ï¼Œä¸€ä¸ªè®¡ç®—$Theta_{grad}$ å¦‚ä½•å¯¹Xå’ŒThetaæ±‚åå¯¼æ•°ï¼Ÿ $$(Theta_{grad}(i, :))^T = \\begin{bmatrix}\\frac {\\partial J} {\\partial \\theta^{(i)}_1} \\\\frac {\\partial J} {\\partial \\theta^{(i)}_2} \\â€¦ \\\\frac {\\partial J} {\\partial \\theta^{(i)}_n}\\end{bmatrix}$$ åŒæ ·ï¼Œæˆ‘ä»¬åªéœ€è€ƒè™‘ç”¨æˆ·å·²ç»è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œç”¨å…¶ä½œä¸ºè®­ç»ƒæ ·æœ¬ å› ä¸ºVectorizationéå¸¸å®¹æ˜“æä¹±å„ä¸ªmatrixï¼Œæ‰€ä»¥å»ºè®®å…ˆæ•´ç†ä¸€ä¸‹å„ä¸ªmatrixçš„sizeï¼Œè®¡ç®—æ—¶å¯ä»¥æ ¹æ®matrixçš„sizeè¿›è¡Œè®¡ç®—ã€‚ 4.3 Implementationæ³¨æ„è¿™é‡Œå¹¶æ²¡æœ‰ç»™å‡ºå®Œæ•´çš„ä»£ç  (Octave/Matlab)ï¼Œéƒ½åªæ˜¯ä¸»è¦çš„éƒ¨åˆ†ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950% Theta : nu x n% X : nm x n% Y : nm x nu% R : nm x nupred = X * Theta'; %nm x nu 'diff = pred - Y;% Cost Function with regularizationJ = 0.5 * sum(sum((diff.^2) .* R));J = J + (lambda * 0.5) * sum(sum(Theta.^2)); % regularized term of theta.J = J + (lambda * 0.5) * sum(sum(X.^2)); % regularized term of x.% calculate Xfor i = 1 : num_movies, % the row vector of all users that have rated movie i idx = find(R(i, :) == 1); % (1 * r) % the list of users who have rated on movie i Theta_temp = Theta(idx, :); % (r * n) Y_temp = Y(i, idx); % (1 * r) X_temp = X(i, :); % (1 * n) % ((1 * n) * (n * r) -(1 * r)) * (r * n) = (1 * n) X_grad(i, :) = (X_temp * Theta_temp' - Y_temp) * Theta_temp; %' % regularization X_grad(i, :) = X_grad(i, :) + lambda * X_temp;end% calculate Thetafor i = 1 : num_users, % the row vector of all movies that user i has rated idx = find(R(:, i) == 1)'; % (1 * r) ' Theta_temp = Theta(i, :); % (1 * n) Y_temp = Y(idx, i); % (r * 1) X_temp = X(idx, :); % (r * n) % ((r * n) * (n * 1) - (r * 1)) * (r * n) = (1 * n) Theta_grad(i, :) = (X_temp * Theta_temp' - Y_temp)' * X_temp; % regularization Theta_grad(i, :) = Theta_grad(i, :) + lambda * Theta_temp;endgrad = [X_grad(:); Theta_grad(:)]; â€‹â€‹â€‹","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Machine Learning - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-SVM/","text":"1. Optimisation Objective$$h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\z = -\\theta^Tx$$ Why we need do that? 2. Hypothesis Function2.1 Logistic Regression$$\\frac 1 m \\sum_{i=1}^m [ y^{(i)} (-log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) (-log(1 - h_\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum_{j=1}^n \\theta_j^2$$ 2.2 Support Vector Machine$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ Analysisï¼š ä¸ºäº†ä½¿å¾—cost functionå–å¾—æœ€å°å€¼ï¼Œæˆ‘ä»¬ä»¤C*W + Péƒ¨åˆ†ä¸­ï¼ŒC*Wä¸ºé›¶ã€‚å³ï¼š å½“ y = 1æ—¶ï¼Œ cost1 = 0ï¼Œæ‰€ä»¥ z &gt;= 1 å½“ y = 0æ—¶ï¼Œ cost0 = 0ï¼Œæ‰€ä»¥ z &lt;= -1 Noteï¼š1. cost0 and cost1 å¯¹åº”çš„æ˜¯ä¸Šå›¾ä¸­å·¦å³ä¸¤è¾¹çš„cost functionï¼Œå› ä¸ºy=0å’Œy=1çš„ç›®æ ‡å‡½æ•°ã€‚ å¸¸æ•°Cå–ä¸€ä¸ªå¾ˆå¤§çš„å€¼æ—¶æ¯”è¾ƒå¥½ã€‚å› ä¸ºC*W + Pï¼Œ æ‰€ä»¥Cå¤§åˆ™Wä¼šå˜å°ï¼Œå³ç›¸å¯¹penalityå°±ä¼šå˜å¤§ï¼ŒWä¼šå˜å° ä¸ºä»€ä¹ˆè¦é‡æ–°é€‰å®šä¸€ä¸ªcost function ï¼Ÿï¼ˆé€»è¾‘å›å½’çš„ä¸´ç•Œç‚¹ä¸º0ï¼Œä½†æ˜¯SVMçš„ä¸´ç•Œç‚¹æ˜¯1ï¼Œæ‰€ä»¥SVMæ›´åŠ ç²¾ç¡®ã€‚ ï¼‰ å¯¹åº”çš„çº¿æ€§é€»è¾‘å›å½’ï¼Ÿå³æ¬¡æ•°ä¸å¤§äº1çš„ï¼Ÿ Decision Boundary ä¸æ˜¯ä¸€æ¡ç›´çº¿çš„æƒ…å†µ 3. Large Margin Classifier12ç»“è®ºï¼šå¸¸æ•°Cå–ä¸€ä¸ªæ¯”è¾ƒå¤§çš„å€¼æ¯”è¾ƒå®¹æ˜“è·å¾—Large Margin ClassifierCå¤§ï¼Œåˆ™æ¯”è¾ƒå®¹æ˜“è·å¾— ä»¥ä¸Šä¸ºä¸¤ç±»åˆ†å¸ƒæ¯”è¾ƒå‡åŒ€çš„æ—¶å€™ï¼ŒDecision Boundaryä¸ºå›¾ä¸­é»‘è‰²çš„çº¿ï¼Œæ‰€æœ‰ç‚¹ç¦»é»‘è‰²çš„è·ç¦»éƒ½ç›¸å¯¹æ¯”è¾ƒå¤§æ¯”è¾ƒå‡åŒ€ï¼Œä½†æ˜¯å½“å­˜åœ¨å¹²æ‰°ç‚¹çš„æ—¶å€™å¦‚ä¸‹å›¾ï¼ŒDecision Boundaryä¼šç”±é»‘è‰²å˜ä¸ºç²‰çº¢è‰²ã€‚æ‰€ä»¥Cçš„å–å€¼ä¸èƒ½å¤ªå¤§ï¼Œä¹Ÿä¸èƒ½å¤ªå°ã€‚éœ€è¦æ±‚å‡ºæœ€ä¼˜è§£ 4. Mathmatics Behind Large Margin Classification4.1 Vector Inner Product Noteï¼š å¦‚ä½•æ±‚æŠ•å½±pçš„å€¼ï¼Ÿ å½“è§’åº¦ &lt; 90Â°ï¼Œpä¸ºæ­£æ•°ã€‚å½“è§’åº¦ &gt; 90Â°æ—¶ï¼Œpä¸ºè´Ÿæ•°ã€‚ å‘é‡å†…ç§¯$$u^Tv = ||u|| Â· ||v|| Â· cosÎ¸ = ||u|| Â· p_{v,u} = ||v|| Â· p_{u,v} = u_1v_1+u_2v_2$$ 4.2 SVM Cost Function$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ å½“Cå–ä¸€ä¸ªä¸€ä¸ªå¾ˆå¤§çš„å€¼æ—¶ï¼Œcost functionåªå‰©ä¸‹åé¢Pçš„éƒ¨åˆ†ã€‚ å‡è®¾Î¸0 = 0$$\\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + â€¦ + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + â€¦ + \\theta^2_n)= \\frac 1 2 ||\\theta||^2$$ æ‰€ä»¥ï¼š$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\p^{(i)}æ˜¯ç‚¹åˆ°å‘é‡\\thetaçš„projectionï¼Œå³ç‚¹åˆ°Decision Boundaryçš„è·ç¦»$$ä¸Šé¢æˆ‘ä»¬è®¨è®ºäº†ï¼Œå½“Cå–åˆ°ä¸€ä¸ªåˆé€‚çš„ã€è¾ƒå¤§çš„æ•°å€¼æ—¶ï¼ŒSVMçš„cost functionå°±åªå‰©ä¸‹åé¢Pçš„éƒ¨åˆ†ï¼Œå³$$\\frac 1 2 ||\\theta||^2$$æˆ‘ä»¬è¦å‡å°cost functionï¼Œæ‰€ä»¥éœ€è¦å‡å°Î¸çš„å€¼ã€‚ å½“Î¸å–åˆ°ä¸€ä¸ªæ¯”è¾ƒå°çš„å€¼çš„æ—¶å€™ï¼Œè¿˜éœ€è¦æ»¡è¶³ä¸Šé¢è®¨è®ºçš„ï¼š$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\$$æ‰€ä»¥Î¸æ¯”è¾ƒå°æ—¶ï¼Œåªèƒ½å¢åŠ pçš„å€¼å»æ»¡è¶³p*||Î¸|| &gt;= 1 æˆ–è€… p*||Î¸||&lt;= -1ã€‚ è¿™æ ·å°±ä¿è¯äº†pçš„å€¼æ¯”è¾ƒå¤§ï¼Œå³ç‚¹åˆ°Decision Boundaryçš„å¤§é—´è·ã€‚ 5. Kernels5.1 Kernels &amp; Similarityé¦–å…ˆï¼Œæˆ‘ä»¬å›æƒ³ä¸€ä¸‹ä¹‹å‰çš„logistic regressionä¸­å¯¹äºnon-linear æƒ…å†µçš„æ‹Ÿåˆã€‚ Predict y = 1, if$$\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3x_2 + \\theta_4x_1^2 + \\theta_5x_2^2 + â€¦ &gt;= 0 \\\\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + â€¦ &gt;= 0$$å³å°†fnå®šä¹‰ä¸ºxçš„å¹‚æ¬¡é¡¹ç»„åˆï¼Œå¦‚ä¸‹ï¼š$$f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, â€¦$$ ä½†æ˜¯åœ¨SVMä¸­ï¼Œæˆ‘ä»¬è¦é‡æ–°å®šä¹‰fnï¼Œå¼•å…¥Kernelçš„æ¦‚å¿µï¼Œå³ç”¨ kernel functionæ¥è¡¨ç¤ºfnã€‚ Note: l æ˜¯landmarkï¼Œä¸”å¦‚æœtraining setsé‡Œé¢çš„æ•°é‡ä¸ºnçš„è¯ï¼Œåˆ™landmarkçš„æ•°é‡ä¹Ÿä¸ºnã€‚ å‡è®¾training setsæ•°é‡ä¸ºnï¼Œåˆ™å¯¹äºä¸€ä¸ªæ–°çš„exampleæ¥è¯´ï¼Œå¯è®¡ç®—å‡ºnä¸ªæ–°çš„ç‰¹å¾f1â€¦fnã€‚ç„¶åç”¨æ–°çš„ç‰¹å¾ï¼Œå¯¹è¯¥exampleè¿›è¡Œåˆ¤æ–­ï¼ˆä½ç»´è½¬ä¸ºé«˜ç»´çš„è¿‡ç¨‹ï¼‰ kernel functionä¸ºguassian functionã€‚å½“xä¸landmark lè¶Šæ¥è¿‘æ—¶ï¼Œä¸¤ç‚¹çš„è·ç¦»è¶Šå°ï¼Œå€¼æ¥è¿‘1 5.2 SVM with Kernels å¯¹æ¯”ä¹‹å‰çš„cost functionï¼Œå¯ä»¥å‘ç°è¿™é‡ŒÎ¸å’Œf(x)è·Ÿä¹‹å‰çš„ä¸åŒã€‚ åœ¨logistic regression ä¸­ï¼ŒÎ¸çš„ç»´åº¦ä¸º(n+1) x 1, åŒ…å«Î¸0ï¼Œ ä¸”nä¸ºå•ä¸ªexampleçš„ç‰¹å¾ä¸ªæ•° åœ¨SVM with kernelä¸­ï¼Œf(x)çš„ä¸ªæ•°ä¸ºmï¼Œå…¶ä¸­mæ˜¯training setsä¸­çš„ä¸ªæ•°ï¼Œæ‰€ä»¥Î¸çš„ç»´åº¦åº”è¯¥æ˜¯(m+1)x1 Steps ç»™å®šä¸€ç»„training setsï¼Œæ ¹æ®æ¯ä¸ªexampleï¼Œé€‰å–mä¸ªlandmarkç‚¹ è®¡ç®—æ¯ä¸€ä¸ªexampleä¸æ‰€æœ‰landmarkçš„ç›¸è¯†åº¦ï¼Œç›¸åŒä¸º1ï¼Œéå¸¸ä¸åŒæ¥è¿‘ä¸º0ã€‚è®¡ç®—ç›¸è¯†åº¦çš„kernel functionä¸ºGaussian Function æœ€ç»ˆï¼Œå¯¹äºæ¯ä¸€ä¸ªexampleé‡Œé¢éƒ½å¯ä»¥è®¡ç®—å‡ºmä¸ªæ–°çš„featureï¼Œæ‰€ä»¥å¯¹äºè¿™ä¸ªtraining setsè€Œè¨€ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªm*mçš„çŸ©é˜µï¼Ÿ å°†å¾—åˆ°çš„m*mçš„çŸ©é˜µï¼Œä»£å…¥åˆ°Hypothesisä¸­ï¼Œè®¡ç®—å‡ºÎ¸çš„å€¼ã€‚ 5.4 SVM parameters C = 1/Î» Large C Small Î» Large Î¸ Lower Bias High Variance Over Fitting Small C Large Î» Small Î¸ Higher Bias Low Variance Under Fitting Ïƒ Large Ïƒ more smoothly Higher Bias Lower Variance Under Fitting Small Ïƒ less smoothly Lower Bias Higher Variance Over Fitting","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Linuxå¸¸ç”¨æŒ‡ä»¤","date":"2017-02-19T01:16:24.000Z","path":"2017/02/19/Linuxå¸¸ç”¨æŒ‡ä»¤ (Chenson's conflicted copy 2019-02-11)/","text":"æœ‰æ—¶å€™è®°ä¸œè¥¿è€æ˜¯è®°ä¸ä½ï¼Œè™½ç„¶æ˜¯ç»å¸¸ä½¿ç”¨ï¼Œæ‰€ä»¥å¯¹å¸¸ç”¨çš„linuxæŒ‡ä»¤åšä¸ªç®€å•è®°å½•ï¼Œæ–¹ä¾¿ä»¥åä½¿ç”¨ï¼ˆä¸æ–­æ›´æ–°â€¦ â€¦ï¼‰ã€‚ 1. scpæ–‡ä»¶ä¼ è¾“ (security copy)12# eg. scp chenson@127.0.0.1:~/Home/test . ~/Home/local_test_pathscp PATH_OF_SRC PATH_OF_DST 2. æµ‹å†…å­˜çš„ä½¿ç”¨ç‡12345678valgrind --tool=massif --pages-as-heap=yesms_print massif æ–‡ä»¶åvalgrind --tool=massif --pages-as-heap=yes ./bwtsearch testfiles/shopping.bwt l.idx \"system\"ms_print massif.out.3116 1find &lt;PATH TO FOLDER&gt; -ls | tr -s &apos; &apos;|cut -d&apos; &apos; -f 7| paste -sd+ | bc 3. å¼ºåˆ¶å†™å…¥123456789# vii commond model, Try the below command:w !sudo tee %Explanation:w â€“ write!sudo â€“ call shell sudo commandtee â€“ the output of write (:w) command is redirected using tee% â€“ current file name 4. æŸ¥çœ‹äºŒè¿›åˆ¶æ–‡ä»¶vimä¸‹ 1docker exec -it -u root spark-notebook /bin/bash:%!xxd linuxä¸‹ 1xxd -b file_name 5. æŸ¥çœ‹å…¬ç½‘IP1curl http://members.3322.org/dyndns/getip 6. æŸ¥çœ‹é‚£ä¸ªè¿›ç¨‹å ç”¨ï¼š1lsof -i:9000 # MAC 7. æŸ¥çœ‹ç›‘å¬ç«¯å£ï¼š12netstat -ntlp # linuxlsof -i:port_num # MAC 8. æµ‹æ–‡ä»¶å¤¹å¤§å°123456find &lt;PATH TO FOLDER&gt; -ls | tr -s ' '|cut -d' ' -f 7| paste -sd+ |bcdu -ah --max-depth=1ï¼Œå…¶ä¸­aè¡¨ç¤ºæ˜¾ç¤ºç›®å½•ä¸‹æ‰€æœ‰çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ï¼ˆä¸å«å­ç›®å½•ï¼‰ï¼Œhè¡¨ç¤ºä»¥äººç±»èƒ½çœ‹æ‡‚çš„æ–¹å¼ï¼Œmax-depthè¡¨ç¤ºç›®å½•çš„æ·±åº¦ã€‚du -sh : æŸ¥çœ‹å½“å‰ç›®å½•æ€»å…±å çš„å®¹é‡ã€‚è€Œä¸å•ç‹¬åˆ—å‡ºå„å­é¡¹å ç”¨çš„å®¹é‡ 9. æ–‡ä»¶ç³»ç»Ÿç©ºé—´ä½¿ç”¨æƒ…å†µ12345678910//æŸ¥çœ‹ç³»ç»Ÿä¸­æ–‡ä»¶çš„ä½¿ç”¨æƒ…å†µdf -h//æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹å„ä¸ªæ–‡ä»¶åŠç›®å½•å ç”¨ç©ºé—´å¤§å°du -sh *//æ–¹æ³•ä¸€ï¼šåˆ‡æ¢åˆ°è¦åˆ é™¤çš„ç›®å½•ï¼Œåˆ é™¤ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶rm -f *//æ–¹æ³•äºŒï¼šåˆ é™¤logsæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œè€Œä¸åˆ é™¤æ–‡ä»¶å¤¹æœ¬èº«rm -rf log/* 10. åŠ å¯†æ–‡ä»¶å¤§å°12# oJ5A93Fhzip -r -P password utils.zip utils 11. æ‰“åŒ…+å‹ç¼© å‹ç¼© 12345678910111213tar czvf my.tar.gz file1 file2 ....fileNtar -cvf jpg.tar *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆtar.jpg tar -czf jpg.tar.gz *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨gzipå‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªgzipå‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.gz tar -cjf jpg.tar.bz2 *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨bzip2å‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªbzip2å‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.bz2tar -cZf jpg.tar.Z *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨compresså‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªumcompresså‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.Zrar a jpg.rar *.jpg //raræ ¼å¼çš„å‹ç¼©ï¼Œéœ€è¦å…ˆä¸‹è½½rar for linuxzip jpg.zip *.jpg //zipæ ¼å¼çš„å‹ç¼©ï¼Œéœ€è¦å…ˆä¸‹è½½zip for linux è§£å‹ 1234567891011tar -xvf file.tar //è§£å‹ taråŒ…tar -xzvf file.tar.gz //è§£å‹tar.gztar -xjvf file.tar.bz2 //è§£å‹ tar.bz2tar -xZvf file.tar.Z //è§£å‹tar.Zunrar e file.rar //è§£å‹rarunzip file.zip //è§£å‹zip 12. æŸ¥æ‰¾æ–‡ä»¶12345678910111213141516171819202122$find ~ -name \"*.txt\" -print #åœ¨$HOMEä¸­æŸ¥.txtæ–‡ä»¶å¹¶æ˜¾ç¤º$find . -name \"*.txt\" -print$find . -name \"[A-Z]*\" -print #æŸ¥ä»¥å¤§å†™å­—æ¯å¼€å¤´çš„æ–‡ä»¶$find /etc -name \"host*\" -print #æŸ¥ä»¥hostå¼€å¤´çš„æ–‡ä»¶$find . -name \"[a-z][a-z][0â€“9][0â€“9].txt\" -print #æŸ¥ä»¥ä¸¤ä¸ªå°å†™å­—æ¯å’Œä¸¤ä¸ªæ•°å­—å¼€å¤´çš„txtæ–‡ä»¶$find . -perm 755 -print$find . -perm -007 -exec ls -l &#123;&#125; \\; #æŸ¥æ‰€æœ‰ç”¨æˆ·éƒ½å¯è¯»å†™æ‰§è¡Œçš„æ–‡ä»¶åŒ-perm 777$find . -type d -print$find . ! -type d -print $find . -type l -print$find . -size +1000000c -print #æŸ¥é•¿åº¦å¤§äº1Mbçš„æ–‡ä»¶$find . -size 100c -print # æŸ¥é•¿åº¦ä¸º100cçš„æ–‡ä»¶$find . -size +10 -print #æŸ¥é•¿åº¦è¶…è¿‡æœŸä½œåºŸ10å—çš„æ–‡ä»¶ï¼ˆ1å—=512å­—èŠ‚ï¼‰$cd /$find etc home apps -depth -print | cpio -ivcdC65536 -o /dev/rmt0$find /etc -name \"passwd*\" -exec grep \"cnscn\" &#123;&#125; \\; #çœ‹æ˜¯å¦å­˜åœ¨cnscnç”¨æˆ·$find . -name \"yao*\" | xargs file$find . -name \"yao*\" | xargs echo \"\" &gt; /tmp/core.log$find . -name \"yao*\" | xargs chmod o-w 13. é€’å½’åˆ é™¤æ‰€æœ‰æ–‡ä»¶å¤¹ä¸‹çš„ç‰¹å®šç±»å‹æ–‡ä»¶1find . -name '*.ttteset' -type f -print -exec rm -rf &#123;&#125; \\; 14. åå°è¿è¡ŒæŒ‡å®šç¨‹åºï¼ˆä¸æŒ‚æ–­åœ°è¿è¡Œå‘½ä»¤ï¼‰12345# no hang upnohup Command [é€‰é¡¹ å‚æ•°] &amp;# e.g ç¨‹åºè¿è¡Œå½“ä¸­çš„æ‰€æœ‰logéƒ½ä¼šè¢«é‡å®šå‘åˆ°file.logé‡Œé¢å»ï¼Œç¼ºçœçš„æƒ…å†µä¸‹æ˜¯nohup.outæ–‡ä»¶ä¸­nohup command &gt; file.log 2&gt;&amp;1 &amp; 123456789101112131415161718nohup python main.py 80 &gt; ~/log/WeChatAccountLog.file 2&gt;&amp;1 &amp;è¯¥å‘½ä»¤çš„ä¸€èˆ¬å½¢å¼ä¸ºï¼šnohup command &amp;å¦‚æœä½¿ç”¨nohupå‘½ä»¤æäº¤ä½œä¸šï¼Œé‚£ä¹ˆåœ¨ç¼ºçœæƒ…å†µä¸‹è¯¥ä½œä¸šçš„æ‰€æœ‰è¾“å‡ºéƒ½è¢«é‡å®šå‘åˆ°ä¸€ä¸ªåä¸ºnohup.outçš„æ–‡ä»¶ä¸­ï¼Œé™¤éå¦å¤–æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼šcommand&gt;out.fileæ˜¯å°†commandçš„è¾“å‡ºé‡å®šå‘åˆ°out.fileæ–‡ä»¶ï¼Œå³è¾“å‡ºå†…å®¹ä¸æ‰“å°åˆ°å±å¹•ä¸Šï¼Œè€Œæ˜¯è¾“å‡ºåˆ°out.fileæ–‡ä»¶ä¸­ã€‚2&gt;&amp;1 æ˜¯å°†æ ‡å‡†å‡ºé”™é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡ºï¼Œè¿™é‡Œçš„æ ‡å‡†è¾“å‡ºå·²ç»é‡å®šå‘åˆ°äº†out.fileæ–‡ä»¶ï¼Œå³å°†æ ‡å‡†å‡ºé”™ä¹Ÿè¾“å‡ºåˆ°out.fileæ–‡ä»¶ä¸­ã€‚æœ€åä¸€ä¸ª&amp;ï¼Œ æ˜¯è®©è¯¥å‘½ä»¤åœ¨åå°æ‰§è¡Œã€‚è¯•æƒ³2&gt;1ä»£è¡¨ä»€ä¹ˆï¼Œ2ä¸&gt;ç»“åˆä»£è¡¨é”™è¯¯é‡å®šå‘ï¼Œè€Œ1åˆ™ä»£è¡¨é”™è¯¯é‡å®šå‘åˆ°ä¸€ä¸ªæ–‡ä»¶1ï¼Œè€Œä¸ä»£è¡¨æ ‡å‡†è¾“å‡ºï¼›æ¢æˆ2&gt;&amp;1ï¼Œ&amp;ä¸1ç»“åˆå°±ä»£è¡¨æ ‡å‡†è¾“å‡ºäº†ï¼Œå°±å˜æˆé”™è¯¯é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡º.ctrl + z å¯ä»¥å°†ä¸€ä¸ªæ­£åœ¨å‰å°æ‰§è¡Œçš„å‘½ä»¤æ”¾åˆ°åå°ï¼Œå¹¶ä¸”å¤„äºæš‚åœçŠ¶æ€ã€‚Ctrl+c ç»ˆæ­¢å‰å°å‘½ä»¤ã€‚jobs æŸ¥çœ‹å½“å‰æœ‰å¤šå°‘åœ¨åå°è¿è¡Œçš„å‘½ä»¤ä½¿ç”¨ fg %n å…³é—­ 15. åå°ä¸Šä¼ /ä¸‹è½½æ•°æ®1å¦å¤–æœ‰ä¸¤ä¸ªå¸¸ç”¨çš„ftpå·¥å…·ncftpgetå’Œncftpputï¼Œå¯ä»¥å®ç°åå°çš„ftpä¸Šä¼ å’Œä¸‹è½½ï¼Œè¿™æ ·å°±å¯ä»¥åˆ©ç”¨è¿™äº›å‘½ä»¤åœ¨åå°ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶äº†ã€‚ 16. Docker åŠ é€Ÿå™¨åœ°å€12https://cr.console.aliyun.com/cn-qingdao/mirrorshttps://q90v31vp.mirror.aliyuncs.com 17. æŸ¥æ‰¾æŒ‡å®šå†…å®¹1find . | xargs grep &quot;keyword&quot; | egrep &quot;keyword&quot; # å½“å‰æ–‡ä»¶ç›®å½•ä¸‹ vsftpd, kaiqi ftp fuwuchoongqi dashuju taojianambari-agent restart lspci https://www.cnblogs.com/YangJieCheng/p/5907166.html","tags":[{"name":"Linux","slug":"Linux","permalink":"http://chenson.com/tags/Linux/"}]},{"title":"Linuxå¸¸ç”¨æŒ‡ä»¤","date":"2017-02-19T01:16:24.000Z","path":"2017/02/19/Linuxå¸¸ç”¨æŒ‡ä»¤/","text":"æœ‰æ—¶å€™è®°ä¸œè¥¿è€æ˜¯è®°ä¸ä½ï¼Œè™½ç„¶æ˜¯ç»å¸¸ä½¿ç”¨ï¼Œæ‰€ä»¥å¯¹å¸¸ç”¨çš„linuxæŒ‡ä»¤åšä¸ªç®€å•è®°å½•ï¼Œæ–¹ä¾¿ä»¥åä½¿ç”¨ï¼ˆä¸æ–­æ›´æ–°â€¦ â€¦ï¼‰ã€‚ 1. scpæ–‡ä»¶ä¼ è¾“ (security copy)12# eg. scp chenson@127.0.0.1:~/Home/test . ~/Home/local_test_pathscp PATH_OF_SRC PATH_OF_DST 2. æµ‹å†…å­˜çš„ä½¿ç”¨ç‡12345678valgrind --tool=massif --pages-as-heap=yesms_print massif æ–‡ä»¶åvalgrind --tool=massif --pages-as-heap=yes ./bwtsearch testfiles/shopping.bwt l.idx \"system\"ms_print massif.out.3116 1find &lt;PATH TO FOLDER&gt; -ls | tr -s &apos; &apos;|cut -d&apos; &apos; -f 7| paste -sd+ | bc 3. å¼ºåˆ¶å†™å…¥123456789# vii commond model, Try the below command:w !sudo tee %Explanation:w â€“ write!sudo â€“ call shell sudo commandtee â€“ the output of write (:w) command is redirected using tee% â€“ current file name 4. æŸ¥çœ‹äºŒè¿›åˆ¶æ–‡ä»¶vimä¸‹ 1:%!xxd linuxä¸‹ 1xxd -b file_name 5. æŸ¥çœ‹å…¬ç½‘IP1curl http://members.3322.org/dyndns/getip 6. æŸ¥çœ‹é‚£ä¸ªè¿›ç¨‹å ç”¨ï¼š1lsof -i:9000 # MAC 7. æŸ¥çœ‹ç›‘å¬ç«¯å£ï¼š12netstat -ntlp # linuxlsof -i:port_num # MAC 8. æµ‹æ–‡ä»¶å¤¹å¤§å°123456find &lt;PATH TO FOLDER&gt; -ls | tr -s ' '|cut -d' ' -f 7| paste -sd+ |bcdu -ah --max-depth=1ï¼Œå…¶ä¸­aè¡¨ç¤ºæ˜¾ç¤ºç›®å½•ä¸‹æ‰€æœ‰çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ï¼ˆä¸å«å­ç›®å½•ï¼‰ï¼Œhè¡¨ç¤ºä»¥äººç±»èƒ½çœ‹æ‡‚çš„æ–¹å¼ï¼Œmax-depthè¡¨ç¤ºç›®å½•çš„æ·±åº¦ã€‚du -sh : æŸ¥çœ‹å½“å‰ç›®å½•æ€»å…±å çš„å®¹é‡ã€‚è€Œä¸å•ç‹¬åˆ—å‡ºå„å­é¡¹å ç”¨çš„å®¹é‡ 9. æ–‡ä»¶ç³»ç»Ÿç©ºé—´ä½¿ç”¨æƒ…å†µ1234567891011# æŸ¥çœ‹ç³»ç»Ÿä¸­æ–‡ä»¶çš„ä½¿ç”¨æƒ…å†µdf -h# æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹å„ä¸ªæ–‡ä»¶åŠç›®å½•å ç”¨ç©ºé—´å¤§å°du -sh *# æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹æ–‡ä»¶å¤¹çš„å¤§å°du -h folder_name# æŸ¥çœ‹è¯¥æ–‡ä»¶å¤¹çš„å¤§å°df -hs folder_name å¯ä»¥çœ‹åˆ°ä¸‹é¢æœ‰ä¸€å—æ•°æ®ç›˜ï¼ŒæŒ‚è½½åœ¨/data1ä¸‹ å¦‚æœç³»ç»Ÿæœ‰å•ç‹¬çš„æ•°æ®ç›˜ï¼Œä¸”æ•°æ®ç›˜æ²¡æœ‰åˆ†åŒºå’ŒæŒ‚è½½ï¼Œä½¿ç”¨df -hå‘½ä»¤æ˜¯çœ‹ä¸åˆ°çš„ï¼Œå¯ä»¥ä½¿ç”¨fdisk -læŸ¥çœ‹ï¼Œå¯ä»¥çœ‹åˆ°æœ‰å“ªäº›ç¡¬ç›˜ ä¸‹å›¾å¯çœ‹åˆ°æ•°æ®ç›˜ä¸ºDisk /dev/vdb å¯ä»¥çœ‹åˆ°ä¸Šå›¾æœ‰53.7Gçš„æœªåˆ†åŒºæ•°æ®ç›˜ï¼Œç„¶åæœ‰1771.7Gçš„å·²åˆ†åŒºçš„æ•°æ®ç›˜ã€‚ References åˆ†åŒºæ•™ç¨‹ UbuntuæŸ¥çœ‹ç£ç›˜ä½¿ç”¨æƒ…å†µ 10. åŠ å¯†æ–‡ä»¶å¤§å°12# oJ5A93Fhzip -r -P password utils.zip utils 11. æ‰“åŒ…+å‹ç¼© å‹ç¼© 12345678910111213tar czvf my.tar.gz file1 file2 ....fileNtar -cvf jpg.tar *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆtar.jpg tar -czf jpg.tar.gz *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨gzipå‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªgzipå‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.gz tar -cjf jpg.tar.bz2 *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨bzip2å‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªbzip2å‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.bz2tar -cZf jpg.tar.Z *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨compresså‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªumcompresså‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.Zrar a jpg.rar *.jpg //raræ ¼å¼çš„å‹ç¼©ï¼Œéœ€è¦å…ˆä¸‹è½½rar for linuxzip jpg.zip *.jpg //zipæ ¼å¼çš„å‹ç¼©ï¼Œéœ€è¦å…ˆä¸‹è½½zip for linux è§£å‹ 1234567891011tar -xvf file.tar //è§£å‹ taråŒ…tar -xzvf file.tar.gz //è§£å‹tar.gztar -xjvf file.tar.bz2 //è§£å‹ tar.bz2tar -xZvf file.tar.Z //è§£å‹tar.Zunrar e file.rar //è§£å‹rarunzip file.zip //è§£å‹zip 12. æŸ¥æ‰¾æ–‡ä»¶12345678910111213141516171819202122$find ~ -name \"*.txt\" -print #åœ¨$HOMEä¸­æŸ¥.txtæ–‡ä»¶å¹¶æ˜¾ç¤º$find . -name \"*.txt\" -print$find . -name \"[A-Z]*\" -print #æŸ¥ä»¥å¤§å†™å­—æ¯å¼€å¤´çš„æ–‡ä»¶$find /etc -name \"host*\" -print #æŸ¥ä»¥hostå¼€å¤´çš„æ–‡ä»¶$find . -name \"[a-z][a-z][0â€“9][0â€“9].txt\" -print #æŸ¥ä»¥ä¸¤ä¸ªå°å†™å­—æ¯å’Œä¸¤ä¸ªæ•°å­—å¼€å¤´çš„txtæ–‡ä»¶$find . -perm 755 -print$find . -perm -007 -exec ls -l &#123;&#125; \\; #æŸ¥æ‰€æœ‰ç”¨æˆ·éƒ½å¯è¯»å†™æ‰§è¡Œçš„æ–‡ä»¶åŒ-perm 777$find . -type d -print$find . ! -type d -print $find . -type l -print$find . -size +1000000c -print #æŸ¥é•¿åº¦å¤§äº1Mbçš„æ–‡ä»¶$find . -size 100c -print # æŸ¥é•¿åº¦ä¸º100cçš„æ–‡ä»¶$find . -size +10 -print #æŸ¥é•¿åº¦è¶…è¿‡æœŸä½œåºŸ10å—çš„æ–‡ä»¶ï¼ˆ1å—=512å­—èŠ‚ï¼‰$cd /$find etc home apps -depth -print | cpio -ivcdC65536 -o /dev/rmt0$find /etc -name \"passwd*\" -exec grep \"cnscn\" &#123;&#125; \\; #çœ‹æ˜¯å¦å­˜åœ¨cnscnç”¨æˆ·$find . -name \"yao*\" | xargs file$find . -name \"yao*\" | xargs echo \"\" &gt; /tmp/core.log$find . -name \"yao*\" | xargs chmod o-w 13. é€’å½’åˆ é™¤æ‰€æœ‰æ–‡ä»¶å¤¹ä¸‹çš„ç‰¹å®šç±»å‹æ–‡ä»¶1find . -name '*.ttteset' -type f -print -exec rm -rf &#123;&#125; \\; 14. åå°è¿è¡ŒæŒ‡å®šç¨‹åºï¼ˆä¸æŒ‚æ–­åœ°è¿è¡Œå‘½ä»¤ï¼‰ nohub 123456789101112131415161718192021nohup python main.py 80 &gt; ~/log/WeChatAccountLog.file 2&gt;&amp;1 &amp;è¯¥å‘½ä»¤çš„ä¸€èˆ¬å½¢å¼ä¸ºï¼šnohup command &amp;å¦‚æœä½¿ç”¨nohupå‘½ä»¤æäº¤ä½œä¸šï¼Œé‚£ä¹ˆåœ¨ç¼ºçœæƒ…å†µä¸‹è¯¥ä½œä¸šçš„æ‰€æœ‰è¾“å‡ºéƒ½è¢«é‡å®šå‘åˆ°ä¸€ä¸ªåä¸ºnohup.outçš„æ–‡ä»¶ä¸­ï¼Œé™¤éå¦å¤–æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼šcommand&gt;out.fileæ˜¯å°†commandçš„è¾“å‡ºé‡å®šå‘åˆ°out.fileæ–‡ä»¶ï¼Œå³è¾“å‡ºå†…å®¹ä¸æ‰“å°åˆ°å±å¹•ä¸Šï¼Œè€Œæ˜¯è¾“å‡ºåˆ°out.fileæ–‡ä»¶ä¸­ã€‚2&gt;&amp;1 æ˜¯å°†æ ‡å‡†å‡ºé”™é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡ºï¼Œè¿™é‡Œçš„æ ‡å‡†è¾“å‡ºå·²ç»é‡å®šå‘åˆ°äº†out.fileæ–‡ä»¶ï¼Œå³å°†æ ‡å‡†å‡ºé”™ä¹Ÿè¾“å‡ºåˆ°out.fileæ–‡ä»¶ä¸­ã€‚æœ€åä¸€ä¸ª&amp;ï¼Œ æ˜¯è®©è¯¥å‘½ä»¤åœ¨åå°æ‰§è¡Œã€‚è¯•æƒ³2&gt;1ä»£è¡¨ä»€ä¹ˆï¼Œ2ä¸&gt;ç»“åˆä»£è¡¨é”™è¯¯é‡å®šå‘ï¼Œè€Œ1åˆ™ä»£è¡¨é”™è¯¯é‡å®šå‘åˆ°ä¸€ä¸ªæ–‡ä»¶1ï¼Œè€Œä¸ä»£è¡¨æ ‡å‡†è¾“å‡ºï¼›æ¢æˆ2&gt;&amp;1ï¼Œ&amp;ä¸1ç»“åˆå°±ä»£è¡¨æ ‡å‡†è¾“å‡ºäº†ï¼Œå°±å˜æˆé”™è¯¯é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡º.# å¯ä»¥å°†ä¸€ä¸ªæ­£åœ¨å‰å°æ‰§è¡Œçš„å‘½ä»¤æ”¾åˆ°åå°ï¼Œå¹¶ä¸”å¤„äºæš‚åœçŠ¶æ€ã€‚ctrl + z # ç»ˆæ­¢å‰å°å‘½ä»¤ã€‚Ctrl+c # æŸ¥çœ‹å½“å‰æœ‰å¤šå°‘åœ¨åå°è¿è¡Œçš„å‘½ä»¤jobs ä½¿ç”¨ fg %n å…³é—­ screen 1 15. åå°ä¸Šä¼ /ä¸‹è½½æ•°æ®1å¦å¤–æœ‰ä¸¤ä¸ªå¸¸ç”¨çš„ftpå·¥å…·ncftpgetå’Œncftpputï¼Œå¯ä»¥å®ç°åå°çš„ftpä¸Šä¼ å’Œä¸‹è½½ï¼Œè¿™æ ·å°±å¯ä»¥åˆ©ç”¨è¿™äº›å‘½ä»¤åœ¨åå°ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶äº†ã€‚ 16. Docker åŠ é€Ÿå™¨åœ°å€12https://cr.console.aliyun.com/cn-qingdao/mirrorshttps://q90v31vp.mirror.aliyuncs.com 17. æŸ¥æ‰¾æŒ‡å®šå†…å®¹1find . | xargs grep &quot;keyword&quot; | egrep &quot;keyword&quot; # å½“å‰æ–‡ä»¶ç›®å½•ä¸‹ vsftpd, kaiqi ftp fuwuchoongqi dashuju taojianambari-agent restart","tags":[{"name":"Linux","slug":"Linux","permalink":"http://chenson.com/tags/Linux/"}]},{"title":"Pythonæ•°æ®åˆ†æç¬”è®°ï¼ˆä¸€ï¼‰","date":"2016-12-15T11:37:30.000Z","path":"2016/12/15/Pythonæ•°æ®åˆ†æç¬”è®°ï¼ˆä¸€ï¼‰/","text":"1. å¸¸è§é—®é¢˜ Pandas.dataframeé‡Œé¢ .values, .iloc, .ix, .loc çš„åŒºåˆ« Different Choices for Indexing loc: only work on index / labeliloc: work on position, from 0, 1, 2, â€¦ â€¦ix: You can get data from dataframe without it being in the indexat: get scalar values. Itâ€™s a very fast lociat: Get scalar values. Itâ€™s a very fast iloc 12345678910111213141516171819202122232425262728&gt;&gt;&gt; df = pd.DataFrame(&#123;'A':['a', 'b', 'c'], 'B':[54, 67, 89]&#125;, index=[100, 200, 300])&gt;&gt;&gt; df A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df.iloc[0] # ç”¨ä½ç½®æ¥ç´¢å¼•A aB 54&gt;&gt;&gt; df.loc[100] # ç”¨åˆå§‹åŒ–æ—¶è®¾ç½®çš„indexæ¥ç´¢å¼•ï¼Œä¹Ÿå°±æ˜¯è‡ªå·±ç»™rowè®¾ç½®çš„labelA aB 54&gt;&gt;&gt; df.loc[100:300] A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df['A'] # ç´¢å¼• columns100 a200 b300 cName: A, dtype: object&gt;&gt;&gt; df.A # ç´¢å¼• columns100 a200 b300 cName: A, dtype: object Pandas å’Œ Numpyä¹‹é—´çš„è½¬æ¢ np.ndarray è½¬åŒ–ä¸º pd.dataframe 1pd.DataFrame(example) pd.dataframe è½¬åŒ–ä¸º np.ndarray 1example.values[:, :] è¯»å†™æ•ˆç‡çš„å¯¹æ¯” npyè¯»å†™æ•ˆç‡æœ€é«˜ï¼Œä½†æœ€è´¹ç¡¬ç›˜ç©ºé—´ï¼Œæ¯”å¦‚np.load(), np.save() csvå…¶æ¬¡ï¼Œæ¯”å¦‚pd.Dataframe.to_csv()ï¼Œpd.load_csv() txtè¯»å†™ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å¾ˆå¿«ï¼Œä½†æ˜¯éœ€è¦é¢‘ç¹çš„splitï¼Œå¯¹æ ¼å¼è§„èŒƒçš„æ•°æ®æ¯”è¾ƒéº»çƒ¦ è‡³äºç®€å•çš„excelå’Œwordï¼Œå¯ä»¥ç”¨xlrd,xlwtæ¥æ“ä½œ 2. Numpy Nç»´æ•°ç»„å¯¹è±¡ï¼Œå¯ä»¥åˆ©ç”¨è¿™ç§æ•°ç»„å¯¹è±¡å¯¹æ•´å—æ•°æ®è¿›è¡Œä¸€äº›ç§‘å­¦è¿ç®—ï¼Œå°±æ˜¯æŠŠarrayå½“åšä¸€ç§å¯¹è±¡é‡Œæ“ä½œã€‚è¿™å’ŒPythonä¸­çš„arrayæ˜¯ä¸åŒçš„ã€‚ ä¸¾ä¸ªæ —å­ï¼š åœ¨Pythonä¸­ 123&gt;&gt;&gt; data = [1, 2, 3]&gt;&gt;&gt; data * 10[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3] Numpyçš„ndarray 12345&gt;&gt;&gt; data2 = np.array([1, 2, 3])&gt;&gt;&gt; data2array([1, 2, 3])&gt;&gt;&gt; data2 * 10array([10, 20, 30]) è½´(axes) å’Œ ç§©(rank) è½´è¡¨ç¤ºçš„æ˜¯ä¸€ç§ç»´åº¦ï¼Œå¦‚ä¸€ç»´çš„æ•°æ®ï¼ŒäºŒç»´çš„æ•°æ®ï¼Œä¸‰ç»´çš„æ•°æ®ç­‰ 12345678910111213141516171819202122&gt;&gt;&gt; data3 = np.array([1, 2, 3]) # æ³¨æ„è¿™é‡Œçš„**æ–¹æ‹¬å·**&gt;&gt;&gt; data3.ndim # æŸ¥çœ‹ç»´åº¦1&gt;&gt;&gt; data3 = np.array([[1,2,3], [1,2,3], [1,2,3]])&gt;&gt;&gt; data3.ndim2&gt;&gt;&gt; data3 = np.array([[[1], [1], [1]], [[1], [1], [1]],[[1], [1], [1]]])&gt;&gt;&gt; data3.ndim3&gt;&gt;&gt; data3.shape(3, 3, 1) # ç»´åº¦ä»æœ€å¤–å±‚åˆ°é‡Œå±‚&gt;&gt;&gt; data3.size9 # 3 * 3 * 1 = 9#ä¸€ä¸ªç”¨æ¥æè¿°æ•°ç»„ä¸­å…ƒç´ ç±»å‹çš„å¯¹è±¡ï¼Œå¯ä»¥é€šè¿‡åˆ›é€ æˆ–æŒ‡å®šdtypeä½¿ç”¨æ ‡å‡†Pythonç±»å‹ã€‚å¦å¤–NumPyæä¾›å®ƒè‡ªå·±çš„æ•°æ®ç±»å‹ã€‚&gt;&gt;&gt; data3.dtype# æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚å¤§å°ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå…ƒç´ ç±»å‹ä¸ºfloat64çš„æ•°ç»„itemsizå±æ€§å€¼ä¸º8(=64/8),åˆå¦‚ï¼Œä¸€ä¸ªå…ƒç´ ç±»å‹ä¸ºcomplex32çš„æ•°ç»„itemå±æ€§ä¸º4(=32/8).&gt;&gt;&gt; data3.itermsize# åŒ…å«å®é™…æ•°ç»„å…ƒç´ çš„ç¼“å†²åŒºï¼Œé€šå¸¸æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨è¿™ä¸ªå±æ€§ï¼Œå› ä¸ºæˆ‘ä»¬æ€»æ˜¯é€šè¿‡ç´¢å¼•æ¥ä½¿ç”¨æ•°ç»„ä¸­çš„å…ƒç´ ã€‚&gt;&gt;&gt; data3.data å¸¸ç”¨çš„æ•°ç»„åˆ›å»ºå‡½æ•° æ‰“å°æ•°ç»„ 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(6) # 1d array&gt;&gt;&gt; print(a)[0 1 2 3 4 5]&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(12).reshape(4,3) # 2d array&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]&gt;&gt;&gt;&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4) # 3d array&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] å¦‚æœä¸€ä¸ªæ•°ç»„ç”¨æ¥æ‰“å°å¤ªå¤§äº†ï¼ŒNumPyè‡ªåŠ¨çœç•¥ä¸­é—´éƒ¨åˆ†è€Œåªæ‰“å°è§’è½ 1234567891011&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt;&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] ç¦ç”¨è¿™ç§reshapeæ¥æ‰“å°æ•´ä¸ªæ•°ç»„ï¼Œéœ€è¦å¯¹printoptionå‚æ•°è¿›è¡Œè®¾ç½® 1&gt;&gt;&gt; np.set_printoptions(threshold='nan') åŸºæœ¬çš„æ•°æ®è¿ç®— 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; b = np.array(list(range(5, 10)))&gt;&gt;&gt; a + barray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; a - barray([-5, -5, -5, -5, -5])&gt;&gt;&gt; a * barray([ 0, 6, 14, 24, 36])&gt;&gt;&gt; a += b&gt;&gt;&gt; aarray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; c = np.ones(5)&gt;&gt;&gt; carray([ 1., 1., 1., 1., 1.])&gt;&gt;&gt; c + aarray([ 6., 8., 10., 12., 14.])&gt;&gt;&gt; c = np.ones(5, dtype=int)&gt;&gt;&gt; c + aarray([ 6, 8, 10, 12, 14]) æ•°ç»„çš„è¿ç®— è¿™å°±ç±»ä¼¼åœ¨Matlab/Octaveä¸­ï¼Œå¯¹matrix/arrayä¸­çš„æ•°æ®æ‰§è¡Œæ‰¹é‡è¿ç®—ï¼Œå³Vectorizationï¼Œå‰ææ˜¯matrix/arrayçš„å¤§å°å¿…é¡»æ»¡è¶³å¯¹åº”çš„è¦æ±‚ã€‚ ç”¨æ•°ç»„è¡¨è¾¾å¼å¯ä»¥ä»£æ›¿å¾ªç¯æ“ä½œï¼ŒçŸ¢é‡åŒ–çš„è¿ç®—æ˜¯Numpyçš„ä¼˜åŠ¿ã€‚ æ•°ç»„è½¬ç½®å’Œè½´å¯¹æ¢ 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.T # æ•°ç»„è½¬ç½®ï¼Œè½´å¯¹æ¢array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]])&gt;&gt;&gt; np.dot(a.T, a) # å†…ç§¯array([[125, 140, 155, 170, 185], [140, 158, 176, 194, 212], [155, 176, 197, 218, 239], [170, 194, 218, 242, 266], [185, 212, 239, 266, 293]])&gt;&gt;&gt; b = np.arange(16).reshape((2, 2, 4))&gt;&gt;&gt; barray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt;&gt;&gt; b.transpose((1, 0 , 2)) # å¯¹é«˜ç»´æ•°ç»„ï¼Œtransposeéœ€è¦å¾—è¦ä¸€ä¸ªç”±è½´ç¼–å·ç»„æˆçš„å…ƒç»„æ‰èƒ½å¯¹è¿™äº›è½´è¿›è¡Œè½¬ç½®array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) ç´¢å¼•å’Œåˆ‡ç‰‡ 1234567891011121314151617181920212223242526272829303132333435# ä¸€ç»´æ•°æ®&gt;&gt;&gt; a = np.arange(5)array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:]array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[1 : 4]array([1, 2, 3])&gt;&gt;&gt; a[1]1# äºŒç»´æ•°æ®&gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6]])array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:, 1:3]array([[2, 3], [5, 6]])&gt;&gt;&gt; b[1][2]6&gt;&gt;&gt; b[1, 2]6# ä¸‰ç»´æ•°ç»„&gt;&gt;&gt; c = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; c[1]array([[ 7, 8, 9], [10, 11, 12]]) é€šç”¨å‡½æ•° P111 ç»™ array æ·»åŠ  columns å’Œ rows 123456789101112# æ–¹æ³•ä¸€np.c_[array1, array2] # æ·»åŠ  columnsnp.r_[array1, array2] # æ·»åŠ  row# æ–¹æ³•äºŒ è¢«æ’å…¥çš„è¡Œnp.insert(a, 2, values=b, axis=1) # æ·»åŠ  columns# æ–¹æ³•ä¸‰a = np.concatenate((a, -np.ones((a.shape[0], 1))), axis=1) # æ·»åŠ  columns# æ–¹æ³•å››np.column_stack((a,b)) 3. Pandasåœ¨Pandasä¸­ï¼ŒSerieså’ŒDataFrameæ˜¯ä¸¤ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„ Series ç±»ä¼¼ä¸€ç»´æ•°ç»„ï¼Œç”±ä¸€ç»„æ•°æ®ï¼ˆå„ç§Numptæ•°æ®ç±»å‹ ( list, dictç­‰ )ï¼‰å’Œä¸€ç»„å¯¹äºçš„æ•°æ®æ ‡ç­¾ç»„æˆ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&gt;&gt;&gt; obj = pd.Series(list(range(3, 8)))&gt;&gt;&gt; obj0 31 42 53 64 7dtype: int64&gt;&gt;&gt; obj.valuesarray([3, 4, 5, 6, 7])&gt;&gt;&gt; obj.index # è¿™é‡Œå°±æ˜¯ index ç´¢å¼•ï¼Œæ²¡æœ‰è®¾ç½®çš„æ˜¯æ—¶å€™ç³»ç»Ÿä¼šè‡ªåŠ¨è®¾ç½®ä¸ºinteger indexRangeIndex(start=0, stop=5, step=1)&gt;&gt;&gt; obj[1] # å¯ä»¥ç”¨è¿™ç´¢å¼•ï¼Œè·Ÿlistç”¨æ³•ç±»ä¼¼ï¼Œä½†æ˜¯listä¸­åªèƒ½æ˜¯æ•°å­—ï¼Œä½†pandasä¸­å¯ä»¥è‡ªå®šä¹‰indexçš„ç´¢å¼•4# è‡ªå®šä¹‰ç´¢å¼•ï¼Œæ™®é€šçš„listå’ŒNumpyçš„æ•°ç»„å°±ä¸è¡Œ&gt;&gt;&gt; obj2 = pd.Series([1,3,4], index=['a', 'b', 'c'])&gt;&gt;&gt; obj2['a']1# ç´¢å¼•å¯ä»¥ç›´æ¥ç”¨æ¥æ•°ç»„è¿ç®—ï¼Œè¿™äº›åœ¨æ•°æ®æ¸…æ´—çš„æ—¶å€™æ¯”è¾ƒå¸¸ç”¨&gt;&gt;&gt; index = obj2 &gt; 2&gt;&gt;&gt; indexa Falseb Truec Truedtype: bool &gt;&gt;&gt; obj2[index]b 3c 4dtype: int64# å…¶ä»–æ“ä½œ&gt;&gt;&gt; np.log(obj2)a 0.000000b 1.098612c 1.386294dtype: float64 # å½“åšå­—å…¸æ¥index&gt;&gt;&gt; 2 in obj2False &gt;&gt;&gt; 3 in obj2False&gt;&gt;&gt; 'a' in obj2 # åªèƒ½æ˜¯index (key)ï¼Œä¸èƒ½æ˜¯valuesTrue# ç”¨å­—å…¸æ¥åˆå§‹åŒ– Series&gt;&gt;&gt; obj3 = &#123;'one': 1, 'two': 2, 'three' : 3&#125;&gt;&gt;&gt; obj3&#123;'one': 1, 'three': 3, 'two': 2&#125;&gt;&gt;&gt; index = ['One', 'Two', 'Three']&gt;&gt;&gt; value = [1, 2, 3]&gt;&gt;&gt; obj4 = pd.Series(value, index)&gt;&gt;&gt; objOne 1Two 2Three 3dtype: int64 # æ‰‹åŠ¨ä¿®æ”¹ç´¢å¼•ï¼Œä¸”ç´¢å¼•çš„å€¼æ˜¯ä¸èƒ½é‡å¤çš„&gt;&gt;&gt; obj4.index = ['one', 'two', 'Three']&gt;&gt;&gt; obj4one 1two 2Three 3dtype: int64 # é‡å»ºç´¢å¼•ï¼Œå¦‚æœç´¢å¼•ä¸å­˜åœ¨çš„å€¼ï¼Œåˆ™å¼•å…¥NaN&gt;&gt;&gt; obj4.reindex(['one', 'two', 'Threeee']) # fill_value=0one 1.0two 2.0Threeee NaNdtype: float64# å¯é€‰ ffill/pad å‘å‰å¡«å……æˆ–è€…bfill/backfill å‘åå¡«å……&gt;&gt;&gt; obj4.ffill()one 1.0two 2.0Threeee 2.0dtype: float64# å½“ç„¶ä¹Ÿå¯ä»¥drop columnsçš„å†…å®¹&gt;&gt;&gt; obj4.drop('Threeee')one 1.0two 2.0dtype: float64 # ä½†æ˜¯ä¸èƒ½è¿™ä¹ˆä¿®æ”¹æ•°å€¼&gt;&gt;&gt; obj4.values = [4, 5, 6]---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2702 else:-&gt; 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):AttributeError: can't set attributeDuring handling of the above exception, another exception occurred:AttributeError Traceback (most recent call last)&lt;ipython-input-41-b4392e8960b0&gt; in &lt;module&gt;()----&gt; 1 obj4.values = [4, 5, 6]/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):-&gt; 2705 object.__setattr__(self, name, value) 2706 2707 # ----------------------------------------------------------------------AttributeError: can't set attribute â€‹ DataFrame DataFrameæ˜¯ä¸€ä¸ªè¡¨æ ¼å‹çš„æ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†ä¸€ç»„æœ‰åºçš„åˆ—ï¼Œæ¯åˆ—å¯ä»¥æ˜¯ä¸åŒçš„å€¼ç±»å‹ã€‚æ‰€æœ‰å¯ä»¥çœ‹åšè¿™æ˜¯ä¸€ä¸ªäºŒç»´çš„æ•°ç»„ï¼Œæœ‰è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼• åˆ›å»ºDataFrameå’ŒåŸºç¡€æ“ä½œ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; dates = pd.date_range('20170305', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6, 4), index=dates, &gt;&gt;&gt; columns=list('ABCD'))&gt;&gt;&gt; df A B C D2017-03-05 -1.616361 0.027641 0.328074 1.0386272017-03-06 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 -0.350594 0.231137 -0.004755 0.6014602017-03-08 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 -0.227631 -0.186816 -0.370522 0.343896# å½“ç„¶ä¹Ÿå¯ä»¥æ‰‹åŠ¨ä¼ è¿›æ¥åˆ›å»ºdf&gt;&gt;&gt; df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)&gt;&gt;&gt; df2Ones A B C D2017-03-05 1 -1.616361 0.027641 0.328074 1.0386272017-03-06 1 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 1 -0.350594 0.231137 -0.004755 0.6014602017-03-08 1 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 1 -0.227631 -0.186816 -0.370522 0.343896# çœ‹DataFrameçš„attributes&gt;&gt;&gt; df.&lt;TAB&gt;df.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D æŸ¥çœ‹DataFrameé‡Œé¢çš„æ•°æ® 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt;&gt; df.head() # é»˜è®¤5è¡Œ A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401&gt;&gt;&gt; df.tail(3) # æ‰‹åŠ¨è®¾ç½®æ‰“å°çš„è¡Œæ•° A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988&gt;&gt;&gt; df.index # index æ˜¯ row çš„ ç´¢å¼•DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df.columnsIndex([u'A', u'B', u'C', u'D'], dtype='object')&gt;&gt;&gt; df.valuesarray([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804&gt;&gt;&gt; df.sort_index(axis=1, ascending=False) # æ˜¯å¦æŒ‰ç…§ columnsçš„å€¼ä¸‹é™æ¥æ’åº D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690&gt;&gt;&gt; df.sort_values(by='B') # æŒ‰ç…§columns B å‡åºæ¥ A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 ç´¢å¼•columnså’Œrows 4. References pythonï¼Œnumpyï¼Œpandasæ•°æ®å¤„ç†ä¹‹å°æŠ€å·§ ####","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.com/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.com/tags/Pandas/"}]}]