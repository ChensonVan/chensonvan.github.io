[{"title":"2015 MacBookPro SSDç¡¬ç›˜æ›´æ¢","date":"2019-11-23T05:09:21.000Z","path":"2019/11/23/2015-MacBookPro-SSDç¡¬ç›˜æ›´æ¢/","text":"1. å‰æœŸå‡†å¤‡ ã€å¿½ç•¥è¢«çŒ«å¼„æˆè¿™æ ·çš„æ¡Œå¸ƒã€‚ã€‚ã€‚ã€‘ ç”µè„‘æ˜¯2015å¹´çš„MacBook Proï¼Œ256çš„ç¡¬ç›˜ã€‚ç”¨äº†å·®ä¸å¤šå››å¹´äº†ï¼Œç”±äºä¸å¤Ÿç”¨ï¼Œè€ƒè™‘è‡ªå·±æ¢å—ç¡¬ç›˜ï¼Œå¯¹æ¯”äº†ä¸‹æ„Ÿè§‰Intel 760Pæ¯”è¾ƒåˆé€‚ã€‚å‡†å¤‡å¦‚ä¸‹ï¼š Intel 760P SSD 512G M.2 NVMEåè®® åŒ11äº¬ä¸œè´­ä¹°çš„ï¼Œ650å·¦å³Intelå®˜æ–¹æ——èˆ°åº—è´­ä¹° nvmeè½¬M.2æ¥å£è½¬æ¢å™¨ é©¬äº‘å®¶ï¼Œä¸åˆ°20å—å§ ç±³å®¶wihaèºä¸åˆ€ é©¬äº‘å®¶ï¼Œ85å— é‡‘æ‰‹æŒ‡é«˜æ¸©èƒ¶å¸¦ é©¬äº‘å®¶ï¼Œä¸‰å—ï¼Œç”¨äºè´´åœ¨ä¸Šé¢è½¬æ¢å™¨ä¸Šçš„ï¼Œé˜²é™ç”µå•¥çš„ã€‚å¦‚æœç¡¬ç›˜æ˜¯é©¬äº‘å®¶ä¹°ï¼Œå¯èƒ½å°±é€ä¸€ç‚¹äº†ï¼Œå› ä¸ºå°±ç”¨äº†2cmå·¦å³ å°ç”µ32G USB3.1ä¼˜ç›˜ äº¬ä¸œæ´»åŠ¨27å—ä¹°çš„ï¼Œç”¨äºåˆ¶ä½œUç›˜å¼•å¯¼ç›˜çš„ Time Machineæ•°æ®å¤‡ä»½ MacOS Mojaveç³»ç»Ÿ éœ€è¦High Sierraä»¥ä¸Šï¼Œä¸ç„¶ä¸æ”¯æŒnvmeåè®®çš„ç¡¬ç›˜ 2. å®‰è£…æ­¥éª¤ Time Machineåšå¥½æ•°æ®å¤‡ä»½ åˆ¶ä½œå¥½Uç›˜å¯åŠ¨ç›˜ æ‹†æœº å…ˆå…³æœº Dé¢ç”¨å°ç±³èºä¸åˆ€P5ï¼Œç„¶åæœ‰ä¸¤é¢—èºä¸æ¯”è¾ƒçŸ­ï¼Œæ‹§å›å»çš„æ—¶å€™éœ€æ³¨æ„ æ‹†å®Œå¦‚ä¸‹å›¾ï¼Œéœ€è¦æ–­å¼€ç”µæ± å’Œä¸»æ¿çš„çº¿ï¼Œç„¶åæ‹†ç¡¬ç›˜ï¼Œæ‹†ç¡¬ç›˜ç”¨T5H ç¡¬ç›˜å¯¹æ¯”ï¼Œæ ‡çº¢å¤„æ˜¯é˜²é™ç”µèƒ¶å¸ƒè´´çš„åœ°æ–¹ æ¥ä¸‹æ¥å°±æ˜¯æ’ä¸Šç¡¬ç›˜ï¼Œæ‹§èºä¸ï¼Œç„¶åå°†ç”µæºå’Œä¸»æ¿çš„çº¿åˆä¸Š åç›–å¯å…ˆä¸æ‹§èºä¸ï¼Œé˜²æ­¢æœ‰é—®é¢˜åˆè¦é‡æ‹† å®‰è£…ç³»ç»Ÿ æ’ä¸ŠUç›˜ï¼Œå¼€æœºæŒ‰Commad + Rï¼Œ è¿›å…¥å·¥å…·ç¨‹å¼ å…ˆè¿›ç£ç›˜å·¥å…·ï¼Œå°†Intelçš„ç¡¬ç›˜æ ¼å¼åŒ–æˆMacçš„æ ¼å¼ é€‰æ‹©å·¦ä¸Šè§’çš„â€œå†…ç½®â€INTELï¼Œç‚¹å‡»â€œç£ç›˜å·¥å…·&gt;æŠ¹æ‰â€ï¼Œåœ¨â€œåç§°â€ä¸­è‡ªå®šä¹‰åç§°ï¼ˆæ­¤åç§°åœ¨ç³»ç»Ÿå¯åŠ¨åå¯éšæ—¶æ›´æ”¹ï¼‰ï¼Œâ€œæ ¼å¼â€é€‰æ‹©â€œMac OS æ‰©å±•ï¼ˆæ—¥å¿—å¼ï¼‰â€ï¼Œæ–¹æ¡ˆé€‰â€œGUIDåˆ†åŒºå›¾â€ï¼Œç‚¹å‡»â€œæŠ¹æ‰â€ï¼Œç„¶åç‚¹â€œå®Œæˆâ€ è¿™æ ·æ–°æ’å…¥çš„SSDå°±å¯ä»¥ç”¨æ¥å®‰è£…Macç³»ç»Ÿäº† å®‰è£…ç³»ç»Ÿ é‡æ–°é€€åˆ°ä¸Šä¸€å±‚ç›®å½•ï¼Œè¿™æ¬¡é€‰å®‰è£…macOSï¼Œç„¶åå°±ä¼šè¯»å–Uç›˜é‡Œé¢çš„ç³»ç»Ÿï¼Œé€‰æ‹©å®‰è£…åˆ°æ–°ç¡¬ç›˜ä¸Šï¼Œç‚¹å‡»ç»§ç»­ã€‚å¤§æ¦‚æ–°ç³»ç»Ÿ20minså·¦å³å®‰è£…å®Œæˆã€‚æ­¤æ—¶å¯ä»¥é€‰æ‹©æ’¸ä¸€ä¸‹çŒ« å®‰è£…å®Œä¹‹åï¼Œæ£€æŸ¥ä¸€ä¸‹ç¡¬ç›˜æ ¼å¼æ˜¯å¦OK çªç„¶å¿˜è®°åœ¨å“ªçœ‹äº†ï¼Œè‡ªå·±æ‰¾æ‰¾å§ ==ï¼ŒNVMEçš„é“¾æ¥å®½å¸¦éœ€è¦ä¸ºx4 ä»Time Machineä¸­æ¢å¤æ•°æ® æ’ä¸Šå¤‡ä»½äº†ç³»ç»Ÿçš„ç§»åŠ¨ç¡¬ç›˜ï¼Œè¿˜æ˜¯Command + Rï¼Œè¿›å…¥ä¹‹å‰çš„å®ç”¨å·¥å…·ï¼Œè¿™æ¬¡ç‚¹å‡»â€œä»æ—¶é—´æœºå™¨å¤‡ä»½è¿›è¡Œæ¢å¤â€ã€‚æ•´ä¸ªæ¢å¤è¿‡ç¨‹æ¯”è¾ƒè€—æ—¶ï¼Œæˆ‘åŸå…ˆæ˜¯256Gï¼Œæ‰€ä»¥ä¼°è®¡3å°æ—¶+å§ï¼Œæˆ‘æ˜¯è®©æœºå™¨è‡ªå·±æ¢å¤å°±å»ç¡è§‰äº†ï¼Œå…·ä½“è€—æ—¶ä¸æ¸…æ¥š æ¢å¤å®Œä¹‹åï¼Œé‡æ–°å®‰è£…macOS å…¶å®ç¬¬4æ­¥ä¼¼ä¹å¯ä»¥è·³è¿‡çš„ï¼Œç›´æ¥ä»Time Machineä¸­æ¢å¤ã€‚ä½†ç”±äºæ¢å¤è¿‡ç¨‹æ¯”è¾ƒä¹…ï¼Œä¸ç¡®å®šç¡¬ç›˜å®‰è£…æ˜¯å¦okï¼Œæˆ‘æ˜¯å…ˆå¿«é€Ÿé‡è£…äº†ä¸€ä¸‹ï¼Œæ£€æŸ¥æ²¡äº‹åæ‰é‡è£…çš„ é‡è£…ä¹‹åæ•°æ®åŸºæœ¬æ˜¯æ¢å¤äº†ï¼Œä½†è¿˜æ˜¯æœ‰äº›å°é—®é¢˜ï¼Œé‡Œé¢å¾ˆå¤šè½¯ä»¶éƒ½æ— æ•ˆäº†ï¼ŒåŒ…æ‹¬ç³»ç»Ÿè‡ªå¸¦çš„Safariã€Previewç­‰ï¼Œè¿˜æœ‰ä¸€äº›è‡ªå·±æŒ‰ç…§çš„è½¯ä»¶ã€‚é‡è¯•äº†æ–°å»ºç”¨æˆ·ã€é‡ç½®SMCå’ŒNVRAMå‡æ— æ•ˆ é€‰å–è‹¹æœèœå• &gt;â€œå…³æœºâ€ã€‚ç­‰ Mac å…³æœºåï¼ŒæŒ‰ä¸‹å†…å»ºé”®ç›˜å·¦ä¾§çš„ Shift-Control-Optionï¼Œç„¶ååŒæ—¶æŒ‰ä¸‹ç”µæºæŒ‰é’®ã€‚æŒ‰ä½è¿™äº›æŒ‰é”®å’Œç”µæºæŒ‰é’® 10 ç§’é’Ÿã€‚å¦‚æœæ‚¨çš„ MacBook Pro å¸¦æœ‰è§¦æ§ IDï¼Œåˆ™è§¦æ§ ID æŒ‰é’®ä¹Ÿæ˜¯ç”µæºæŒ‰é’®ã€‚æ¾å¼€æ‰€æœ‰æŒ‰é”®ã€‚å†æ¬¡æŒ‰ä¸‹ç”µæºæŒ‰é’®ä»¥å¼€å¯ Macã€‚ 2ï¼‰å¦‚ä½•é‡ç½® NVRAM å°† Mac å…³æœºï¼Œç„¶åå¼€æœºå¹¶ç«‹å³åŒæ—¶æŒ‰ä½ä»¥ä¸‹å››ä¸ªæŒ‰é”®ï¼šOptionã€Commandã€P å’Œ Rã€‚æ‚¨å¯ä»¥åœ¨å¤§çº¦ 20 ç§’åæ¾å¼€è¿™äº›æŒ‰é”®ï¼Œåœ¨æ­¤ æœŸé—´æ‚¨çš„ Mac å¯èƒ½çœ‹ä¼¼åœ¨é‡æ–°å¯åŠ¨ã€‚ åœ¨å‘å‡ºå¯åŠ¨å£°çš„ Mac ç”µè„‘ä¸Šï¼Œæ‚¨å¯ä»¥åœ¨ä¸¤æ¬¡å¯åŠ¨å£°ä¹‹åæ¾å¼€è¿™äº›æŒ‰é”®ã€‚åœ¨ iMac Pro ä¸Šï¼Œæ‚¨å¯ä»¥åœ¨ Apple æ ‡å¿—ç¬¬äºŒæ¬¡å‡ºç°å¹¶æ¶ˆå¤±åæ¾å¼€è¿™äº›æŒ‰é”®ã€‚å¦‚æœæ‚¨çš„ Mac ä½¿ç”¨äº†å›ºä»¶å¯†ç ï¼Œè¿™ä¸ªç»„åˆé”®å°†ä¸èµ·ä»»ä½•ä½œç”¨æˆ–å¯¼è‡´æ‚¨çš„ Mac ä» macOS æ¢å¤åŠŸèƒ½å¯åŠ¨ã€‚è¦é‡ç½® NVRAMï¼Œè¯·å…ˆå…³é—­å›ºä»¶å¯†ç ã€‚ åœ¨æ‚¨çš„ Mac å®Œæˆå¯åŠ¨åï¼Œæ‚¨å¯èƒ½éœ€è¦æ‰“å¼€â€œç³»ç»Ÿåå¥½è®¾ç½®â€å¹¶è°ƒæ•´å·²é‡ç½®çš„ä»»ä½•è®¾ç½®ï¼Œä¾‹å¦‚éŸ³é‡ã€æ˜¾ç¤ºå±åˆ†è¾¨ç‡ã€å¯åŠ¨ç£ç›˜é€‰æ‹©æˆ–æ—¶åŒºã€‚ æ‰€ä»¥æ²¡åŠæ³•ï¼Œåˆå…³æœºäº†ï¼ŒCommand + Rè¿›å…¥ä¹‹å‰çš„å®ç”¨å·¥å…·ï¼Œé€‰æ‹©ç¬¬äºŒä¸ªå®‰è£…macOSã€‚ è¿™æ¬¡å®‰è£…æ˜¯ä¸ä¼šåˆ é™¤ç”µè„‘ä¸Šçš„æ•°æ®çš„ï¼Œåªä¼šå®‰è£…ä¸€éç³»ç»Ÿè€Œå·²ï¼Œä¸”ä¹‹å‰çš„é…ç½®ç­‰å…¶ä»–æ•°æ®éƒ½ä¸ä¼šæœ‰å½±å“ã€‚è¿™éƒ¨åˆ†å¤§æ¦‚è€—æ—¶1hå·¦å³å§ï¼Œé‡è£…ä¹‹åç³»ç»Ÿè½¯ä»¶æ¢å¤æ­£å¸¸ï¼Œä½†è‡ªå·±å®‰è£…çš„ç¬¬ä¸‰æ–¹è½¯ä»¶ä¾æ—§æ— æ³•è¿è¡Œï¼Œéœ€è¦é‡æ–°å®‰è£… é‡ç½®SMCå’ŒNVRAM æŒ‰ç…§ä¸Šé¢çš„æ–¹æ³•é‡ç½®SMCå’ŒNVRAMï¼Œè™½ç„¶æˆ‘ä¹Ÿæ²¡å»äº†è§£å¹²å˜›çš„ã€‚ã€‚ã€‚ 3. ä½¿ç”¨æ„Ÿå— æµ‹äº†ä¸‹é€Ÿåº¦ï¼Œå†™çš„å¥½åƒæ²¡æœ‰è¾¾åˆ°åˆ«äººçš„1500+Mb/sï¼Œå¤§æ¦‚800+çš„æ ·å­å§ï¼Œè¯»å–çš„å·®ä¸å¤š1300+Mb/sã€‚å¯èƒ½ä¹°äº†å—å‡ç¡¬ç›˜ï¼Ÿï¼Ÿï¼Ÿä¸è¿‡æ³¢åŠ¨ä¸€ç›´æ¯”è¾ƒå¤§ï¼Œæœ‰æ—¶å€™åˆ1000å¤šï¼Œå¯èƒ½ä¹Ÿæ˜¯æ­£å¸¸å§ ç”¨ç”µä¼¼ä¹æ²¡å•¥æ„Ÿè§‰ï¼Œæœ¬æ¥ç”µæ± å¥åº·å°±74%å·¦å³äº†ï¼Œå¹³å¸¸ä¸€ç›´æ¥ç”µæºç”¨ã€‚ç„¶åå»ºç«‹ç´¢å¼•å•¥çš„ä¹Ÿæ²¡å•¥æ„Ÿè§‰ï¼ŒCPUå•¥çš„ä¹Ÿæ¯”è¾ƒæ­£å¸¸ ç¡çœ æ²¡æœ‰æ­»æœºçš„æƒ…å†µï¼Œç›®å‰ä½¿ç”¨ä¸¤å¤© ä»¥ä¸Šï¼Œç”¨æ—¶å‡ ä¸ªå°æ—¶ï¼ŒèŠ±è´¹750å·¦å³ï¼Œ256å‡çº§åˆ°Intel 512Gï¼ŒOverã€‚ 4. References Macbook Pro15æ¬¾ï¼Œå‡çº§ssd MacBook Pro 2015å‡çº§Intel 760P 1TB NVME SSDå›ºæ€ç¡¬ç›˜ï¼Œ2018.6.20äº²æµ‹æˆåŠŸ https://post.smzdm.com/p/ag89eved/","tags":[]},{"title":"ç±»åˆ«ä¸å¹³è¡¡æ•°æ®å°ç»“","date":"2019-11-09T05:39:56.000Z","path":"2019/11/09/ç±»åˆ«ä¸å¹³è¡¡æ•°æ®å°ç»“/","text":"1. ç›¸å…³ä»‹ç»ä¹‹å‰åœ¨æŸé“¶è¡Œçš„æ‰‹æœºé“¶è¡Œäº¤æ˜“åæ¬ºè¯ˆé¡¹ç›®ä¸­ï¼Œé‡åˆ°äº†æ ·æœ¬æ¯”è¾ƒåçš„é—®é¢˜ã€‚ç„¶åè¿™å‡ å¤©ä¹Ÿæ˜¯åˆšå¥½çœ‹åˆ°ä¸€ç¯‡æ¯”è¾ƒå¥½çš„ï¼Œæœ‰å…³äºæ•°æ®ä¸å¹³è¡¡çš„æ–‡ç« ï¼Œæ‰€ä»¥å°±åšä¸ªç¬”è®°ï¼Œé¡ºä¾¿æ€»ç»“ä¸€ä¸‹ä¹‹å‰é¡¹ç›®ä¸­é‡åˆ°çš„é—®é¢˜ï¼Œä»¥åŠå“ªäº›æ–¹é¢å¯ä»¥æ”¹è¿›çš„ã€‚ å‡è®¾é—®é¢˜éƒ½æ˜¯å…³äºäºŒåˆ†ç±»çš„ï¼Œæ­£ç±»ä¸ºæå°‘æ•°çš„éƒ¨åˆ†ï¼Œè´Ÿç±»ä¸ºæ•°æ®é‡å¤§çš„ä¸€ç±»ã€‚ å¦‚æœæ›´å…³æ³¨æ­£ç±»çš„åˆ†ç±»èƒ½åŠ›ï¼Œåˆ™æ˜¯è¿™é‡Œæƒ³è¦è®¨è®ºçš„ç±»åˆ«ä¸å¹³è¡¡çš„é—®é¢˜ï¼› å¦‚æœæ›´å…³æ³¨è´Ÿç±»çš„åˆ†ç±»èƒ½åŠ›ï¼Œé—®é¢˜åˆ™å˜ä¸ºAnomaly Detectionï¼Œå³å¼‚å¸¸å€¼æ£€æµ‹é—®é¢˜ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ ·æœ¬ä¸å¹³è¡¡å¾€å¾€ä¼šå½±å“æ¨¡å‹çš„å­¦ä¹ æ€§èƒ½ï¼Œå¯èƒ½çš„åŸå› æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ ·æœ¬ä¸å¹³è¡¡å½±å“äº†ç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚å¯¹å°‘æ•°ç±»çš„è¯¯åˆ†ç±»å½±å“ä¸ä¼šå¤ªå¤§ã€‚æ­¤æ—¶å¸¸è§çš„ä¸€äº›è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ä¹Ÿä¼šæœ‰ä¸€å®šçš„è¯¯å¯¼æ€§ï¼Œæ¯”å¦‚ACCï¼ŒKSï¼ŒAUCç­‰ï¼Œå› ä¸ºè¿™äº›æŒ‡æ ‡å¾€å¾€ä¼šæ¯”å…¶ä»–æ­£å¸¸æ¨¡å‹é‡Œé¢çš„å€¼éƒ½ä¼šé«˜ä¸€äº›ï¼ŒAUCå¾€å¾€éƒ½æ˜¯0.9å‡ äº†ã€‚æ­¤æ—¶å¯ä»¥è€ƒè™‘ä¸€ä¸‹å…¶ä»–æŒ‡æ ‡ï¼š å¬å›ç‡Recall \\frac {TP} {TP + FN} å‡†ç¡®ç‡Precisionï¼ˆæœ‰äº›åœºæ™¯ä¸‹ä¹Ÿä¼šè½¬æ¢æˆè¯¯æŠ¥ç‡æ¥çœ‹ï¼‰ \\frac {TP} {TP + FP} F1-Score 2 \\times \\frac {Recall \\times Precision } {Recall + Precision} G-Mean \\sqrt{(Recall \\times Precision)} MCC \\frac {TP \\times TN - FP \\times FN} {\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} 2. è§£å†³æ–¹æ¡ˆå¯¹äºæ­¤ç±»ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œå¯ä»¥ä»æ•°æ®å±‚é¢å’Œç®—æ³•å±‚é¢æ¥ç¼“è§£ã€‚ 2.1 æ•°æ®å±‚é¢æ•°æ®å±‚é¢ä¸Šç®—æ˜¯ä»é—®é¢˜çš„æ ¹æœ¬ä¸Šå‡ºå‘ï¼Œå°±æ˜¯ç¼“è§£æ•°æ®ä¸å¹³è¡¡ï¼Œè®©æ•°æ®ç¨å¾®å¹³è¡¡ä¸€äº›ã€‚ å‡å°‘å¤šæ•°ç±»çš„æ ·æœ¬ æ¬ é‡‡æ ·/ä¸‹é‡‡æ ·ã€RUSã€NearMissã€ENNã€Tomeklinkç­‰ï¼ˆä½†æˆ‘éƒ½æ²¡è¯•è¿‡ == ï¼‰ã€‘ã€å¯èƒ½ä¼šå½±å“åŸå§‹æ ·æœ¬çš„åˆ†å¸ƒã€‘ å¢åŠ å°‘æ•°ç±»çš„æ ·æœ¬ è¿‡é‡‡æ ·/ä¸Šé‡‡æ ·ã€SMOTEã€ADASYNã€Borderline-SMOTEç­‰ï¼ˆåªäº†è§£SMOTEç›¸å…³çš„ ==ï¼‰ã€‘ ç”±äºæ ·æœ¬çš„ç¨€ç–æ€§ï¼Œæ­£ç±»å°‘æ•°æ ·æœ¬å¾ˆéš¾æœ‰ä¸ªæ¸…æ™°çš„ç»“æ„ï¼Œæ‰€ä»¥è¿‡é‡‡æ ·ç”Ÿæˆçš„æ ·æœ¬å¯èƒ½å¸¦æ¥ä¸€éƒ¨åˆ†å™ªéŸ³ï¼ŒåŒæ—¶å¯¼è‡´ç©ºé—´ä¸Šçš„ä¸å¹³æ»‘åˆ†å¸ƒã€‚å¼•ç”¨ä¸€ä¸‹æ–‡ç« é‡Œé¢çš„å¼•ç”¨ï¼š In cases with such a high imbalance the minority class is often poorly represented and lacks a clear structure. Therefore, straightforward application of pre-processing methods that rely on relations between minority objects (like SMOTE) can actually deteriorate the classification performance. Using randomized methods may also be inadvisable due to a high potential variance induced by the imbalance ratio. Methods that will be able to empower the minority class and predict or reconstruct a potential class structure seem to be a promising direction. -Krawczyk, Bartosz. â€œLearning from imbalanced data: open challenges and future directions.â€ Progress in Artificial Intelligence 5.4 (2016): 221-232. ç»“åˆä¸Šè¿°ä¸¤ç§æ–¹æ³•åŒæ—¶é‡‡æ · é‡‡æ ·å¯ä»¥åˆ†æˆéšæœºå’Œééšæœºçš„ï¼Œéšæœºçš„è¯å°±æ˜¯éšæœºä»å¤§ç±»æ ·æœ¬ä¸­æŠ½å–ä¸€å®šæ¯”ä¾‹æ ·æœ¬å†åŠ ä¸Šå°ç±»æ ·æœ¬è®­ç»ƒã€‚ä½†éšæœºçš„è¯å¯èƒ½ä¼šå¸¦æ¥ä¸€äº›å…¶ä»–é—®é¢˜ï¼Œå¦‚è¿‡é‡‡æ ·ä¸­é€‰å–äº†ä¸€äº›å™ªéŸ³æ•°æ®ï¼Œæ¬ é‡‡æ ·æ”¹å˜äº†æ•°æ®çš„åˆ†å¸ƒç­‰ã€‚ é™¤æ­¤ä¹‹è¿˜æœ‰çš„è¯ï¼Œæœ‰äº›åœºæ™¯ä¸‹ä¼šä¸å¤ªä¸€æ ·ã€‚æ¯”å¦‚åŸºäºæŸäº›ç»´åº¦æ¥é‡‡æ ·ï¼Œæ¯”å¦‚æŒ‰å®¢æˆ·ç»´åº¦æ¯”ä¾‹éšæœºæŠ½å–ç­‰ï¼Œæˆ–è€…å¯¹æ¯å¤©æŒ‰ä¸€å®šæ¯”ä¾‹æŠ½å–ç­‰ï¼Œæˆ–è€…æ˜¯å¯ä»¥é€šè¿‡å‰ç½®çš„ä¸€äº›ç®—æ³•ï¼ŒæŒ‰æŸäº›ç»´åº¦æ¥æŠ½å–ã€‚ 2.2 ç®—æ³•å±‚é¢åˆšåˆšæåˆ°äº†ï¼Œç”±äºç±»åˆ«ä¸å¹³è¡¡ï¼Œå¯¹äºç›®æ ‡å‡½æ•°ä¼˜åŒ–å¯èƒ½æœ‰å½±å“ï¼Œå› ä¸ºå¯¹äºå°‘æ•°ç±»è¯¯åˆ†ä¹Ÿæ²¡å¤ªå¤§å½±å“ï¼Œæ‰€ä»¥ä»ç®—æ³•å±‚é¢ä¸Šæœ‰ä»£ä»·æ•æ„Ÿå­¦ä¹ ï¼ˆcost-sensitive learningï¼‰ï¼Œå­—é¢æ„æ€å°±æ˜¯ç»™å°‘æ•°ç±»åˆ†é…è¾ƒé«˜çš„è¯¯åˆ†ç±»ä»£ä»·ã€‚æˆ‘ä¹‹å‰åœ¨é¡¹ç›®ä¸­å°±æ˜¯ä¿®æ”¹äº†æ­£è´Ÿæ ·æœ¬çš„æƒé‡ï¼Œåœ¨é‡‡æ ·ä¹‹åï¼ŒåŒæ—¶ç»™æ­£ç±»è¾ƒé«˜çš„æƒé‡ã€‚ æˆ–è€…ç»“åˆBoostingçš„æ–¹æ³•ï¼Œæ¯”å¦‚åœ¨æ¬ é‡‡æ ·ä¸­ï¼Œå¤šæ•°ç±»å¯ä»¥åˆ†æˆå‡ éƒ¨åˆ†å’Œå°‘æ•°ç±»ä¸€èµ·è®­ç»ƒï¼Œç„¶åå†ç»“åˆæ¨¡å‹æ¥ä½¿ç”¨ã€‚ 2.3 æ•°æ®+ç®—æ³•å±‚é¢å¤§æ¦‚å°±æ˜¯å€Ÿé‰´äº†ä¸Šé¢ä¸¤ç§æ€è·¯ï¼Œæ¯”å¦‚å‘¨å¿—åè€å¸ˆçš„BalanceCascadeï¼Œç»“åˆäº†æ¬ é‡‡æ ·+Ensembleçš„æ–¹æ³•ã€‚ è¯¥æ–¹æ³•æ˜¯ä¸€ç§å…¸å‹çš„undersampling + ensembleæ–¹æ³•ï¼ŒåŸºå­¦ä¹ å™¨ä¸ºAdaBoostç±»å†³ç­–æ ‘ã€‚BCåœ¨æ¯ä¸€æ¬¡è¿­ä»£ä¸­ä»å¤šæ•°ç±»é‡‡æ ·ç©ºé—´ä¸­éšæœºé™é‡‡æ ·å¾—åˆ°ä¸€ä¸ª1:1çš„è®­ç»ƒé›†ï¼Œä»¥ä»è¯¥è®­ç»ƒé›†å¾—åˆ°çš„Adaboostæ¨¡å‹è¾“å‡ºä¸ºä¾æ®ï¼Œæ¯ä¸€æ¬¡è¿­ä»£éƒ½ä»å¤šæ•°ç±»é‡‡æ ·ç©ºé—´ä¸­ä¸¢å¼ƒä¸€éƒ¨åˆ†å·²ç»è¢«å¾ˆå¥½åœ°åˆ†ç±»çš„å¤šæ•°ç±»æ ·æœ¬ã€‚è¯¥å¤šæ¬¡è¿­ä»£è¿‡ç¨‹åœ¨é€æ­¥ç¼©å°é‡‡æ ·ç©ºé—´é™ä½random undersamplingçš„éšæœºæ€§çš„åŒæ—¶ï¼Œåœ¨BCè¿‡ç¨‹åæœŸäº§ç”Ÿçš„å­¦ä¹ å™¨ä¼šæ›´åŠ å…³æ³¨ä¸æ˜“åˆ†çš„æ ·æœ¬ï¼Œå¹¶é€šè¿‡ensembleçš„æ–¹å¼æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªé²æ£’çš„å¼ºåˆ†ç±»å™¨ã€‚ è¿™ç§æ–¹æ³•æœ‰ç‚¹ç±»ä¼¼æˆ‘ä¹‹å‰åœ¨å·¥ç¨‹ä¸­ç”¨åˆ°çš„ï¼Œå°±æ˜¯å…ˆé€šè¿‡é‡‡æ ·è®­ç»ƒä¸€ä¸ªbaselineæ¨¡å‹å‡ºæ¥ï¼Œç„¶åä¸¢å¼ƒä¸€äº›å¯ä»¥éå¸¸å¥½åˆ†ç±»çš„å¤§ç±»ï¼Œå¢åŠ ä¸€äº›åˆ†æ•°éå¸¸é«˜çš„å¤§ç±»ä½œä¸ºé€‰å®šçš„æ ·æœ¬ï¼Œç„¶åé‡å¤å‡ æ¬¡ï¼Œå³å¯é€‰å‡ºä¸€æ‰¹æ¯”è¾ƒç‰¹æ®Šçš„æ ·æœ¬äº†ã€‚ ä¸Šè¿°æ–¹æ³•è¿˜æœ‰ä¸€ä¸ªå¥½å¤„å°±æ˜¯ï¼Œå¯ä»¥å°†æŠ½æ ·å¸¦æ¥çš„æ ·æœ¬åˆ†å¸ƒå½±å“é™ä½ï¼Œé€šè¿‡Ensembleçš„æ–¹æ³•ï¼Œä½†å°±æ˜¯ä¼šå¸¦æ¥äº†è¾ƒé«˜çš„è®¡ç®—é‡ï¼Œå¢åŠ æ¨¡å‹çš„è®­ç»ƒ+è¿­ä»£+ä¼˜åŒ–çš„éš¾åº¦ï¼ŒåŒæ—¶æœ€ç»ˆEnsembleå‡ºæ¥çš„ç»“æœæœ‰é—®é¢˜æ—¶ï¼Œæ¯”è¾ƒéš¾æ’é™¤ã€‚ ä½†è¿™ä¸ªæ–¹æ³•åœ¨æˆ‘æ‰€æ¥è§¦åˆ°çš„é¡¹ç›®é‡Œé¢åˆæ¯”è¾ƒç‰¹æ®Šï¼Œå› ä¸ºè¿™ä¸ªåšæ³•æœ‰ä¸ªå‰æå‡è®¾ï¼Œå°±æ˜¯å¤§ç±»ä¸€å®šæ˜¯å¤§ç±»ï¼Œä½†å®é™…ä¸­å¯èƒ½ä¸å¤ªä¸€æ ·ã€‚æ¯”å¦‚é‡‘èæ¬ºè¯ˆçš„åœºæ™¯ä¸‹ï¼Œè¿˜æ˜¯å­˜åœ¨ä¸€äº›æ¬ºè¯ˆæ ·æœ¬æœªè¢«å‘ç°ï¼Œæ­¤æ—¶æ¨¡å‹ç»™äº†ä¸€ä¸ªè¾ƒé«˜çš„åˆ†æ•°ï¼Œä½†æ˜¯æ ¹æ®ä¸Šè¿°çš„åšæ³•ï¼Œè¿™ç±»æ ·æœ¬æœ€ç»ˆä¼šè¢«é€‰è¿›è®­ç»ƒæ ·æœ¬ï¼Œä¸”å½“åˆæ­£å¸¸æ ·æœ¬æ¥ä½¿ç”¨ï¼Œå¯¹äºæ¨¡å‹å¯èƒ½ä¼šå¸¦æ¥ä¸€äº›å™ªéŸ³å¹²æ‰°ï¼Œä¹Ÿé™ä½äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½†è·Ÿä¸šåŠ¡æ²Ÿé€šä¹‹åï¼Œä¸”æ ¹æ®å®é™…æ¨¡å‹çš„æ•ˆæœï¼Œè¿™ç§åšæ³•è¿˜æ˜¯æœ‰äº›æé«˜çš„ã€‚ ä¸Šè¿°çš„åšæ³•ï¼Œå¤§æ¦‚å°±æ˜¯ä¸ºäº†åœ¨ä¸‹é¢bå›¾ä¸­ï¼Œæ‰¾åˆ°å’Œæ­£ç±»é‡å éƒ¨åˆ†çš„è´Ÿç±»ï¼Œå³åˆ†ç±»éš¾åº¦æ¯”è¾ƒé«˜çš„é‚£äº›æ ·æœ¬ã€‚ å¼•ç”¨ä¸€ä¸‹æœ€è¿‘çœ‹çš„é‚£ç¯‡æ–‡ç« é‡Œé¢æåˆ°çš„é—®é¢˜ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ä¹‹å‰åœ¨é¡¹ç›®ä¸­æ¯”è¾ƒæ‹…å¿ƒçš„é—®é¢˜ï¼š åˆå§‹random sampleå¯¼è‡´çš„è¯¯å·®ç´¯ç§¯ï¼šåœ¨extremely imbalanceçš„æƒ…å†µä¸‹åˆå§‹çš„å‡ è½®1:1é‡‡æ ·é¢ä¸´ç€ç›¸å½“é«˜çš„é™é‡‡æ ·ç‡ï¼Œæ­¤æ—¶é™é‡‡æ ·å¾—åˆ°çš„å¤šæ•°ç±»æ ·æœ¬å­é›†å¹¶ä¸ä¸€å®šèƒ½å¤Ÿè¾ƒå¥½åœ°ååº”æ€»ä½“æ•°æ®çš„çœŸå®åˆ†å¸ƒ(namely, Bad undersampling)ï¼Œä½¿ç”¨è¿™äº›bad undersamplingå¾—åˆ°çš„æ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„å­¦ä¹ å™¨å­¦ä¹ åˆ°çš„è‡ªç„¶ä¹Ÿæ˜¯ä¸€ä¸ªbiasedåˆ†å¸ƒï¼Œè¿™æ„å‘³ç€ä½¿ç”¨è¯¥å­¦ä¹ å™¨è¾“å‡ºæ‰€ä¸¢å¼ƒçš„é‚£ä¸€éƒ¨åˆ†æ ·æœ¬å¯èƒ½å¹¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦ä¸¢å¼ƒçš„â€softâ€æ ·æœ¬(é”™è¯¯åœ°ä¸¢å¼ƒäº†å«æœ‰é‡è¦ä¿¡æ¯çš„æ ·æœ¬)ã€‚åˆç”±äºåœ¨å‰å‡ è½®æˆ‘ä»¬ä¸¢å¼ƒäº†å¤šæ•°ç±»æ ·æœ¬ä¸­ä¸€äº›é‡è¦çš„ç»“æ„ä¿¡æ¯ï¼Œåœ¨åç»­è¿­ä»£ä¸­å¾—åˆ°bad undersamplingçš„æ¦‚ç‡å˜å¾—æ›´é«˜ï¼Œä»è€Œé™·å…¥äº†(bad sampling -&gt; bad base learner -&gt; bad drop -&gt; even worse sampling)çš„æ¶æ€§å¾ªç¯ã€‚ åæœŸä¸€å‘³å…³æ³¨æ ·æœ¬çš„åˆ†ç±»éš¾åº¦å¹¶ä¸å¯å–ï¼šä¸extremely imbalanceéšä¹‹è€Œæ¥çš„é€šå¸¸æœ‰ç±»åˆ«åˆ†å¸ƒåœ¨æ ·æœ¬ç©ºé—´çš„ä¸¥é‡é‡å (overlapping)ã€‚è®¾æƒ³ä¸¤ä¸ªç±»åˆ«å„è‡ªæœä»ä¸€ä¸ªåªæœ‰ä¸­å¿ƒç‚¹ä¸åŒçš„é«˜æ–¯åˆ†å¸ƒGmajä¸Gminï¼Œå°‘æ•°ç±»æ ·æœ¬åœ¨Gminä¸Šé‡‡æ ·100æ¬¡ï¼Œå¤šæ•°ç±»æ ·æœ¬åœ¨Gmajä¸Šé‡‡æ ·1e5æ¬¡ (IR=1000)ã€‚æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°å³ä½¿ä¸¤ä¸ªç±»åˆ«æ‰€å±çš„åˆ†å¸ƒçœ‹èµ·æ¥å¯ä»¥è¢«è½»æ˜“åŒºåˆ†å¼€æ¥ï¼Œä½†åœ¨ä¸¤è€…çš„æ ·æœ¬æ•°é‡å·®è·æ‚¬æ®Šçš„æƒ…å†µä¸‹ï¼Œå³ä¾¿åœ¨å°‘æ•°ç±»åˆ†å¸ƒæœ€å¯†é›†çš„åœ°æ–¹ä¹Ÿå­˜åœ¨ç›¸å½“å¤šçš„å¤šæ•°ç±»æ ·æœ¬(è§ä¸‹å›¾)ã€‚å¦‚æœä»ç„¶ç§‰æŒå•çº¯å…³æ³¨æ ·æœ¬çš„hardnessçš„æ–¹æ³•è®ºï¼Œæœ€åå‡ è½®è¿­ä»£ä¸­è¿™éƒ¨åˆ†overlapping areaä¸­å¤šæ•°ç±»æ ·æœ¬çš„å æ¯”åªä¼šè¶Šæ¥è¶Šå¤šï¼Œå¯æƒ³è€ŒçŸ¥ä½¿ç”¨è¿™äº›æ ·æœ¬è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ä¼šå…·æœ‰ç›¸å½“ç³Ÿç³•çš„æ³›åŒ–èƒ½åŠ›(æ ¹æ®æˆ‘è‡ªå·±çš„å®éªŒç»“æœå‡ ä¹ç­‰äºæ²¡æœ‰æ³›åŒ–èƒ½åŠ› = =)ã€‚ 3. å¯æ”¹è¿›ä¹‹å¤„ æ•°æ®å±‚é¢ é‡‡æ ·æ–¹é¢å…¶å®è¿˜æ˜¯å¯ä»¥åœ¨æ·±å…¥ç ”ç©¶ä¸€ä¸‹ï¼ŒåŒ…æ‹¬ç”¨ä¸Šé¢æåˆ°çš„å‡ ç§ç®—æ³•è¯•ä¸€ä¸‹ï¼› ç”±äºæ ·æœ¬æ˜¯éå¸¸çš„ä¸å¹³è¡¡ï¼Œå…¶å®æ˜¯å¯ä»¥å°è¯•ä¸€ä¸‹è¿‡é‡‡æ ·çš„ã€‚ä½†ç”±äºæˆ‘ä¹‹å‰æ¥è§¦çš„åæ¬ºè¯ˆé¡¹ç›®ï¼Œæ¬ºè¯ˆæ ·æœ¬æ˜¯å’Œåæ¬ºè¯ˆä¸æ–­æŠ—è¡¡çš„ä¸€ä¸ªè¿‡ç¨‹ï¼Œä¸æƒ³å…¶ä»–ä¸å¹³è¡¡é¡¹ç›®ä¸­ï¼Œç›¸å¯¹æ¯”è¾ƒç¨³å®šã€‚æ‰€ä»¥è¿‡é‡‡æ ·å‡ºæ¥çš„æ ·æœ¬è¿˜æ˜¯å¾—double checkä¸€ä¸‹ï¼› æ­£ç±»æ ·æœ¬ä¸­ä¸€äº›è¢«é€‰æ‹©çš„éƒ¨åˆ†ï¼Œä¹Ÿéœ€è¦å†æ£€æŸ¥ä¸€ä¸‹ï¼Œæ¯•ç«Ÿæœ‰ä¸€éƒ¨åˆ†æ˜¯é—æ¼äº†çš„æ¬ºè¯ˆæ ·æœ¬ï¼Œä½†è¿™éƒ¨åˆ†å°±æ¯”è¾ƒè€—æ—¶äº†ã€‚ ç®—æ³•å±‚é¢ æ•´ä¸ªç»“æ„æˆ‘è§‰å¾—å¯ä»¥å†ä¼˜åŒ–ä¸€äº›ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ä¸€ä¸‹ä¸Šé¢å‘¨è€å¸ˆçš„é‚£ä¸ªç®—æ³•ï¼Œæˆ–è€…æ¨¡å‹é‚£ä¸ªç»“æ„ï¼ŒæŠŠæ•´ä¸ªpipelineçš„æ¢³ç†å¥½ï¼Œæ–¹ä¾¿ä»¥åæ¨¡å‹åˆ·æ–°è¿­ä»£ï¼› 4. References æç«¯ç±»åˆ«ä¸å¹³è¡¡æ•°æ®ä¸‹çš„åˆ†ç±»é—®é¢˜S01ï¼šå›°éš¾ä¸æŒ‘æˆ˜ æç«¯ç±»åˆ«ä¸å¹³è¡¡æ•°æ®ä¸‹çš„åˆ†ç±»é—®é¢˜S02ï¼šé—®é¢˜æ¦‚è¿°ï¼Œæ¨¡å‹é€‰æ‹©åŠäººç”Ÿç»éªŒ","tags":[]},{"title":"TensorFlowé«˜çº§APIä¹‹Dataset","date":"2019-08-26T12:04:38.000Z","path":"2019/08/26/TensorFlowé«˜çº§APIä¹‹Dataset/","text":"1. Introductionä¸Šä¸€ç¯‡ä¸»è¦ä»‹ç»çš„æ˜¯tfçš„Estimatorï¼Œå±äºæ¨¡å‹éƒ¨åˆ†çš„ã€‚è¿™é‡Œä¸»è¦è®²çš„æ˜¯Datasetç›¸å…³çš„å†…å®¹ï¼Œä¸»è¦æ˜¯åŠ è½½æ•°æ®ï¼Œä¹Ÿä¼šè®²åˆ°ä¸€äº›ç‰¹å¾å·¥ç¨‹ç›¸å…³çš„ã€‚ ä¸ºä»€ä¹ˆä½¿ç”¨Datasetå‘¢ï¼Œä¸»è¦è€ƒè™‘åˆ°ä»¥ä¸‹å‡ ç‚¹ ä¹‹å‰éƒ½æ˜¯placeholder+feed-dictï¼Œä½çº§APIã€‚æŸç¯‡åšä¸»é‡Œé¢è¯´feed-dictæ˜¯æœ€æ…¢çš„æ–¹å¼ã€å­˜ç–‘ï¼Œå¾…æŸ¥è¯ã€‘ã€‚é€‚ç”¨äºä»pythonå°†æ•°æ®å–‚ç»™åç«¯ã€‚ Datasetæä¾›äº†pipelineçš„æ“ä½œæ–¹å¼ï¼Œæ–¹ä¾¿ï¼Œå¤ç”¨æ€§å’Œæ€§èƒ½ä¹Ÿæ¯”è¾ƒé«˜ï¼Œä¹Ÿæ¯”è¾ƒé€‚åˆå¤§æ‰¹é‡ï¼Œåˆ†å¸ƒå¼çš„æƒ…å†µã€åŸå› ã€‘ã€‚ 2. åŠ è½½æ•°æ®2.1 ä»numpayåŠ è½½123456789# å¼€å¯# ä¸”éœ€è¦ä»¥å¼€å§‹å°±å¼€å¯ï¼Œå¦åˆ™ä¼šæŠ¥å¦‚ä¸‹é”™è¯¯# ValueError: tf.enable_eager_execution must be called at program startup.tf.enable_eager_execution()dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))iterator = dataset.make_one_shot_iterator()for i in iterator: print(i) å¦‚æœæœªå¼€å¯çš„è¯ï¼Œå°±éœ€è¦æ–°å»ºtf.sessionæ¥æŸ¥çœ‹ä¸Šé¢æ•°æ®çš„ç»“æœäº†ï¼Œå¦åˆ™ä¼šæŠ¥å¦‚ä¸‹é”™ å› ä¸ºtfé‡‡ç”¨çš„æ˜¯ç¬¦å·å¼ç¼–ç¨‹(symbolic style programs)æ¨¡å¼ï¼Œè€Œä¸æ˜¯å¸¸è§çš„å‘½ä»¤å¼ç¼–ç¨‹(imperative style programs)æ¨¡å¼ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªSessionæ‰å¯ä»¥è¿è¡Œç¨‹åº ç¬¦å·å¼ç¼–ç¨‹ï¼š å‘½ä»¤å¼ç¼–ç¨‹ åŒæ—¶ä¹Ÿå¯ä»¥å°†np.arrayä»¥å…ƒç»„çš„å½¢å¼è¾“å…¥è¿›å»ï¼Œæ¯”å¦‚(feaures, labels) é—®é¢˜ï¼š èƒ½å¦ç›´æ¥ä»pd.DataFrameä¸­ç›´æ¥åŠ è½½ 2.2 ä»tf.tensors/tf.placeholderä¸­åŠ è½½123456789# ä»tensorç›´æ¥åˆ›å»ºdataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))# ä»placeholderåŠ è½½# å¦‚æœå¼€å¯äº†eagerâ€”â€”executionæ¨¡å¼çš„è¯ï¼Œä¼šæŠ¥å¦‚ä¸‹é”™è¯¯# RuntimeError: tf.placeholder() is not compatible with eager execution.# é—®é¢˜ï¼šå¦‚ä½•æŠŠæ•°å–‚è¿›å» feed_dictä¹ˆï¼Ÿx = tf.placeholder(tf.float32, shape=[None,2])dataset = tf.data.Dataset.from_tensor_slices(x) 2.3 ä»gendratorä¸­åŠ è½½2.4 åŠ è½½csvæ–‡ä»¶12345CSV_PATH = 'tf_dataset_test_data.csv'dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)for i in dataset.take(5): print(i) ä¸Šé¢æ˜¯æœ€ç®€å•è¯»å–csvçš„æ–¹å¼ï¼Œä¹Ÿæœ‰å¤„ç†å¤æ‚ä¸€ç‚¹çš„æƒ…å†µ å¯ä»¥å…ˆå°†csvæ–‡ä»¶ä»¥textçš„å½¢å¼è¯»è¿›æ¥ï¼Œåœ¨è¯»æˆcsvæ–‡ä»¶ 123456# å¯ä»¥ä¼ ä¸€ä¸ªæ–‡ä»¶çš„pathï¼Œä¹Ÿå¯ä»¥æ˜¯æ–‡ä»¶pathçš„list# ä¸”ä¸èƒ½å¸¦header# ä¸èƒ½å¤„ç†mixted typesdataset = tf.data.TextLineDataset(CSV_PATH2)for i in dataset.take(2): tf.decode_csv(i, record_defaults=[[_] for _ in df.select_dtypes(include='float').iloc[0].fillna(-999).values], field_delim=',') è¯»å–çš„æ—¶å€™é‡åˆ°äº†äº›å‘ï¼Œä¸€ç›´æŠ¥é”™ï¼Œç„¶åçœ‹äº†å®˜æ–¹æ–‡æ¡£-decode_csvé‡Œé¢å‚æ•°ä»‹ç»ï¼Œä¸€ä¸ªrecoderé‡Œé¢ï¼Œæ‰€æœ‰æ ¼å¼å¿…é¡»ç›¸åŒï¼Œæ‰€ä»¥ä¸Šé¢æ··ç€object/float/intçš„å°±ä¸è¡Œäº† Args: records: A Tensor of type string. ==Each string is a record/row in the csv and all records should have the same format.== record_defaults: A list of Tensor objects with specific types. Acceptable types are float32, float64, int32, int64, string. One tensor per column of the input record, with either a scalar default value for that column or an empty vector if the column is required. field_delim: An optional string. Defaults to &quot;,&quot;. char delimiter to separate fields in a record. use_quote_delim: An optional bool. Defaults to True. If false, treats double quotation marks as regular characters inside of the string fields (ignoring RFC 4180, Section 2, Bullet 5). name: A name for the operation (optional). na_value: Additional string to recognize as NA/NaN. select_cols: Optional sorted list of column indices to select. If specified, only this subset of columns will be parsed and returned. æŠŠä¸åŒçš„ç±»å‹çš„å…ˆå»äº†å°±å¯ä»¥äº†ï¼Œä¹‹åå†æ‰¾è§£å†³æ–¹æ¡ˆå§ã€‚ tf.data.experimental.make_csv_datasetå¯ä»¥è¯»æ··åˆç±»å‹çš„csvï¼Œå¯å‚è€ƒå®˜æ–¹æ–‡æ¡£ ç„¶åå¯ä»¥æŠŠè¿™ä¸ªä»£ç å°è£…èµ·æ¥ï¼Œæ–¹ä¾¿ä¹‹åè®­ç»ƒæ¨¡å‹ä½¿ç”¨ã€‚ 12345678910111213141516171819202122232425262728293031323334353637# åˆ©ç”¨datasetè§£æCSVæ–‡ä»¶_CSV_COLUMNS = ['field1', 'field2', 'field3', 'field4']# è§£æçš„æ‘¸ç‰ˆï¼Œå¿…é¡»æ˜¯äºŒç»´æ•°ç»„ï¼Œå¦åˆ™æŠ¥é”™_CSV_COLUMN_DEFAULTS = [[''], [''], [0.0], [0.0]]def input_fn(data_file, shuffle=False, batch_size=64, label_col=None, field_delim='\\t'): \"\"\" :param data_file: :param shuffle: :param batch_size: \"\"\" def parse_csv(value, label_col=label_col, field_delim=field_delim): columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS, field_delim=',') features = dict(zip(_CSV_COLUMNS, columns)) if label_col: labels = features.pop(label_col) return features, tf.equal(labels, 1.0) else: return features # Extract lines from input files using the Dataset API. dataset = tf.data.TextLineDataset(data_file) if shuffle: dataset = dataset.shuffle(buffer_size=100000) dataset = dataset.map(parse_csv, num_parallel_calls=100) # We call repeat after shuffling, rather than before, to prevent separate # epochs from blending together. # dataset = dataset.repeat() dataset = dataset.batch(batch_size) return datasetdataset = input_fn(CSV_PATH2, field_delim=',')for i in dataset.take(2): print(i) é‚£ä¹ˆå›é¡¾ä¸€ä¸‹ä¹‹å‰é‡åˆ°æ··åˆç±»å‹æ•°æ®çš„csvï¼Œå¦‚æœå†å¤æ‚ä¸€ç‚¹çš„æƒ…å†µå‘¢ï¼Œæ¯”å¦‚é‡Œé¢æŸä¸€åˆ—æ˜¯ä¸ªjsonæ•°æ®ï¼Œé‚£ä¹ˆ 3. è¯»å–æ•°æ®ä¹‹å‰æ˜¯åŠ è½½æ•°æ®ï¼Œå½“ç„¶ä¹Ÿæœ‰è¯»å–çœ‹äº†ä¸€ä¸‹é‡Œé¢çš„æ•°æ®ï¼Œè¿™åªæ˜¯è¯»å–æ•°æ®çš„å…¶ä¸­ä¸€ç§æ–¹æ³•ã€‚ 3.1 One-shotone-shotå¾ˆç®€å•ï¼Œå°±è·Ÿä¹‹å‰ä½¿ç”¨çš„ä¸€æ · 12345678910x = np.random.sample((100,2))# make a dataset from a numpy arraydataset = tf.data.Dataset.from_tensor_slices(x)# create the iteratoriter = dataset.make_one_shot_iterator()# å’Œæ­£å¸¸çš„è¿­ä»£å™¨ä¸€æ ·ï¼Œå¯ä»¥ç”¨get_next()è·å–ä¸‹ä¸€ä¸ª# create the iteratoriter = dataset.make_one_shot_iterator()el = iter.get_next() 3.2 Initializableä¹‹å‰æœ‰æåˆ°ï¼ŒåŠ è½½æ•°æ®çš„æ—¶å€™ä¹Ÿå¯ä»¥ä»placeholderä¸­åŠ è½½ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰å®é™…å–‚å…¥æ•°æ®ï¼Œè¿™è¾¹è·Ÿä¹‹å‰ç”¨feed_dictå·®ä¸å¤šã€‚ä½†æ˜¯è¿™é‡Œä¹Ÿå’Œä¸Šé¢ä¸€æ ·ï¼Œplaceholderä¸æ”¯æŒeageræ¨¡å¼ 12345678910# using a placeholderx = tf.placeholder(tf.float32, shape=[None,2])dataset = tf.data.Dataset.from_tensor_slices(x)data = np.random.sample((100,2))iter = dataset.make_initializable_iterator() # create the iteratorel = iter.get_next()with tf.Session() as sess: # feed the placeholder with data sess.run(iter.initializer, feed_dict=&#123; x: data &#125;) print(sess.run(el)) # output [ 0.52374458 0.71968478] 3.3 Reinitializable3.4 Feedable4. ä½¿ç”¨æ•°æ®123456789101112131415# using two numpy arraysfeatures, labels = (np.array([np.random.sample((100,2))]), np.array([np.random.sample((100,1))]))dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()x, y = iter.get_next()# make a simple modelnet = tf.layers.dense(x, 8) # pass the first value from iter.get_next() as inputnet = tf.layers.dense(net, 8)prediction = tf.layers.dense(net, 1)loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as labeltrain_op = tf.train.AdamOptimizer().minimize(loss) 5. ç‰¹å¾å·¥ç¨‹5 References How to use Dataset in TensorFlow tf.io.decode_csv ç”¨ tf.data åŠ è½½ CSV æ•°æ®","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://chenson.cc/tags/TensorFlow/"}]},{"title":"TensorFlowåˆ†å¸ƒå¼è®­ç»ƒåˆæ¢","date":"2019-08-25T09:21:49.000Z","path":"2019/08/25/TensorFlowåˆ†å¸ƒå¼è®­ç»ƒåˆæ¢/","text":"ç”±äºå·¥ä½œä¸­å¯èƒ½ä¼šç”¨åˆ°ç®€å•çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸”ç”±äºæ•°æ®é‡å·¨å¤§ï¼Œå¯èƒ½éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒã€‚è™½ç„¶ç”¨çš„å¹³å°å¯èƒ½æœ‰ç°æˆçš„æ¡†æ¶ï¼Œä½†è¿˜æ˜¯äº†è§£ä¸€ä¸‹ã€‚ 1. ç›¸å…³ä»‹ç»1.1 æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ 1.2 åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ æ¨¡å‹å¹¶è¡Œï¼ˆin-graph-replicationï¼‰ æ¨¡å‹å¾ˆå¤§ï¼Œæ— æ³•åœ¨ä¸€ä¸ªè®¾å¤‡ä¸Šè·‘å®Œï¼Œéœ€è¦åˆ†å¼€è·‘ã€‚æ¯”å¦‚ä¸€å°æœºå™¨è·‘encoderï¼Œä¸€å°æœºå™¨è·‘decoderç­‰ æ•°æ®å¹¶è¡Œï¼ˆbetween-graph-replicationï¼‰ è®­ç»ƒæ•°æ®åˆ†å‰²æˆå°å—ï¼Œåœ¨å¤šä¸ªä¸åŒçš„è®¾å¤‡ä¸Šï¼Œæ¯ä¸ªè®¾å¤‡ä¸Šæœ‰å®Œæ•´çš„æ¨¡å‹ï¼Œäº’ä¸å½±å“ é—®é¢˜ï¼šå¦‚ä½•ä¿è¯ä¸åŒè®¾å¤‡ä¸Šçš„æ¨¡å‹å‚æ•°æ˜¯ä¸€è‡´çš„ï¼Ÿ åŒæ­¥ï¼ˆsynchronousï¼‰ æ•°æ®ä¹Ÿæ˜¯åƒä¹‹å‰åˆ†å¼€åœ¨å¤šä¸ªè®¾å¤‡ä¸Šï¼Œæ¯ä¸ªè®¾å¤‡æ›´æ–°éœ€è¦ç­‰è¯¥è½®æ‰€æœ‰è®¾å¤‡å®Œæˆè®­ç»ƒ-è¿­ä»£åï¼Œæ”¶é›†æ‰€æœ‰çš„å‚æ•°ï¼Œç»Ÿä¸€æ›´æ–°ã€‚ å¼‚æ­¥ï¼ˆasynchronousï¼‰ æ•°æ®ä¹Ÿæ˜¯åƒä¹‹å‰åˆ†å¼€åœ¨å¤šä¸ªè®¾å¤‡ä¸Šï¼Œå…¶ä¸­ä¸€ä¸ªè®¾å¤‡å®Œæˆä¸€è½®è¿­ä»£ï¼Œä¸éœ€è¦ç­‰å¾…å…¶ä»–èŠ‚ç‚¹ï¼Œç›´æ¥å»æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚ä¸åŒè®¾å¤‡è¯»å–å‚æ•°å–å€¼çš„æ—¶é—´ä¸ä¸€æ ·ï¼Œæ‰€ä»¥å¾—åˆ°çš„å€¼ä¹Ÿæœ‰å¯èƒ½ä¸ä¸€æ ·ã€‚ é—®é¢˜ - æ¢¯åº¦å¤±æ•ˆï¼ˆstale gradientsï¼‰ åˆšå¼€å§‹æ‰€æœ‰è®¾å¤‡é‡‡ç”¨ç›¸åŒçš„å‚æ•°æ¥è®­ç»ƒï¼Œä½†æ˜¯å¼‚æ­¥æƒ…å†µä¸‹ï¼ŒæŸä¸ªè®¾å¤‡å®Œæˆä¸€è½®è®­ç»ƒåï¼Œå¯èƒ½å‘ç°æ¨¡å‹å‚æ•°å·²ç»è¢«å…¶å®ƒè®¾å¤‡æ›´æ–°è¿‡äº†ï¼Œæ­¤æ—¶è¿™ä¸ªè®¾å¤‡è®¡ç®—å‡ºçš„æ¢¯åº¦å°±è¿‡æœŸäº†ï¼ˆæˆ‘ç†è§£çš„è¿‡æœŸæ˜¯æŒ‡å½“å‰åå‘ä¼ æ’­è®¡ç®—å‡ºæ¥çš„æœ€ä¼˜å€¼å·²ç»ä¸é€‚åº”æœ€æ–°çš„å‚æ•°äº†ï¼‰ã€‚ç”±äºæ¢¯åº¦å¤±æ•ˆé—®é¢˜ï¼Œå¼‚æ­¥è®­ç»ƒå¯èƒ½é™·å…¥æ¬¡ä¼˜è§£ 1.3 åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„ Parameter Serveræ¶æ„ Ring AllReduceæ¶æ„ 1.4 RPC - Remot Precedure Call ä¸HTTPçš„åŒºåˆ« æ¶ˆæ¯é˜Ÿåˆ— 2. TensorFlowçš„é«˜é˜¶API åœ¨äº†è§£çš„è¿‡ç¨‹ï¼Œçœ‹åˆ°å¾ˆå¤šæ–‡ç« éƒ½åœ¨ä»‹ç»tf.train.ClusterSpec å’Œ tf.train.Server è¿™ç±»APIå®ç°çš„ï¼Œä¸”éƒ¨ç½²çš„éœ€è¦ä¿®æ”¹çš„åœ°æ–¹æ¯”è¾ƒå¤šã€‚ç„¶åçœ‹åˆ°åœ¨1.3ä¹‹åï¼Œå®˜æ–¹ç»™å‡ºäº†é«˜çº§APIï¼šEstimatorï¼Œå±…ç„¶éƒ¨ç½²çš„æ—¶å€™å•æœºå’Œåˆ†å¸ƒå¼çš„ä»£ç åŸºæœ¬ä¸€è‡´ï¼Œå·¥ç¨‹åŒ–æ¯”è¾ƒæ–¹ä¾¿ã€‚ä¸”ç»™äººçš„æ„Ÿè§‰å¾ˆåƒsklearné‡Œé¢çš„æ¥å£ï¼Œå®é™…ä¸Šå¥½åƒä¹Ÿæ˜¯å—é‚£ä¸ªå¯å‘ï¼Œç»Ÿä¸€äº†æ¥å£ã€‚ ç„¶ååˆæœ‰äººè¯´2.0ä¹‹åå®˜æ–¹å¼€å§‹æ¨ tf.keras äº†ï¼Œç”±äºtfä¸æ˜¯å¾ˆäº†è§£ï¼Œå…ˆçœ‹çœ‹è¿™äº›å§ã€‚ å¯¹äº tf 1.xæ¥è¯´ï¼Œå®˜æ–¹æ¨èçš„æ˜¯ tf.data.Dataset + tf.estimator.Estimator å¯¹äº tf 2.x æ¥è¯´ï¼Œå®˜æ–¹æ¨èçš„æ˜¯ tf.data.Dataset + tf.keras å¦‚ä¸Šå›¾ï¼Œé«˜é˜¶çš„APIæœ‰Estimatorï¼ŒDatasetï¼Œç­‰ 2.1 Dataset2.2 Experment2.3 Estimator ä¸»è¦çš„æ–¹æ³•æœ‰train, predictå’Œevaluate æŒ‰ç…§å®˜æ–¹çš„æ•™ç¨‹ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š 2.3.1 ç¼–å†™å‡½æ•°æ¨¡å‹12345def my_model_fn( features, # This is batch_features from input_fn labels, # This is batch_labels from input_fn mode, # An instance of tf.estimator.ModeKeys params): # Additional configuration å‰ä¸¤ä¸ªå‚æ•°æ˜¯ä»è¾“å…¥å‡½æ•°ä¸­è¿”å›çš„ç‰¹å¾å’Œæ ‡ç­¾ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œfeatures å’Œ labels æ˜¯æ¨¡å‹å°†ä½¿ç”¨çš„æ•°æ®çš„å¥æŸ„ã€‚mode å‚æ•°è¡¨ç¤ºè°ƒç”¨ç¨‹åºæ˜¯trainã€predictè¿˜æ˜¯evaluateã€‚ è°ƒç”¨ç¨‹åºå¯ä»¥å°† params ä¼ é€’ç»™ Estimator çš„æ„é€ å‡½æ•°ã€‚ä¼ é€’ç»™æ„é€ å‡½æ•°çš„æ‰€æœ‰ params è½¬è€Œåˆä¼ é€’ç»™ model_fnã€‚åœ¨ custom_estimator.py ä¸­ï¼Œä»¥ä¸‹è¡Œå°†åˆ›å»º Estimator å¹¶è®¾ç½®å‚æ•°æ¥é…ç½®æ¨¡å‹ã€‚æ­¤é…ç½®æ­¥éª¤ä¸æˆ‘ä»¬é…ç½® tf.estimator.DNNClassifierï¼ˆåœ¨é¢„åˆ›å»ºçš„ Estimator ä¸­ï¼‰çš„æ–¹å¼ç›¸ä¼¼ã€‚ 12345678910111213141516171819202122232425# åˆ›å»ºç‰¹å¾åˆ—# Feature columns describe how to use the input.# train_x è¿™é‡Œæ˜¯my_feature_columns = []for key in train_x.keys(): my_feature_columns.append(tf.feature_column.numeric_column(key=key))classifier = tf.estimator.Estimator( model_fn=my_model, params=&#123; 'feature_columns': my_feature_columns, # Two hidden layers of 10 nodes each. 'hidden_units': [10, 10], # The model must choose between 3 classes. 'n_classes': 3, &#125;)# è®­ç»ƒclassifier.train(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500))# é¢„æµ‹classifier.predict(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500))# è¯„ä¼°classifier.evaluate(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500)) çœ‹åˆ°å¾ˆå¤šåœ°æ–¹éƒ½æåˆ°äº†first-class functionsï¼Œäº†è§£ä¸€ä¸‹ï¼Œå³å‡½æ•°å¯ä½œä¸ºå‚æ•°æ¥ç”¨ã€‚ In computer science, a programming language is said to have first-class functions if it treats functions) as first-class citizens. This means the language supports passing functions as arguments to other functions, returning them as the values from other functions, and assigning them to variables or storing them in data structures.[1] Some programming language theorists require support for anonymous functions (function literals) as well.[2] In languages with first-class functions, the names) of functions do not have any special status; they are treated like ordinary variables) with a function type.[3] The term was coined by Christopher Stracheyin the context of â€œfunctions as first-class citizensâ€ in the mid-1960s.[4] First-class functions are a necessity for the functional programming style, in which the use of higher-order functions is a standard practice. A simple example of a higher-ordered function is the map function, which takes, as its arguments, a function and a list, and returns the list formed by applying the function to each member of the list. For a language to support map, it must support passing a function as an argument. 2.3.2 å®šä¹‰æ¨¡å‹å…¶å®è¿™é‡Œå°±è·Ÿä¹‹å‰ç”¨ä½çº§çš„APIå®šä¹‰æ¨¡å‹å·®ä¸å¤šï¼Œå·®ä¸å¤šæ˜¯æŠŠæ•´ä¸ªæ¨¡å‹çš„ç»“æ„å®šä¸‹æ¥ï¼Œè¿™æ˜¯ä¸å†éœ€è¦æ‰‹åŠ¨å»åšè¿ç®—ï¼Œç„¶åæ±‚å¯¼ï¼Œåå‘ä¼ æ’­å’Œæ›´æ–°å‚æ•°ä¹‹ç±»çš„ã€‚ 12345678910111213141516171819# 1. å®šä¹‰è¾“å…¥å±‚# Use `input_layer` to apply the feature columns.net = tf.feature_column.input_layer(features, params['feature_columns'])# 2. å®šä¹‰éšè—å±‚# Build the hidden layers, sized according to the 'hidden_units' param.# units å‚æ•°ä¼šå®šä¹‰æŒ‡å®šå±‚ä¸­è¾“å‡ºç¥ç»å…ƒçš„æ•°é‡ã€‚# activation å‚æ•°ä¼šå®šä¹‰æ¿€æ´»å‡½æ•° - åœ¨è¿™ç§æƒ…å†µä¸‹ä¸º Reluã€‚# è¿™é‡Œçš„å˜é‡ net è¡¨ç¤ºç½‘ç»œçš„å½“å‰é¡¶å±‚ã€‚åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œnet è¡¨ç¤ºè¾“å…¥å±‚ã€‚åœ¨æ¯æ¬¡å¾ªç¯è¿­ä»£æ—¶ï¼Œtf.layers.dense éƒ½ä½¿ç”¨å˜é‡ net åˆ›å»ºä¸€ä¸ªæ–°å±‚ï¼Œè¯¥å±‚å°†å‰ä¸€å±‚çš„è¾“å‡ºä½œä¸ºå…¶è¾“å…¥ã€‚# tf.layers.dense æä¾›å¾ˆå¤šå…¶ä»–åŠŸèƒ½ï¼ŒåŒ…æ‹¬è®¾ç½®å¤šç§æ­£åˆ™åŒ–å‚æ•°çš„åŠŸèƒ½ã€‚ä¸è¿‡ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªæ¥å—å…¶ä»–å‚æ•°çš„é»˜è®¤å€¼ã€‚for units in params['hidden_units']: net = tf.layers.dense(net, units=units, activation=tf.nn.relu) # 3. å®šä¹‰è¾“å‡ºå±‚(ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°)# Compute logits (1 per class).logits = tf.layers.dense(net, params['n_classes'], activation=None)# Compute predictions.predicted_classes = tf.argmax(logits, 1) 2.3.4 åˆ†å¸ƒå¼å‚æ•°è®¾ç½®2.3.5 ç›¸å…³é—®é¢˜ estimator å’Œ sessionçš„å…³ç³»ï¼Ÿåªæ˜¯å°è£…äº†sessionä¹ˆï¼Ÿ datasetå’Œfeed_dictçš„åŒºåˆ« 1.3ä¹‹å‰ï¼Œä½¿ç”¨placeholder + feed_dictè¯»å†…å­˜ä¸­çš„æ•°æ® 1.3ä¹‹å‰ï¼Œä½¿ç”¨æ–‡ä»¶åé˜Ÿåˆ—ï¼ˆstring_input_producerï¼‰ä¸å†…å­˜é˜Ÿåˆ—ï¼ˆreaderï¼‰è¯»ç¡¬ç›˜ä¸­çš„æ•°æ® tf.slim / tf.kerasçš„åŒºåˆ« keraså’Œtf.kerasçš„åŒºåˆ« 3. References åˆ›å»ºè‡ªå®šä¹‰ Estimator ä½¿ç”¨ Estimator æ„å»ºçº¿æ€§æ¨¡å‹ Higher-Level APIs in TensorFlow Denoising Autoencoder as TensorFlow estimator å¦‚ä½•ç”¨tensorflowå®ç°åˆ†å¸ƒå¼è®­ç»ƒ æ„å»ºåˆ†å¸ƒå¼Tensorflowæ¨¡å‹ç³»åˆ—:Estimator","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://chenson.cc/tags/TensorFlow/"},{"name":"åˆ†å¸ƒå¼","slug":"åˆ†å¸ƒå¼","permalink":"http://chenson.cc/tags/åˆ†å¸ƒå¼/"}]},{"title":"HJJåæ¬ºè¯ˆåˆ†äº«ç¬”è®°","date":"2019-08-07T15:16:57.000Z","path":"2019/08/07/HJJåæ¬ºè¯ˆåˆ†äº«ç¬”è®°/","text":"1. IntroductionçŸ¥ä¹ç½‘çº¢é»„å§å§æ¥æˆ‘å¸çš„åˆ†äº«ä¼šï¼ŒæŒºæ—©å°±æœ‰å…³æ³¨å¥¹çš„ï¼Œè¿˜æŒºæƒ³å¬ä¸€å¬çš„ã€‚ä½†ç”±äºäººåœ¨å¤–åœ°å‡ºå·®ï¼Œå°±å°†åŒäº‹åšçš„noteæ•´ç†ä¸€ä¸‹ï¼Œä¹‹åå¯èƒ½ç”¨çš„åˆ°ã€‚ 2. åæ¬ºè¯ˆç‰¹å¾è¡ç”Ÿ LBSï¼Œipç½‘æ ¼åˆ’åˆ†ï¼ˆè¿çº¦ç‡åœ°å›¾ï¼‰ ç´§æ€¥è”ç³»äººé€šè¯è®°å½•è¡ç”Ÿï¼Œè®¢å•è¡ç”Ÿ æ‹’ç»çš„äººç‰¹å¾æŒ–æ˜ï¼ˆæ‹’ç»çš„äººã€é»‘æ ·æœ¬ã€æ­£å¸¸è¿˜æ¬¾ï¼Œä¸‰è€…å¯¹æ¯”ï¼‰ APPList èµŒåšç±»appï¼Œå¥—ç°ç±»appï¼Œæé¢ç±»appï¼Œè´·æ¬¾ç±»ï¼Œå½©ç¥¨ç±»ï¼Œappç»„åˆï¼ˆeg:1ä¸ªèµŒåš+3ä¸ªå¥—ç°ï¼‰ï¼Œä½œä¸šç±»appï¼Œå­¦ä¹ ç±»ï¼Œç½‘èµšç±»ï¼ˆé‡‘ç‰Œè¯•å®¢ï¼‰ appåˆ†ç±»æ˜¯æ›´è¯¦ç»†ï¼Œä¸ªæ¨æä¾›æ¯”è¾ƒæ›´è¯¦ç»†çš„appåˆ†ç±»ï¼Œæ¯”å¦‚æ£€æµ‹å‡ºæ¸…æœºè½¯ä»¶ã€å®šä½è½¯ä»¶ã€æ¨¡æ‹Ÿè½¯ä»¶ã€é€šè®¯å½•åŒæ­¥åŠ©æ‰‹ åŸ‹ç‚¹æ•°æ®ipçš„å˜åŠ¨ï¼ŒäºŒæ¬¡ç™»é™†ï¼ŒåŠ¨æ”¯æ—¶é—´å·®ï¼ˆæ²‰é»˜ç”¨æˆ·ç”¨æ¬¾ï¼‰ï¼Œç‚¹å‡»è¡Œä¸ºåºåˆ—ï¼ˆæ¬¡æ•°ï¼Œå æ¯”ï¼Œåºåˆ—ï¼‰ çŸ­ä¿¡ç‰¹å¾è¡ç”Ÿï¼šâ€œç½‘è´·ä¸­ä»‹â€ï¼Œâ€œå°è´·â€ï¼Œâ€œèµŒâ€ï¼Œä¸è‰¯ä¿¡ç”¨å¡è¿˜æ¬¾ä¿¡æ¯ï¼Œå‚¬æ”¶ä¿¡æ¯ï¼Œæ³¨å†Œç½‘è´·ï¼Œâ€œè€å¸ˆâ€ï¼Œâ€œä½œä¸šâ€ï¼Œâ€œå­¦æ ¡â€ï¼Œâ€œä½œä¸šæ²¡å†™å®Œâ€ é€šè®¯å½•ä¸è¿è¥å•†æ•°æ®å¯¹æ¯”ã€è¯¦å•å’Œé€šè®¯å½•äº¤å‰å æ¯” ç”µè¯é‚¦æ•°æ®ç­›é€‰é€šè®¯å½•ä¿¡æ¯ï¼ˆå¿«é€’ç±»ï¼Œå¤–å–ç±»ï¼Œä¸­ä»‹ç±»ç­‰ï¼‰ 3. åæ¬ºè¯ˆæ–¹æ³• é£é™©ç‚¹ï¼ˆé±¼éª¨å›¾ï¼‰(æ³¨å†Œ â€” ç”³è¯· â€” ä¸‹å• â€” è¿˜æ¬¾) SNSç¤¾äº¤ç½‘ç»œæ¨¡å‹ èšç±»æ¨¡å‹ å¼‚å¸¸ç‚¹æ£€æµ‹ï¼ˆå­¤ç«‹æ£®æ—ï¼Œauto_encoderï¼‰ 4. æ¸ é“åˆ†æ xè½´ï¼šç”³è¯·å®Œæˆç‡ï¼Œyè½´ï¼šåŠ¨æ”¯é‡‘é¢ï¼Œzè½´ï¼šæˆä¿¡é‡‘é¢ åˆ’åˆ† èšç±»åˆ†æ 5. å…¶ä»– APPå½•å±éœ€æ±‚ ä¼ é”€ï¼šé£é™©ç‚¹é›†ä¸­æš´é›· å®è§‚è°ƒæ§ èµ°å·¥ä½œ å·ç +æ—¶é—´ è®¾å¤‡æŒ‡çº¹ï¼ˆsensorIDï¼‰ é»‘ç™½åå•æ ¸å¯¹ï¼Œéœ€è¦ç¡®å®šæ˜¯é»‘äººçš„æ—¶é—´ 6. References é»„å§å§HJJçŸ¥ä¹ è¿è¥å•†å¤„å¥³åœ° é»„é‡‘æ•°æ®ï¼ˆsensorIDï¼‰ ç´§æ€¥è”ç³»äºº é‚£äº›listèƒŒåçš„ç„æœº","tags":[]},{"title":"Snippets for Python","date":"2019-06-28T02:07:00.000Z","path":"2019/06/28/Snippets-for-Python/","text":"æ”¶é›†å¹³å¸¸é‡åˆ°çš„å„ç§ä¸å¸¸ç”¨çš„PythonæŠ€å·§ï¼ŒæŒç»­æ›´æ–°ingã€‚ 1. çª—å£å‡½æ•°æ»‘çª—å‡½æ•°å¹³å¸¸è¿˜æ˜¯ç”¨çš„å¾ˆå¤šçš„ã€‚ç»†åˆ†çš„è¯å¯ä»¥åˆ†æŒ‰æ—¶é—´ç»´åº¦ã€æ¬¡æ•°ç»´åº¦æˆ–æ˜¯æ’åºç»´åº¦æ»‘çª—è®¡ç®—ã€‚ 1.1 Codes æ—¶é—´ç»´åº¦æ»‘çª— 123df = pd.Series(np.random.randn(600), index=pd.date_range('7/1/2018', freq='D', periods=600))r = df.rolling(window=5) åŒæ—¶ä¹Ÿæ”¯æŒè‡ªå®šä¹‰çš„èšåˆå‡½æ•° 1.2 Refernces2. è‡ªå®šèšåˆå‡½æ•°2.1 Codes æµ‹è¯•ä¸€ä¸ªSUMçš„èšåˆå‡½æ•° 123from functools import reducedef self_define_sum(series): return reduce(lambda x, y : x + y, series) è¿‡æ»¤éç©ºå€¼ 123def not_null(series): analyst_list = filter(lambda analyst: pd.notnull(analyst), series) return list(analyst_list) ç»Ÿè®¡ç©ºå€¼ 12def count_nulls(series): return len(series) â€” series.count() 2.2 References Writing custom aggregation functions with Pandas 3. sort - UDF3.1 Codes123456s = ['(1, 10)', '(100, 1000)', '(20, 30)', '(100, 10000)', '(50, 100)']print(sorted(s))s.sort(key=lambda x : int(x.split(',')[0][1:]))print(s) 3.2 References4. Sklearnå¹¶è¡Œè®¡ç®—4.1 Codes1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = 'all'import pandas as pdimport numpy as npimport time, os, subprocess# Tutorial: https://stackoverflow.com/questions/38601026/easy-way-to-use-parallel-options-of-scikit-learn-functions-on-hpcfrom sklearn.externals.joblib import Parallel, parallel_backend, register_parallel_backend, delayed# ä¸q'cudef reader(f): time.sleep(5) return pd.read_csv(f) # method 1def mlp_reader(file_list, n_job=20, typ='threading'): \"\"\" By default the following backends are available: â€˜lokyâ€™: single-host, process-based parallelism (used by default), # å¯èƒ½ä¸æ˜¯æˆ‘è£…çš„0.19ç‰ˆæœ¬çš„å‚æ•°ï¼ŒæŠ¥é”™ï¼Œæ–‡æ¡£æ¥ç€å®˜æ–¹0.22çš„æ–‡æ¡£ â€˜threadingâ€™: single-host, thread-based parallelism, â€˜multiprocessingâ€™: legacy single-host, process-based parallelism. \"\"\" with parallel_backend(typ): df_tmp = Parallel(n_jobs=n_job)(delayed(reader, check_pickle=False)(f) for f in file_list) df_tmp = pd.concat(df_tmp) # return df_tmp# method 2def mlp_reader_2(file_list, n_job=20): df_tmp = Parallel(n_jobs=n_job)(delayed(reader, check_pickle=False)(f) for f in file_list) df_tmp = pd.concat(df_tmp) return df_tmp # æµ‹ç®—æ—¶é—´def measure_time_using(n=10, p=10): r = [] for i in range(1, n): file_list = ['20190525_spark_test_data.csv' for _ in range(i * p)] # print(file_list) t1 = time.time() df_result2 = mlp_reader_2(file_list) t2 = time.time() t3 = time.time() df_result = mlp_reader(file_list) t4 = time.time() r.append([i, i * 10, t2-t1, t4-t3]) return pd.DataFrame(r, columns=['iter', 'total_file', 'mlp_reader_2', 'mlp_reader']) åœ¨æˆ‘çš„å•æœºMac Proä¸Šï¼Œä¸ç”¨parallel_backendä¼šç¨å¾®æ…¢ä¸€äº›ï¼Œè¿™å’Œæˆ‘åœ¨å®é™…å»ºæ¨¡å·¥ç¨‹ä¸­ç›¸åäº†ï¼Œä¸çŸ¥é“æ˜¯å› ä¸ºå½“æ—¶ä½¿ç”¨çš„æ˜¯æœåŠ¡å™¨æœ‰å…³ç³»ä¸ã€‚ åœ¨æˆ‘çš„å•æœºMac Proä¸Šï¼Œå¤šçº¿ç¨‹çš„ç¨å¾®å¿«ä¸€ç‚¹ç‚¹ã€‚ 4.2 References Easy way to use parallel options of scikit-learn functions on HPC Model selection / joblib does not actually use dask / ipyparallel distributed backends #7168 Document - sklearn.utils.parallel_backend 5. Subprocess5.1 Codes12345678910import subprocess, os# å¼€ä¸€ä¸ªå­è¿›ç¨‹p = subprocess.Popen(['python', 'python_file_for_runing.py'], stdout='logfile_name', stderr=subprocess.STDOUT, preexec_fn=os.setsid)# æ‰“å°å­è¿›ç¨‹å·print(\"subprocess's ID :\", p.pid) 5.2 References Python multiprocessing within mpi 6. è®¡ç®—å‡½æ•°è¿è¡Œæ—¶é—´çš„è£…é¥°å™¨6.1 Codes123456789101112131415161718192021222324252627282930313233343536373839import timefrom functools import wrapsdef timefn(fn): \"\"\"è®¡ç®—æ€§èƒ½çš„ä¿®é¥°å™¨\"\"\" precision = 4 @wraps(fn) def measure_time(*args, **kwargs): t1 = time.time() result = fn(*args, **kwargs) t2 = time.time() print(f'INFO ï¼šfunc &#123;fn.__name__&#125; using &#123;round(t2 - t1, precision)&#125; second.') return result return measure_time# ä½¿ç”¨æ–¹å¼1## åœ¨å®šä¹‰å‡½æ•°å‰è¿›è¡Œè£…é¥°@timefndef foo(): s = 0 for i in range(1000000): s += 1 return sfoo()# ä½¿ç”¨æ–¹å¼2## ä½œä¸ºå‚æ•°ä¼ è¿›è£…é¥°å™¨def bar(): s = 0 for i in range(1000000): s += 1 return stimefn(bar)()&gt;&gt;&gt; INFO ï¼šfunc foo using 0.0898 second.&gt;&gt;&gt; 1000000&gt;&gt;&gt; INFO ï¼šfunc bar using 0.1557 second.&gt;&gt;&gt; 1000000 6.2 References Python multiprocessing within mpi 7. è‡ªå®šèšåˆå‡½æ•°7.1 Codes12 2.2 References8. è‡ªå®šèšåˆå‡½æ•°8.1 Codes12 8.2 ReferencesReferences Pythonè¯­è¨€å°å†Œ","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.cc/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.cc/tags/Pandas/"}]},{"title":"å¸¸è§é™ç»´ç®—æ³•å°ç»“ï¼ˆäºŒï¼‰","date":"2019-06-08T02:49:29.000Z","path":"2019/06/08/å¸¸è§é™ç»´ç®—æ³•å°ç»“ï¼ˆäºŒï¼‰/","text":"1. Introduction 2. æµå½¢å­¦ä¹ 3. t-SNE4. Embedding5. AutoEncoder6. Word2Vec7. References æµå½¢å­¦ä¹ -é«˜ç»´æ•°æ®çš„é™ç»´ä¸å¯è§†åŒ–","tags":[]},{"title":"å¸¸è§é™ç»´ç®—æ³•å°ç»“","date":"2019-05-30T08:23:01.000Z","path":"2019/05/30/å¸¸è§é™ç»´ç®—æ³•å°ç»“/","text":"1. Introductionç”±äºæ•°å­¦ä¸æ˜¯å¾ˆå¥½ï¼Œç»å¸¸å¯¹ä¸€äº›çŸ©é˜µåˆ†è§£ä¹‹ç±»ç›¸å…³çš„ç®—æ³•ç†è§£å¾—æœ‰äº›æ¨¡ç³Šï¼Œå¤§è‡´çŸ¥é“ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼Œå¾€å¾€å´æ²¡ç†è§£åˆ°å…¶æ•°å­¦è§’åº¦çš„æœ¬è´¨åŒºåˆ«ï¼Œæ‰€ä»¥æœ‰å¿…è¦å¥½å¥½æ•´ç†ä¸€ä¸‹ç›¸å…³çš„çŸ¥è¯†ç‚¹ã€‚å°±ä»¥é™ç»´ä¸ºèµ·ç‚¹ï¼Œæ¢³ç†ä¸€ä¸‹ç›¸å…³çš„ç®—æ³•ã€‚ï¼ˆå¯èƒ½æœ‰äº›å¹¶æ²¡æœ‰å½’ç±»åˆ°é™ç»´ç›¸å…³çš„ç®—æ³•ä¸Šï¼Œä½†æœ€ç»ˆå´æœ‰ç€é™ç»´çš„æ•ˆæœï¼‰ 2. PCA - Principal Components Analysisä¸»æˆæˆåˆ†åˆ†æ(PCA)å±äºçº¿æ€§ã€æ— ç›‘ç£çš„å…¨å±€é™ç»´ç®—æ³• 2.1 æœ€å¤§åŒ–æŠ•å½±æ–¹å·®ä»æœ€å¤§æ–¹å·®è§’åº¦ç†è§£PCAï¼Œå°±æ˜¯å°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°äº†ä½ç»´ä¸Šï¼Œå¹¶ä¿è¯äº†æ•°æ®ä¹‹é—´çš„çš„æ–¹å·®æ˜¯å°½å¯èƒ½æœ€å¤§çš„ã€‚å› ä¸ºä»ä¿¡å·çš„è§’åº¦ä¸Šæ¥çœ‹å¤§æ–¹å·®å¾€å¾€å…·æœ‰è¾ƒé«˜çš„ä¿¡å™ªæ¯” å‡è®¾æ•°æ®çš„åŸå§‹çŸ©é˜µä¸º$V = \\{\\overrightarrow {v_1}, \\overrightarrow {v_2}, â€¦, \\overrightarrow {v_n}\\}$ï¼Œç»´åº¦ä¸º$n\\times m$ï¼Œå…¶å‡å€¼ä¸º$\\overrightarrow u = \\frac 1 n \\sum_{i=1}^n \\overrightarrow {v_i}$ ç®—æ³•æ­¥éª¤ï¼š 2.1.1 å¯¹æ ·æœ¬æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–å¤„ç†ï¼Œå¾—åˆ°å¤„ç†åæ ·æœ¬$X$ \\begin {align} X &= [\\overrightarrow {v_1} - \\overrightarrow {u_1}, \\overrightarrow {v_2} - \\overrightarrow {u_2},..., \\overrightarrow {v_n} - \\overrightarrow {u_n}] \\\\ &= [{\\overrightarrow {x_1}, \\overrightarrow {x_2}, ..., \\overrightarrow {x_n}} ] \\end {align}å‘é‡çš„å†…ç§¯åœ¨å‡ ä½•ä¸Šè¡¨ç¤ºä¸ºç¬¬ä¸€ä¸ªå‘é‡åˆ°ç¬¬äºŒä¸ªå‘é‡ä¸Šçš„é•¿åº¦ï¼Œå‘é‡$\\overrightarrow {x_i}$åœ¨å•ä½å‘é‡$\\overrightarrow {w}$ä¸Šçš„æŠ•å½±å¯ä»¥è¡¨ç¤ºä¸º$ (\\overrightarrow {x_i}, \\overrightarrow {w}) = x_i^Tw$ï¼Œ ä¸”å…·ä½“å€¼ä¸º \\begin {align} x_i^T w &= \\frac 1 n \\sum_{i=1}^n x_i^Tw \\\\ &= (\\frac 1 n \\sum_{i=1}^n x_i^T)w \\\\ &= 0 \\end {align}å› ä¸ºå†…éƒ¨çš„æ•°æ®ä¸­å¿ƒåŒ–åå’Œä¸º0 2.1.2 æ±‚ä¸­å¿ƒåŒ–å¤„ç†åæ ·æœ¬çš„åæ–¹å·®çŸ©é˜µæŠ•å½±åçš„æ–¹å·®å¯ä»¥è¡¨ç¤ºä¸º \\begin {align} D(x) &= \\frac 1 n \\sum_{i=1}^n (x_i^Tw)^2 \\\\ &= \\frac 1 n \\sum_{i=1}^n (x_i^Tw)^T(x_i^Tw) \\\\ &= \\frac 1 n \\sum_{i=1}^n w^Tx_ix_i^Tw \\\\ &= w^T(\\frac 1 n \\sum_{i=1}^n x_ix_i^T)w \\tag 1 \\end {align}å…¬å¼(1)çš„æ‹¬å·å†…éƒ¨å…¶å®æ˜¯æ ·æœ¬åæ–¹å·®çŸ©é˜µï¼Œå°†å…¶ç®€åŒ–å†™ä½œ$\\Sigma$ï¼Œåˆ™æœ‰ \\left\\{ \\begin{array}{} \\max\\{w^T\\Sigma w\\} \\\\ s.t \\space\\space\\space\\space w^Tw=1 \\end{array} \\right.ä»¥ä¸Šé—®é¢˜å°±å˜æˆäº†å¸¦çº¦æŸçš„æœ€å¤§åŒ–é—®é¢˜ï¼Œä¹‹å‰æ•´ç†çš„å¸¸è§ä¼˜åŒ–ç®—æ³•æ—¶æåˆ°æ‹‰æ ¼æœ—æ—¥ä¹˜å­å°±æ˜¯ç”¨æ¥è§£è¿™æ ·é—®é¢˜çš„ã€‚ æ‹‰æ ¼æœ—æ—¥(Lagrangian)å‡½æ•° L(x, y, \\lambda) = f(x, y) - \\lambda(g(x, y) - c)è¿™é‡Œ$f(x, y)$æ˜¯ç›®æ ‡å‡½æ•°ï¼Œ$g(x, y) = c$æ˜¯çº¦æŸæ¡ä»¶ã€‚ å°†ä¸Šé¢çš„å¼å­ä»£å…¥åˆ°æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼Œæœ‰ï¼ˆè¿™é‡Œ$\\lambda$æ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼‰ J(w) = w^T\\Sigma w - \\lambda(w^Tw - 1) \\tag 2å°†ä¸Šè¿°å…¬å¼(2)å¯¹$w$æ±‚å¯¼ï¼Œå¹¶ä»¤å…¶ä¸º0ï¼Œåˆ™æœ‰ \\Sigma w - \\lambda w = 0å³æœ‰$\\lambda$ä¸º$\\Sigma$çš„ç‰¹å¾å€¼ï¼Œæ­¤æ—¶æœ‰ w^T\\Sigma w = w^T\\lambda w = \\lambda w^Tw = \\lambdaå³æœ‰$x$æŠ•å½±åçš„æ–¹å·®å°±æ˜¯åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼$\\rarr$æœ€å¤§åŒ–æ–¹å·®å°±æ˜¯æœ€å¤§åŒ–åæ–¹å·®çŸ©é˜µæœ€å¤§çš„ç‰¹å¾å€¼ $\\rarr$æœ€ä½³æŠ•å½±æ–¹å‘å°±æ˜¯æœ€å¤§ç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ æ•°å­¦è¡¨ç¤º$Mv_i = \\lambda_i v_i$ï¼Œè¿™é‡Œ$M$æ˜¯ä¸ª$n\\times n$çš„æ–¹é˜µï¼Œ$v_i$æ˜¯ä¸ªnç»´çš„ç‰¹å¾å‘é‡ï¼Œ$\\lambda_i$æ˜¯æ–¹é˜µ$M$çš„ä¸€ä¸ªç‰¹å¾å€¼ï¼Œè€Œ$v_i$æ˜¯æ–¹é˜µ$M$çš„ç‰¹å¾å€¼$\\lambda_i$æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ ç‰¹å¾å€¼åˆ†è§£ä¸»è¦å°†ä¸€ä¸ªæ–¹é˜µåˆ†è§£æˆå·¦è¾¹æ ¼å¼$M=Q\\Sigma Q^{-1}$ è¿™é‡Œ$Q$æ˜¯æ–¹é˜µ$M$çš„ç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µ $\\Sigma$æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œæ¯ä¸€ä¸ªå¯¹è§’çº¿ä¸Šçš„å…ƒç´ å°±æ˜¯ä¸€ä¸ªç‰¹å¾å€¼ ä¸€èˆ¬çŸ©é˜µAï¼Œå°†Aä¸å…¶è½¬ç½®ç›¸ä¹˜$A^TA$ï¼Œå°†ä¼šå¾—åˆ°ä¸€ä¸ªæ–¹é˜µï¼Œä¾¿å¯æ±‚å¾—è¯¥æ–¹é˜µçš„ç‰¹å¾å€¼$A^TAv_i=\\lambda_iv_i$ ç‰¹å¾å€¼è¡¨ç¤ºçš„æ˜¯è¿™ä¸ªç‰¹å¾åˆ°åº•æœ‰å¤šé‡è¦ï¼Œç±»ä¼¼äºæƒé‡ï¼Œè€Œç‰¹å¾å‘é‡åœ¨å‡ ä½•ä¸Šå°±æ˜¯ä¸€ä¸ªç‚¹ï¼Œä»åŸç‚¹åˆ°è¯¥ç‚¹çš„æ–¹å‘è¡¨ç¤ºå‘é‡çš„æ–¹å‘ ç‰¹å¾å€¼åˆ†è§£å¯ä»¥å¾—åˆ°ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡ï¼Œç‰¹å¾å€¼è¡¨ç¤ºçš„æ˜¯è¿™ä¸ªç‰¹å¾åˆ°åº•æœ‰å¤šé‡è¦ï¼Œè€Œç‰¹å¾å‘é‡è¡¨ç¤ºè¿™ä¸ªç‰¹å¾æ˜¯ä»€ä¹ˆ ç‰¹å¾å‘é‡çš„ä»£æ•°ä¸Šå«ä¹‰æ˜¯ï¼šå°†çŸ©é˜µä¹˜æ³•è½¬æ¢ä¸ºæ•°ä¹˜æ“ä½œ ç‰¹å¾å‘é‡çš„å‡ ä½•å«ä¹‰æ˜¯ï¼šç‰¹å¾å‘é‡é€šè¿‡æ–¹é˜µAå˜æ¢åªè¿›è¡Œä¼¸ç¼©ï¼Œè€Œä¿æŒç‰¹å¾å‘é‡çš„æ–¹å‘ä¸å˜ è¿™é‡ŒåŒæ—¶éœ€è¦å¼ºè°ƒä¸€ä¸‹ï¼šçŸ©é˜µä¹˜æ³•å…¶å®æ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢çš„è¿‡ç¨‹ æ¯”å¦‚ä¸‹é¢çš„å¼å­å˜åŒ–å¦‚ä¸‹å›¾ \\left[\\begin{array}{} 3 &0\\\\ 0&1 \\end{array} \\right] \\left[\\begin{array}{} x\\\\ y \\end{array} \\right] = \\left[\\begin{array}{} 3x\\\\ y \\end{array} \\right] åŒç†ï¼Œä¹‹å‰å¾—åˆ°çš„å¯¹è§’çŸ©é˜µ$\\Sigma$ï¼Œæ¯ä¸€ä¸ªå¯¹è§’çº¿ä¸Šçš„å…ƒç´ å°±æ˜¯ä¸€ä¸ªç‰¹å¾å€¼ï¼Œæè¿°çš„ä¹Ÿå¦‚ä¸Šé¢çš„æ“ä½œï¼Œæè¿°äº†è¿™ä¸ªçŸ©é˜µçš„å˜åŒ–æ–¹å‘ï¼ŒåŒæ—¶ä»å¤§åˆ°å°çš„æ’åˆ—ä»£è¡¨äº†é‡è¦ç¨‹åº¦ 2.2 æœ€å°å›å½’è¯¯å·®2.1.3 å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå°†ç‰¹å¾å€¼ä»å¤§åˆ°å°æ’åˆ—2.1.4 å–ç‰¹å¾å€¼å‰då¤§å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œå°†nç»´æ•°æ®æŠ•å½±åˆ°dç»´ä¸Š2.2 æœ€å°å¹³æ–¹è¯¯å·®3. SVD - Singular Value Decomposition3.1 å›¾è§£SVDä¹‹å‰åœ¨è®²PCAçš„æ—¶å€™ï¼Œæœ‰æåˆ°ç‰¹å¾å€¼å’Œç‰¹å¾å€¼åˆ†è§£ï¼Œè¿™äº›éƒ½æ˜¯é’ˆå¯¹è¡Œä¸åˆ—å€¼ç›¸ç­‰çš„æ–¹é˜µè€Œè¨€çš„ã€‚ä½†ä¹ˆå¯¹äºéæ–¹é˜µï¼Œå°±å¯ä»¥ç”¨å¥‡å¼‚å€¼åˆ†è§£(SVD)æ¥åˆ†è§£ï¼Œåˆ†è§£å‡ºæ¥çš„å¥‡å¼‚å€¼ç›¸å½“äºæ–¹é˜µä¸­çš„ç‰¹å¾å€¼ã€‚ å¥‡å¼‚å€¼åˆ†è§£å³å¯¹ä»»æ„çŸ©é˜µMè¡¨ç¤ºä¸ºä¸‹é¢è¿™ç§å½¢å¼ M = U\\Sigma V^T \\tag 3è¿™é‡Œæœ‰ $U$æ˜¯å·¦å¥‡å¼‚(singular)çŸ©é˜µï¼Œ$n \\times n$çš„æ­£äº¤é˜µï¼Œè¡¨ç¤ºåŸå§‹åŸŸçš„æ ‡å‡†æ­£äº¤åŸº Væ˜¯å³å¥‡å¼‚çŸ©é˜µ$m \\times m$çš„æ­£äº¤é˜µï¼Œè¡¨ç¤ºç»è¿‡M å˜æ¢åçš„co-domainçš„æ ‡å‡†æ­£äº¤åŸº $\\Sigma$æ˜¯$m \\times n$çš„å¯¹è§’çŸ©é˜µï¼Œé™¤äº†ä¸»å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ä»¥å¤–å…¨ä¸º0ï¼Œä¸»å¯¹è§’çº¿ä¸Šçš„æ¯ä¸ªå…ƒç´ éƒ½ç§°ä¸ºå¥‡å¼‚å€¼(é™åºæ’åˆ—)ã€‚è¡¨ç¤ºäº†V ä¸­çš„å‘é‡ä¸uä¸­ç›¸å¯¹åº”å‘é‡ä¹‹é—´çš„å…³ç³» 3.2 æ±‚è§£$U, \\Sigma, V$ä¸‰ä¸ªçŸ©é˜µ V æ ¹æ®ä¹‹å‰æåˆ°çš„$M^TMv_i = \\lambda_i v_i$ï¼Œæ­¤æ—¶å³å¯æ±‚å‡ºçŸ©é˜µV = {$v_1, v_2, â€¦, v_n$} U å°†çŸ©é˜µ$M$å’Œ$M$çš„è½¬ç½®åšçŸ©é˜µä¹˜æ³•ï¼Œä¼šçš„åˆ°ä¸€ä¸ª$m \\times m$çš„æ–¹é˜µ$MM^T$ï¼Œæ­¤æ—¶æœ‰$MM^Tu_i = \\lambda_i u_i$ï¼ŒçŸ©é˜µU={$u_1, u_2, â€¦, u_m$} é—®é¢˜1ï¼šä¸ºä»€ä¹ˆ$M^TM$å¯¹åº”çš„æ˜¯çŸ©é˜µ$M$ï¼Œ$MM^T$å¯¹åº”çš„æ˜¯çŸ©é˜µ$U$?ä»…ä»…æ ¹æ®å¯¹åº”çŸ©é˜µçš„å¤§å°ä¹ˆï¼Ÿ é—®é¢˜2ï¼šè¿™é‡Œçš„å¥‡å¼‚å€¼$\\lambda_i$æ˜¯ä¹‹å‰å…¬å¼é‡Œé¢çš„å¥‡å¼‚å€¼ä¹ˆï¼Ÿ $\\Sigma$ ç”±äº$\\Sigma$é™¤äº†å¯¹è§’çº¿ä¸Šæ˜¯å¥‡å¼‚å€¼å…¶ä»–ä½ç½®éƒ½æ˜¯0ï¼Œé‚£æˆ‘ä»¬åªéœ€è¦æ±‚å‡ºæ¯ä¸ªå¥‡å¼‚å€¼$\\sigma$å¯ä»¥äº†ã€‚ å°†ä¸Šé¢çš„ç­‰å¼(3)ä¸¤è¾¹åŒæ—¶ä¹˜ä»¥çŸ©é˜µVï¼Œåˆ™æœ‰ï¼ˆVçš„è½¬ç½®çŸ©é˜µ$V^T$ç­‰äºVçš„é€†çŸ©é˜µ$V^{-1}$ï¼Œ$VV^T=I$ï¼Œå¯¹è§’çº¿å€¼ä¸º1çš„å•ä½çŸ©é˜µï¼Œå•ä½çŸ©é˜µ$I$Ã—ä»»æ„çŸ©é˜µ$A$=$A$ï¼‰ \\begin {align} MV &= U \\Sigma V^TV \\\\ &= U \\Sigma \\\\ Mv_i &= \\sigma_i u_i \\\\ \\sigma_i &= \\frac {M v_i} {u_i} \\end {align}è¿™é‡Œå°†çŸ©é˜µVè¡¨ç¤ºä¸º{$v_1, v_2, â€¦, v_n$}ï¼ŒçŸ©é˜µUè¡¨ç¤ºä¸º{$Av_1, Av_2, â€¦, Av_n$} é—®é¢˜ï¼šçŸ©é˜µUå¤§å°ä¸º$m\\times m$ï¼Œè¿™é‡Œä¼¼ä¹$m \\times n$ï¼Œä¸å¯¹ï¼Ÿ ä¹‹å‰å°±å·²ç»æ±‚å‡ºäº†$V$å’Œ$U$äº†ï¼Œå³å¯ä»¥æ±‚å‡ºå¥‡å¼‚å€¼çŸ©é˜µ$\\Sigma$ è¿™é‡Œè¯æ˜ä¸€ä¸‹ä¹‹å‰æå‡ºçš„é—®é¢˜ï¼Œä¸ºä»€ä¹ˆä¸ºä»€ä¹ˆ$M^TM$å¯¹åº”çš„æ˜¯çŸ©é˜µ$M$ï¼Œ$MM^T$å¯¹åº”çš„æ˜¯çŸ©é˜µ$U$ã€‚ å…ˆè¯æ˜å‰è€…ï¼Œæ ¹æ®ä¹‹å‰çš„å…¬å¼ \\begin {align} M &= U\\Sigma V^T \\tag 4\\\\ M^T &= V\\Sigma^TU^T \\tag 5\\\\ M^TM &= V\\Sigma^TU^T U\\Sigma V^T \\tag 6\\\\ &= V\\Sigma^TI\\Sigma V^T \\\\ &= V\\Sigma^2V^T \\\\ M^TM V &= V\\Sigma^2V^TV \\tag 7\\\\ &= V\\Sigma^2 \\end {align}å°†å…¬å¼(4)(5)ç›¸ä¹˜å¯å¾—å…¬å¼(6)ï¼Œä¸¤è¾¹åŒæ—¶ä¹˜ä»¥çŸ©é˜µ$V$ï¼Œå¾—åˆ°å…¬å¼(7) é—®é¢˜ï¼šå…³äºå…¬å¼é‡Œé¢çš„$\\Sigma^2$æ˜¯æ€ä¹ˆæ¥çš„ï¼Œä»¥åŠå’Œä¸Šé¢æåˆ°çš„$\\lambda_i$çš„å…³ç³»ï¼Ÿ å›ç­”ï¼š$\\Sigma$æ˜¯å¥‡å¼‚å€¼çŸ©é˜µï¼Œ$\\lambda_i$æ˜¯ç‰¹å¾å€¼ï¼Œä¸¤è€…å…³ç³»å¦‚ä¸‹ \\begin {align} M^TMV &= V\\Sigma^2 \\\\ M^TMv_i &=\\sigma_i^2v_i \\\\ &= \\lambda_i v_i \\\\ \\sigma_i &= \\sqrt \\lambda_i \\tag 8 \\end {align}æ‰€ä»¥$\\sigma_ğ‘–$å¯ä»¥é€šè¿‡$\\frac {Mğ‘£_ğ‘–} {ğ‘¢_ğ‘–}$æ¥è®¡ç®—ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ±‚å‡º$M^TM$çš„ç‰¹å¾å€¼$\\lambda_i$å–å¹³æ–¹æ ¹æ¥æ±‚å¥‡å¼‚å€¼ è¯æ˜åé¢çš„å…¬å¼ï¼ŒåŒç†æœ‰ \\begin {align} M &= U\\Sigma V^T \\tag 4\\\\ M^T &= V\\Sigma^TU^T \\tag 5\\\\ MM^T &= U\\Sigma V^TV\\Sigma^TU^T \\tag 9\\\\ &= U\\Sigma^TI\\Sigma U^T \\\\ &= U\\Sigma^2U^T \\\\ M^TM U &= U\\Sigma^2U^TU \\tag {10}\\\\ &= U\\Sigma^2 \\end {align}æ‰€ä»¥æ¢³ç†ä¸€ä¸‹æ±‚è§£æ­¥éª¤ M \\rarr M^TM , MM^T \\rarr \\lambda_i \\rarr v_i, u_i \\\\ \\lambda_i \\rarr \\sigma_i \\rarr \\Sigmaå…·ä½“è®¡ç®—ä¾‹å­ï¼Œåœ¨åšå®¢å¥‡å¼‚å€¼åˆ†è§£(SVD)åŸç†ä¸åœ¨é™ç»´ä¸­çš„åº”ç”¨é‡Œæœ‰ï¼Œå°±ä¸ä¸¾ä¾‹äº†ã€‚ 3.3 SVDå¸¸è§ç”¨é€”ä¸Šé¢çš„è®¡ç®—æ¨å¯¼å…¬å¼ä¸­ï¼Œæœ€åå¾—åˆ°äº†å¥‡å¼‚å€¼çŸ©é˜µ$\\Sigma$ï¼Œè¿™å’ŒçŸ©é˜µåˆ†è§£ä¸­çš„ç‰¹å¾å€¼å¾ˆåƒï¼Œä»£è¡¨äº†è¿™ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼Œä¸”å€¼ä»å·¦ä¸Šåˆ°å³ä¸‹æ˜¯é™åºçš„ï¼Œç»å¸¸å‰top 10%çš„å€¼å°±å äº†å¥‡å¼‚å€¼çš„99%ï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥ç”¨è¿™10%æ›¿ä»£æ•´ä¸ªçŸ©é˜µäº†ã€‚æ ¹æ®è¿™ä¸€ç‰¹æ€§ï¼ŒSVDå¸¸è§ç”¨é€”å¦‚ä¸‹ã€‚ 3.3.1 æ¨èç®—æ³•ä»¥å‰çš„åšå®¢é‡Œé¢æœ‰æåˆ°è¿‡ï¼Œé€‰å–å¥‡å¼‚å€¼å‰kä¸ªï¼Œå…·ä½“åº”ç”¨å¦‚ä¸‹å›¾ 3.3.2 PCAé™ç»´3.3.3 NLP-æ½œåœ¨è¯­ä¹‰ç´¢å¼•ï¼ˆLSIï¼‰3.3.4 CVä¸­ç”¨äºå›¾åƒå‹ç¼©å…·ä½“ä¾‹å­è§çŸ¥ä¹ä¸“æ å¥‡å¼‚å€¼çš„ç‰©ç†æ„ä¹‰ï¼Œè¿˜æŒºæœ‰æ„æ€çš„ã€‚å¹¶ä¸”è¿™ç¯‡æ–‡ç« åé¢å…³äºSVDçš„å‡ ä½•å«ä¹‰ä¹Ÿå†™å¾—å¾ˆä¸é”™ï¼Œå€¼å¾—ä¸€çœ‹ã€‚ 3.4 SVDçš„å‡ ä½•å«ä¹‰4. LDA - Linea5. References Relationship between SVD and PCA. How to use SVD to perform PCA? çŸ©é˜µåˆ†è§£ï¼ˆMatrix Decompositionï¼‰ çŸ©é˜µè®ºç¬”è®°ï¼ˆ5ï¼‰â€”â€”çŸ©é˜µåˆ†æå’Œåº”ç”¨ æ‹‰æ ¼æœ—æ—¥ä¹˜æ³•å­ ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åŸç†æ€»ç»“ å¥‡å¼‚å€¼åˆ†è§£(SVD)åŸç†ä¸åœ¨é™ç»´ä¸­çš„åº”ç”¨ ç‰¹å¾å€¼åˆ†è§£ å’Œ SVDåˆ†è§£ å¥‡å¼‚å€¼çš„ç‰©ç†æ„ä¹‰ æœºå™¨å­¦ä¹ ä¸­çš„æ•°å­¦(5)-å¼ºå¤§çš„çŸ©é˜µå¥‡å¼‚å€¼åˆ†è§£(SVD)åŠå…¶åº”ç”¨","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"PCA","slug":"PCA","permalink":"http://chenson.cc/tags/PCA/"},{"name":"LDA","slug":"LDA","permalink":"http://chenson.cc/tags/LDA/"},{"name":"SVD","slug":"SVD","permalink":"http://chenson.cc/tags/SVD/"}]},{"title":"æœºå™¨å­¦ä¹ ä¸é‡åŒ–äº¤æ˜“è¯¾ç¨‹ç¬”è®°","date":"2019-05-12T06:51:59.000Z","path":"2019/05/12/æœºå™¨å­¦ä¹ ä¸é‡åŒ–äº¤æ˜“è¯¾ç¨‹ç¬”è®°/","text":"è¯„ä»·ï¼šâ˜…â˜†â˜†â˜†â˜†å¤§æ¦‚å°±åªèƒ½ç»™ä¸€é¢—æ˜Ÿã€‚æ•´ä¸ªè¯¾ç¨‹åŸºæœ¬éƒ½æ˜¯åœ¨è®²æœºå™¨å­¦ä¹ ï¼Œå…¶å®å’Œé‡åŒ–å·²ç»æ²¡æœ‰ä»€ä¹ˆå…³ç³»äº†ï¼Œåªæ˜¯å¥—äº†ä¸€ä¸ªé‡åŒ–äº¤æ˜“çš„å¤–å£³è€Œå·²ã€‚å…¶ä¸­æˆ‘è§‰å¾—å’Œé‡åŒ– (å½“ç„¶æŒ‡çš„æ˜¯ä½é¢‘) ç›¸å…³çš„å‡ ç‚¹éƒ½æ²¡æœ‰æåˆ°ï¼ˆç›®å‰å¬å®Œç¬¬å…­è¯¾ï¼‰ï¼š é‡åŒ–äº¤æ˜“ä¸­æ¯”è¾ƒé‡è¦çš„æ—¶é—´åºåˆ—æ•°æ®åŠç›¸å…³ç®—æ³•æ²¡æœ‰æåŠï¼› å¦‚ä½•é’ˆå¯¹è¿™ç±»çš„æ—¶é—´åºåˆ—ç‰¹å¾åšç‰¹å¾è¡ç”Ÿä¹Ÿä¸€ç‚¹æ²¡æœ‰æåˆ°ã€‚ä»å¤´åˆ°å°¾å…³äºç‰¹å¾éƒ¨åˆ†å°±æåˆ°äº†å‡ ä¸ªæŒ‡æ ‡ï¼Œåè€Œåœ¨æ¨¡å‹ä¸Šå¤§åšæ–‡ç« ã€‚æ€ä¹ˆçš„ä¹Ÿå¾—ä»‹ç»ä¸€äº›ç‰¹å¾è¡ç”Ÿç›¸å…³çš„åŒ…å§ï¼Œæ¯”å¦‚ta-libç­‰ï¼› å¦‚ä½•å›æµ‹æ•°æ®æˆ–è·Ÿå›æµ‹ç³»ç»Ÿç›¸å…³çš„å†…å®¹ã€‚ ä»¥ä¸‹æ˜¯æ•´ä¸ªè¯¾ç¨‹çš„ç¬”è®°ï¼Œéå¸¸çš„ç®€å•ï¼Œå•çº¯çš„è®°å½•ä¸€ä¸‹è€Œå·²ã€‚ ç¬¬ä¸€è¯¾ç¬¬äºŒè¯¾ç¬¬ä¸‰è¯¾1. æ•°æ®è·å– 2. æ—¶é—´åºåˆ—åˆ†ææ—¶é—´åºåˆ—ï¼šå°æ•°æ®é‡ï¼ŒåŸºäºå‡å€¼ï¼ŒåŸºäºåŠ¨é‡ã€‚ æœºå™¨å­¦ä¹ ï¼š éšæœºæ¸¸èµ°(ä¸ä¸Šä¸€åˆ»æ²¡æœ‰å…³ç³»)ï¼Œæ²¡æ³•é¢„æµ‹å’Œç»Ÿè®¡å¥—åˆ©ã€‚ æ‰¾å‡ºå¹³ç¨³çš„éšæœºè¿‡ç¨‹çš„åºåˆ—ï¼ˆåœ¨æŸä¸ªå‡å€¼å‘¨å›´æ³¢åŠ¨ï¼‰ ç›®æ ‡æ˜¯ï¼šæ‰¾å‡ºä¸€ä¸ªæŠ•èµ„ç»„åˆï¼Œè¿™ä¸ªç»„åˆæ˜¯å¹³ç¨³çš„ Mean Reversion and Ornstein-Uhlenbeck process dx_t = \\theta(\\mu - x_t)d_t + \\sigma dW_t $\\theta$ ï¼šå›å½’åˆ°meanå€¼çš„é€Ÿç‡ $\\mu$ ï¼š å‡å€¼ $\\sigma$ ï¼šæ—¶é—´åºåˆ—çš„æ–¹å·® d ï¼šå¸ƒæœ—è¿åŠ¨ æ—¶é—´åºåˆ—ä»·æ ¼æ³¢åŠ¨ åˆ¤æ–­æ˜¯å¦å¹³ç¨³ $x_1, x_2, â€¦, x_t$ ADF - Test Hurst Exponent ä½œä¸š ä»å•ä¸€è‚¡ç¥¨åˆ°æŠ•èµ„ç»„åˆ Cadf.py y(t) = \\beta x(t) - \\epsilon(t) $\\beta$ $\\epsilon$ éšæœºå™ªå£° ç»Ÿè®¡å¥—åˆ© æ‰¾å‡ºä¸¤åªç›¸å…³çš„è‚¡ç¥¨ p-value t-statistics æ³›åŒ–èƒ½åŠ› - OOT &amp; å›æµ‹ å…¶ä»– å½’ä¸€åŒ–çš„é‡è¦æ€§ æ­£åˆ™åŒ–ï¼šç»“æ„æ€§é£é™©ï¼Œè¶Šå¤æ‚é£é™©è¶Šé«˜ï¼Œå¥¥å¡å§†å‰ƒåˆ€ ç¬¬å››è¯¾å¥½åƒè¿™èŠ‚è¯¾æ²¡å•¥å¥½è®°çš„ï¼ŒåŸºæœ¬éƒ½æ˜¯ä¸€äº›æœºå™¨å­¦ä¹ ç›¸å…³çš„åŸºç¡€å†…å®¹å’Œä»‹ç»ã€‚ 1. OLS - Ordinary Least Square2. Ridgeå¸¸è§çº¿æ€§å›å½’ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå¥½ï¼Œä½†æ²¡æœ‰ç‰¹å¾é€‰æ‹©çš„è¿‡ç¨‹ã€‚ 3. Lassoå‡ ä½•æ„ä¹‰ 4. Kernelså¯¹è¾“å…¥çš„æ•°æ®åšäº†éçº¿æ€§å˜åŒ–ï¼Œæ‹Ÿåˆçš„æ—¶å€™è¿˜æ˜¯ä½¿ç”¨çš„çº¿æ€§æ¨¡å‹ æ ¸ä¸ä¸€å®šæ˜¯é«˜æ–¯å‡½æ•°ï¼Œåªæ˜¯ä¸€ç§ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚ å¯é€šè¿‡k-meansç¡®å®šæ ¸çš„ä¸ªæ•°ï¼ŒRBFéœ€è¦æŒ‡å®šå¤šå°‘ä¸ªæ ¸çš„ã€‚ æœºå™¨å­¦ä¹ æŠ€æ³•è¯¾ç¨‹å­¦ä¹ ç¬”è®°14â€” Radial Basis Function Network RBF - Radial basis function å¾„å‘åŸºå‡½æ•° Kernel 5. Cross-Validationç¬¬äº”è¯¾ - ç­–ç•¥å»ºæ¨¡ç»¼è¿°1. Transform data into training setåŸå§‹çš„é‡‘èæ•°æ®åˆ°ç»“æ„åŒ–çš„è®­ç»ƒé›† é€‰æ‹©ç‰¹å¾æ˜¯quantå¯¹é—®é¢˜çš„ä¸€ç§ç†è§£ï¼Œæ•…å› å­çš„é€‰æ‹©å–å†³äºå¯¹é—®é¢˜çš„ç†è§£ã€‚ e.g. ä½ è®¤ä¸ºæ²ªæ·±ä¸‰ç™¾è‚¡æŒ‡å’Œä»€ä¹ˆäº‹æƒ…æœ‰å…³ï¼Ÿ å¸¸è§çš„Features Time lags CCI MA - Moving Average ROC Bollinger Bands â€¦ â€¦ More Feature Indicators å¦‚ä½•éªŒè¯ è®¡ç®—R-squareå€¼ æ–‡æœ¬æ•°æ® TF_IDF Word2Vector æŒ‡æ ‡æ•° 2. Building predictive models3. building event driving back - test pipelinesåŸºäºäº‹ä»¶çš„å›æµ‹ ç¬¬å…­è¯¾1. ç‰¹å¾é€‰æ‹©2. é—ä¼ ç®—æ³•3. æ·±å…¥ç†è§£BPç®—æ³•4. RNNReferences Youtubeè¯¾ç¨‹é“¾æ¥","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"Quant","slug":"Quant","permalink":"http://chenson.cc/tags/Quant/"}]},{"title":"åœ°ç†è§£æä¸é€†è§£æ","date":"2019-05-12T01:07:36.000Z","path":"2019/05/12/åœ°ç†è§£æä¸é€†è§£æ/","text":"1. ä¸¤ä¸ªç»çº¬åº¦ç‚¹ä¹‹é—´çš„è·ç¦»å’Œæ–¹ä½è§’1.1 è®¡ç®—è·ç¦»12345678910111213141516171819from math import radians, cos, sin, asin, sqrt, atan, tan, acos, pidef haversine(lngA, latA, lngB, latB): \"\"\" è®¡ç®—ç»™å®šAã€Bä¸¤ç‚¹ä¹‹é—´çš„è·ç¦» lng: ç»åº¦ -180~180 lat: çº¬åº¦ -90~90 è¿”å›å•ä½ä¸ºå…¬é‡Œ \"\"\" # åœ°çƒå¹³å‡åŠå¾„ï¼Œå•ä½ä¸ºå…¬é‡Œ r = 6370.99681 # å°†åè¿›åˆ¶åº¦æ•°è½¬åŒ–ä¸ºå¼§åº¦ # å¦‚åœ¨Hiveæˆ–MySQLä¸­æ— radianså‡½æ•°ï¼Œå¯æ‰‹åŠ¨è®¡ç®— radiasn(lat) == lat / 180 * pi lngA, latA, lngB, latB = map(radians, [lngA, latA, lngB, latB]) return r * acos(sin(latA) * sin(latB) + cos(latA) * cos(latB) * cos(lngA - lngB))haversine(110.0123, 23, 110.0123, 23.02) 1.2 è®¡ç®—æ–¹ä½è§’12345678910111213141516def cal_degree(lngA, latA, lngB, latB, d=360): \"\"\" è®¡ç®—ç»™å®šAã€Bä¸¤ç‚¹ä¹‹é—´çš„æ–¹ä½è§’ lng: ç»åº¦ -180~180 lat: çº¬åº¦ -90~90 è¿”å›å•ä½ä¸ºå…¬é‡Œ \"\"\" lngA, latA, lngB, latB = map(radians, [lngA, latA, lngB, latB]) y = sin(lngB - lngA) * cos(latB) x = cos(latA) * sin(latB) - sin(latA) * cos(latB) * cos(lngB - lngA) # è¿”å›çš„è§’åº¦ä¸º0~360Â°ï¼Œå¯æ ¹æ®éœ€è¦æ”¹ä¸º180Â°æˆ–90Â°ç­‰ return(degrees(atan2(y, x)) + 360) % dcal_degree(110.0123, 23.32435, 110.1344, 25.5465)cal_degree(110.1344, 25.5465, 110.0123, 23.32435) 2. ç™¾åº¦åœ°å›¾API3. é«˜å¾·åœ°å›¾API3.1 addr2lnglat123456789101112131415161718def addr2lnglat(addr): \"\"\" addr: åœ°å€ \"\"\" import requests parameters = &#123;'address': addr, 'key': '*****'&#125; base = 'https://restapi.amap.com/v3/geocode/geo' res = requests.get(base, parameters, timeout=50) try: if res.status_code == 200: loc = res.json()['geocodes'][0]['location'] return loc else: return res.status_code except Exception as e: return repr(e) addr2lnglat('åŒ—äº¬å¸‚æœé˜³åŒºé˜œé€šä¸œå¤§è¡—6å·') 3.2 lnglat2addr12345678910111213141516171819def lnglat2addr(lng, lat): \"\"\" lng: ç»åº¦ -180~180 lat: çº¬åº¦ -90~90 \"\"\" from urllib.request import urlopen, quote parameters = &#123;'location': f'&#123;lng&#125;,&#123;lat&#125;', 'key': '*****', 'output' : 'json'&#125; base = 'https://restapi.amap.com/v3/geocode/regeo' res = requests.get(base, parameters, timeout=50) try: if res.status_code == 200: addr = res.json()['regeocode']['formatted_address'] return addr else: return res.status_code except Exception as e: return repr(e) lnglat2addr(101.3879394531, 31.2028176382) 4. References Python åœ°å€ç¼–ç å·¥å…·ç®±geopy(èƒ½æ ¹æ®ç»çº¬åº¦ç®—è·ç¦»ï¼ŒåŒ…å«Vincenty) é«˜å¾·åœ°å›¾å®˜æ–¹æ–‡æ¡£ ç™¾åº¦åœ°å›¾å®˜æ–¹æ–‡æ¡£ Pythonè°ƒç”¨é«˜å¾·åœ°å›¾APIå®ç°ç»çº¬åº¦æ¢ç®—ã€åœ°å›¾å¯è§†åŒ– pythonè°ƒç”¨ç™¾åº¦åœ°å›¾APIå®ç°ç»çº¬åº¦æ¢ç®—ã€çƒ­åŠ›åœ°å›¾å…¨æµç¨‹æŒ‡å— GPSæŸ¥è¯¢","tags":[{"name":"LBS","slug":"LBS","permalink":"http://chenson.cc/tags/LBS/"}]},{"title":"å¸¸è§ä¼˜åŒ–ç®—æ³•å°ç»“","date":"2019-05-12T01:02:06.000Z","path":"2019/05/12/å¸¸è§ä¼˜åŒ–ç®—æ³•å°ç»“/","text":"1. Preview åœ¨Referencesé‡Œé¢çš„ç¬¬ä¸€ç¯‡æ–‡ç« é‡Œï¼Œå¯¹äºä¼˜åŒ–ç®—æ³•çš„åˆ†ç±»æ€»ç»“çš„æŒºå¥½çš„ï¼Œåˆ†ä¸ºå…¬å¼è§£ã€æ•°å€¼ä¼˜åŒ–å’Œå…¶ä»–æ–¹æ³•ã€‚è¿™é‡Œé‡ç‚¹æ€»ç»“çš„æ˜¯æ•°å€¼ä¼˜åŒ–æ³•ã€‚ ä¸¾ä¸ªä¾‹å­ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­å¯¹äºæœ‰ç›‘ç£å­¦ä¹ è€Œè¨€ï¼Œé€šå¸¸éƒ½ä¼šæœ‰ä¸€ä¸ªå‡è®¾çš„Hypothesiså‡½æ•°ï¼Œé€šè¿‡è¿™æ ·ä¸€ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬è¾“å…¥ä¸€ä¸ªæ•°æ®xï¼Œä¼šè¾“å‡ºä¸€ä¸ªé¢„æµ‹å€¼$\\hat y$ï¼Œä¸”ä¸ºäº†èƒ½å¤Ÿä½¿å¾—$\\hat y$å’ŒçœŸå®å€¼$y$ä¹‹é—´çš„å·®è·éå¸¸å°ï¼Œå³é€šè¿‡æŸå¤±å‡½æ•°$L(w, x_i, y_i)$æ¥è®¡ç®—ã€‚ä¸”éœ€è¦è€ƒè™‘åˆ°æ•´ä½“çš„æƒ…å†µã€‚ä¼šéœ€è¦è®¡ç®—æŸä¸€æ‰¹é‡æ•°æ®çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œä¸”éœ€è¦ä¸æ–­æœ€å°åŒ–è¿™ä¸ªè¯¯å·®ï¼ˆé™ä½è®­ç»ƒè¯¯å·®ï¼‰ã€‚å³æœ‰äº†å¦‚ä¸‹å…¬å¼ \\min_w \\frac 1 N \\sum_{i=1}^N L(w, x_i, y_i) + \\lambda ||w||_2^2 ä¸Šé¢è¿™ä¸ªå…¬å¼çš„ç›®çš„å°±æ˜¯å¸Œæœ›è°ƒæ•´æ¨¡å‹çš„å‚æ•°æƒé‡$w$ï¼Œä½¿å¾—è¿™ä¸ªæ‰¹é‡çš„æ•°æ®çš„æŸå¤±å‡½æ•°è¾¾åˆ°æœ€å°å€¼ã€‚ å½“ç„¶ä¹Ÿå¯ä»¥ä»å¦å¤–ä¸€ä¸ªè§’åº¦æ€è€ƒï¼Œä¸¤è¾¹å–å¯¹æ•°ï¼Œå°†æ±‚æœ€å°å€¼çš„é—®é¢˜ï¼Œè½¬æ¢æˆæ±‚æœ€å¤§å€¼çš„é—®é¢˜ã€‚å³æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜çš„æ¦‚ç‡å¯†åº¦å‡½æ•°$p(x)$ï¼Œä½¿å¾—å¯¹è®­ç»ƒæ ·æœ¬çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°æå¤§åŒ–ï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ã€‚ \\max \\sum_{i=1}^l \\ln p(x_i; \\theta)è¿™é‡Œ $\\theta$ æ˜¯éœ€è¦æ±‚è§£çš„æ¨¡å‹å‚æ•°ï¼Œæ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å‚æ•°ã€‚ æ‰€ä»¥æœºå™¨å­¦ä¹ ä¸­å¾ˆé‡è¦çš„å°±æ˜¯ï¼ŒåŸºäºç»™å®šçš„è¿™äº›æ•°æ®ï¼Œå¦‚ä½•æ‰¾åˆ°æœ€ä¼˜çš„å‚æ•°ï¼Œä½¿å¾—æ•´ä½“çš„æŸå¤±æœ€å°ã€‚ ä»¥ä¸‹æ€»ç»“äº†æœºå™¨å­¦ä¹ ä¸­å¸¸è§çš„ä¼˜åŒ–ç®—æ³•ã€‚ 2 æ¢¯åº¦ä¸‹é™æ³•2.1 Hypothesis h(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx2.2 è¯„ä»·å‡½æ•° J(\\theta) = \\frac 1 2 \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2ä»¥ä¸Šä¸¤ä¸ªå‡½æ•°å‡è®¾æ˜¯å¼•ç”¨è‡ªå´æ©è¾¾è€å¸ˆè¯¾ä»¶çš„å†…å®¹ï¼Œå…¶ä¸­$\\theta$ æ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œ$J(\\theta)$å¯ä»¥çœ‹æˆå…³äº$\\theta$çš„å¤šå…ƒå‡½æ•°ã€‚ å¯¹äºè¿™æ ·çš„ä¸€ä¸ªå¤šå…ƒå‡½æ•°ï¼Œå¯¹å‡½æ•°è¿›è¡Œæ±‚åå¯¼ï¼Œå¯å¾—åˆ°è¯¥å‡½æ•°çš„æ¢¯åº¦ï¼Œæ²¿ç€è¯¥æ¢¯åº¦æœ€å¤§æ–¹å‘ï¼Œå‡½æ•°çš„å€¼å¢é•¿æœ€å¿«ï¼›åä¹‹æ²¿ç€è´Ÿæ¢¯åº¦çš„æ–¹å‘ï¼Œè¯¥å‡½æ•°çš„å€¼ä¸‹é™æœ€å¿«ï¼ˆåé¢ä¼šè¯æ˜ï¼‰ã€‚æ‰€ä»¥å¯¹äºè¿™æ ·çš„ä¸€ä¸ªå¤šå…ƒå‡½æ•°ï¼Œå…¶è¿­ä»£å…¬å¼ä¸º \\theta_j := \\theta_j - \\alpha \\frac \\partial {\\partial\\theta} J(\\theta)2.3 è¿­ä»£å…¬å¼ä»£å…¥ä¸Šé¢çš„å…¬å¼æœ‰ å¯ä»¥çœ‹åˆ°ä¸Šé¢åœ¨æ±‚åå¯¼çš„æ—¶å€™ï¼Œæ˜¯æ±‚çš„å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼Œæ‰€ä»¥æ˜¯å±äºä¸€é˜¶ä¼˜åŒ–ç®—æ³•ã€‚æ ¹æ®å‡½æ•°çš„ä¸€é˜¶æ³°å‹’å±•å¼€ï¼Œåœ¨è´Ÿæ¢¯åº¦æ–¹å‘ï¼Œå‡½æ•°å€¼æ˜¯ä¼šä¸‹é™çš„ã€‚ æ³°å‹’å±•å¼€å¼ [éœ€è¦çœ‹ç‹ä¼Ÿçš„PPT] æœºå™¨å­¦ä¹ ç¬”è®°1â€”æ³°å‹’å±•å¼€å¼å’Œç‰›é¡¿æ³• 2.4 æ¨å¯¼è¿‡ç¨‹ç†è®ºæ¨å¯¼å…ˆä»¥ä¸€ç»´çš„æ¢¯åº¦ä¸‹é™æ¥æ¨å¯¼ç†è§£ï¼Œç„¶åå†ç†è§£å¤šç»´çš„ã€‚é¦–å…ˆå¯¹äºä¸€ä¸ªå¯å¯¼çš„ä¸€å…ƒå‡½æ•°ï¼Œå®ƒçš„æ³°å‹’å…¬å¼å±•å¼€å¦‚ä¸‹: f(x+\\Delta x) =f(x) + f'(x)\\Delta x + \\frac 1 2 f''(x)(\\Delta x)^2 + ... + \\frac 1 n f^{(n)}(x)(\\Delta x)^næ ¹æ®å·²çŸ¥çš„ä¸€å…ƒå‡½æ•°å¯¼æ•°çš„ç»“è®ºï¼š å¦‚æœåœ¨æŸä¸€ç‚¹å¤„å¯¼æ•°çš„å€¼å¤§äº0ï¼Œåˆ™å‡½æ•°åœ¨æ­¤å¤„æ˜¯å¢å‡½æ•°ï¼ŒåŠ å¤§$x$çš„å€¼å‡½æ•°å€¼ä¼šå¢åŠ ï¼Œå‡å°$x$çš„å€¼å‡½æ•°ä¼šå‡å°ï¼› å¦‚æœåœ¨æŸä¸€ç‚¹å¤„å¯¼æ•°çš„å€¼å°äº0ï¼Œåˆ™å‡½æ•°åœ¨æ­¤å¤„æ˜¯å‡å‡½æ•°ï¼Œå¢åŠ $x$çš„å€¼å‡½æ•°å€¼ä¼šå‡å°ï¼Œå‡å°$x$çš„å€¼å‡½æ•°ä¼šå¢åŠ ï¼› å‰é¢é‡Œæœ‰ä¸ªç»“è®ºï¼š æ²¿ç€è¯¥æ¢¯åº¦æœ€å¤§æ–¹å‘ï¼Œå‡½æ•°çš„å€¼å¢é•¿æœ€å¿« åä¹‹æ²¿ç€è´Ÿæ¢¯åº¦çš„æ–¹å‘ï¼Œè¯¥å‡½æ•°çš„å€¼ä¸‹é™æœ€å¿« ç»“åˆä¸Šé¢çš„ä¸¤ç‚¹ç†è§£ï¼š å½“$fâ€™(x) &gt; 0$æ—¶ï¼Œæƒ³è¦å‡å°‘å‡½æ•°çš„å€¼çš„è¯ï¼Œåªèƒ½å‡å°‘$x$çš„å€¼ï¼› å½“$fâ€™(x)&lt;0$æ—¶ï¼Œæƒ³è¦å‡å°‘å‡½æ•°çš„å€¼çš„è¯ï¼Œåªèƒ½å¢å¤§xçš„å€¼ã€‚ å³å½“ $x := x - \\eta fâ€™(x)$ æ—¶å‡½æ•°çš„å€¼ä¸‹é™æœ€å¿«ã€‚ å…¬å¼æ¨å¯¼å¯¹äºæ³°å‹’å±•å¼€å…¬å¼å¤ªå¤æ‚äº†ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆè€ƒè™‘ä¸€é˜¶å±•å¼€ï¼Œå³æœ‰ é—®é¢˜ï¼šæ³°å‹’å…¬å¼ä¸­å¯¹äº$\\epsilon$æ˜¯å¦æœ‰å–å€¼æ–¹å‘çš„é™å®šï¼Œå¦‚å¿…é¡»å¤§äº0 f(x+\\epsilon) \\approx f(x) + \\epsilon f'(x)æ­¤æ—¶å¯ä»¥ä»¤$\\epsilon = -\\eta fâ€™(x) \\approx 0$ï¼Œä¸”$\\eta &gt; 0$ï¼Œä»£å…¥åˆ°ä¸Šé¢çš„å…¬å¼ä¸­æœ‰ f(x - \\eta f'(x)) \\approx f(x) - \\eta f'(x)^2æ­¤æ—¶å½“$fâ€™(x) \\neq 0$æ—¶ï¼Œæœ‰$\\eta fâ€™(x)^2 &gt;0$ï¼ˆå› ä¸ºå‰é¢å‡è®¾äº†$\\eta &gt; 0$ï¼‰ï¼Œæ‰€æœ‰æœ‰å¦‚ä¸‹å…¬å¼ï¼š f(x - \\eta f'(x)) â‰² f(x)æ‰€ä»¥é€šè¿‡$x := x - \\eta fâ€™(x)$æ¥è¿­ä»£$x$ï¼Œå‡½æ•°$f(x)$çš„å€¼å¯èƒ½ä¼šé™ä½ é—®é¢˜1ï¼šå‡è®¾$\\epsilon = -\\eta fâ€™(x) \\approx 0$æ€ä¹ˆæ¥çš„ï¼Œå¦‚æœæ²¡æœ‰è´Ÿå·çš„æƒ…å†µå‘¢ï¼Ÿ ç­”1ï¼šæ¨å¯¼å‡ºæ¥çš„å…¬å¼å°±æ˜¯$f(x) â‰² f(x + \\eta fâ€™(x))$ï¼Œå³æ²¿ç€æ¢¯åº¦æ–¹å‘ï¼Œå‡½æ•°å€¼æ˜¯ä¸Šå‡çš„ã€‚ é—®é¢˜2ï¼šå¦‚ä½•ç†è§£æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ï¼Œä¸‹é™æ˜¯æœ€å¿«çš„ï¼Ÿ ç­”2ï¼šä¸‹é¢å°è¯•ç€è¯æ˜ å¤šç»´æ¢¯åº¦ä¸‹é™ä¸Šé¢è®¨è®ºçš„ä»…ä»…æ˜¯ä¸€ç»´æƒ…å†µï¼Œå¯¹äºå¤šç»´åº¦$x$çš„æƒ…å†µï¼ˆå‡è®¾ç»´åº¦ä¸º$d$ï¼‰ï¼Œç›®æ ‡å‡½æ•°$f(x)$çš„æ¢¯åº¦å°±æ˜¯ä¸€ä¸ªç»´åº¦ä¸º$d$çš„åå¯¼æ•°å‘é‡ï¼Œå…·ä½“å¦‚ä¸‹ \\nabla_xf(x) = [\\frac {\\partial f(x)} {\\partial x_1}, \\frac {\\partial f(x)} {\\partial x_2},..., \\frac {\\partial f(x)} {\\partial x_d}]ä¸”ç›®æ ‡å‡½æ•°$f(x)$åœ¨$x$ä¸Šæ²¿ç€$u$æ–¹å‘çš„æ–¹å‘å¯¼æ•°ä¸ºï¼š \\begin{align} D_uf(x) =& \\lim_{h\\rightarrow 0} \\frac {f(x + hu) - f(x)} {h} \\\\ =& \\nabla f(x) \\cdot u \\\\ =& ||\\nabla f(x)|| \\cdot ||u|| \\cdot \\cos(\\theta) \\\\ =& ||\\nabla f(x)|| \\cdot \\cos(\\theta) \\end{align} è¿™é‡Œ$\\nabla$æ˜¯æ¢¯åº¦ç®—å­, $u$æ˜¯å•ä½å‘é‡ï¼Œä¸”$||u|| = 1$ æ–¹å‘å¯¼æ•°$D_uf(x)$ç»™å‡ºäº†$f$åœ¨$x$ä¸Šæ²¿ç€æ‰€æœ‰å¯èƒ½æ–¹å‘çš„å˜åŒ–ç‡ å› ä¸º$||\\nabla f(x)||$çš„å€¼æ˜¯å›ºå®šçš„ï¼Œåªè¦å½“$\\theta = \\pi$æ—¶å€™ï¼Œ$cos(\\theta)=-1$ï¼Œæ–¹å‘å¯¼æ•°$D_uf(x)$æ­¤æ—¶å€¼æ˜¯æœ€å°çš„ x := x - \\eta \\nabla f(x)ä»¥ä¸‹ä¸‰ç§é‡‡ç”¨çš„åŸºæœ¬æ€æƒ³éƒ½æ˜¯GDï¼Œåªæ˜¯åœ¨æ›´æ–°æ—¶é€‰ç”¨çš„æ•°æ®æœ‰äº›è®¸ä¸åŒã€‚ 1.2 æ‰¹é‡æ¢¯åº¦ä¸‹é™ - Batch Gradient Descent ä¼ªä»£ç  1234# é‡ç‚¹åœ¨äºwhole_datasetfor i in range(n_epochs): params_grad = evaluate_gradient(loss_func, whole_dataset, params) params = params - learning_rate * params_grad ä¼˜ç¼ºç‚¹ 1.3 éšæœºæ¢¯åº¦ä¸‹é™ - Stochastic Gradient Descent ä¼ªä»£ç  123456# é‡ç‚¹åœ¨äºsingle_datafor i in range(n_epochs): np.random.shuffle(whole_dataset) for single_data in whole_dateset: params_grad = evaluate_gradient(loss_func, single_data, params) params = params - learning_rate * params_grad ä¼˜ç¼ºç‚¹ 1.4 æœ€å°æ‰¹æ¢¯åº¦ä¸‹é™ - Mini-Batch Gradient Descent ä¼ªä»£ç  123456# é‡ç‚¹åœ¨äºepoches + batch_size=50for i in range(n_epochs): np.random.shuffle(whole_dataset) for batch in get_batches(whole_dataset, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad ä¼˜ç¼ºç‚¹ 2. åŸºäºæ¢¯åº¦ä¸‹é™æ³•è¡ç”Ÿçš„ç®—æ³•2.1 åŠ¨é‡æ³•2.2 AdaGradç®—æ³•2.3 RMSPropç®—æ³•2.4 AdaDeltaç®—æ³•2.5 Adamç®—æ³•3. ç‰›é¡¿æ³•ç‰›é¡¿æ³•å’Œæ¢¯åº¦ä¸‹é™æ³•ä¸€æ ·ï¼Œä¹Ÿæ˜¯åˆ©ç”¨å¯¼æ•°çš„æ€æƒ³ï¼Œä¸æ–­è¿­ä»£æ¥æ‰¾åˆ°ç›®æ ‡å‡½æ•°çš„æå€¼ç‚¹ã€‚ä¸åŒçš„æ˜¯ï¼Œæ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œæˆ‘ä»¬ç”¨åˆ°çš„æ˜¯å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°ï¼Œè€Œç‰›é¡¿æ³•ç”¨åˆ°äº†å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ã€‚ç‰›é¡¿æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ åœ¨æŸç‚¹å¤„ç”¨äºŒæ¬¡å‡½æ•°æ¥è¿‘ä¼¼ç›®æ ‡å‡½æ•°ï¼Œå¾—åˆ°å¯¼æ•°ä¸º0çš„æ–¹ç¨‹ï¼Œæ±‚è§£è¯¥æ–¹ç¨‹ï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ªè¿­ä»£ç‚¹ã€‚å› ä¸ºæ˜¯ç”¨äºŒæ¬¡å‡½æ•°è¿‘ä¼¼ï¼Œå› æ­¤å¯èƒ½ä¼šæœ‰è¯¯å·®ï¼Œéœ€è¦åå¤è¿™æ ·è¿­ä»£ï¼Œç›´åˆ°åˆ°è¾¾å¯¼æ•°ä¸º0çš„ç‚¹å¤„ã€‚ 3.1 ç‰›é¡¿æ³•åŒä¸Šï¼Œè¿™é‡Œæˆ‘ä»¬å…ˆä»ä¸€å…ƒå‡½æ•°å¼€å§‹æ¨å¯¼ç†è§£ï¼Œå¯¹ä¸€å…ƒçš„ç›®æ ‡å‡½æ•°åœ¨$x_0$ç‚¹å‡ºåšæ³°å‹’å±•å¼€ï¼Œæœ‰ \\begin {align} f(x) =& f(x_0) + f'(x_0)(x-x_0) + \\frac 1 2 f''(x_0)(x-x_0)^2 + ... + \\frac 1 n f^{(n)}(x_0)(x-x_0)^n \\\\ =& f(x_0) + f'(x_0)(x-x_0) + \\frac 1 2 f''(x_0)(x-x_0)^2 \\end {align}å› ä¸ºè¿™é‡Œåªç”¨åˆ°äºŒé˜¶å¯¼æ•°ï¼Œä¸Šé¢å…¬å¼å¿½ç•¥äº†äºŒé˜¶ä»¥ä¸Šçš„é¡¹ã€‚å› ä¸ºç›®çš„æ˜¯ä¸ºäº†æ‰¾åˆ°å¯¼æ•°ä¸º0çš„æ–¹ç¨‹ï¼Œå¯¹ä¸Šé¢çš„å¼å­å¯¹$x$æ±‚å¯¼å¹¶ç­‰äº0ï¼Œæœ‰ f'(x) = f'(x_0) + f''(x_0)(x-x_0) = 0ç®€åŒ–æœ‰ x = x_0 - \\frac {f'(x_0)} {f''(x_0)}è¿™é‡Œ$x$æ˜¯$x_0$ä¸‹ä¸€ä¸ªç‚¹çš„ä½ç½®ï¼Œå³è¿­ä»£å…¬å¼å¦‚ä¸‹ x_{t+1} = x_t - \\frac {f'(x_t)} {f''(x_t)} é—®é¢˜1ï¼šè¿™é‡Œè¿­ä»£å…¬å¼çœ‹ç€æœ‰ç‚¹åƒæ¢¯åº¦ä¸‹é™æ³•é‡Œçš„ï¼Œåªæ˜¯å¤šç”¨äº†ä¸€ä¸ªäºŒé˜¶å¯¼æ•°ï¼Ÿ åŒç†æ¨å¹¿åˆ°å¤šå…ƒå‡½æ•°ä¸Šï¼Œæœ‰ï¼ˆå¿½ç•¥äºŒé˜¶ä»¥ä¸Šçš„é¡¹ï¼‰ \\begin {align} f(x) =& f(x_0) + \\nabla f(x_0)^T(x - x_0) + \\frac 1 2 (x-x_0)^T\\nabla^2f(x_0)(x-x_0) \\end {align}åŒä¸Šï¼ŒåŒæ—¶å¯¹ä¸Šé¢å¼å­ä¸¤è¾¹å¯¹$x$æ±‚æ¢¯åº¦ï¼Œå¹¶ä»¤å·¦å³ä¸¤è¾¹ç­‰äº0ï¼Œå¾—åˆ°å‡½æ•°çš„å¯¼æ•°ï¼ˆæ¢¯åº¦å‘é‡ï¼‰ä¸º \\nabla f(x) = \\nabla f(x_0) + \\nabla^2f(x_0)(x-x_0) = 0è¿™é‡Œ$\\nabla^2f(x_0)$å°±æ˜¯è¿™ç« çš„é‡ç‚¹é»‘å¡çŸ©é˜µ)ï¼Œé€šå¸¸ç”¨$H$æ¥è¡¨ç¤ºï¼Œé»‘å¡çŸ©é˜µå…·ä½“è¡¨ç¤ºå¦‚ä¸‹ï¼š ç®€åŒ–ä¸Šé¢çš„å¼å­ï¼Œæœ‰ \\begin {align} x_{t+1} =& x_t - \\frac {\\nabla f(x_t)} {\\nabla^2 f(x_t)} \\\\ =& x_t - \\frac {g(x_t)} {H(x_t)} \\\\ =& x_t - H^{-1}g \\end {align}æ²¿ç€ä¸Šè¿°çš„è¿­ä»£å…¬å¼ï¼Œå‡½æ•°ä¼šæœ€ç»ˆä¼šåˆ°è¾¾å‡½æ•°çš„é©»ç‚¹å¤„ã€‚å…¶ä¸­$-H^{-1}g$ç§°ä¸ºç‰›é¡¿æ–¹å‘ã€‚è¿­ä»£ç»ˆæ­¢çš„æ¡ä»¶æ˜¯æ¢¯åº¦çš„æ¨¡æ¥è¿‘äº0ï¼Œæˆ–è€…å‡½æ•°å€¼ä¸‹é™å°äºæŒ‡å®šé˜ˆå€¼ã€‚ ç›´è§‚ç†è§£ä»¥ä¸Šæ˜¯é€šè¿‡å…¬å¼çš„æ¨å¯¼ï¼Œä½†æ˜¯ä¸å¤Ÿç›´è§‚ï¼Œå¯ä»¥ä»å‡ ä½•æ„ä¹‰ä¸Šç†è§£ä¸€ä¸‹ ã€è«åå…¶å¦™è¿™é‡Œå†™çš„ä¸€å¤§æ®µå°±ä¸è§äº†ï¼Œå´©æºƒã€‚ä¸‹é¢å°±ç›´æ¥çœ‹Referenceåšå®¢é‡Œçš„å†…å®¹å§ï¼Œä¸‹æ¬¡å¤ä¹ çš„æ—¶å€™å†è¡¥ä¸Šå§:(ã€‘ 3.2 æ‹Ÿç‰›é¡¿æ³•ä¸Šé¢è®²äº†ç‰›é¡¿æ³•åœ¨æ¯æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œè®¡ç®—é»‘å¡çŸ©é˜µçš„é€†ï¼Œç„¶åæ±‚è§£ä¸€ä¸ªä»¥è¯¥çŸ©é˜µä¸ºç³»æ•°çŸ©é˜µçš„çº¿æ€§æ–¹ç¨‹ç»„ï¼Œè¿™éœ€è¦å¤§é‡çš„è®¡ç®—éå¸¸è€—æ—¶ï¼Œå¦å¤–HessiançŸ©é˜µå¯èƒ½ä¸å¯é€†ã€‚ æ‹Ÿç‰›é¡¿æ³•çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯é€šè¿‡ç”¨ä¸€ä¸ª$n$é˜¶çŸ©é˜µ$G_k=G(x^{k})$æ¥è¿‘ä¼¼ä»£æ›¿$H^{-1}(x^{k})$ï¼Œå³æ„é€ ä¸€ä¸ªè¿‘ä¼¼HessiançŸ©é˜µæˆ–å…¶é€†çŸ©é˜µçš„æ­£å®šå¯¹ç§°çŸ©é˜µã€‚ åœ¨å…¬å¼æ¨å¯¼ç‰›é¡¿æ³•çš„æ—¶å€™ä»¤$x_0$ä¸ºç‚¹$x_k$ï¼Œä¸‹ä¸€ä¸ªç‚¹ä¸º$x_{k+1}$ï¼Œæœ‰å¦‚ä¸‹å…¬å¼ï¼ˆè€ƒè™‘å¿½ç•¥äºŒæ¬¡é¡¹ä»¥ä¸Šçš„å€¼ï¼Œå–$\\approx$ï¼‰ \\begin {align} \\nabla f(x) - \\nabla f(x_0) \\approx& \\nabla^2f(x_0)(x -x_0) \\\\ g(x) - g(x_0) \\approx& H_{x}(x - x_0) \\\\ g_{k+1} - g_k \\approx& H_{k+1}(x_{k+1} - x_k) \\end {align}ä»¤ \\begin {align} s_k =& x_{k+1} - x_k \\\\ y_k =& g_{k+1} - g_k \\end {align}æœ‰$y_k \\approx H_{k+1}s_k$ æˆ– $H^{-1}_{k+1}y_k \\approx s_k$ï¼Œä¸”è¯¥ç­‰å¼ç§°ä¹‹ä¸ºæ‹Ÿç‰›é¡¿æ¡ä»¶ï¼Œç”¨æ¥è¿‘ä¼¼ä»£æ›¿HessiançŸ©é˜µçš„çŸ©é˜µéœ€è¦æ»¡è¶³æ­¤æ¡ä»¶ã€‚æ ¹æ®è¿™ä¸ªæ¡ä»¶ï¼Œæœ‰ä¸åŒçš„æ‹Ÿç‰›é¡¿è§£æ³•ï¼Œå…·ä½“å¦‚ä¸‹ï¼š ä»¥ä¸Šç®—æ³•çœ‹ç€éƒ½æŒºå¤æ‚çš„ï¼Œè¿˜æ˜¯ä»¥åæœ‰æ—¶é—´å†å¦èµ·ç¯‡å¹…ç†è§£å§ã€‚ 4. å¸¸è§é—®é¢˜ å±€éƒ¨æœ€å°å€¼ç‚¹ éç‚¹ æ³°å‹’å±•å¼€å…¬å¼ HessiançŸ©é˜µ å…³äºæœ‰ç›‘ç£/æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œæ˜¯å¦‚ä½•åº”ç”¨å¦‚ä¸Šçš„ä¸€äº›ä¼˜åŒ–å‡½æ•°çš„ï¼Ÿ 5. References æœºå™¨å­¦ä¹ ä¸­çš„æœ€ä¼˜åŒ–ç®—æ³•æ€»ç»“ An overview of gradient descent optimization algorithms ä¸€ä¸ªæ¡†æ¶çœ‹æ‡‚ä¼˜åŒ–ç®—æ³•ä¹‹å¼‚åŒ SGD/AdaGrad/Adam ä¸€æ–‡çœ‹æ‡‚å¸¸ç”¨çš„æ¢¯åº¦ä¸‹é™ç®—æ³• ç›‘ç£å­¦ä¹ ä¹‹æ¢¯åº¦ä¸‹é™â€”â€”Andrew Ngæœºå™¨å­¦ä¹ ç¬”è®°ï¼ˆä¸€ï¼‰ ç†è§£æ¢¯åº¦ä¸‹é™æ³• ç†è§£ç‰›é¡¿æ³• ç‰›é¡¿æ³•åŠå…¶å‡ ä½•æ„ä¹‰ç†è§£","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"ä¼˜åŒ–ç®—æ³•","slug":"ä¼˜åŒ–ç®—æ³•","permalink":"http://chenson.cc/tags/ä¼˜åŒ–ç®—æ³•/"}]},{"title":"Metabaseåˆæ¢","date":"2019-04-21T07:22:36.000Z","path":"2019/04/21/Metabaseåˆæ¢/","text":"1. Introduction1.1 ä¸ºä»€ä¹ˆé€‰æ‹©Metabase Metabaseæ˜¯ä¸€æ¬¾åŠŸèƒ½ä¸°å¯Œã€æ“ä½œç®€å•çš„BIå·¥å…·ï¼Œéå¸¸æ³¨é‡éæŠ€æœ¯äººå‘˜ï¼ˆå¦‚äº§å“ã€è¿è¥ç­‰ï¼‰åœ¨ä½¿ç”¨è¿™ä¸ªå·¥å…·æ—¶çš„ä½“éªŒï¼Œè®©ä»–ä»¬èƒ½å¤Ÿè‡ªç”±æ¢ç´¢æ•°æ®ï¼Œå›ç­”è‡ªå·±çš„é—®é¢˜ï¼ŒåŒæ—¶è¿˜æ˜¯å¼€æºå…è´¹çš„ï¼Œè¿™ç‚¹éå¸¸æ£’ã€‚è€Œå…¶ä»–çš„BIå·¥å…·è¦ä¹ˆæ˜¯ä»˜è´¹çš„ï¼ˆå¦‚Tableauï¼‰ï¼Œè¦ä¹ˆå°±åªèƒ½çœ‹é¢„å…ˆå»ºå¥½çš„Dashboardï¼Œå¯¹äºä¸æ‡‚SQLæˆ–è€…æ•°æ®åº“ç»“æ„çš„ä»–ä»¬æ¥è¯´ï¼Œæ˜¯å¾ˆéš¾è‡ªå·±å»æ‘¸ç´¢çš„ã€‚åŒæ—¶å¯¹äºæŠ€æœ¯äººå‘˜æ¥è¯´ï¼Œå®˜ç½‘çš„æ–‡æ¡£å†™çš„æŒºæ£’çš„ï¼Œæ¯”è¾ƒå…¨ï¼Œç„¶åè®¾è®¡ä¹Ÿå¥½çœ‹ã€‚ å½“ç„¶äº†ï¼Œè¿˜å­˜åœ¨å…¶ä»–æ›´åŠ é«˜å¤§ä¸Šçš„BIæŠ¥è¡¨å·¥å…·ï¼Œåªæ˜¯è¿™ä¸ªè¿™ä¸ªå¯¹äºæˆ‘ç›®å‰çš„éœ€æ±‚æ¥è¯´ï¼Œæ˜¯æœ€ä¸è´¹æ—¶æ•ˆæœè¿˜æ¯”è¾ƒOKçš„ä¸€æ¬¾ã€‚ 1.2 Metabaseä¼˜ç‚¹ æ”¯æŒä»¥ä¸‹æ•°æ®åº“ Postgresql MySQL Druid SQL Server Redshift MongoDB Google BigQuery SQLite H2 Oracle Vertica Presto Snowflake æƒé™ç®¡ç† Metabaseæœ‰è‡ªå·±çš„æƒé™ç®¡ç†ç³»ç»Ÿï¼Œç”¨æˆ·åˆ†ä¸ºç”¨æˆ·ä¸ç”¨æˆ·ç»„ï¼Œæƒé™åˆ†ä¸ºæ•°æ®æºä¸é›†åˆã€‚é›†åˆå¯ä»¥ç†è§£ä¸ºä¸ªäººæˆ–è€…å…¬å…±çš„ç›®å½•ï¼Œç®¡ç†å‘˜å¯ä»¥ä¸ºç”¨æˆ·ä¸ç”¨æˆ·ç»„åˆ†é…ä¸åŒçš„æ•°æ®æŸ¥è¯¢ä¸é›†åˆæƒé™ã€‚ ä¾¿æ·çš„æ•°æ®æ¢ç´¢æ–¹å¼ Metabaseæ•°æ®æ¢ç´¢æ–¹å¼ç®€å•æ˜“ç”¨ï¼Œå¯ç›´æ¥ä½¿ç”¨â€œæ‰˜æ‹‰æ‹½â€çš„æ–¹å¼å®ç°æ•°æ®åˆ†æä»¥åŠå›¾è¡¨çš„éœ€æ±‚ï¼Œè¿™ä¸ªå¯¹äºé‚£äº›éæŠ€æœ¯äººå‘˜ï¼ˆå¦‚äº§å“ï¼Œè¿è¥ç­‰ï¼‰æ˜¯éå¸¸å‹å¥½çš„ï¼Œä¸éœ€è¦ç”¨æˆ·å…·å¤‡SQLåŠå…¶ä»–çš„ä¸“ä¸šçŸ¥è¯†ã€‚åŒæ—¶è¿™ä¸ªä¹Ÿæ¯”è¾ƒé€‚åˆç»„å†…å†…éƒ¨çš„ä¸€äº›BIæŠ¥è¡¨å¼€å‘ï¼Œå¯ä»¥ä¸€å®šç¨‹åº¦é¿å…ä¸å¼€å‘å›¢é˜Ÿäº¤æ¶‰ï¼Œæé«˜æ•ˆç‡ã€‚ å‹å¥½çš„Dashboardå±•ç¤º 1.3 Metabaseç¼ºç‚¹2. å®‰è£…ç•¥ï¼Œè¿™éƒ¨åˆ†æ„Ÿè§‰çœ‹å®˜ç½‘çš„æ–‡æ¡£æ¯”è¾ƒå¥½ï¼Œå†™çš„å¾ˆå…¨ã€‚ ç›®å‰æˆ‘é€‰æ‹©çš„å®‰è£…æ–¹å¼æ˜¯é€šè¿‡Jaræ–‡ä»¶å®‰è£…ï¼ˆMacæœ‰å¯¹åº”Metabaseçš„Applicationï¼‰ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒDockerã€Herokuç­‰æ–¹å¼ä½¿ç”¨ã€‚æˆ‘çš„å®‰è£…ç¯å¢ƒæ˜¯ OSX 10.14 java version â€œ1.8.0_92â€ 3. å¿«é€Ÿä¸Šæ‰‹3.1 Custome3.2 Advance SQL4. References Metabaseå®˜æ–¹æ–‡æ¡£ Metabaseä½¿ç”¨æ•™ç¨‹ æ•°æ®å¯è§†åŒ–çš„å¼€æºæ–¹æ¡ˆ: Superset vs Redash vs Metabase (ä¸€) æ•°æ®å¯è§†åŒ–çš„å¼€æºæ–¹æ¡ˆ: Superset vs Redash vs Metabase (äºŒ)","tags":[{"name":"Metabase","slug":"Metabase","permalink":"http://chenson.cc/tags/Metabase/"},{"name":"BIæŠ¥è¡¨","slug":"BIæŠ¥è¡¨","permalink":"http://chenson.cc/tags/BIæŠ¥è¡¨/"}]},{"title":"é—ä¼ ç®—æ³•åˆæ¢","date":"2019-04-09T11:37:00.000Z","path":"2019/04/09/é—ä¼ ç®—æ³•åˆæ¢/","text":"1. Genetic Algorithm é—ä¼ ç®—æ³• (genetic algorithm) è¿›åŒ–ç­–ç•¥ (evolution strategy) é—ä¼ è§„åˆ’ (genetic programming) / è¿›åŒ–è§„åˆ’ (evolution programming) GPå¸¸ç”¨äºå…³äºæ¨¡å‹ç»“æ„çš„è‡ªåŠ¨åŒ–è®¾è®¡å’Œä¼˜åŒ–çš„é¢†åŸŸï¼Œå¦‚ç”µè·¯è®¾è®¡ã€æœºå™¨ç»“æ„è®¾è®¡å’Œç”Ÿç‰©å­¦ä¸­çš„è¯ç‰©åˆ†å­è®¾è®¡ç­‰ã€‚ åœ¨æ•°æ®ç›¸å…³é¢†åŸŸï¼Œå¸¸ç”¨äºç¬¦å·å›å½’ã€ç‰¹å¾æå–å’Œé€‰æ‹©ç­‰ã€‚ é—ä¼ ç®—æ³•å¹¶ä¸æ˜¯ç”¨æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°çš„ 2. åˆå§‹åŒ–åœ¨é—ä¼ ç®—æ³•ä¸­ï¼Œå‡éœ€è¦æŠŠé—®é¢˜çš„å¯èƒ½ç­”æ¡ˆç¼–ç æˆä¸€ä¸ªå®¹æ˜“è¢«è¿›åŒ–ç®—æ³•æ“çºµçš„å½¢å¼ï¼Œæœ€å¸¸ä½¿ç”¨çš„äºŒè¿›åˆ¶01ç¼–ç ã€‚è‹¥é‡‡ç”¨ç”Ÿç‰©å­¦ä¸­çš„æœ¯è¯­ï¼Œåˆ™ç§°ä¹‹ä¸ºè¡¨ç°å‹(phenotype)ä¸åŸºå› å‹(genotype)ä¹‹é—´çš„äº’ç›¸æ˜ å°„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ ä»¥ä¸‹ä»¥èƒŒåŒ…ç®—æ³•ä½œä¸ºä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚ åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œä»¥ä¸Šä¸‰ä¸ªæ¦‚å¿µå¯¹åº”çš„æ˜¯ï¼š Gene - å•ä¸ªç‰¹å¾ Chromosome - ç‰¹å¾å­é›† Population - æ‰€æœ‰ç‰¹å¾å­é›†çš„é›†åˆ 3. é€‚åº”åº¦å‡½æ•°é€‚åº”åº¦å‡½æ•°å°±æ˜¯ä¸ºäº†è®¡ç®—Chromosomeçš„å¯¹åº”çš„é€‚åº”åº¦åˆ†æ•°ï¼Œåˆ†æ•°è¶Šé«˜çš„å³è¶Šæœ‰å¯èƒ½è¢«ç•™ä¸‹æ¥ã€‚ é‚£ä¹ˆå¯¹åº”æœºå™¨å­¦ä¹ é‡Œé¢å‘¢ï¼Œå°±æ˜¯è¿™ä¸ªç‰¹å¾å­é›†å¯¹åº”è®­ç»ƒå‡ºæ¨¡å‹çš„åˆ†æ•°ï¼Œè¿™ä¸ªæŒ‡æ ‡å¯ä»¥æ ¹æ®éœ€æ±‚æ¥å®šï¼ŒAUCï¼ŒKSï¼Œå‡†ç¡®ç‡ç­‰ç­‰ã€‚ ä»¥ä¸Šé¢å›¾ç‰‡ä¸Šä¸ºä¾‹ï¼ˆèƒŒåŒ…é—®é¢˜ï¼‰ï¼Œè®¡ç®—å‰ä¸¤æ¡æŸ“è‰²ä½“çš„é€‚åº”åº¦åˆ†æ•°ï¼ˆè¿™é‡Œä»¥survival pointsä¸ºå‡†ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥è€ƒè™‘èƒŒåŒ…çš„é‡é‡ï¼‰ A1ã€100110ã€‘æœ‰ A2ã€001110ã€‘æœ‰ 4. é€‰æ‹© - SELECTå‡è®¾æœ‰mç»„æŸ“è‰²ä½“ï¼Œä¸”è®¡ç®—å¥½äº†å…¶å¯¹åº”çš„survival pointï¼Œåˆ™è¿™ä¸ªsurvival pointçš„æ€»å’Œå°±æ˜¯åˆ†æ¯ï¼Œå•ä¸ªæŸ“è‰²ä½“çš„survival pointå°±æ˜¯åˆ†å­ã€‚ç„¶åå¯ä»¥è®¡ç®—æ¯æ¡æŸ“è‰²ä½“survival pointçš„å æ¯”ã€‚ç„¶ååšé€‰æ‹©çš„æ—¶å€™å°±ç±»ä¼¼è½¬ç›˜è½¬ä¸€ä¸‹ï¼Œå æ¯”è¶Šå¤§çš„å°±è¶Šæœ‰å¯èƒ½è¢«é€‰å‡ºæ¥ã€‚è¿™ä¸ªä¹Ÿæ˜¯make senseçš„ã€‚å¯ä»¥åƒç¬¬ä¸€ä¸ªè½¬ç›˜ä¸€ä¸‹é€‰ä¸¤æ¬¡ï¼Œä¹Ÿå¯ä»¥åƒç¬¬äºŒä¸ªè½¬ç›˜ä¸€æ ·ï¼ŒåŒæ—¶é€‰ä¸¤æ¬¡ã€‚ 5. äº¤å‰ - CROSSOVERå°±æ˜¯ç”Ÿç‰©å­¦ä¸­è¯´çš„æŸ“è‰²ä½“äº¤å‰ï¼Œæœ‰å•ç‚¹äº¤å‰å’Œå¤šç‚¹äº¤å‰ã€‚ å•ç‚¹äº¤å‰ å¤šç‚¹äº¤å‰ ç®€å•çš„è¯´å°±æ˜¯æ ¹æ®è¿™äº›ç‰¹å¾ï¼Œåšä¸€äº›å›ºå®šçš„ç‰¹å¾ç»„åˆ(æ ¹æ®ç‰¹å¾çš„idx) 6. å˜å¼‚ - MUTATE å˜å¼‚çš„è¯å°±æ˜¯æŸ“è‰²ä½“ä¸Šå‘ç”Ÿéšæœºçš„å˜åŒ–ï¼Œä¹Ÿå°±æ˜¯æœºå™¨å­¦ä¹ ï¼Œæ·±åº¦å­¦ä¹ ä¸­å¸¸æ‰€çš„å¼•å…¥ä¸€å®šçš„éšæœºæ€§ã€‚ 7. è¿›åŒ– - EVALUATE å®Œæˆå˜å¼‚ä¹‹åï¼Œå°±ä¼šäº§ç”Ÿä¸€ä¸ªæ–°çš„æŸ“è‰²ä½“ï¼Œç„¶åå†ç”¨é€‚åº”åº¦å‡½æ•°å»è¯„ä¼°æ˜¯å¦ä¿ç•™ï¼Œæˆ–è€…æ›¿æ¢å·²å­˜åœ¨çš„ä¸€äº›æŸ“è‰²ä½“ã€‚ 8. ç»ˆæ­¢æ¡ä»¶è·Ÿæœºå™¨å­¦ä¹ ç®—æ³•ä¸­è¿­ä»£çš„åœæ­¢æ¡ä»¶ä¹Ÿæ¯”è¾ƒç›¸ä¼¼ï¼Œæœ‰ å›ºå®šçš„è¿­ä»£æ¬¡æ•°Nï¼Œå³è¿›åŒ–çš„æ¬¡æ•° Xè½®ä¹‹å†…æ€»ä½“çš„è¯„ä¼°æŒ‡æ ‡æ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼Œå³early stopping é€‚åº”åº¦å‡½æ•°å·²ç»è¾¾åˆ°äº†é¢„å…ˆå®šä¹‰çš„å€¼ â€¦ â€¦ 9. ç¼–ç å®ç°9.1 Types - é€‰å¥½è§£å†³é—®é¢˜çš„ç±»å‹é€‰æ‹©ä½ è¦è§£å†³çš„é—®é¢˜ç±»å‹,ç¡®å®šè¦æ±‚è§£çš„é—®é¢˜ä¸ªæ•°,æœ€å¤§å€¼è¿˜æ˜¯æœ€å°å€¼ 1234567from deap import base, creator# creating class# weights 1.0, æ±‚æœ€å¤§å€¼,-1.0 æ±‚æœ€å°å€¼# (1.0,-1.0,)æ±‚ç¬¬ä¸€ä¸ªå‚æ•°çš„æœ€å¤§å€¼,æ±‚ç¬¬äºŒä¸ªå‚æ•°çš„æœ€å°å€¼creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))creator.create(\"Individual\", list, fitness=creator.FitnessMin) 9.2 Initialization - æ³¨å†Œä¸ªä½“ã€ç§ç¾¤ç­‰å‡½æ•°åˆå§‹åŒ–åŸºå› ç¼–ç ä½æ•°,åˆå§‹å€¼,ç­‰åŸºæœ¬ä¿¡æ¯ 12345678910111213141516from deap import basetoolsbox = base.Toolbox()# ç§ç¾¤æ•°IND_SIZE = 300 # toolbox.register(\"attr_bool\", random.randint, 0, 1) toolbox.register(\"attribute\", random.random, 0, 1)# è°ƒç”¨randon.randomä¸ºæ¯ä¸€ä¸ªåŸºå› ç¼–ç ç¼–ç åˆ›å»º éšæœºåˆå§‹å€¼ ä¹Ÿå°±æ˜¯èŒƒå›´[0, 1]toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attribute, n=IND_SIZE)# æ˜¯æŠŠä¹‹å‰åˆ›å»ºçš„individualæ”¾å…¥äº†populationä¸­toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) 9.3 Operator - ç®—å­é€‰æ‹©ï¼Œäº¤å‰ã€å˜å¼‚ã€é€‰æ‹©ã€è¿›åŒ–ç­‰å¾…æ“ä½œ,è®¾è®¡evaluateå‡½æ•°,åœ¨å·¥å…·ç®±ä¸­æ³¨å†Œå‚æ•°ä¿¡æ¯:äº¤å‰,å˜å¼‚,ä¿ç•™ä¸ªä½“,è¯„ä»·å‡½æ•° 12345678# è¯„ä¼°å‡½æ•°def evaluate(individual): return sum(individual),toolbox.register(\"mate\", tools.cxTwoPoint) # ä¸¤ä¸ªåŸºå› ä¹‹é—´äº¤å‰toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1) # åŸºå› å˜å¼‚toolbox.register(\"select\", tools.selTournament, tournsize=3) # é€‰æ‹©ä¿ç•™çš„æœ€ä½³ä¸ªä½“toolbox.register(\"evaluate\", evaluate) # è¿›åŒ– 9.4 Algorithms - å°†ä¸Šé¢æ³¨å†Œçš„å‡½æ•°ç­‰åº”ç”¨èµ·æ¥1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def main(): pop = toolbox.population(n=300) CXPB = 0.5 # the probability with which two individuals are crossed MUTPB = 0.2 # the probability for mutating an individual NGEN = 40 # the number of generations for which the evolution runs # Evaluate the entire population fitnesses = map(toolbox.evaluate, pop) for ind, fit in zip(pop, fitnesses): ind.fitness.values = fit # Begin the evolution for g in range(NGEN): print(\"-- Generation %i --\" % g) # Select the next generation individuals offspring = toolbox.select(pop, len(pop)) # Clone the selected individuals offspring = map(toolbox.clone, offspring) # Apply crossover and mutation on the offspring for child1, child2 in zip(offspring[::2], offspring[1::2]): # cross two individuals with probability CXPB if random.random() &lt; CXPB: toolbox.mate(child1, child2) # fitness values of the children # must be recalculated later del child1.fitness.values del child2.fitness.values for mutant in offspring: # mutate an individual with probability MUTPB if random.random() &lt; MUTPB: toolbox.mutate(mutant) del mutant.fitness.values # Evaluate the individuals with an invalid fitness invalid_ind = [ind for ind in offspring if not ind.fitness.valid] fitnesses = map(toolbox.evaluate, invalid_ind) for ind, fit in zip(invalid_ind, fitnesses): ind.fitness.values = fit # The population is entirely replaced by the offspring pop[:] = offspring # Gather all the fitnesses in one list and print the stats fits = [ind.fitness.values[0] for ind in pop] length = len(pop) mean = sum(fits) / length sum2 = sum(x*x for x in fits) std = abs(sum2 / length - mean**2)**0.5 print(\" Min %s\" % min(fits)) print(\" Max %s\" % max(fits)) print(\" Avg %s\" % mean) print(\" Std %s\" % std) print(\"-- End of (successful) evolution --\") best_ind = tools.selBest(pop, 1)[0] print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values)) return pop 9.5 ä»£ç å°è£…æ•´åˆå°†GAçš„ä»£ç å°è£…æˆsklearnçš„æ¥å£ï¼Œç›®å‰è¿˜éœ€è¦ä¿®æ”¹çš„æœ‰ä»¥ä¸‹å‡ ä¸ªåœ°æ–¹ Fitness Functionæ”¯æŒæ›´å¤šçš„æŒ‡æ ‡ï¼Œä¸ä»…ä»…åªæ˜¯f1_score é€Ÿåº¦å¤ªæ…¢äº†ï¼Œéœ€è¦æå‡ä¸€ä¸‹æ€§èƒ½ã€‚å¤§æ¦‚çœ‹äº†ä¸‹è€—æ—¶ï¼Œä»¥ä¸‹å‡ ä¸ªåœ°æ–¹å¯ä»¥ä¼˜åŒ–ä¸€ä¸‹ fitnesses = list(map(self.toolbox.evaluate, pop))ï¼Œè¿™è¾¹åˆå§‹åŒ–è€—æ—¶å¤ªä¹…äº† è¿­ä»£è¿›åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œæ˜¯å¦å¯ä»¥å¤šæ ¸ã€å¤šè¿›ç¨‹ç­‰å¹¶è¡Œä¼˜åŒ– éœ€è¦å¢åŠ è‡ªå®šä¹‰ç»ˆæ­¢æ¡ä»¶ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280import randomimport numpy as npfrom deap import base, creator, toolsclass FitenessFunction: def __init__(self, n_splits=5, *args, **kwargs): \"\"\" :param n_splits: int, Number of splits for cv :param *args: array-like, shape (n_samples,) :param **kwargs: Other estimator specific parameters :return : \"\"\" self.n_splits = n_splits self.estimator_ = None def calculate_fitness(self, estimator, X, y): \"\"\" :param estimator: :param X: :param y: \"\"\" self.estimator_ = clone(estimator) cv_set = np.repeat(-1., X.shape[0]) skf = StratifiedKFold(n_splits=self.n_splits) for train_index, test_index in skf.split(X, y): x_train, x_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] if x_train.shape[0] != y_train.shape[0]: raise Exception() self.estimator_.fit(x_train, y_train) y_pred = self.estimator_.predict(x_test) cv_set[test_index] = y_pred return f1_score(y, cv_set) class FeatureSelectionGA: \"\"\" FeaturesSelectionGA This class uses Genetic Algorithm to find out the best features for an input estimator using Distributed Evolutionary Algorithms in Python(DEAP) package. Default toolbox is used for GA but it can be changed accordingly. \"\"\" def __init__(self, estimator, X, y, cv_split=5, verbose=0): \"\"\" :param estimator: scikit-learn supported estimator, :param X: &#123;array-like&#125;, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. :param y: &#123;array-like&#125;, shape = [n_samples] :param cv_split: int, Number of splits for cross_validation to calculate fitness. :param verbose: 0 or 1 \"\"\" self.estimator_ = clone(estimator) self.n_features = X.shape[1] self.toolbox = None self.creator = self._create() self.cv_split = cv_split self.X = X self.y = y self.verbose = verbose if self.verbose == 1: print(\"estimator &#123;&#125; will select best features among &#123;&#125; features using cv_split :&#123;&#125;.\".format( estimator, X.shape[1], cv_split)) print(\"Shape od train_x: &#123;&#125; and target: &#123;&#125;\".format(X.shape, y.shape)) self.final_fitness = [] self.fitness_in_generation = &#123;&#125; self.best_ind = None def evaluate(self, individual): fit_func = FitenessFunction(self.cv_split) np_ind = np.asarray(individual) if np.sum(np_ind) == 0: fitness = 0.0 else: feature_idx = np.where(np_ind == 1)[0] # è®¡ç®—é€‚åº”åº¦ fitness = fit_func.calculate_fitness(self.estimator_, self.X[:, feature_idx], self.y) if self.verbose == 1: print(\"Individual: &#123;&#125; Fitness_score: &#123;&#125; \".format(individual, fitness)) return fitness, def _create(self): creator.create(\"FeatureSelect\", base.Fitness, weights=(1.0,)) creator.create(\"Individual\", list, fitness=creator.FeatureSelect) return creator def create_toolbox(self): \"\"\" Custom creation of toolbox. :return Initialized toolbox \"\"\" self._init_toolbox() return toolbox def register_toolbox(self, toolbox): \"\"\" Register custom created toolbox. Evalute function will be registerd in this method. :param toolbox: Registered toolbox with crossover,mutate,select tools except evaluate \"\"\" toolbox.register(\"evaluate\", self.evaluate) self.toolbox = toolbox def _init_toolbox(self): toolbox = base.Toolbox() toolbox.register(\"attr_bool\", random.randint, 0, 1) # Structure initializers toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, self.n_features) toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) return toolbox def _default_toolbox(self): toolbox = self._init_toolbox() toolbox.register(\"mate\", tools.cxTwoPoint) toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1) toolbox.register(\"select\", tools.selTournament, tournsize=3) toolbox.register(\"evaluate\", self.evaluate) return toolbox def get_final_scores(self, pop, fits): self.final_fitness = list(zip(pop, fits)) def generate(self, n_pop, cxpb=0.5, mutxpb=0.2, ngen=5, set_toolbox=False): \"\"\" Generate evolved population :param: n_pop: int, population size :param: cxpb: float, crossover probablity :param: mutxpb: float, mutation probablity :param: n_gen: int, number of generations :param: set_toolbox: boolean, If True then you have to create custom toolbox before calling method. If False use default toolbox. :return: Fittest population \"\"\" import time t = time.time() if self.verbose == 1: print(\"Population: &#123;&#125;, crossover_probablity: &#123;&#125;, mutation_probablity: &#123;&#125;, total generations: &#123;&#125;\".format( n_pop, cxpb, mutxpb, ngen)) if not set_toolbox: self.toolbox = self._default_toolbox() else: raise Exception( \"Please create a toolbox.Use create_toolbox to create and register_toolbox to register. Else set set_toolbox = False to use defualt toolbox\") pop = self.toolbox.population(n_pop) CXPB, MUTPB, NGEN = cxpb, mutxpb, ngen print('&gt;&gt;&gt; time 1 =', time.time() - t) t = time.time() # Evaluate the entire population print(\"EVOLVING.......\") fitnesses = list(map(self.toolbox.evaluate, pop)) for ind, fit in zip(pop, fitnesses): ind.fitness.values = fit print('&gt;&gt;&gt; time 2 =', time.time() - t) t = time.time() # TODO: è¿­ä»£ï¼Œéœ€è¦ä¼˜åŒ–æ¬¡æ•° for g in range(NGEN): print(\"-- GENERATION &#123;&#125; --\".format(g + 1)) offspring = self.toolbox.select(pop, len(pop)) self.fitness_in_generation[str( g + 1)] = max([ind.fitness.values[0] for ind in pop]) # Clone the selected individuals offspring = list(map(self.toolbox.clone, offspring)) # Apply crossover and mutation on the offspring for child1, child2 in zip(offspring[::2], offspring[1::2]): if random.random() &lt; CXPB: self.toolbox.mate(child1, child2) del child1.fitness.values del child2.fitness.values for mutant in offspring: if random.random() &lt; MUTPB: self.toolbox.mutate(mutant) del mutant.fitness.values # Evaluate the individuals with an invalid fitness weak_ind = [ind for ind in offspring if not ind.fitness.valid] fitnesses = list(map(self.toolbox.evaluate, weak_ind)) for ind, fit in zip(weak_ind, fitnesses): ind.fitness.values = fit print(\"Evaluated %i individuals\" % len(weak_ind)) # The population is entirely replaced by the offspring pop[:] = offspring print('&gt;&gt;&gt; time g =', time.time() - t) t = time.time() # Gather all the fitnesses in one list and print the stats fits = [ind.fitness.values[0] for ind in pop] length = len(pop) mean = sum(fits) / length sum2 = sum(x * x for x in fits) std = abs(sum2 / length - mean**2)**0.5 if self.verbose == 1: print(\" Min %s\" % min(fits)) print(\" Max %s\" % max(fits)) print(\" Avg %s\" % mean) print(\" Std %s\" % std) print(\"-- Only the fittest survives --\") self.best_ind = tools.selBest(pop, 1)[0] print(\"Best individual is %s, %s\" % (self.best_ind, self.best_ind.fitness.values)) self.get_final_scores(pop, fits) return pop from sklearn.base import BaseEstimator, MetaEstimatorMixin, clonefrom sklearn.utils import check_array, safe_maskfrom sklearn.feature_selection.base import SelectorMixinclass SelectFromGA(BaseEstimator, SelectorMixin, MetaEstimatorMixin): def __init__(self, estimator, n_pop=50, cv_split=5, cxpb=0.5, mutxpb=0.2, ngen=5, set_toolbox=False, verbose=0): self.estimator = estimator # self.threshold = threshold # # self.prefit = prefit # select model can be used directly if prefit == True # self.norm_order = norm_order # used in getting importance self.n_pop = n_pop self.cv_split = cv_split self.cxpb = cxpb self.mutxpb = mutxpb self.ngen = ngen self.set_toolbox = set_toolbox self.verbose = verbose def fit(self, X, y=None, **fit_params): \"\"\" :param X : array-like of shape (n_samples, n_features) :param y : array-like, shape (n_samples,) :param **fit_params : Other estimator specific parameters :return : \"\"\" self.estimator_ = clone(self.estimator) self.fsga = FeatureSelectionGA(self.estimator_, np.array(X), np.array(y), self.cv_split, self.verbose) self.population = self.fsga.generate(self.n_pop, self.cxpb, self.mutxpb, self.ngen, self.set_toolbox) self.best_ind = self.fsga.best_ind def _get_support_mask(self): if hasattr(self, 'fsga'): fsga = self.fsga else: raise ValueError('Please fit SelectFromGA before transform.') return np.array(fsga.best_ind) &gt; 0 def transform(self, X): \"\"\" Reduce X to the selected features. :param X: array of shape [n_samples, n_features] The input samples. :return X_r: array of shape [n_samples, n_selected_features] The input samples with only the selected features. \"\"\" # è€ƒè™‘ç§»é™¤ï¼Œéœ€æ·»åŠ  force_all_finite=False åå¯æ”¯æŒç¼ºå¤±å€¼ X = check_array(X, accept_sparse='csr', force_all_finite=False) mask = self.get_support() if not mask.any(): warn(\"No features were selected: either the data is\" \" too noisy or the selection test too strict.\", UserWarning) return np.empty(0).reshape((X.shape[0], 0)) if len(mask) != X.shape[1]: raise ValueError(\"X has a different shape than during fitting.\") return X[:, safe_mask(X, mask)] 10. References ä¸€æ–‡è¯»æ‡‚é—ä¼ ç®—æ³•å·¥ä½œåŸç† ä½¿ç”¨Pythonå’Œé—ä¼ è§„åˆ’(Genetic Programming)ç©è½¬Flappy Bird pythoné—ä¼ ç®—æ³•ï¼ˆGAï¼‰DEAP-Overviewå­¦ä¹ æ‘˜è¦ Deapå®˜æ–¹æ–‡æ¡£ Deap: pythonä¸­çš„é—ä¼ ç®—æ³•å·¥å…·ç®± Pythoné—ä¼ ç®—æ³•å·¥å…·ç®±çš„ä½¿ç”¨ï¼ˆä¸€ï¼‰æ±‚è§£å¸¦çº¦æŸçš„å•ç›®æ ‡ä¼˜åŒ–","tags":[{"name":"Genetic Algorithm","slug":"Genetic-Algorithm","permalink":"http://chenson.cc/tags/Genetic-Algorithm/"}]},{"title":"æ¨èç³»ç»Ÿå°ç»“","date":"2019-03-25T02:35:17.000Z","path":"2019/03/25/æ¨èç³»ç»Ÿå°ç»“/","text":"1. Factorization Machine (FM) å‡ºç°èƒŒæ™¯ ä¼ ç»Ÿç”¨çš„One-Hotç¼–ç ä¼šå¸¦æ¥æ•°æ®ç¨€ç–çš„é—®é¢˜ã€‚ æ¯”å¦‚æ•°æ®å¦‚ä¸‹ï¼ˆæ —å­æ¥è‡ªç¾å›¢æŠ€æœ¯åšå®¢ï¼‰ï¼š åšäº†One-Hotç¼–ç å å½“é‡Œé¢æŸäº›ç‰¹å¾çš„ç»´åº¦æ¯”è¾ƒé«˜çš„æ—¶å€™ï¼Œæ•°æ®ç»´åº¦å¯èƒ½è¾¾åˆ°ç™¾ä¸‡çº§åˆ«ã€‚ è€Œä¼ ç»Ÿçš„LRæ¨¡å‹ä¹Ÿå¹¶æ²¡è€ƒè™‘åˆ°ç‰¹å¾ä¹‹é—´çš„å…³è” y = w_0 + \\sum_{i=1}^n w_i x_i å¦‚éœ€è¦è¡¨è¿°ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¯é‡‡ç”¨å¤šé¡¹å¼æ¨¡å‹ ( Poly2ï¼Œåªè€ƒè™‘ä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„ç›¸äº’å…³ç³») y = w_0 + \\sum_{i=1}^nw_ix_i + \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n w_{ij}x_ix_jç›¸æ¯”ä¹‹å‰çš„å•ä¸€ç‰¹å¾ï¼Œè€ƒè™‘ç‰¹å¾ä¹‹é—´çš„ä¸¤ä¸¤å…³ç³»ï¼Œæ­¤æ—¶å¤šå‡ºäº† $n * (n-1) / 2$ ä¸ªäºŒé˜¶ç‰¹å¾ã€‚ è¿™æ ·ä¼šå¸¦äº†å¦å¤–ä¸€äº›é—®é¢˜ ç”±äºåšäº†One-Hotç¼–ç ï¼Œæ•°æ®éå¸¸ç³»æ•°ï¼Œ$x_i * x_j$ å¾ˆå¤šéƒ½æ˜¯ä¸º0çš„ï¼Œè¿™æ ·å°±ä¼šå¯¼è‡´å¯¹åº”çš„æƒé‡ $w_{ij}$ è®­ç»ƒä¸å‡ºæ¥ï¼Œæˆ–è€…å­¦å‡ºæ¥çš„æƒé‡ä¸é è°± è®­ç»ƒå’Œå­˜å‚¨çš„å¼€é”€ä¼šå¢å¤§å¾ˆå¤š å¯¹äºäºŒé˜¶ç‰¹å¾çš„æƒé‡ $\\hat W$ï¼Œæ­¤æ—¶å¼•å…¥äº†è¾…åŠ©å‘é‡ $V$ï¼Œæœ‰ \\hat W = VV^T \\\\ V = \\left [ \\begin{matrix} \\boldsymbol{v}_1 \\\\ \\boldsymbol{v}_2 \\\\ ... \\\\ \\pmb{v}_n \\end{matrix} \\right ] = \\left [ \\begin{matrix} v_{11} & v_{12} & ... & v_{1k}\\\\ v_{21} & v_{22} & ... & v_{2k} \\\\ ... & ... & ... & ... \\\\ v_{n1} & v_{n2} & ... & v_{nk} \\end{matrix} \\right ] _{n\\times k}æ­¤æ—¶å°±å¯ä»¥ç”¨åˆ†è§£åäº§ç”Ÿçš„$V$æ¥è¡¨è¾¾$\\hat W$ï¼Œå°†å‚æ•°ç”±$n^2$é™ä½åˆ° $kn$ä¸ªã€‚å…¶ä¸­$V$çŸ©é˜µçš„æ¯ä¸€è¡Œ$v_i$å°±æ˜¯ç¬¬iç»´ç‰¹å¾çš„éšå‘é‡ï¼Œä¸€ä¸ªéšå‘é‡åŒ…å«äº†kä¸ªæè¿°ç¬¬iç»´ç‰¹å¾çš„å› å­ï¼Œæ‰€ä»¥å«å› å­åˆ†è§£ã€‚ é—®é¢˜ å’ŒSVDå¥‡å¼‚çŸ©é˜µåˆ†è§£çš„åŒºåˆ«ï¼Ÿ å…¬å¼æ¨å¯¼ çŸ©é˜µåˆ†è§£ Itâ€™s well known that for any ==positive definite matrix $W$==, there exists a matrix $V$ such that $W$ = $V$Â·$V^T$ provided that $k$ is sufficiently large. This shows that a $FM$ can express any interaction matrix $W$ if $k$ is chosen large enough. Nevertheless in sparse settings, typically a small $k$ should be chosen because there is not enough data to estimate complex interactions $W$. Restricting $k$ - and thus the expressiveness of the $FM$ - leads to better generalization and thus improved interaction matrices under sparsity. s æ±‚è§£å‘é‡Vã€ä¸æ˜¯å¾ˆç†è§£ï¼Œéœ€è¦æ·±å…¥äº†è§£ã€‘ åŸºæœ¬å…¬å¼ ((a + b + c)^2 - a^2 - b^2 - c^2) å…·ä½“è¿‡ç¨‹ - é—®é¢˜ [ä¸ºä»€ä¹ˆFMèƒ½å¤Ÿè§£å†³å‚æ•°è®­ç»ƒçš„é—®é¢˜](https://blog.csdn.net/songbinxu/article/details/79662665) ![image-20190325154016889](2019-03-25-æ¨èç³»ç»Ÿå°ç»“/image-20190325154016889.png) æ¨¡å‹ç»“æ„ å’Œå…¶ä»–æ¨¡å‹å¯¹æ¯” äºŒé˜¶å¤šé¡¹å¼çš„SVM MFæ¨¡å‹ SVD++æ¨¡å‹ ä»£ç å®ç° æ‰‹æ’¸ä¸€é è°ƒç”¨xLearnåŒ… 2. Field-aware Factorization Machine (FFM) å‡ºç°èƒŒæ™¯ ä¹‹å‰æœ‰æåˆ°åœ¨FMä¸­ï¼Œ$V$çŸ©é˜µçš„æ¯ä¸€åˆ—$v_i$å°±æ˜¯ç¬¬iç»´ç‰¹å¾çš„éšå‘é‡ï¼Œä¸€ä¸ªéšå‘é‡åŒ…å«äº†kä¸ªæè¿°ç¬¬iç»´ç‰¹å¾çš„å› å­ï¼Œæ­¤æ—¶$v_i$æ˜¯ä¸ªä¸€ç»´çš„éšå‘é‡ã€‚ FFMæ˜¯åœ¨FMçš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº†Fieldçš„æ¦‚å¿µï¼Œå³åŸŸ/ç±»åˆ«çš„æ¦‚å¿µã€‚ç»§ç»­ä»¥FMä¸­çš„ä¾‹å­ç†è§£ å¯¹äºè¿™ä¸‰ä¸ªç‰¹å¾Countryã€Dayå’ŒAd_type Countryè¡ç”Ÿå‡ºçš„One-Hotç‰¹å¾å•ç‹¬å±äºä¸€ä¸ªåŸŸ Dayè¡ç”Ÿå‡ºçš„One-Hotç‰¹å¾å•ç‹¬å±äºä¸€ä¸ªåŸŸ Ad_typeè¡ç”Ÿå‡ºçš„One-Hotç‰¹å¾ä¹Ÿå•ç‹¬å±äºä¸€ä¸ªåŸŸ è¿™ä¸ªå¾ˆå¥½ç†è§£ï¼Œå› ä¸ºç”±ä¸€ä¸ªç‰¹å¾è¡ç”Ÿå‡ºæ¥çš„One-Hotç‰¹å¾å…¶å®éƒ½æ˜¯å±äºä¸€ç±»çš„ï¼Œæ¯”å¦‚éƒ½æ˜¯å›½å®¶ç±»å‹ã€æ—¥æœŸå’Œå¹¿å‘Šç±»å‹ç­‰ã€‚ åŒæ—¶å¯¹äºåšå®ŒOne-Hotçš„ç‰¹å¾$x_i$ï¼Œå¯¹æ¯ä¸€ä¸ªFieldéƒ½ä¼šå­¦ä¹ å‡ºä¸€ä¸ªéšå‘é‡$v_{i, f_j}$ï¼Œä¸åŒçš„ç‰¹å¾åœ¨è·ŸåŒä¸€ä¸ªFieldè¿›è¡Œå…³è”æ—¶ï¼Œä½¿ç”¨çš„æ˜¯ä¸åŒçš„éšå‘é‡ã€‚è€Œåœ¨FMä¸­ï¼Œåªæœ‰ä¸€ä¸ª$v_i$çš„éšå‘é‡ã€‚è¿™ä¸ªæ˜¯ä»–ä»¬æœ€å¤§çš„ä¸åŒã€‚ \\hat W = VV^T = ç»§ç»­ä¹‹å‰çš„æ —å­ One-Hot Label $x_1$ $x_2$ $x_3$ $x_4$ $x_5$ $x_6$ $x_7$ Clicked C=USA C=China D=26/11/15 D=1/7/14 D=19/2/15 A=Movie A=Game 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 Field-Table Field Name Field Index Feature Name Feature Index Country 1 USA 1 China 2 Day 2 26/11/15 3 1/7/14 4 19/2/15 5 Ad_type 3 Movie 6 Game 7 äºŒé˜¶ç‰¹å¾äº¤å‰ \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n w_{ij}x_ix_j FFMè¡¨ç¤º ç”±äºäºŒé˜¶äº¤å‰æœ‰ç‚¹å¤šï¼Œæ¯ä¸ªéƒ½å†™å¥½ç´¯ï¼Œå°±æŒ‘å…¶ä¸­å››ä¸ªçœ‹å§ã€‚ w_{1, 3} â‹… x_1 x_3 = â‹… x_1 x_3 \\\\ w_{1, 6} â‹… x_1 x_6 = â‹… x_1 x_6 \\\\ w_{1, 7} â‹… x_1 x_7 = â‹… x_1 x_7 \\\\ w_{3, 7} â‹… x_3 x_7 = â‹… x_3 x_7å¯ä»¥çœ‹å‡ºæ¥ï¼Œä¸€ä¸ªç‰¹å¾$x_i$ å¯¹äºåŒä¸€ä¸ªFieldçš„ç‰¹å¾ï¼Œä½¿ç”¨çš„éšå‘é‡æ˜¯ä¸€æ ·çš„ï¼› å¯¹äºä¸åŒçš„Fieldçš„ç‰¹å¾ï¼Œä½¿ç”¨çš„éšå‘é‡æ˜¯ä¸åŒçš„ã€‚ æ¨¡å‹ç»“æ„ ç‰¹å¾å¤„ç†ç»†èŠ‚ æ ·æœ¬å½’ä¸€åŒ–ï¼šFFMé»˜è®¤æ˜¯è¿›è¡Œæ ·æœ¬æ•°æ®çš„å½’ä¸€åŒ–ï¼Œå³ ä¸ºçœŸï¼›è‹¥æ­¤å‚æ•°è®¾ç½®ä¸ºå‡ï¼Œå¾ˆå®¹æ˜“é€ æˆæ•°æ®infæº¢å‡ºï¼Œè¿›è€Œå¼•èµ·æ¢¯åº¦è®¡ç®—çš„nané”™è¯¯ã€‚å› æ­¤ï¼Œæ ·æœ¬å±‚é¢çš„æ•°æ®æ˜¯æ¨èè¿›è¡Œå½’ä¸€åŒ–çš„ã€‚ ç‰¹å¾å½’ä¸€åŒ–ï¼šCTR/CVRæ¨¡å‹é‡‡ç”¨äº†å¤šç§ç±»å‹çš„æºç‰¹å¾ï¼ŒåŒ…æ‹¬æ•°å€¼å‹å’Œcategoricalç±»å‹ç­‰ã€‚ä½†æ˜¯ï¼Œcategoricalç±»ç¼–ç åçš„ç‰¹å¾å–å€¼åªæœ‰0æˆ–1ï¼Œè¾ƒå¤§çš„æ•°å€¼å‹ç‰¹å¾ä¼šé€ æˆæ ·æœ¬å½’ä¸€åŒ–åcategoricalç±»ç”Ÿæˆç‰¹å¾çš„å€¼éå¸¸å°ï¼Œæ²¡æœ‰åŒºåˆ†æ€§ã€‚ä¾‹å¦‚ï¼Œä¸€æ¡ç”¨æˆ·-å•†å“è®°å½•ï¼Œç”¨æˆ·ä¸ºâ€œç”·â€æ€§ï¼Œå•†å“çš„é”€é‡æ˜¯5000ä¸ªï¼ˆå‡è®¾å…¶å®ƒç‰¹å¾çš„å€¼ä¸ºé›¶ï¼‰ï¼Œé‚£ä¹ˆå½’ä¸€åŒ–åç‰¹å¾â€œsex=maleâ€ï¼ˆæ€§åˆ«ä¸ºç”·ï¼‰çš„å€¼ç•¥å°äº0.0002ï¼Œè€Œâ€œvolumeâ€ï¼ˆé”€é‡ï¼‰çš„å€¼è¿‘ä¼¼ä¸º1ã€‚ç‰¹å¾â€œsex=maleâ€åœ¨è¿™ä¸ªæ ·æœ¬ä¸­çš„ä½œç”¨å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œè¿™æ˜¯ç›¸å½“ä¸åˆç†çš„ã€‚å› æ­¤ï¼Œå°†æºæ•°å€¼å‹ç‰¹å¾çš„å€¼å½’ä¸€åŒ–åˆ° æ˜¯éå¸¸å¿…è¦çš„ã€‚ çœç•¥é›¶å€¼ç‰¹å¾ï¼šä»FFMæ¨¡å‹çš„è¡¨è¾¾å¼å¯ä»¥çœ‹å‡ºï¼Œé›¶å€¼ç‰¹å¾å¯¹æ¨¡å‹å®Œå…¨æ²¡æœ‰è´¡çŒ®ã€‚åŒ…å«é›¶å€¼ç‰¹å¾çš„ä¸€æ¬¡é¡¹å’Œç»„åˆé¡¹å‡ä¸ºé›¶ï¼Œå¯¹äºè®­ç»ƒæ¨¡å‹å‚æ•°æˆ–è€…ç›®æ ‡å€¼é¢„ä¼°æ˜¯æ²¡æœ‰ä½œç”¨çš„ã€‚å› æ­¤ï¼Œå¯ä»¥çœå»é›¶å€¼ç‰¹å¾ï¼Œæé«˜FFMæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹çš„é€Ÿåº¦ï¼Œè¿™ä¹Ÿæ˜¯ç¨€ç–æ ·æœ¬é‡‡ç”¨FFMçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ ä»£ç å®ç° æ‰‹æ’¸ä¸€é- libffm-python è°ƒç”¨libffm æ•°æ®æ ¼å¼ $label$ $field_idx_1$ï¼š$feature_idx_1$ï¼š$value$ $field_idx_2$ï¼š$feture_idx_2$ï¼š$value$ æ‰€ä»¥ä¸Šé¢çš„æ —å­å¯ä»¥è¡¨ç¤ºä¸º 1 1:1:1 2:3:1 3:6:1 0 1:2:1 2:4:1 3:7:1 1 1:2:1 2:5:1 3:7:1 å¯èƒ½è¿™é‡Œä¼šå›°æƒ‘ä¸ºå•¥æœ€åéƒ½æ˜¯1è¿˜è¦ä¿ç•™ç€ï¼Ÿèƒ½å¦å…¨éƒ¨çœç•¥äº†ï¼ŸåŸå› æ˜¯å¯èƒ½ç‰¹å¾å­˜åœ¨æ•°å€¼å‹å·çš„ç‰¹å¾ï¼Œæ¯”å¦‚äººå£ç‰¹å¾ä¸­çš„å¹´é¾„ç­‰ï¼Œæœ€åçš„valueå€¼æ­¤æ—¶ä¸ºæ•°å€¼å‹ï¼Œé0/1å˜é‡ã€‚(æ‰€æœ‰çš„æ•°å€¼ç‰¹å¾å…±äº«åŒä¸€ä¸ªfeature_idx) DEMO è°ƒç”¨xLearn 12345678910111213141516171819202122232425import xlearn as xl# Training taskffm_model = xl.create_ffm() # Use field-aware factorization machine (ffm)ffm_model.setTrain(\"./small_train.txt\") # Set the path of training datasetffm_model.setValidate(\"./small_test.txt\") # Set the path of validation dataset# Parameters:# 0. task: binary classification# 1. learning rate: 0.2# 2. regular lambda: 0.002# 3. evaluation metric: accuracyparam = &#123;'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'acc'&#125;# Start to train# The trained model will be stored in model.outffm_model.fit(param, './model.out')# Prediction taskffm_model.setTest(\"./small_test.txt\") # Set the path of test datasetffm_model.setSigmoid() # Convert output to 0-1# Start to predict# The output result will be stored in output.txtffm_model.predict(\"./model.out\", \"./output.txt\") 3. DeepFM å‡ºç°èƒŒæ™¯ FMåªè®¡ç®—äº†äºŒé˜¶ç‰¹å¾ç»„åˆï¼Œå¦‚éœ€è¦è€ƒè™‘é«˜é˜¶ç‰¹å¾çš„ç»„åˆï¼Œå¯èƒ½é‡‡ç”¨DNNæ¨¡å‹ã€‚ä½†ç”±äºç‰¹å¾åšäº†One-Hotï¼Œè¾“å…¥ä¸€èˆ¬æ˜¯éå¸¸é«˜çš„ç»´åº¦ï¼Œå¯¼è‡´è¾“å…¥çš„å‚æ•°å¤ªå¤šäº†ã€‚ è€ƒè™‘åˆ°ç”¨FFMä¸­Fieldçš„æ¦‚å¿µï¼Œå¯ä»¥å°†åŒä¸€ä¸ªç‰¹å¾One-Hotå‡ºæ¥çš„ç‰¹å¾è½¬æ¢æˆä¸€ä¸ªDense Vectorï¼Œç„¶ååœ¨å†åŸºäºFieldçš„Denseæ•°æ®ï¼Œå†è¾“å…¥åˆ°DNNä¸­å­¦ä¹ ä¸åŒç‰¹å¾ä¹‹é—´çš„é«˜é˜¶ç»„åˆã€‚(é‚£ä¸ºå•¥ä¸å«DeepFFMï¼Ÿ) æ¨¡å‹ç»“æ„ è¾“å…¥çš„Embeddingå±‚ Embeddingå±‚çš„è¾“å‡ºä¸º a^{(0)}=[e_1, e_2, ..., e_m]å…¶ä¸­$e_i$æ˜¯åµŒå…¥çš„ç¬¬iä¸ªfiledï¼Œmæ˜¯fieldçš„ä¸ªæ•°ï¼Œå‰å‘è¿‡ç¨‹å°†åµŒå…¥å±‚çš„è¾“å‡ºï¼Œè¾“å…¥åˆ°éšè—å±‚ä¸º a^{(l+1)} = \\sigma (W^{(l)}a^{(l)} + b^{(l)}) FMéƒ¨åˆ† DNNéƒ¨åˆ† é¢„æµ‹è¾“å‡º \\hat y = sigmoid(y_{FM} + y_{DNN}) 4. Wide &amp; Deep Network (WDN)Wide-and-Deepæ¨¡å‹åˆæ¢ 5. Deep &amp; Cross Network (DCN)DCNè¿›é˜¶ç‰ˆä¹‹xDeepFM æ¨èç³»ç»Ÿé‡ä¸Šæ·±åº¦å­¦ä¹ (äºŒåäºŒ)â€”DeepFMå‡çº§ç‰ˆXDeepFMæ¨¡å‹å¼ºåŠ¿æ¥è¢­ 6. References æ¨èç³»ç»Ÿé‡ä¸Šæ·±åº¦å­¦ä¹ (ä¸€)â€”FMæ¨¡å‹ç†è®ºå’Œå®è·µ ä¸€æ–‡è¯»æ‡‚FMç®—æ³•ä¼˜åŠ¿ FMç³»åˆ—ç®—æ³•è§£è¯»ï¼ˆFM+FFM+DeepFMï¼‰ å› å­åˆ†è§£æœºï¼ˆlibffm+xlearnï¼‰ ç¾å›¢æŠ€æœ¯åšå®¢ - æ·±å…¥FFMåŸç†ä¸å®è·µ FFMæ¨¡å‹åœ¨ç‚¹å‡»ç‡é¢„ä¼°ä¸­çš„åº”ç”¨å®è·µ äº‘è„‘-ç”µå•†æ¨èç³»ç»Ÿ(ç‰¹å¾å·¥ç¨‹éƒ¨åˆ†) libffm-python libffm-C++ DeepFM: A Factorization-Machine based Neural Network for CTR Prediction","tags":[{"name":"Wide&Deep","slug":"Wide-Deep","permalink":"http://chenson.cc/tags/Wide-Deep/"},{"name":"æ¨èç³»ç»Ÿ","slug":"æ¨èç³»ç»Ÿ","permalink":"http://chenson.cc/tags/æ¨èç³»ç»Ÿ/"},{"name":"FM","slug":"FM","permalink":"http://chenson.cc/tags/FM/"},{"name":"FFM","slug":"FFM","permalink":"http://chenson.cc/tags/FFM/"},{"name":"DeepFM","slug":"DeepFM","permalink":"http://chenson.cc/tags/DeepFM/"},{"name":"DCN","slug":"DCN","permalink":"http://chenson.cc/tags/DCN/"}]},{"title":"AutoMLå¹³å°è¯•ç”¨ä½“éªŒ","date":"2019-03-20T10:44:55.000Z","path":"2019/03/20/AutoMLå¹³å°è¯•ç”¨ä½“éªŒ/","text":"1. èš‚èšé‡‘æœ Morse2. èš‚èšé‡‘æœ Pi3. ç™¾åº¦å¤©é“¾ 4. ç¬¬å››èŒƒå¼","tags":[{"name":"Auto-ML","slug":"Auto-ML","permalink":"http://chenson.cc/tags/Auto-ML/"}]},{"title":"Mac OSXå®‰è£…Fiddler","date":"2019-03-07T09:59:54.000Z","path":"2019/03/07/Mac-OSXå®‰è£…Fiddler/","text":"1. å®‰è£…Monoé¦–å…ˆï¼ŒMacä¸‹éœ€è¦ä½¿ç”¨.Netç¼–è¯‘åçš„ç¨‹åºï¼Œéœ€è¦ç”¨åˆ°è·¨å¹³å°çš„æ–¹æ¡ˆMono(ç°é˜¶æ®µå¾®è½¯å·²æ¨å‡ºè·¨å¹³å°çš„æ–¹æ¡ˆ.Net Coreï¼Œä¸è¿‡æš‚æ—¶åªæ”¯æŒæ§åˆ¶å°ç¨‹åº)ã€‚ä»¥ä¸Šæ˜¯ç½‘ä¸Šçš„æè¿°ï¼Œç»™æˆ‘æ„Ÿè§‰æœ‰ç‚¹ç±»ä¼¼ä¸€ä¸ªè™šæ‹Ÿæœºæˆ–è€…dockerä¸€ä¸‹çš„ä¸œè¥¿ï¼Œè¿™æ ·å°±èƒ½åœ¨OSXçš„ç¯å¢ƒä¸‹ï¼Œè¿è¡Œexeæ–‡ä»¶äº†ã€‚ ä¸‹è½½Monoï¼Œå¹¶å®‰è£… å®‰è£…å®Œåï¼Œæ‰§è¡ŒæŒ‡ä»¤ 12345# version == 5.16.0/Library/Frameworks/Mono.framework/Versions/5.16.0/bin/mozroots --import --sync/Library/Frameworks/Mono.framework/Versions/5.16.0/bin/cert-sync --import --sync/Library/Frameworks/Mono.framework/Versions/5.18.0/bin/cert-sync --import --sync/Library/Frameworks/Mono.framework/Versions/5.18.0/bin/mozroots --import --sync æ­¤æ­¥æ˜¯ä¸ºäº†ä»Mozilla LXRä¸Šä¸‹è½½æ‰€æœ‰å—ä¿¡ä»»çš„rootè¯ä¹¦ï¼Œå­˜äºMonoçš„è¯ä¹¦åº“é‡Œã€‚rootè¯ä¹¦èƒ½ç”¨äºè¯·æ±‚httpsåœ°å€ã€‚ æŠ¥é”™ ä¿®æ”¹ç¯å¢ƒå˜é‡ï¼Œå¹¶æ¿€æ´»ä¿®æ”¹åçš„æ–‡ä»¶ sudo vi ~/.bash_profile æˆ–è€…/etc/profile 12export MONO_HOME=/Library/Frameworks/Mono.framework/Versions/5.16.0export PATH=$PATH:$MONO_HOME/bin 2. å®‰è£…Fiddler ä¸‹è½½Fiddlerï¼Œå¹¶å®‰è£…ï¼ˆè§£å‹åˆ°éä¸­æ–‡å­—ç¬¦çš„è·¯å¾„ä¸‹ï¼‰ å®‰è£…å®Œåï¼Œè¿›å…¥åˆ°åˆšåˆšè§£å‹çš„Fiddlerçš„è·¯å¾„ï¼Œå¹¶å‘½ä»¤æ‰§è¡Œ 1sudo mono Fiddler.exe ä»¥ä¸Šéƒ½æ˜¯ç½‘ä¸Šçš„æ•™ç¨‹ï¼Œåœ¨è¿è¡Œæœ€åä¸€æ­¥çš„æ—¶å€™ï¼Œå¤±è´¥äº†ã€‚è¯•äº†å„ç§æ–¹æ³•éƒ½æ²¡æˆåŠŸï¼Œä¸ç¡®å®šæ˜¯æˆ‘æ“ä½œçš„é—®é¢˜è¿˜æ˜¯è¿™æ–¹æ³•æ¯”è¾ƒä¹…äº†ã€‚æ‰€ä»¥æˆ‘çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œæ”¹ç”¨Charles : ) 3. References Mac OS å®‰è£…Fiddler macä¸‹Fiddlerçš„å®‰è£…-å¯åŠ¨ Mono å®˜ç½‘ Fiddler å®˜ç½‘ macOS10.14 å®‰è£…Fiddler","tags":[{"name":"Mac","slug":"Mac","permalink":"http://chenson.cc/tags/Mac/"},{"name":"Fiddler","slug":"Fiddler","permalink":"http://chenson.cc/tags/Fiddler/"}]},{"title":"Centos7å®‰è£…Nvidaé©±åŠ¨å’Œæ˜¾å¡é©±åŠ¨","date":"2019-03-07T09:54:49.000Z","path":"2019/03/07/Centos7å®‰è£…Nvidaé©±åŠ¨å’Œæ˜¾å¡é©±åŠ¨/","text":"1. å®‰è£…1.1 å®‰è£…æ–¹å¼æœ‰ä¸‰ç§ CUDAï¼ˆ.runï¼‰ä¸‹è½½ä»¥åå®‰è£…å¸¦ä¸Šæ˜¾å¡é©±åŠ¨ï¼Œ ä¸‹è½½å¦‚ä¸‹ 1234567891011121314# æŸ¥çœ‹æ˜¾å¡ç‰ˆæœ¬lspci | grep -i nvidia# ä¸‹è½½# https://www.nvidia.cn/Download/index.aspx?lang=cn å¯åœ¨è¿™ä¸ªç½‘ç«™çœ‹å‡ºå¯¹åº”çš„ç‰ˆæœ¬wget http://cn.download.nvidia.com/tesla/410.79/NVIDIA-Linux-x86_64-410.79.run# å®‰è£…sh NVIDIA-Linux-x86_64-410.79.run# å¸è½½sh NVIDIA-Linux-x86_64-410.79.run --uninstallnvidia-uninstall æ˜¾å¡é©±åŠ¨.runæ–‡ä»¶å®‰è£… é›†æˆè½¯ä»¶åŒ…å®‰è£…ï¼ˆyumç­‰ï¼‰ 12345# ç¡®è®¤æ˜¯å¦ä¸ºyumå®‰è£…yum list installed | grep nvidia# å¸è½½yum remove nvidia-detect.x86_64 1.2 å®‰è£…æ­¥éª¤ æ·»åŠ EIRepoæº 1234rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # æ˜¯å¦æŒ‡å®šç‰ˆæœ¬ï¼Ÿrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm å®‰è£…æ˜¾å¡é©±åŠ¨æ£€æŸ¥ 1yum install nvidia-detect è¿è¡Œæ˜¾å¡æ£€æµ‹é©±åŠ¨ 1234nvidia-detect -v# è¿è¡Œç»“æœkmod-nvidia ç³»ç»Ÿå‡†å¤‡å·¥ä½œ 12345678910111213# æ³¨æ„è¿™æ˜¯å‡çº§ç³»ç»Ÿç›¸å…³çš„åŒ…ï¼Œè€—æ—¶è¾ƒé•¿yum -y updateyum -y groupinstall \"GNOME Desktop\" \"Development Tools\"# å®‰è£…åŸºç¡€åŒ…yum install kernel-devel kernel-doc kernel-headers gcc* glibc* glibc-*# å®‰è£…åˆšæ‰æ£€æµ‹åˆ°çš„é©±åŠ¨yum install kmod-nvidia yum -y install epel-releaseyum -y install dkms ç¼–è¾‘grubæ–‡ä»¶ 123456vim /etc/default/grub# åœ¨â€œGRUB_CMDLINE_LINUXâ€ä¸­æ·»åŠ # rd.driver.blacklist=nouveau nouveau.modeset=0# éšåç”Ÿæˆé…ç½®grub2-mkconfig -o /boot/grub2/grub.cfg åˆ›å»ºblacklist 12345# /lib/modprobe.d/dist-blacklist.confvim /etc/modprobe.d/blacklist.conf# æ·»åŠ , å±è”½é»˜è®¤å¸¦æœ‰çš„nouveaublacklist nouveau æ›´æ–°é…ç½® (é‡å»ºinitramfs image) 12mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.imgdracut /boot/initramfs-$(uname -r).img $(uname -r) é‡å¯ 1reboot ç¡®è®¤ç¦ç”¨äº†nouveau 12# è‹¥æ— è¾“å‡ºåˆ™ç¦ç”¨æˆåŠŸlsmod | grep nouveau å¼€å§‹å®‰è£…é©±åŠ¨ 1sh NVIDIA-Linux-x86_64-410.79.run æŸ¥çœ‹æ˜¾å¡ä½¿ç”¨æƒ…å†µ 12# nvidia-smiæ˜¯ç”¨æ¥æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µçš„nvidia-smi 1.3 å¸¸ç”¨æŒ‡ä»¤ æŸ¥çœ‹å®‰è£…çš„nvidiaæ¨¡å—å‘½ä»¤ 12# rpm -qarpm -qa|grep -i nvid|sort æŸ¥çœ‹ç³»ç»Ÿç‰ˆæœ¬ 123456uname -alsb_release -a# ä»¥ä¸‹æ–¹æ³•é€‚ç”¨äºRedHat,CentOScat /etc/redhat-release 2. å®‰è£…CUDA æ£€æµ‹GPUæ˜¯å¦OK 1lspci | grep -i nvidia ä¸‹è½½cuda 12# https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=CentOS&amp;target_version=7&amp;target_type=runfilelocalwget https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda_10.0.130_410.48_linux å®‰è£…gccã€g++ç¼–è¯‘å™¨ 12yum install gccyum install g++ å®‰è£…kernel-develå’Œkernel-headers 12yum install kernel-develyum install kernel-headers èµ‹äºˆæƒé™ï¼Œå’Œå®‰è£… 12chmod 755 cuda_9.0.176_384.81_linux.run./cuda_9.0.176_384.81_linux.run å®‰è£…è¿‡ç¨‹ 1234567891011121314install NVIDIA Accelerated Graphics Driver ... -no # ä¸éœ€è¦å†å®‰è£…æ˜¾å¡é©±åŠ¨Install OpenGL ... -no # è¿™ä¸ªç»„ä»¶ä¸€å®šä¸è¦å®‰è£…ï¼Œå¦‚æœå‡ºç°é€‰æ‹©ï¼Œé€‰ NoInstall CUDA 9.0 Toolkit ... -yesToolkit location /usr/local/cuda-9.0 ... [Enter] # é»˜è®¤å®‰è£…ä½ç½®å³å¯Install a symbolic link at ... -yesInstall CUDA 9.0 Samples ... -yes # å®‰è£…ä¸€äº›ä¾‹ç¨‹Enter CUDA Samples Location ... [Enter] # é»˜è®¤å®‰è£…ä½ç½®...Finished Driver : Not SelectedToolkit : Installed in /usr/local/cuda-9.0Samples : Installed in /root, but missing recommended libraries***WARNING: Incomplete installation! å‡ºç°è­¦å‘Šï¼Œå®‰è£…ä¸å®Œå…¨ï¼Œä½†æ˜¯æ²¡æœ‰å½±å“ é…ç½®ç³»ç»Ÿè·¯å¾„ 123456# vim /etc/profile...export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH# source /etc/profile ; ä½¿ç¯å¢ƒå˜é‡ç«‹å³ç”Ÿæ•ˆ CUDAæµ‹è¯• 12345678# cuda ; æŒ‰ä¸¤ä¸‹ tab é”®cudafe cuda-gdb cuda-install-samples-9.0.shcudafe++ cuda-gdbserver cuda-memcheck# nvcc --versionnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2017 NVIDIA CorporationBuilt on Fri_Sep__1_21:08:03_CDT_2017Cuda compilation tools, release 9.0, V9.0.176 3. å®‰è£…cuDNN ä¸‹è½½ 1234567wget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/cudnn-9.0-linux-x64-v7.4.2.24.tgz# Runtime Libyarywget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/RHEL7_3-x64/libcudnn7-devel-7.4.2.24-1.cuda9.0.x86_64.rpm# Developer Librarywget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/RHEL7_3-x64/libcudnn7-devel-7.4.2.24-1.cuda9.0.x86_64.rpm 4. Anacondaå®‰è£… ä¸‹è½½ 123# æ‰€æœ‰å†å²ç‰ˆæœ¬ï¼šhttps://repo.continuum.io/archive/# anaconda 3.5.1wget https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh è·¯å¾„åŠ å…¥åˆ°ç³»ç»Ÿé…ç½®æ–‡ä»¶ä¸­ ç”Ÿæˆç§˜é’¥ 12345from notebook.auth import passwdpasswd()Enter password: kn88888Verify password: kn88888'sha1:25f17927d3a2:74b098a697b5e7bd76931badf00df1000eccfe17' ç”Ÿæˆjupyterçš„é…ç½®æ–‡ä»¶ 12[root@s3-aidc-dl-prod-01 ~]# jupyter notebook --generate-configWriting default config to: /root/.jupyter/jupyter_notebook_config.py ä¿®æ”¹é…ç½®æ–‡ä»¶ 12345678910111213vim /root/.jupyter/jupyter_notebook_config.py# çº¦174è¡Œc.NotebookApp.ip = &apos;localhost&apos;# çº¦240è¡Œc.NotebookApp.port = 8888# çº¦220è¡Œc.NotebookApp.open_browser = False# çº¦229è¡Œc.NotebookApp.password = &apos;sha1:25f17927d3a2:74b098a697b5e7bd76931badf00df1000eccfe17&apos; ä½¿ç”¨sshç§æœ‰é€šé“ 12ssh -N -f -L [remote port]:localhost:[local port] -p [ssh port] -l [username] [å…¬ç½‘IP]ssh -N -f -L 7864:10.105.50.178:8888 fanchangxun@182.254.211.45 -p36000 å®‰è£…JupyterHub 1conda install -c conda-forge jupyterhub jupyter é…ç½® 5. References centos7 nvidiaé©±åŠ¨å®‰è£…å¤±è´¥é—®é¢˜çš„è§£å†³åŠæ³• æˆ‘çš„AIä¹‹è·¯ â€”â€” ä»è£¸æœºæ­å»ºGPUç‰ˆæœ¬çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒ ã€å·²è§£å†³ã€‘æ±‚åŠ©ï¼ŒNå¡å®˜æ–¹é©±åŠ¨å®‰è£…ä¸èƒ½ CentOS 7 å®‰è£… Cuda çš„ç»å† Centos7 é‡è£…è‹±ä¼Ÿè¾¾æ˜¾å¡é©±åŠ¨+Cuda9.0+Cudnn7 centos7ç³»ç»Ÿ å®‰è£…NVIDIAæ˜¾å¡é©±åŠ¨ CentOS 7.0å®‰è£…Nvidiaé©±åŠ¨ Centos7 ä¸Šä¸ºkaldiå®‰è£…/å¸è½½ nvidiaæ˜¾å¡é©±åŠ¨å’ŒCUDA CUDAä¹‹nvidia-smiå‘½ä»¤è¯¦è§£ CentOS 7 å®‰è£… NVIDIA æ˜¾å¡é©±åŠ¨å’Œ CUDA Toolkit DKMSç®€ä»‹ 6. Error Logs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114ERROR: Failed to run `/usr/sbin/dkms build -m nvidia -v 410.79 -k 3.10.0-862.11.6.el7.x86_64`: Error! echo Your kernel headers for kernel 3.10.0-862.11.6.el7.x86_64 cannot be found at /lib/modules/3.10.0-862.11.6.el7.x86_64/build or /lib/modules/3.10.0-862.11.6.el7.x86_64/source. # é”™è¯¯ï¼šGoogle failed to run /usr/sbin/dkms centos# è§£å†³æ–¹æ¡ˆsudo apt-get install dkms build-essential linux-headers-$(uname -r)ERROR: Failed to install the kernel module through DKMS. No kernel module was installed; please try installing again without DKMS, or check the DKMS logs for more information.ERROR: Installation has failed. Please see the file &apos;/var/log/nvidia-installer.log&apos; for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.nvidia-installer log file &apos;/var/log/nvidia-installer.log&apos;creation time: Mon Dec 24 15:39:55 2018installer version: 410.79PATH: /data1/anaconda3/bin:/usr/local/services/java/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/services/go/bin:/root/binnvidia-installer command line: ./nvidia-installerUnable to load: nvidia-installer ncurses v6 user interfaceUsing: nvidia-installer ncurses user interface-&gt; Detected 28 CPUs online; setting concurrency level to 28.-&gt; Installing NVIDIA driver version 410.79.-&gt; There appears to already be a driver installed on your system (version: 410.79). As part of installing this driver (version: 410.79), the existing driver will be uninstalled. Are you sure you want to continue? (Answer: Continue installation)-&gt; Would you like to register the kernel module sources with DKMS? This will allow DKMS to automatically build a new module, if you install a different kernel later. (Answer: Yes)-&gt; Installing both new and classic TLS OpenGL libraries.-&gt; Installing both new and classic TLS 32bit OpenGL libraries.-&gt; Install NVIDIA&apos;s 32-bit compatibility libraries? (Answer: No)-&gt; Will install GLVND GLX client libraries.-&gt; Will install GLVND EGL client libraries.-&gt; Skipping GLX non-GLVND file: &quot;libGL.so.410.79&quot;-&gt; Skipping GLX non-GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLX non-GLVND file: &quot;libGL.so&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so.410.79&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so.1&quot;-&gt; Uninstalling the previous installation with /usr/bin/nvidia-uninstall.Looking for install checker script at ./libglvnd_install_checker/check-libglvnd-install.sh executing: &apos;/bin/sh ./libglvnd_install_checker/check-libglvnd-install.sh&apos;... Checking for libglvnd installation. Checking libGLdispatch... Checking libGLdispatch dispatch table Checking call through libGLdispatch All OK libGLdispatch is OK Checking for libGLX libGLX is OK Checking for libEGL libEGL is OK Checking entrypoint library libOpenGL.so.0 Checking call through libGLdispatch Checking call through library libOpenGL.so.0 All OK Entrypoint library libOpenGL.so.0 is OK Checking entrypoint library libGL.so.1 Checking call through libGLdispatch Checking call through library libGL.so.1 All OK Entrypoint library libGL.so.1 is OK Found libglvnd libraries: libGL.so.1 libOpenGL.so.0 libEGL.so.1 libGLX.so.0 libGLdispatch.so.0 Missing libglvnd libraries: libglvnd appears to be installed.Will not install libglvnd libraries.-&gt; Skipping GLVND file: &quot;libOpenGL.so.0&quot;-&gt; Skipping GLVND file: &quot;libOpenGL.so&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so.1.2.0&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so.1&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so.2.1.0&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so.2&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so&quot;-&gt; Skipping GLVND file: &quot;libGLdispatch.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1.7.0&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libGL.so&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1.1.0&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libEGL.so&quot;-&gt; Skipping GLVND file: &quot;./32/libGL.so.1.7.0&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libGL.so&quot;-&gt; Skipping GLVND file: &quot;./32/libGLX.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so&quot;-&gt; Skipping GLVND file: &quot;./32/libEGL.so.1.1.0&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libEGL.so&quot;Will install libEGL vendor library config file to /usr/share/glvnd/egl_vendor.d-&gt; Searching for conflicting files:-&gt; done.-&gt; Installing &apos;NVIDIA Accelerated Graphics Driver for Linux-x86_64&apos; (410.79): executing: &apos;/usr/sbin/ldconfig&apos;...-&gt; done.-&gt; Driver file installation is complete.-&gt; Installing DKMS kernel module:ERROR: Failed to run `/usr/sbin/dkms build -m nvidia -v 410.79 -k 3.10.0-862.11.6.el7.x86_64`: Error! echoYour kernel headers for kernel 3.10.0-862.11.6.el7.x86_64 cannot be found at/lib/modules/3.10.0-862.11.6.el7.x86_64/build or /lib/modules/3.10.0-862.11.6.el7.x86_64/source.-&gt; error.ERROR: Failed to install the kernel module through DKMS. No kernel module was installed; please try installing again without DKMS, or check the DKMS logs for more information.ERROR: Installation has failed. Please see the file &apos;/var/log/nvidia-installer.log&apos; for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.","tags":[{"name":"Centos7","slug":"Centos7","permalink":"http://chenson.cc/tags/Centos7/"},{"name":"Nvida","slug":"Nvida","permalink":"http://chenson.cc/tags/Nvida/"},{"name":"GPU","slug":"GPU","permalink":"http://chenson.cc/tags/GPU/"}]},{"title":"ç”¨æˆ·ç”»åƒåˆæ¢","date":"2019-02-27T02:33:21.000Z","path":"2019/02/27/ç”¨æˆ·ç”»åƒåˆæ¢/","text":"1. ä»€ä¹ˆæ˜¯ç”¨æˆ·ç”»åƒä»æµ·é‡çš„ç”¨æˆ·æ•°æ®ä¸­ï¼Œå»ºæ¨¡æŠ½è±¡å‡ºæ¯ä¸ªç”¨æˆ·çš„å±æ€§æ ‡ç­¾ä½“ç³»ï¼Œè¿™äº›æ ‡ç­¾ä½“ç³»é€šå¸¸è¦æœ‰ä¸€å®šçš„å•†ä¸šä»·å€¼ï¼Œæœ‰æ˜ç¡®çš„å±‚çº§åˆ’åˆ†å’Œå¯ç†è§£æ€§ï¼Œèƒ½å¤Ÿç›´æ¥æŒ‡å¯¼å•†ä¸šè¿è¥ã€‚å¸¸è§çš„æ ‡ç­¾åŒ…æ‹¬ä½†ä¸é™äºï¼šäººå£å±æ€§ã€è¡Œä¸ºè½¨è¿¹ã€ç”¨æˆ·åˆ†ç¾¤ã€ç”Ÿæ´»åœºæ™¯ã€æ¶ˆè´¹åå¥½ç­‰ï¼ˆå¦‚å›¾ï¼‰ã€‚é€šå¸¸æ ¹æ®ä¸åŒçš„é¢†åŸŸï¼Œä¸åŒçš„ä¸šåŠ¡åœºæ™¯å’Œä¸åŒçš„å•†ä¸šç›®æ ‡ï¼Œç”¨æˆ·ç”»åƒçš„å±‚çº§åˆ’åˆ†å·®å¼‚ä¼šæ¯”è¾ƒå¤§ã€‚ ä¸¾ä¸ªä¾‹å­ï¼Œä»¥ç”µå•†çš„ç”¨æˆ·ç”»åƒå’Œåˆ¸å•†çš„ç”¨æˆ·ç”»åƒä¸ºä¾‹ï¼Œå¦‚å›¾ã€‚ 2. å¦‚ä½•å»ºç«‹ç”¨æˆ·ç”»åƒæ ‡ç­¾ä½“ç³»é€šå¸¸å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼Œç¬¬ä¸€ç±»å°±æ˜¯æœ‰æ˜ç¡®çš„æ ‡ç­¾ï¼Œç„¶åè¿›è¿‡ç­–ç•¥çš„ç»Ÿè®¡åˆ†æç›´æ¥å¾—åˆ°ï¼Œæœ€ç®€å•çš„ä¾‹å­ï¼Œé€šå¸¸å°±æ˜¯ç”¨æˆ·çš„äººå£å±æ€§è¿™ç±»çš„é™æ€ç‰¹å¾ã€‚å¦å¤–ä¸€ç§æ˜¯æ²¡æœ‰æ˜ç¡®çš„æ•°æ®æ ‡ç­¾ï¼Œéœ€è¦æˆ‘ä»¬å¯¹ç”¨æˆ·çš„è¿™äº›æ•°æ®é€šè¿‡æœºå™¨å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååŸºäºæ¨¡å‹åšå‡ºé¢„æµ‹å¾—å‡ºç”¨æˆ·çš„æ ‡ç­¾ï¼Œæ¯”å¦‚è¯´ç”¨æˆ·çš„å…´è¶£çˆ±å¥½ã€‚ å¦‚æœè¯´æ¯”è¾ƒç½‘ç«™çš„è®¨è®ºå¦‚ä½•å»ºç«‹ç”¨æˆ·ç”»åƒçš„æ ‡ç­¾ä½“ç³»ï¼Œæˆ‘è§‰å¾—å¯èƒ½éœ€è¦ä»æ•°æ®çš„æœ€ä¸Šæ¸¸è¯´åˆ°æœ€ä¸‹æ¸¸ï¼Œå¯å¤§è‡´åˆ†æˆ8ä¸ªéƒ¨åˆ†æ¥è¯´ã€‚ æ•°æ®æ”¶é›†ï¼Œæ¸…æ´—ï¼Œæ±‡æ€»åˆ°æ•°ä»“ ç‰¹å¾å®½è¡¨çš„è®¡ç®—ï¼Œä»æ•°æ®åˆ°ç‰¹å¾æ˜¯æ•´ä¸ªç¯å¢ƒä¸­è‡³å…³é‡è¦çš„ä¸€æ­¥ï¼Œè¿™ä¸ªå†³å®šäº†ç»“æœçš„ä¸Šé™ã€‚ ç‰¹å¾å®½è¡¨çš„ç›‘æ§ï¼šå±•ç¤ºå„ç§ç»Ÿè®¡æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šmax-minå€¼ï¼Œå‡å€¼ï¼Œæ–¹å·®ï¼Œè¦†ç›–ç‡ã€ç¼ºå¤±ç‡ç­‰ï¼Œä¸”éœ€è¦æŒ‰å°æ—¶ã€æŒ‰å¤©æŒ‰å‘¨ç­‰è¿›è¡Œç»Ÿè®¡å±•ç¤ºã€‚ç„¶åè®¾å®šå¯¹åº”çš„æŠ¥è­¦é˜ˆå€¼ï¼Œå¦‚æœè¶…è¿‡è¿™ä¸ªé˜ˆå€¼çš„è¯å°±åšå‡ºé¢„è­¦é€šçŸ¥ç›¸å…³äººå‘˜ã€‚ æ¨¡å‹è®­ç»ƒé¢„æµ‹ï¼Œå…¶ä¸­ä¼šåŒ…æ‹¬ç‰¹å¾é€‰æ‹©ã€æ¨¡å‹è®­ç»ƒã€æ•ˆæœè¯„ä¼°ã€ä¾‹è¡Œé¢„æµ‹ç­‰ã€‚ åº”ç”¨æ¥å£ å•†ä¸šåº”ç”¨ 3. å»ºç«‹è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„é—®é¢˜3.1 å¦‚ä½•é€‰å–ç”¨æˆ·çš„å”¯ä¸€æ ‡è¯†3.2 å¦‚ä½•ç®¡ç†ç‰¹å¾åº“3.3 å¦‚ä½•è®¡ç®—ç‰¹å¾ ç¦»çº¿ç‰¹å¾ Spark + Hive å®æ—¶ç‰¹å¾ Kafka + Stome Kafka é«˜ååé‡çš„åˆ†å¸ƒå¼æ¶ˆæ¯ç³»ç»Ÿ Stomeæ˜¯Twitterå¼€æºçš„åˆ†å¸ƒå¼å®æ—¶å¤§æ•°æ®å¤„ç†æ¡†æ¶ 3.4 å°æ ·æœ¬é—®é¢˜ é‡‡æ · ä¸‹é‡‡æ · è¿‡é‡‡æ · æ¬ é‡‡æ · åŠç›‘ç£å­¦ä¹  å·²æ ‡æ³¨æ•°æ®å’Œæ— æ ‡æ³¨æ•°æ®ï¼šLu Learning (Learning from Labeled and Unlableled Exampels) æ¯ä¸ªç±»åˆ«çš„æ•°æ®åŒ…å«å°‘é‡çš„å·²æ ‡æ³¨æ•°æ®å’Œå¤§é‡çš„æ— æ ‡æ³¨æ•°æ® æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨çš„EMç®—æ³•ï¼Œä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„è¿­ä»£ç®—æ³• Co-Training è‡ªå­¦ä¹ ç®—æ³• Transductive SVM ç›´æ¨å¼æ”¯æŒå‘é‡æœº åŸºäºå›¾çš„æ–¹æ³• æ­£ä¾‹å’Œæ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ ï¼šPu Learning (Learning from Positive and Unlableled Exampels) å‡è®¾è§£å†³çš„æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œä½†è®­ç»ƒæ•°æ®æ˜¯ç”±æ­£ä¾‹å’Œæ— æ ‡æ³¨æ•°æ®ç»„æˆï¼Œå…¶ä¸­ä¸å«æœ‰åä¾‹æ•°æ® ç›´æ¥æ³• å•æ ·æœ¬å»ºæ¨¡ V-SVMã€One-Class-SVMã€Biased-SVM ä¸¤æ­¥æ³• å‘ç°å¯é çš„è´Ÿä¾‹ï¼šSPyã€1DNã€NBç®—æ³•ç­‰ åŸºäºä¸Šè¿°çš„æ•°æ®ï¼Œç”¨å¸¸ç”¨çš„æ¨¡å‹è®­ç»ƒ ä»¥ä¸Šçš„æ¨¡å‹å’Œæ–¹æ³•éƒ½æœ‰æ¯”è¾ƒå¤šçš„å‡è®¾ 3.5 æ ¹æ®ä¸åŒä¸šåŠ¡éœ€æ±‚ï¼Œé€‰æ‹©ä¸åŒçš„æ¨¡å‹åŒä¸€ä¸ªéœ€æ±‚ï¼Œå¯èƒ½å¯ä»¥ç”¨ä¸åŒçš„æ¨¡å‹æ¥å®ç°ã€‚ ç»Ÿè®¡é—®é¢˜ å¹³æ»‘/å½’ä¸€ ç­–ç•¥æ‰“åˆ† è¯­ä¹‰åˆ†æ åˆ†è¯ LDA DNN â€¦ â€¦ é«˜ç»´åå¥½ CF MF/FM å›å½’é—®é¢˜ å›å½’æ ‘ æ™®é€šæœ€å°äºŒä¹˜ Lasso Ridge SVR â€¦ â€¦ åˆ†ç±»é—®é¢˜ äºŒåˆ†ç±» LR / SVM / NB RF / AdaBoost / GBDT / XGBoost â€¦ â€¦ å¤šåˆ†ç±» RF/ GBDT / XGBoost MaxEn One V.S All èšç±»é—®é¢˜ K-Means é«˜æ–¯æ··åˆ One-Class-SVM EM â€¦ â€¦ å…³è”è§„åˆ™ Aprior FP-growth 4. References - æ•°æ®æŒ–æ˜ä¹‹éƒ¨åˆ†ç›‘ç£å­¦ä¹  ç¤¾äº¤å¹³å°èˆ†æƒ…åˆ†æé¡¹ç›®çš„æ€»ç»“å’Œæ„Ÿæƒ³ï¼ˆLUå­¦ä¹ ï¼ŒEMï¼ŒKNNï¼‰(äºŒ)","tags":[{"name":"ç”¨æˆ·ç”»åƒ","slug":"ç”¨æˆ·ç”»åƒ","permalink":"http://chenson.cc/tags/ç”¨æˆ·ç”»åƒ/"}]},{"title":"hexoé”™è¯¯è®°å½•","date":"2019-02-23T06:30:57.000Z","path":"2019/02/23/hexoé”™è¯¯è®°å½•/","text":"è®²å®è¯ï¼Œæˆ‘ä¸æ˜¯ç‰¹åˆ«äº†è§£hexoã€‚å¤§æ¦‚å°±çŸ¥é“æ˜¯ä¸ªç”¨æ¥ç”Ÿæˆé™æ€çš„åšå®¢åŒ…orå·¥å…·ï¼Ÿä¾èµ–äº†npm(node package manager)ã€‚ç„¶åå¹³å¸¸ä½¿ç”¨çš„æ—¶å€™æœ‰æ—¶ä¼šé‡åˆ°ä¸€äº›æ¯”è¾ƒå¤´å¤§çš„é—®é¢˜ï¼Œåœ¨æ²¡æœ‰åŠ¨hexoå’Œnpmçš„æƒ…å†µä¸‹ã€‚ æ‰€ä»¥ä¸ºäº†ä»¥åé‡åˆ°ç±»ä¼¼çš„é—®é¢˜æ–¹ä¾¿å¤„ç†ï¼Œå¯¹hexoçš„é”™è¯¯å¤„ç†åšäº†ä¸€ä¸ªç®€å•çš„è®°å½•(æŒç»­æ›´æ–°ing)ã€‚ 1. hexo not found in XXX é—®é¢˜æè¿° å¤§æ¦‚æ˜¯ä¸€ä¸ªå¤šæœˆæ²¡å†™æ–‡ç« äº†ï¼Œåœ¨è¿è¡Œhexo n titleçš„æ—¶å€™æŠ¥äº†è¿™ä¸ªé”™è¯¯ï¼Œè¯´æ˜¯æœ¬åœ°æœ¬åœ°æ²¡æœ‰å‘ç°hexoï¼Ÿï¼Ÿï¼Ÿåæ­£æ˜¯ä¸€è„¸æ‡µé€¼ è§£å†³æ–¹æ¡ˆ å°è¯•è¿‡æœªæˆåŠŸçš„æ–¹æ³• æ›´æ–°npmå’Œnode å¸è½½nodeï¼Œé‡è£… æœ€ç»ˆæˆåŠŸçš„æ–¹æ³• 12345678# å¼ºåˆ¶åˆ é™¤åšå®¢ä¸‹node_modulesæ–‡ä»¶å¤¹$ rm -rf node_modules# é‡è£…$ npm install # ç»“æœå¤±è´¥ï¼Œæ”¹ç”¨äº†æ·˜å®çš„é•œåƒåæˆåŠŸ$ npm install cnpm -g --registry=https://registry.npm.taobao.org æ³¨æ„ï¼Œç»å¸¸ä¼šæŠ¥permission deniedçš„Errorï¼Œæ”¹ç”¨sudoè¿è¡ŒæŒ‡ä»¤ã€‚ ä¹‹åhexo n titleå¯ä»¥æ­£å¸¸ç”Ÿæˆæ–°çš„æ–‡ç« ã€‚ é—®é¢˜æè¿° è§£å†³æ–¹æ¡ˆ æŒ‰ç…§ä¸Šé¢ç»™çš„è§£å†³æ–¹æ¡ˆå¹¶æ²¡å•¥ç”¨ï¼Œä¾æ—§æŠ¥è¿™ä¸ªé”™è¯¯ã€‚ å°è¯•è¿‡ä½†æœªæˆåŠŸçš„æ–¹æ³• å…³äºHex Local hexo not found in XXXçš„ä¸€ç§è§£å†³æ–¹æ¡ˆ 1sudo npm install -g hexo ä¾æ—§æŠ¥é”™ï¼ï¼ï¼ æœäº†å¦å¤–ä¸€ä¸ªæ–¹æ³•ï¼ŒHexo s æç¤º Local hexo not found in XXX çš„è§£å†³æ–¹æ³•ï¼Œè¿˜æ˜¯æŠ¥ä¸€å¼€å§‹Næ—©å‰çš„ä¸€ä¸ªé”™è¯¯ã€‚node.jsçœŸæ˜¯ä¸ªå‘å•Šã€‚ å®åœ¨æ²¡åŠæ³•ï¼Œå®šä½ä¸€ä¸‹å…·ä½“é—®é¢˜å‡ºç°åœ¨å“ª 1hexo g --debug æœäº†åŠå¤©ä¹Ÿæ²¡è§£å†³æ–¹æ¡ˆã€‚ 2. hexo Error: Cannot find module â€˜highlight.js é—®é¢˜æè¿° æ¥è‡ªä¸Šä¸€ä¸ªé—®é¢˜çš„å­é—®é¢˜ï¼Œè™½ç„¶å¯ä»¥ç”Ÿæˆæ–°çš„æ–‡ç« ï¼Œä½†æ˜¯åœ¨gendrateç”Ÿæˆé™æ€åšå®¢æ–‡ä»¶çš„æ—¶å€™å¤±è´¥äº†ï¼ŒæŠ¥äº†æ²¡æœ‰highlights.jsè¿™ä¸ªæ¨¡å—çš„é”™è¯¯ã€‚ çœ‹é—®é¢˜åº”è¯¥æ˜¯npmæ²¡æœ‰å®‰è£…ä¸Šhighlightsè¿™ä¸ªæ¨¡å—ã€‚ è§£å†³æ–¹æ¡ˆ å°è¯•è¿‡æœªæˆåŠŸçš„æ–¹æ³• å®‰è£…highlights.jsæ¨¡å— 1$ npm install highlight.js --save æŠ¥é”™å…³äºfseventsæ¨¡å—çš„é—®é¢˜ï¼ŒçœŸçš„æ˜¯å¤´å¤§ğŸ˜¡ï¼ï¼ï¼ é‡è£…npmçš„æ¨¡å— 123$ sudo npm cache clean -f$ sudo npm install -g n$ sudo n stable å®‰è£…fseventsæ¨¡å— 123$ sudo npm i fsevents # å¤±è´¥$ sudo npm install -D fsevents # å¤±è´¥ åŒæ ·æŠ¥é”™Error: EACCES: permission denied, mkdir &#39;/Users/Chenson/Dropbox/PROJECTS/myBlog/node_modules/fsevents/build 1$ sudo npm install -g fsevents # å¤±è´¥ æ³¨æ„åˆ°é‡Œé¢æœ‰ä¸€ä¸ªæŠ¥é”™ npm WARN babel-eslint@10.0.1 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself. ä¼¼ä¹æ‰‹åŠ¨å®‰è£…å‡ ä¸ªåŒ…ï¼Œè¯•ä¸€ä¸‹çœ‹ 1$ sudo npm install eslint å¦ˆäº†ä¸ªé¸¡ï¼ŒåˆæŠ¥ä¸Šé¢åŒæ ·çš„é”™è¯¯ï¼Œè¦ç–¯äº†ğŸ˜ ã€‚ åŒæ ·è¿˜æ˜¯é”™è¯¯gyp ERR! stack Error: EACCES: permission denied, mkdir node_modules/fsevents/build æœ€ç»ˆæˆåŠŸçš„æ–¹æ³• çœ‹ä¸Šé¢å°è¯•çš„æ–¹æ³•ï¼Œå…³é”®åœ¨äºæƒé™é—®é¢˜ï¼Œå°è¯•æ”¹äº†node_moulesçš„æƒé™ä¸º777ï¼Œä¾æ—§å¤±è´¥ã€‚ åé¢å‚è€ƒç­”æ¡ˆInstallation error: permission denied for node-sasï¼Œé‡Œé¢ä»¥ä¸ºè€å“¥è¯´è®¾ç½®nodeçš„æƒé™ä¸ºroot 12345# éå¸¸é‡è¦ï¼ï¼ï¼$ npm config set user root$ sudo npm install eslint $ sudo npm install fsevents$ sudo npm install highlight.js --save æœ€åè¿™ä¿©å±…ç„¶å®‰è£…ä¸Šäº†ï¼ ç„¶åä¹Ÿå¯ä»¥æ­£å¸¸è¿è¡ŒæŒ‡ä»¤hexo gï¼Œç”Ÿæˆé™æ€åšå®¢ã€‚ 3. æ¸²æŸ“å¤æ‚å…¬å¼ é—®é¢˜æè¿° ç›®å‰hexoå¯¹äºç®€å•çš„å…¬å¼æ¸²æŸ“æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä½†å¤æ‚ç‚¹çš„ä¼¼ä¹å°±æ¸²æŸ“å¤±è´¥äº†ï¼Œå…·ä½“å¦‚ä¸‹ã€‚ å°è¯•ä½†æœªæˆåŠŸçš„è§£å†³æ–¹æ¡ˆã€æœ‰bugã€‘ Hexoæ„å»ºblogæ—¶æ¸²æŸ“LaTeXæ•°å­¦å…¬å¼çš„é—®é¢˜ è§£å†³æ–¹æ¡ˆæ˜¯æ›´æ¢markdownå¼•æ“ è§£å†³æ–¹æ¡ˆå°±ä¸æ”¾äº†ï¼Œå› ä¸ºæˆ‘è¯•äº†ä¹‹åï¼Œhexoå°±æŒ‚äº†TATã€‚ 4. è¡¨æ ¼æ˜¾ç¤ºé—®é¢˜ é—®é¢˜æè¿° æœ‰æ—¶å€™åœ¨blogé‡Œé¢æ’å…¥è¡¨æ ¼ï¼Œmarkdownä¼¼ä¹æ— æ³•æ¸²æŸ“æˆåŠŸï¼Œå…·ä½“å¦‚ä¸‹å›¾ è§£å†³æ–¹æ¡ˆ è¡¨æ ¼æ— æ³•æ­£ç¡®æ˜¾ç¤º #16","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chenson.cc/tags/hexo/"}]},{"title":"è¯»ä¹¦ç¬”è®°-å¢é•¿é»‘å®¢","date":"2019-02-23T05:19:54.000Z","path":"2019/02/23/è¯»ä¹¦ç¬”è®°-å¢é•¿é»‘å®¢/","text":"è¯„ä»·ï¼šâ˜…â˜…â˜…â˜…â˜†æ–­æ–­ç»­ç»­çœ‹äº†å‰å‡ ç« ï¼Œæ²¡æœ‰é¢„è®¡çš„é‚£ä¹ˆå¥½çš„æ„Ÿè§‰ã€‚ä¸€æ–¹é¢å¯èƒ½æ˜¯è¿˜æ²¡å…¨éƒ¨çœ‹å®Œçš„åŸå› ï¼Œå¦ä¸€æ–¹é¢å¯èƒ½ä¸€ç›´æ˜¯ä»ç®—æ³•çš„è§’åº¦æ¥æœŸå¾…è¿™æœ¬ä¹¦ï¼Œè€Œéè¿è¥ï¼Œæ‰€ä»¥å’Œé¢„è®¡æœ‰äº›è½å·®ã€‚ä¸”è‡ªå·±å¥½åƒæ€»æ˜¯å¸Œæœ›èƒ½ä»ä¹¦ä¸­è·å–ä¸€äº›æ‰€è°“çš„å¹²è´§ï¼Œç±»ä¼¼å®è·µé¡¹ç›®çš„ä¸€äº›ç»éªŒï¼Œæ€»æ˜¯æƒ³å€Ÿé‰´åˆ«äººçš„ç»éªŒèµ°ä¸€äº›æ·å¾„ï¼Œæœ‰æ—¶å€™è§‰å¾—è¿™æ ·æŒºä¸å¥½çš„ï¼Œæœ‰ç‚¹å¤ªæ€¥åŠŸè¿‘åˆ©äº†ï¼Œåè€Œå¿˜è®°äº†å»æ€è€ƒï¼Œç‰¹åˆ«æ˜¯è¿™ç§å’Œä¸šåŠ¡æ¯”è¾ƒè´´è¿‘çš„é¡¹ç›®ï¼Œå¾ˆéœ€è¦è‡ªå·±å»æ€è€ƒã€‚","tags":[{"name":"è¯»ä¹¦ç¬”è®°","slug":"è¯»ä¹¦ç¬”è®°","permalink":"http://chenson.cc/tags/è¯»ä¹¦ç¬”è®°/"},{"name":"æ™ºèƒ½è¥é”€","slug":"æ™ºèƒ½è¥é”€","permalink":"http://chenson.cc/tags/æ™ºèƒ½è¥é”€/"}]},{"title":"è¯»ä¹¦ç¬”è®°-æ•°æ®æŒ–æ˜ä¸æ•°æ®åŒ–è¿è¥å®æˆ˜","date":"2019-02-23T05:15:17.000Z","path":"2019/02/23/è¯»ä¹¦ç¬”è®°-æ•°æ®æŒ–æ˜ä¸æ•°æ®åŒ–è¿è¥å®æˆ˜/","text":"è¯„ä»·ï¼šâ˜…â˜…â˜…â˜…â˜†ç¬¬å…«ç« 8.2 æ•°æ®æŠ½æ ·","tags":[{"name":"è¯»ä¹¦ç¬”è®°","slug":"è¯»ä¹¦ç¬”è®°","permalink":"http://chenson.cc/tags/è¯»ä¹¦ç¬”è®°/"},{"name":"æ™ºèƒ½è¥é”€","slug":"æ™ºèƒ½è¥é”€","permalink":"http://chenson.cc/tags/æ™ºèƒ½è¥é”€/"}]},{"title":"è¯»ä¹¦ç¬”è®°-é‡æ„å¤§æ•°æ®ç»Ÿè®¡","date":"2019-02-23T04:41:05.000Z","path":"2019/02/23/è¯»ä¹¦ç¬”è®°-é‡æ„å¤§æ•°æ®ç»Ÿè®¡/","text":"è¯„ä»·ï¼šâ˜…â˜…â˜…â˜†â˜†1. å•å˜é‡åŸºæœ¬ç»Ÿè®¡é‡1.1 å¸¸è§çš„æ•°æ®ç»Ÿè®¡é‡ å‡å€¼ \\overline X = \\frac 1 n \\sum_{i=1}^n X_i æ–¹å·® S_n^2 = \\frac 1 {n-1} \\sum_{i=1}^n (X_i - \\overline X)^2åˆ†æ¯ä¸ºn-1ï¼Œåˆ™ç§°ä¹‹ä¸º$S_n^2$çš„è‡ªç”±åº¦(degree of freedom) é—®é¢˜ï¼šä¸æ˜¯å¾ˆç†è§£è‡ªç”±åº¦è¿™ä¸ªæ¦‚å¿µ æ ‡å‡†å·® - Standard Deviation S_n = \\sqrt {\\frac 1 {n-1} \\sum_{i=1}^n (X_i - \\overline X)^2} å˜å¼‚ç³»æ•° - Coefficient of Variation, CV \\frac {S_n} {\\overline X}æ ‡å‡†å·®å’Œå‡å€¼çš„æ¯”å€¼ï¼Œä¸€ä¸ªæ— é‡çº²çš„é‡ï¼Œç”¨æ¥åˆ»ç”»æ•°æ®çš„ç›¸å¯¹åˆ†æ•£æ€§ æ ‡å‡†è¯¯ - Standard Error \\frac {\\sqrt {\\frac 1 {(n-1)} \\sum_{i=1}^n(X_i - \\overline X)^2}} {\\sqrt n} = \\sqrt{\\frac 1 {n(n-1)}\\sum_{i=1}^n(X_i - \\overline X)^2}æ ‡å‡†è¯¯æ˜¯ç”±æ ·æœ¬çš„æ ‡å‡†å·®é™¤ä»¥æ ·æœ¬ä¸ªæ•°çš„å¼€æ–¹è®¡ç®—å¾—åˆ°çš„ï¼Œæ ‡å‡†è¯¯ä»£è¡¨çš„æ˜¯æ ·æœ¬å‡æ•°å¯¹æ€»ä½“å‡æ•°çš„ç›¸å¯¹è¯¯å·®ã€‚ ké˜¶åŸç‚¹çŸ© - Moment \\frac 1 n \\sum_{i=1}^nX_i^kk=1çš„æ—¶å€™ï¼Œå°±æ˜¯å‡å€¼ ké˜¶ä¸­å¿ƒçŸ© \\frac 1 n \\sum_{i=1}^nï¼ˆX_i - \\overline Xï¼‰^kk=1çš„æ—¶å€™ï¼Œä¸€é˜¶ä¸­å¿ƒçŸ©æ’ç­‰äº0 ä¸‹é¢ä¸¤ä¸ªç»Ÿè®¡é‡å¯ç”¨æ¥æè¿°æ ·æœ¬æ•°æ®åˆ†å¸ƒå½¢çŠ¶ ååº¦ - Skewness \\frac {\\sqrt n \\sum_{i=1}^n(X_i - \\overline X)^3} {[\\sum_{i=1}^n(X_i - \\overline X)^2]^{\\frac 3 2}}ååº¦æ˜¯ç”¨æ¥åˆ»ç”»æ•°æ®å¯¹ç§°æ€§çš„æŒ‡æ ‡ã€‚å…³äºå‡å€¼å¯¹ç§°çš„æ•°æ®ï¼Œå…¶ååº¦ç³»æ•°ä¸º0ï¼› è‹¥å·¦ä¾§æ•°æ®æ¯”è¾ƒåˆ†æ•£ï¼Œåˆ™ååº¦ç³»æ•°å°äº0ï¼› è‹¥å³ä¾§æ•°æ®æ¯”è¾ƒåˆ†æ•£ï¼Œåˆ™ååº¦ç³»æ•°å¤§äº0ã€‚ å³°åº¦ - Kurotsis \\frac { n \\sum_{i=1}^n(X_i - \\overline X)^4} {[\\sum_{i=1}^n(X_i - \\overline X)^2]^{2}} - 3å³°åº¦å¯ä»¥æè¿°æ ·æœ¬æ•°æ®åˆ†å¸ƒå½¢æ€ç›¸å¯¹äºæ­£æ€åˆ†å¸ƒçš„é™¡ç¼“æˆéƒ½ è‹¥ Kurotsis = 0ï¼Œåˆ™ä¸çŠ¶æ€åˆ†å¸ƒçš„ç›¸åŒé™¡ç¼“ç¨‹åº¦ç›¸åŒï¼› è‹¥ Kurotsis &gt; 0ï¼Œåˆ™æ¯”çŠ¶æ€åˆ†å¸ƒçš„é«˜å³°æ›´åŠ é™¡å³­ï¼Œè¡¨ç°ä¸ºå°–é¡¶å³°ï¼› è‹¥ Kurotsis &gt; 0ï¼Œåˆ™æ¯”çŠ¶æ€åˆ†å¸ƒçš„é«˜å³°æ˜¾å¾—å¹³ç¼“ï¼Œè¡¨ç°ä¸ºå¹³é¡¶å³°ã€‚ 1.2 å…³äºä¸Šé¢å…¬å¼çš„æ€è€ƒ ğŸ¤”çœ‹åˆ°ä¸Šé¢çš„ä¸€äº›å…¬å¼ï¼Œå‘ç°ä¸€ä¸ªå…±åŒç‚¹ï¼ŒåŸºæœ¬éƒ½æ˜¯ä¼šç”¨åˆ°å‡å€¼ $\\overline X$ï¼Œè¿™å°±æ„å‘³ç€æˆ‘ä»¬åœ¨è®¡ç®—è¿™äº›æ•°æ®çš„æ—¶å€™ï¼Œå¾—å…ˆæ‰«æä¸€éæ‰€æœ‰æ•°æ®ï¼Œæ±‚å‡ºå‡å€¼åå†ç¬¬äºŒæ¬¡æ‰«ææ‰€æœ‰çš„æ•°æ®ï¼Œè®¡ç®—è¿™äº›ç»Ÿè®¡é‡ã€‚ å¦‚æœæˆ‘ä»¬æƒ³æ‰«ä¸€æ¬¡æ•°æ®ä¹‹åï¼Œå°±ç®—å‡ºè¿™äº›ç»Ÿè®¡å˜é‡çš„è¯ï¼Œå°±å¾—å®æ—¶è®¡ç®—å‡ºå½“å‰çš„å‡å€¼ï¼Œä½†å¾ˆå¤šçš„ç»Ÿè®¡é‡éƒ½æ˜¯éœ€è¦å®æ—¶å°†å½“å‰çš„å˜é‡ä¸$\\overline X$ åšå·®ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿåœ¨éœ€è¦æ—¶å€™è¿™äº›ç»Ÿè®¡å˜é‡çš„æ—¶å€™ï¼Œå†è®¡ç®—å‡ºè¿™äº›å€¼çš„è¯ï¼Œæ˜¯ä¸æ˜¯å°±å¯ä»¥è¾¹Xçš„å‡å€¼è¾¹ç¼“å­˜æ‰€æœ‰çš„Xå°±å¯æ‰«æä¸€éè®¡ç®—å‡ºç»“æœã€‚ä½†è¿™æ˜¯è¿™æ ·çš„ä»£ä»·å°±æ˜¯å¾—ç¼“å­˜æ‰€æœ‰çš„Xï¼Œå¯ä»¥è¯´æ˜¯éå¸¸å¤§çš„ï¼Œç‰¹åˆ«æ˜¯å¯¹æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™ã€‚èƒ½ä¸èƒ½æœ‰å…¶ä»–çš„æ–¹å¼å‘¢ï¼Ÿå…¶å®å¯ä»¥æ€è€ƒä¸€ä¸‹ä¸‹é¢è¿™ä¸ªå…¬å¼çš„å˜åŒ– \\sum_{i=1}^n = (X_i - \\overline X)^2 \\\\ = \\sum_{i=1}^n (X_i^2 - 2X_i\\overline X + \\overline X^2) \\\\ = \\sum_{i=1}^n X_i^2 - 2\\overline X \\sum_{i=1}^n X_i + \\sum_{i=1}^n \\overline X^2 \\\\ = \\sum_{i=1}^n X_i^2 - 2\\overline X Â· n\\overline X + n\\overline X^2 \\\\ = \\sum_{i=1}^b X_i^2 - n \\overline X^2 æœ€ç»ˆæˆ‘ä»¬å…¶å®å¯ä»¥åœ¨æ‰«ä¸€éæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å‡º$\\sum_{i=1}^n X_i^2$ã€ $\\overline X$ å’Œn åŒç†å¯ä»¥æ¨å¯¼å…¬å¼ æ‰€ä»¥æ ¹æ®ä¸Šé¢çš„æ€è€ƒç»“æœï¼ŒåŸºæœ¬æˆ‘ä»¬å°±å¯ä»¥å°†å¸¸è§çš„å‡ ä¸ªæ•°æ®ç»Ÿè®¡é‡ç»™è®¡ç®—å‡ºæ¥ã€‚åŸºæœ¬æˆ‘ä»¬åœ¨æ‰«ä¸€éçš„æ—¶å€™ï¼Œè®¡ç®—å‡ºå¦‚ä¸‹å‡ ä¸ªå€¼ï¼Œnï¼Œsumï¼Œsum2ï¼Œsum3ï¼Œsum4ç­‰ count = n \\\\ sum = \\sum_{i=1}^n X_i \\\\ sum2 = \\sum_{i=1}^n X_i^2 \\\\ sum3 = \\sum_{i=1}^n X_i^3 \\\\ sum4 = \\sum_{i=1}^n X_i^41.3 ä¸éœ€è¦å°±å¯è®¡ç®—çš„æ¬¡åºç»Ÿè®¡é‡ ç›®çš„ï¼šè¯»å–ä¸€éæ•°æ®åŒæ—¶å°±è·å–æœ€å¤§çš„å‰Kä¸ªå€¼å’Œæœ€å°çš„å‰Kä¸ªå€¼ è§£å†³æ–¹æ¡ˆï¼šåŒæ—¶ç»´æŠ¤ä¸¤ä¸ªé˜Ÿåˆ— ä¸€ä¸ªæ˜¯æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ï¼Œæ¯æ¬¡è¯»å–åˆ°æ–°çš„æ•°æ®åŠ åˆ°é˜Ÿåˆ—ä¸­ï¼Œç„¶ååªä¿ç•™å‰kä¸ªæ•°ï¼Œå°†å¤šå‡ºçš„æ•°æ®åˆ é™¤ å¦ä¸€ä¸ªé˜Ÿåˆ—æ˜¯æŒ‰ç…§ä»å°åˆ°å¤§çš„é¡ºåºæ’åˆ—ï¼Œæ–¹æ³•åŒä¸Šï¼Œä½†æ˜¯ä¿ç•™å‰kä¸ªæ•°ã€‚ 1.4 åŸºäºé¢‘æ•°ä¿¡æ¯è®¡ç®—æ¬¡åºç»Ÿè®¡é‡ ç›®çš„ï¼šç»Ÿè®¡æŸä¸ªæ•°æ®çš„çš„é¢‘ç‡ï¼ˆå‡è®¾å·²ç»æ±‚å‡ºé¢‘ç‡ï¼‰ï¼Œå¹¶æ’åº ä¾‹å­ï¼š Items Counts A 4 B 2 C 1 D 3 è§£å†³æ–¹æ¡ˆï¼š å¯¹é¢‘ç‡æŒ‰å…ƒç´ ç”±å°åˆ°å¤§çš„é¡ºåºæ’åºï¼ˆè‹¥å·²æ’å¥½åºï¼Œåˆ™å¯å¿½ç•¥ï¼‰ï¼Œè®¾æ’å¥½åºçš„é¢‘ç‡å¯¹ä¸ºï¼ˆitems[i], counts[i]ï¼‰ | Itmes | Counts | | ----- | ------ | | C | 1 | | B | 2 | | D | 3 | | A | 4 | å¯¹æ¯ä¸ªå…ƒç´ items[i] è®¡ç®— cntScan[i], å…¶æ•°å€¼ä¸Šç­‰äºcount[0], count[1], â€¦, count[i-1]çš„å’Œ,å®é™…æ„ä¹‰æ˜¯å°†å…¨éƒ¨æ•°æ®ç”±å°åˆ°å¤§æ’åºï¼Œç¬¬ä¸€æ¬¡å‡ºç°å…ƒç´ items[i]æ—¶å¯¹åº”çš„ä½ç½®ã€‚ å¯¹äºè¦å»çš„åˆ†ä½æ•°ï¼Œé¦–å…ˆæ˜¯è®¡ç®—å‡ºå¯¹åº”æ’å¥½åºçš„æ•°åˆ—çš„ä½ç½®kï¼Œåˆ©ç”¨cntScanï¼Œä½¿ç”¨äºŒåˆ†æ³•æœç´¢å‡ºå¯¹åº”çš„æ•°æ®å…ƒç´ ç¼–å·jï¼Œåˆ™items[j]å³ä¸ºæ‰€æ±‚ã€‚ 1.5 ä¸­ä½æ•°ã€ä¼—æ•°å’Œå‡å€¼ä¹‹é—´çš„å…³ç³» å½“æ•°æ®ä¸ºå¯¹ç§°åˆ†å¸ƒçš„æ—¶å€™ï¼Œè¿™ä¸‰ä¸ªæ•°æ˜¯ç›¸åŒçš„ å½“æ•°æ®ä¸ºå•å³°çš„åæ€åˆ†å¸ƒçš„æ—¶å€™ï¼Œåˆ™åœ¨å‡å€¼çš„ä¸¤ä¾§ï¼Œæ•°æ®çš„ä¸ªæ•°ä¸åŒã€‚ä¸­ä½æ•°ä¼šåœ¨æ•°æ®ä¸ªæ•°æ ¡å¯¹çš„ä¸€ä¾§ï¼Œå‡å€¼ä½äºå¹³è¡¡ç‚¹ã€‚ é˜…è¯»ä¸­ï¼ŒæŒç»­æ›´æ–° â€¦ â€¦ 2. References","tags":[{"name":"Big Data","slug":"Big-Data","permalink":"http://chenson.cc/tags/Big-Data/"},{"name":"è¯»ä¹¦ç¬”è®°","slug":"è¯»ä¹¦ç¬”è®°","permalink":"http://chenson.cc/tags/è¯»ä¹¦ç¬”è®°/"}]},{"title":"Sparkç¬”è®°-åº”ç”¨ç¯‡","date":"2019-01-24T09:36:58.000Z","path":"2019/01/24/Sparkç¬”è®°-åº”ç”¨ç¯‡/","text":"","tags":[]},{"title":"Sparkç¬”è®°-åŸºç¡€æ“ä½œç¯‡","date":"2019-01-24T09:36:35.000Z","path":"2019/01/24/Sparkç¬”è®°-åŸºç¡€æ“ä½œç¯‡/","text":"æœ¬ç¯‡ä¸»è¦æ˜¯ä¸ºäº†ç†Ÿæ‚‰PySparkçš„åŸºæœ¬æ“ä½œï¼Œæ²¡å•¥ç‰¹åˆ«çš„å†…å®¹ã€‚ 1. å‡†å¤‡å·¥ä½œ å¯åŠ¨Hadoop å¯åŠ¨Spark åœ¨Jupyterä¸­æµ‹è¯•PySpark 123456789101112131415161718import pysparkfrom pyspark import SparkContext as scfrom pyspark import SparkConffrom pyspark.sql import SparkSession# conf = SparkConf().setAppName('spark-basic-oprations') \\ .setMaster('local[*]')sc = sc.getOrCreate(conf)# spark = SparkSession.builder \\ .appName('spark-basic-oprations') \\ .master('local[*]') \\ .config('spark.some.config.option', 'some-value') \\ .config('spark.debug.maxToStringFields', '50') \\ .getOrCreate() 2. RDDç›¸å…³æ“ä½œ2.1 RDDçš„åˆ›å»º è¯»å–å¤–éƒ¨æ•°æ® æ•°æ®ä½äºHDFSä¸Š æ•°æ®ä½äºæœ¬åœ°ç£ç›˜ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ‰€æœ‰çš„worker nodesä¸Šéƒ½éœ€è¦èƒ½å¤Ÿè®¿é—®è¿™ä¸ªæœ¬åœ°è·¯å¾„ ä»£ç  12345678# æ•°æ®ä½äºHDFSä¸Šhdfs_file = '/user/hadoop/google_news.txt'# æ•°æ®ä½äºæœ¬åœ°ç£ç›˜ï¼Œæ ¼å¼ä¸º(file:// + æœ¬åœ°çš„ç»å¯¹è·¯å¾„)local_file = 'file:///usr/local/spark/data/google_news.txt'lines = sc.textFile(hdfs_file)lines = sc.textFile(local_file) textFileçš„æ•°æ®å¯ä»¥æ˜¯æ–‡ä»¶åã€ç›®å½•å’Œå‹ç¼©æ–‡ä»¶ç­‰ã€‚åé¢è¿˜å¯ä»¥è·Ÿä¸€ä¸ªå‚æ•°ï¼Œç”¨æ¥æŒ‡å®šåˆ†åŒºçš„æ•°ç›® è¯»å–Driverä¸­å·²ç»å­˜åœ¨çš„æ•°æ®é›†åˆ 123nums = list(10)data = sc.parallelize()data.collect() åˆ›å»ºKey-Values-Pair RDD è¿™ä¸ªè·ŸMapReduceä¸­çš„Key-Vlaueé”®å€¼å¯¹æ˜¯å·®ä¸å¤šçš„ï¼Œç»å¸¸éƒ½æ˜¯ (key, value) è¿™æ ·çš„æ ¼å¼ã€‚ä¹‹æ‰€ä»¥ä»‹ç»è¿™æ ·çš„ä¸€ç§æ ¼å¼ï¼Œæ˜¯å› ä¸ºè¿™ä¸ªä¹‹åå¾ˆå¤šéƒ½ä¼šå°†æ•°æ®è½¬æ¢æˆç±»ä¼¼çš„æ•°æ®æ ¼å¼ï¼Œç„¶åå¯¹RDDåšå„ç§transformationåšç»Ÿè®¡ï¼Œæ¯”å¦‚reduceByKeyã€groupByKeyç­‰ã€‚ 123lines = sc.textFile(local_file)kv_pairs = lines.flatMap(lambda x : x.split(' ')).map(lambda x : (x, 1))kv_pairs.take(10) 2.2 RDDå¸¸ç”¨æ“ä½œ Transformation map(func): å°†æ¯ä¸ªå…ƒç´ ä¼ é€’åˆ°å‡½æ•°funcä¸­ï¼Œå¹¶å°†ç»“æœè¿”å›ä¸ºä¸€ä¸ªæ–°çš„æ•°æ®é›† filter(func): ç­›é€‰å‡ºæ»¡è¶³å‡½æ•°funcçš„å…ƒç´ ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®é›† 1234567lines = sc.textFile(local_file)# åˆ¤æ–­æ˜¯å¦ä¸ºç©ºè¡Œï¼Œè¿™é‡Œè¿”å›å€¼ä¸ºTrue/Falselines.filter(lambdas x : len(x) &gt; 0).collect()# è¿‡æ»¤éç©ºè¡Œçš„ï¼Œè¿™é‡Œè¿”å›éç©ºçš„å†…å®¹lines.filter(lambdas x : len(x) &gt; 0).collect() flatMap(func): ä¸map()ç›¸ä¼¼ï¼Œä½†æ¯ä¸ªè¾“å…¥å…ƒç´ éƒ½å¯ä»¥æ˜ å°„åˆ°0æˆ–å¤šä¸ªè¾“å‡ºç»“æœ reduceByKey(func): åº”ç”¨äº (key, value) é”®å€¼å¯¹çš„æ•°æ®é›†æ—¶ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„ (key, value) å½¢å¼çš„æ•°æ®é›†ï¼Œå…¶ä¸­çš„æ¯ä¸ªå€¼æ˜¯å°†æ¯ä¸ªkeyä¼ é€’åˆ°å‡½æ•°funcä¸­è¿›è¡Œèšåˆã€‚å¦‚æœä¹‹å‰æœ‰äº†è§£è¿‡MapReduceçš„è¯ï¼Œå‘ç°å’Œé‡Œé¢çš„Reduceæ“ä½œä¼šæ¯”è¾ƒç›¸ä¼¼ã€‚ groupByKey(): åº”ç”¨äº (key, value) é”®å€¼å¯¹çš„æ•°æ®é›†æ—¶ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„(key, iterable)å½¢å¼çš„æ•°æ®é›†ï¼Œè¿™ä¸ªå’ŒreduceByKeyæœ‰é‚£ä¹ˆä¸€ç‚¹åŒºåˆ«ï¼Œå…¥å‚ä¸éœ€è¦funcã€‚ sortByKey(): å­—é¢æ„æ€ï¼Œæ ¹æ®é”®æ’åºçš„RDD mapValues(func): å…¥å‚çš„funcåªåº”ç”¨äºvalueä¸Šï¼Œä¸å¯¹keyåšä¿®æ”¹ã€‚ join: è¿™ä¸ªæ“ä½œæ¥è‡ªäºdatabaseä¸­ï¼Œæ‰€ä»¥è¿˜å¯ä»¥ç»†åˆ†ä¸ºå†…è¿æ¥()ã€å³å¤–è¿æ¥(rightOuterJoin)å’Œå·¦å¤–è¿æ¥(leftOuterJoin)ç­‰ã€‚ Action count(): è¿”å›æ•°æ®é›†ä¸­çš„å…ƒç´ ä¸ªæ•° collect(): ä»¥æ•°ç»„çš„å½¢å¼è¿”å›æ•°æ®é›†ä¸­çš„æ‰€æœ‰å…ƒç´  first(): è¿”å›æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´  take(): ä»¥æ•°ç»„çš„å½¢å¼è¿”å›æ•°æ®é›†ä¸­çš„å‰nä¸ªå…ƒç´  reduce(func): é€šè¿‡å‡½æ•°func(è¾“å…¥ä¸¤ä¸ªå‚æ•°å¹¶è¿”å›ä¸€ä¸ªå€¼)èšåˆæ•°æ®é›†ä¸­çš„å…ƒç´  1234567lines = sc.textFile(local_file)# è®¡ç®—å•è¡Œçš„é•¿åº¦lines_length = lines.map(lambdas x : len(x))# è®¡ç®—æ€»é•¿åº¦total_length = lines_length.reduce(lambda a, b : a + b) foreach(func): å°†æ•°æ®é›†ä¸­çš„æ¯ä¸ªå…ƒç´ ä¼ é€’åˆ°å‡½æ•°funcä¸­è¿è¡Œ 2.3 RDDæŒä¹…åŒ– éæŒä¹…åŒ– 1234567891011data = [\"Hadoop\", \"Spark\", \"Hive\"]rdd = sc.parallelize(data)# éæŒä¹…åŒ–ï¼Œé‡å¤è®¡ç®—äº†## è¡ŒåŠ¨æ“ä½œï¼Œè§¦å‘ä¸€æ¬¡çœŸæ­£ä»å¤´åˆ°å°¾çš„è®¡ç®—print(rdd.count()) # &gt;&gt; 3## è¡ŒåŠ¨æ“ä½œï¼Œè§¦å‘ä¸€æ¬¡çœŸæ­£ä»å¤´åˆ°å°¾çš„è®¡ç®—print(', '.join(rdd.collect())) # &gt;&gt;&gt; Hadoop, Spark, Hive æŒä¹…åŒ– persist() persistæ¥å—ä¸€ä¸ªå‚æ•° MEMORY_ONLY è¡¨ç¤ºå°†RDDä½œä¸ºååºåˆ—åŒ–çš„å¯¹è±¡å­˜å‚¨äºJVMä¸­ï¼Œå¦‚æœå†…å­˜ä¸è¶³ï¼Œå°±è¦æŒ‰ç…§LRUåŸåˆ™æ›¿æ¢ç¼“å­˜ä¸­çš„å†…å®¹ MEMORY_AND_DISK è¡¨ç¤ºå°†RDDä½œä¸ºååºåˆ—åŒ–çš„å¯¹è±¡å­˜å‚¨åœ¨JVMä¸­ï¼Œå¦‚æœå†…å­˜ä¸è¶³ï¼Œè¶…å‡ºçš„åˆ†åŒºå°†ä¼šè¢«å­˜æ”¾åœ¨ç¡¬ç›˜ä¸Š ä»£ç  12345678910111213# æŒä¹…åŒ–data = [\"Hadoop\", \"Spark\", \"Hive\"]rdd = sc.parallelize(data)# ä¼šè°ƒç”¨persist(MEMORY_ONLY)ï¼Œä½†æ˜¯ï¼Œè¯­å¥æ‰§è¡Œåˆ°è¿™é‡Œï¼Œå¹¶ä¸ä¼šç¼“å­˜rddï¼Œè¿™æ˜¯rddè¿˜æ²¡æœ‰è¢«è®¡ç®—ç”Ÿæˆï¼Œæ¥ä¸‹æ¥ç¬¬ä¸€æ¬¡actionçš„æ—¶å€™æ‰ä¼šç¼“å­˜ç»“æœrdd.cache() # ç¬¬ä¸€æ¬¡è¡ŒåŠ¨æ“ä½œï¼Œè§¦å‘ä¸€æ¬¡çœŸæ­£ä»å¤´åˆ°å°¾çš„è®¡ç®—ï¼Œè¿™æ—¶æ‰ä¼šæ‰§è¡Œä¸Šé¢çš„rdd.cache()ï¼ŒæŠŠè¿™ä¸ªrddæ”¾åˆ°ç¼“å­˜ä¸­print(rdd.count()) # &gt;&gt;&gt; 3# ç¬¬äºŒæ¬¡è¡ŒåŠ¨æ“ä½œï¼Œä¸éœ€è¦è§¦å‘ä»å¤´åˆ°å°¾çš„è®¡ç®—ï¼Œåªéœ€è¦é‡å¤ä½¿ç”¨ä¸Šé¢ç¼“å­˜ä¸­çš„rddprint(', '.join(rdd.collect())) # &gt;&gt;&gt; Hadoop, Spark, Hive ç§»é™¤æŒä¹…åŒ– å¯ä»¥ä½¿ç”¨unpersist()æ–¹æ³•æ‰‹åŠ¨åœ°æŠŠæŒä¹…åŒ–çš„RDDä»ç¼“å­˜ä¸­ç§»é™¤ 2.4 åˆ†åŒº RDDæ˜¯å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œé€šå¸¸RDDå¾ˆå¤§ï¼Œä¼šè¢«åˆ†æˆå¾ˆå¤šä¸ªåˆ†åŒºï¼Œåˆ†åˆ«ä¿å­˜åœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚RDDåˆ†åŒºçš„ä¸€ä¸ªåˆ†åŒºåŸåˆ™æ˜¯ä½¿å¾—åˆ†åŒºçš„ä¸ªæ•°å°½é‡ç­‰äºé›†ç¾¤ä¸­çš„CPUæ ¸å¿ƒ(core)æ•°ç›®ã€‚ å¯¹äºä¸åŒçš„Sparkéƒ¨ç½²æ¨¡å¼è€Œè¨€(åˆ†åˆ«æ˜¯æœ¬åœ°æ¨¡å¼ã€Standaloneæ¨¡å¼ã€YARNæ¨¡å¼ã€Mesosæ¨¡å¼)ï¼Œéƒ½å¯ä»¥é€šè¿‡è®¾ç½®spark.default.parallelismè¿™ä¸ªå‚æ•°çš„å€¼ï¼Œæ¥é…ç½®é»˜è®¤çš„åˆ†åŒºæ•°ç›®ï¼Œä¸€èˆ¬è€Œè¨€ï¼š æœ¬åœ°æ¨¡å¼ï¼šé»˜è®¤ä¸ºæœ¬åœ°æœºå™¨çš„CPUæ•°ç›®ï¼Œè‹¥è®¾ç½®äº†local[N],åˆ™é»˜è®¤ä¸ºNï¼› Apache Mesosï¼šé»˜è®¤çš„åˆ†åŒºæ•°ä¸º8ï¼› Standaloneæˆ–YARNï¼šåœ¨â€œé›†ç¾¤ä¸­æ‰€æœ‰CPUæ ¸å¿ƒæ•°ç›®æ€»å’Œâ€å’Œâ€œ2â€äºŒè€…ä¸­å–è¾ƒå¤§å€¼ä½œä¸ºé»˜è®¤å€¼ï¼› 2.5 å…±äº«å˜é‡åœ¨Sparkåˆ†å¸ƒå¼è®¡ç®—ä¸­ï¼Œä¸åŒçš„è®¡ç®—èŠ‚ç‚¹ä¸Šçš„ä¸åŒäººç‰©ä¼šè°ƒç”¨åŒä¸€ä¸ªå‡½æ•°ï¼Œé‡Œé¢ä¼šè®¾è®¡åˆ°ä¸€äº›æ•°æ®çš„ä½¿ç”¨ã€‚ å¦‚æœæ¯ä¸ªèŠ‚ç‚¹åªæ˜¯è¿›è¡Œè¯»æ“ä½œï¼Œé‚£ä¹ˆå¯ä»¥å°†è¯¥ä»½æ•°æ®æ‹·è´åˆ°æ¯å°èŠ‚ç‚¹çš„ä¸åŒä»»åŠ¡ä¸Šã€‚ å¦‚æœéœ€è¦åœ¨ä¸åŒèŠ‚ç‚¹ä¸åŒä»»åŠ¡ä¹‹é—´ï¼Œè¯¥å˜é‡ï¼Œå³å­˜åœ¨è¯»å†™çš„æ“ä½œã€‚æ­¤æ—¶åº”è¯¥è®¾ç½®è¿™äº›å˜é‡ä¸ºå…±äº«å˜é‡ã€‚ æ ¹æ®ä»¥ä¸Šéœ€æ±‚ï¼ŒSparkä¸­æä¾›äº†ä¸¤ç§ç±»å‹çš„å˜é‡ å¹¿æ’­å˜é‡(broadcast variables) æ¯å°æœºå™¨ç¼“å­˜ä¸€ä¸ªåªè¯»å˜é‡(éæ¯ä¸ªä»»åŠ¡)ï¼Œè¿™æ ·å¯ä»¥éå¸¸é«˜æ•ˆçš„ç»™æ¯ä¸ªèŠ‚ç‚¹æä¾›ä¸€ä¸ªå¤§çš„è¾“å…¥æ•°æ®é›†çš„å‰¯æœ¬ã€‚ éœ€è¦è¢«ç¼“å­˜çš„æ•°æ®åœ¨è¢«å¹¿æ’­åï¼Œä¼šå…ˆç»“æœåºåˆ—åŒ–ï¼Œç„¶ååœ¨è¢«è°ƒç”¨çš„æ—¶å€™å†è¿›è¡Œååºåˆ—åŒ–ã€‚ Question ä¸ºä»€ä¹ˆhaodoopéœ€è¦åºåˆ—åŒ–æ•°æ®ï¼Ÿ å½“éœ€è¦å¤šæ¬¡å¤šåœ°æ–¹ä½¿ç”¨çš„æ—¶å€™ï¼Œå¤šæ¬¡çš„ååºåˆ—åŒ–æ˜¯å¦å½±å“æ•ˆç‡ï¼Ÿ å…·ä½“æ“ä½œ 1234broadcast_data = sc.broadcast([1, 2, 3])broadcast_data.value&gt;&gt;&gt; [1, 2, 3] ä¸Šé¢çš„SparkContext.broadcast(v)æ˜¯åœ¨æ™®é€šçš„å˜é‡ä¸­åˆ›å»ºä¸€ä¸ªå¹¿æ’­å˜é‡ ç´¯åŠ å™¨(accumulators) 3. DataFrameç›¸å…³æ“ä½œ3.1 Dataframe v.s. RDD RDD RDDæ˜¯åˆ†å¸ƒå¼çš„Javaå¯¹è±¡çš„é›†å’Œï¼Œå¦‚ä¸Šå›¾çš„RDDæ˜¯ä»¥ä¸ºPersonä¸ºç±»å‹å‚æ•°ï¼Œä½†Personç±»çš„å†…éƒ¨ç»“æœå¯¹äºRDDæ˜¯ä¸å¯çŸ¥çš„(æœ‰æ ¡éªŒæœºåˆ¶ä¹ˆï¼Ÿ) DataFrame DFçœ‹ç€å’ŒPythonä¸­çš„DataFrameå·®ä¸å¤šï¼Œæ˜¯ä»¥RDDä¸ºåŸºç¡€çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œä¹Ÿæ˜¯åˆ†å¸ƒå¼çš„Rowå¯¹è±¡çš„é›†å’Œï¼Œæä¾›äº†æ•°æ®çš„è¯¦ç»†ç»“æ„ä¿¡æ¯ï¼Œå³Schemaã€‚ åŒæ—¶RDDä¹Ÿæ˜¯æƒ°æ€§æœºåˆ¶çš„ï¼Œåªè®°å½•äº†å„ç§çš„é€»è¾‘è½¬æ¢çš„è·¯çº¿ï¼Œå³DAGå›¾ã€‚ç­‰éœ€è¦è®¡ç®—ç»“æœçš„æ—¶å€™ï¼Œå†è®¡ç®—ã€‚ 3.2 åˆ›å»ºDataFrame ç›´æ¥åˆ›å»º 12345678910111213141516171819202122232425262728spark = SparkSession.builder \\ .appName('spark-basic-oprations') \\ .master('local[*]') \\ .config('spark.some.config.option', 'some-value') \\ .config('spark.debug.maxToStringFields', '50') \\ .getOrCreate() df = spark.read.text(file)df.show()# å’Œ Python ä¸­çš„DataFrameå¾ˆç›¸ä¼¼&gt;&gt;&gt; +--------------------+ | value| +--------------------+ |As of 2013, Googl...| | | |The service cover...| | | |On December 1, 20...| | | |The layout of Goo...| | | |On July 14, 2011,...| | | |Additionally in J...| | | |In June 2017, the...| +--------------------+ RDDè½¬æ¢æˆDataFrame åˆ©ç”¨åå°„æœºåˆ¶æ¨æ–­åŒ…å«ç‰¹å®šç±»å‹å¯¹è±¡çš„RDDçš„Schemaï¼Œé€‚ç”¨å¯¹å·²çŸ¥çš„æ•°æ®ç»“æ„ ä½¿ç”¨ç¼–ç¨‹æ¥å£ï¼Œæ„é€ ä¸€ä¸ªSchemaå¹¶å°†å…¶åº”ç”¨åœ¨å·²çŸ¥çš„RDDä¸Š 3.2 DataFrameå¸¸ç”¨æ“ä½œä¸‹é¢è¿™äº›æ“ä½œå’ŒPythonä¸­é˜ŸDataFrame 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# æ‰“å°æ¨¡å¼ä¿¡æ¯df.printSchema()&gt;&gt;&gt; root |-- age: long (nullable = true) |-- name: string (nullable = true) # é€‰æ‹©å¤šåˆ—df.select(df.name,df.age + 1).show()&gt;&gt;&gt; +-------+---------+ | name|(age + 1)| +-------+---------+ |Michael| null| | Andy| 31| | Justin| 20| +-------+---------+ # æ¡ä»¶è¿‡æ»¤df.filter(df.age &gt; 20 ).show()&gt;&gt;&gt; +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ # åˆ†ç»„èšåˆdf.groupBy(\"age\").count().show()&gt;&gt;&gt; +----+-----+ | age|count| +----+-----+ | 19| 1| |null| 1| | 30| 1| +----+-----+ # æ’åºdf.sort(df.age.desc()).show()&gt;&gt;&gt; +----+-------+ | age| name| +----+-------+ | 30| Andy| | 19| Justin| |null|Michael| +----+-------+ # å¤šåˆ—æ’åºdf.sort(df.age.desc(), df.name.asc()).show()&gt;&gt;&gt; +----+-------+ | age| name| +----+-------+ | 30| Andy| | 19| Justin| |null|Michael| +----+-------+ # å¯¹åˆ—è¿›è¡Œé‡å‘½ådf.select(df.name.alias(\"username\"),df.age).show()&gt;&gt;&gt; +--------+----+ |username| age| +--------+----+ | Michael|null| | Andy| 30| | Justin| 19| +--------+----+ 4. Spark Streaming æ•°æ®æ€»ä½“ä¸Šå¯ä»¥åˆ†ä¸ºé™æ€æ•°æ®å’Œæµæ•°æ®ã€‚å¯¹é™æ€æ•°æ®å’Œæµæ•°æ®çš„å¤„ç†ï¼Œå¯¹åº”ç€ä¸¤ç§æˆªç„¶ä¸åŒçš„è®¡ç®—æ¨¡å¼ï¼šæ‰¹é‡è®¡ç®—å’Œå®æ—¶è®¡ç®—ã€‚ æ‰¹é‡è®¡ç®—ä»¥â€œé™æ€æ•°æ®â€ä¸ºå¯¹è±¡ï¼Œå¯ä»¥åœ¨å¾ˆå……è£•çš„æ—¶é—´å†…å¯¹æµ·é‡æ•°æ®è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œè®¡ç®—å¾—åˆ°æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚Hadoopå°±æ˜¯å…¸å‹çš„æ‰¹å¤„ç†æ¨¡å‹ï¼Œç”±HDFSå’ŒHBaseå­˜æ”¾å¤§é‡çš„é™æ€æ•°æ®ï¼Œç”±MapReduceè´Ÿè´£å¯¹æµ·é‡æ•°æ®æ‰§è¡Œæ‰¹é‡è®¡ç®—ã€‚ æµæ•°æ®å¿…é¡»é‡‡ç”¨å®æ—¶è®¡ç®—ï¼Œå®æ—¶è®¡ç®—æœ€é‡è¦çš„ä¸€ä¸ªéœ€æ±‚æ˜¯èƒ½å¤Ÿå®æ—¶å¾—åˆ°è®¡ç®—ç»“æœï¼Œä¸€èˆ¬è¦æ±‚å“åº”æ—¶é—´ä¸ºç§’çº§ã€‚å½“åªéœ€è¦å¤„ç†å°‘é‡æ•°æ®æ—¶ï¼Œå®æ—¶è®¡ç®—å¹¶ä¸æ˜¯é—®é¢˜ï¼›ä½†æ˜¯ï¼Œåœ¨å¤§æ•°æ®æ—¶ä»£ï¼Œä¸ä»…æ•°æ®æ ¼å¼å¤æ‚ã€æ¥æºä¼—å¤šï¼Œè€Œä¸”æ•°æ®é‡å·¨å¤§ï¼Œè¿™å°±å¯¹å®æ—¶è®¡ç®—æå‡ºäº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œé’ˆå¯¹æµæ•°æ®çš„å®æ—¶è®¡ç®—â€”â€”æµè®¡ç®—ï¼Œåº”è¿è€Œç”Ÿã€‚ å¤„ç†Spark Streamingï¼Œå…¶ä»–æµè®¡ç®—æ¡†æ¶æœ‰ Twitter Storm(å¯å®ç°æ¯«ç§’çº§æµè®¡ç®—)ã€Yahoo S4ã€‚ æµè®¡ç®—å¤„ç†è¿‡ç¨‹åŒ…æ‹¬å¦‚ä¸‹å‡ éƒ¨åˆ†ï¼š æ•°æ®å®æ—¶é‡‡é›† å¸¸è§çš„å¼€æºåˆ†å¸ƒå¼æ—¥å¿—é‡‡é›†ç³»ç»Ÿ Facebookçš„Scribe LinkedInçš„Kafka æ·˜å®çš„TimeTunnel åŸºäºHadoopçš„Chukwaå’ŒFlume å®é™…å®æ—¶è®¡ç®— å®æ—¶æŸ¥è¯¢æœåŠ¡ 4.1 åˆ›å»ºStreamingContextå¯¹è±¡1234567891011121314# åœ¨pysparkä¸­åˆ›å»ºfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextssc = StreamingContext(sc, 1)# ç¼–å†™ä¸€ä¸ªç‹¬ç«‹çš„Spark Streamingç¨‹åºï¼Œè€Œä¸æ˜¯åœ¨pysparkä¸­è¿è¡Œfrom pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextconf = SparkConf().setAppName('spark-streaming').setMaster('local[*]') sc = SparkContext(conf=conf)ssc = StreamingContext(sc, 1) ç›‘å¬æœ¬åœ°æ–‡ä»¶æµ ä¸Šé¢ä¾‹å­ä¸­ï¼Œå¼€å¯sscçš„ç›‘å¬åï¼Œæ¯éš”5ç§’(ä¹‹å‰åˆå§‹åŒ–è®¾å®šçš„å€¼)ä¼šæŸ¥è¯¢ä¸€æ¬¡ï¼Œä½†åªå¤„ç†æ–°å¢çš„æ–‡ä»¶ï¼Œå¯¹å·²ç»å­˜åœ¨çš„å†å²æ–‡ä»¶ä¸åšå¤„ç†ã€‚ ç›‘å¬å¥—æ¥å­—æµ 12345678910111213141516171819202122from __future__ import print_functionimport sysfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextif __name__ == \"__main__\": if len(sys.argv) != 3: print(\"Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;\", file=sys.stderr) exit(-1) sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\") ssc = StreamingContext(sc, 1) lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2])) counts = lines.flatMap(lambda line: line.split(\" \")) \\ .map(lambda word: (word, 1)) \\ .reduceByKey(lambda a, b: a + b) counts.pprint() ssc.start() ssc.awaitTermination() ç›‘å¬RDDé˜Ÿåˆ—æµ 12345678910111213141516171819202122232425import time from pyspark import SparkContextfrom pyspark.streaming import StreamingContext if __name__ == \"__main__\": sc = SparkContext(appName=\"PythonStreamingQueueStream\") ssc = StreamingContext(sc, 1) # Create the queue through which RDDs can be pushed to # a QueueInputDStream rddQueue = [] for i in range(5): rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)] # Create the QueueInputDStream and use it do some processing inputStream = ssc.queueStream(rddQueue) mappedStream = inputStream.map(lambda x: (x % 10, 1)) reducedStream = mappedStream.reduceByKey(lambda a, b: a + b) reducedStream.pprint() ssc.start() time.sleep(6) ssc.stop(stopSparkContext=True, stopGraceFully=True) ç›‘å¬Kafka ç›‘å¬Flume","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.cc/tags/Spark/"},{"name":"PySpark","slug":"PySpark","permalink":"http://chenson.cc/tags/PySpark/"}]},{"title":"Sparkç¬”è®°-åŸç†ç¯‡","date":"2019-01-24T08:28:11.000Z","path":"2019/01/24/Sparkç¬”è®°-åŸç†ç¯‡/","text":"è¿˜åœ¨UNSWè¯»ç ”çš„æ—¶å€™ï¼Œä¿®è¿‡ä¸€é—¨COMP9313ï¼Œå¤§æ•°æ®ç›¸å…³è¯¾ç¨‹çš„æ—¶å€™æœ‰å­¦è¿‡ä¸€ç‚¹Sparkï¼Œä½†é‚£ä¼šåªæ˜¯åšè¿‡ä¸€äº›ç®€å•çš„Projectsï¼Œç®€å•çš„åº”ç”¨è¿‡Sparkå’ŒMapReduceè¿™ä¿©è®¡ç®—æ¡†æ¶ï¼Œå¹¶æ²¡æœ‰å®é™…å·¥ç¨‹ä¸Šçš„ç»éªŒã€‚æœ€è¿‘å› ä¸ºå·¥ä½œçš„åŸå› ï¼Œéœ€è¦ä½¿ç”¨Sparkåšä¸€äº›ç‰¹å¾å·¥ç¨‹çš„è®¡ç®—ï¼Œå› ä¸ºæ•°æ®é‡è¿˜æ˜¯æŒºå¤§çš„ï¼Œå¥½å‡ åäº¿æ¡çš„æ•°æ®å§ã€‚ç”¨SQLçš„è¯ï¼Œä¸æ˜¯é‚£ä¹ˆçš„çµæ´»ï¼Œç”¨Pythonè®¡ç®—è¿™äº›çš„è¯ï¼Œä¼°è®¡è¿˜æ˜¯æŒºå‘›çš„ï¼Œæ‰€ä»¥Sparkæ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚ å…³äºå­¦ä¹ è®¡åˆ’ï¼Œæ‰“ç®—å…ˆç†Ÿæ‚‰ä¸€ä¸‹Sparkçš„ä¸€äº›åŸºæœ¬åŸç†å’ŒPySparkçš„åŸºæœ¬æ“ä½œï¼Œç„¶åæ¥ä¸‹æ¥å†è®°å½•ä¸€ä¸‹åšç‰¹å¾å·¥ç¨‹ç›¸å…³çš„å†…å®¹ã€‚ 1. Sparkçš„è®¾è®¡ä¸è¿è¡ŒåŸç†1.1 Sparkç”Ÿæ€ç³»ç»Ÿ Spark Core - åŸºæœ¬åŠŸèƒ½æ¨¡å—ï¼Œå¦‚å†…å­˜è®¡ç®—ã€ä»»åŠ¡è°ƒåº¦ã€éƒ¨ç½²æ¨¡å¼ã€æ•…éšœæ¢å¤ã€å­˜å‚¨ç®¡ç†ç­‰ Spark SQL - å¯ä»¥ç›´æ¥æ“ä½œSparkä¸­çš„RDD Spark Streaming - ç”¨äºè®¡ç®—æµæ•°æ®çš„ï¼Œæ”¯æŒé«˜ååé‡ã€å¯å®¹é”™å¤„ç†çš„å®æ—¶æµæ•°æ®å¤„ç†ï¼Œå…¶æ ¸å¿ƒæ€è·¯æ˜¯å°†æµå¼è®¡ç®—åˆ†è§£æˆä¸€ç³»åˆ—çŸ­å°çš„æ‰¹å¤„ç†ä½œä¸š Spark MLLib - æœºå™¨å­¦ä¹ çš„åŒ…ï¼Œé«˜çº§çš„æ¥å£æœ‰ML GraphX - ç”¨äºå›¾è®¡ç®— 1.2 SparkåŸºæœ¬æ¦‚å¿µ RDD - å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(Resilient Distributed Dataset)ï¼Œæ˜¯åˆ†å¸ƒå¼å†…å­˜çš„ä¸€ä¸ªæŠ½è±¡æ¦‚å¿µï¼Œæä¾›äº†ä¸€ç§é«˜åº¦å—é™çš„å…±äº«å†…å­˜æ¨¡å‹ï¼ˆé«˜åº¦å—é™ä¸æ˜¯å¾ˆç†è§£ï¼‰ DAG - æœ‰å‘æ— ç¯å›¾ (Directed Acyclic Graph)ï¼Œä¸»è¦åæ˜ äº†RDDä¹‹é—´çš„ä¾èµ–å…³ç³» Executor - è¿è¡Œåœ¨å·¥ä½œèŠ‚ç‚¹(Work Node)ä¸Šçš„ä¸€ä¸ªè¿›ç¨‹ åº”ç”¨ - ç”¨æˆ·ç¼–å†™çš„Sparkåº”ç”¨ç¨‹åº ä»»åŠ¡ - è¿è¡Œåœ¨Excutorä¸Šçš„å·¥ä½œå•å…ƒ ä½œä¸š - ä¸€ä¸ªä½œä¸šåŒ…å«å¤šä¸ªRDDåŠä½œç”¨äºç›¸åº”RDDä¸Šçš„å„ç§æ“ä½œ é˜¶æ®µ - ä½œä¸šçš„åŸºæœ¬è°ƒåº¦å•ä½ï¼Œä¸€ä¸ªä½œä¸šä¼šåˆ†ä¸ºå¤šç»„ä»»åŠ¡ï¼Œæ¯ç»„ä»»åŠ¡è¢«ç§°ä¸ºâ€œé˜¶æ®µâ€ï¼Œæˆ–è€…ä¹Ÿè¢«ç§°ä¸ºâ€œä»»åŠ¡é›†â€ 1.3 Sparkæ¶æ„è®¾è®¡ä¸€ä¸‹ä¸¤éƒ¨åˆ†å°±çœ‹çœ‹å°±å¥½äº†ï¼ŒåˆæœŸä¸ä¸€å®šéœ€è¦å¾ˆæ·±å…¥çš„äº†è§£ã€‚åˆæœŸçš„é‡ç‚¹æˆ‘è§‰å¾—åº”è¯¥æ˜¯å…ˆç†è§£RDDéƒ¨åˆ†ã€‚ 1.4 Sparkè¿è¡ŒåŸºæœ¬æµç¨‹ 1.5 Sparkæ ¸å¿ƒéƒ¨åˆ†ä¹‹ä¸€RDDSparkç›¸æ¯”MapReduceæ›´åŠ é«˜æ•ˆï¼Œå¾ˆé‡è¦ä¸€ç‚¹æ˜¯å»ºç«‹åœ¨ç»Ÿä¸€çš„æŠ½åƒRDDä¹‹ä¸Šã€‚MapReduceåœ¨è®¡ç®—çš„æ—¶å€™ï¼Œä¼šæ‹†åˆ†æˆå¤šä¸ªMapså’ŒReducesï¼Œè¿™äº›éƒ½ä¼šè¢«åˆ†é…åœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šï¼Œæ¯ä¸ªèŠ‚ç‚¹ä¼šå°†è‡ªå·±è®¡ç®—å¥½çš„ä¸­é—´ç»“æœå†™å…¥åˆ°HDFSä¸Šã€‚é‚£ä¹ˆè¿™ä¸ªè¿‡ç¨‹å°±é€ æˆå¾ˆå¤šçš„é‡å¤è®¡ç®—ã€æ•°æ®å¤åˆ¶ã€ç£ç›˜IOå’Œåºåˆ—åŒ–å¼€é”€ã€‚è€Œåœ¨Sparkä¸­ï¼ŒRDDçš„å­˜åœ¨å°±ä¼šé¿å…äº†ä¸Šé¢æåˆ°çš„è¿™å‡ ç‚¹ï¼Œä»è€Œä½¿Sparkæ›´åŠ é«˜æ•ˆã€‚ é‚£ä¹ˆä»€ä¹ˆæ˜¯RDDå‘¢ï¼Ÿä»æœ€åä¸€ä¸ªå•è¯çœ‹ï¼Œè¿™ä¸ªæ˜¯ä¸€ä¸ªDatasetï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªæ•°æ®é›†åˆï¼Œä¸€ä¸ªåˆ†å¸ƒå¼å¯¹è±¡é›†åˆ(ä¸æ˜¯å¾ˆç†è§£)ã€‚åœ¨ä¸€å¼€å§‹æåˆ°RDDçš„æ—¶å€™ï¼Œè¯´äº†RDDæä¾›äº†ä¸€ç§é«˜åº¦å—é™çš„å…±äº«å†…å­˜æ¨¡å‹ï¼Œå¯¹é«˜åº¦å—é™ä¸æ˜¯å¾ˆç†è§£ï¼Œçœ‹äº†ä¸‹å¥½åƒæ˜¯å› ä¸ºè¿™ä¸ªé›†åˆæ˜¯åªè¯»çš„ã€‚å³ä¸€ä¸ªRDDé‡Œé¢æ˜¯å¯ä»¥æœ‰å¤šä¸ªè®°å½•åˆ†åŒºçš„ï¼Œä¹Ÿå¯ä»¥åˆ†å¸ƒåœ¨é›†ç¾¤ä¸­ä¸åŒçš„èŠ‚ç‚¹ä¸Šï¼Œä½†æ˜¯è¿™äº›æ•°æ®é›†æ˜¯åªè¯»çš„ï¼Œä¸èƒ½ç›´æ¥ä¿®æ”¹ã€‚(æœ‰ç‚¹åƒC++ä¸­çš„å…±äº«å†…å­˜æ¨¡å‹) 1.5.1 RDDçš„æ“ä½œ Transformation è½¬æ¢çš„æ„æ€ï¼Œä»å­—é¢ä¸Šç†è§£å°±æ˜¯è¿™ä¸ªè½¬åˆ°é‚£ä¸ªï¼Œé‚£ä¸ªè½¬åˆ°å¦å¤–ä¸€ä¸ªã€‚å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªç±»ä¼¼æµç¨‹å›¾ä¸€æ ·ï¼Œé‚£ä¹ˆæ¯èµ°ä¸€æ­¥å°±æ˜¯æŒ‡å®šäº†å…¶ä¾èµ–å…³ç³»ï¼ŒRDDä¹‹é—´çš„ç›¸äº’ä¾èµ–å…³ç³»(ä¸å¤ªç†è§£ä¸ºä»€ä¹ˆæ˜¯RDDä¹‹é—´ï¼Œè€Œä¸æ˜¯RDDä¹‹å†…)ã€‚ å¸¸è§çš„Transformationæœ‰Mapã€Filterã€Groupbyã€Joinç­‰ï¼Œå…¶è¾“å…¥æ˜¯ä¸€ä¸ªRDDï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªRDDã€‚ Questions: å¦‚æœå¯¹ä¸€ä¸ªRDDè¿ç»­æ‰§è¡ŒMapã€Filterã€Joinç­‰ï¼Œé‚£ä¹ˆä¸­é—´è¿‡ç¨‹ä¼šæœ‰å¤šå°‘ä¸ªRDDå‘¢ï¼Ÿå¦‚æœæœ‰å¤šä¸ªè¿™äº›RDDä¼šä¸ä¼šå¾ˆå å†…å­˜ç­‰ï¼Ÿ Action å­—é¢æ„æ€å°±æ˜¯è¡ŒåŠ¨ï¼Œå¤§è‡´å°±æ˜¯éœ€è¦è¾“å‡ºä¸€ä¸ªç»“æœã€‚å¸¸è§çš„Actionæœ‰Countï¼ŒCollectç­‰ï¼Œå…¶è¾“å…¥æ˜¯ä¸€ä¸ªRDDï¼Œä½†æ˜¯è¾“å‡ºä¸ºä¸€ä¸ªå€¼æˆ–è€…ç»“æœã€‚ 1.5.2 RDDçš„æ‰§è¡Œæµç¨‹ RDDè¯»å…¥å¤–éƒ¨æˆ–è€…å†…å­˜çš„é›†åˆè¿›è¡Œåˆ›å»º(è¯»å®Œä¹‹åå½“å‰åªæ˜¯ä¸€ä¸ªRDD)ï¼› ç»è¿‡å¤šä¸ªTransformationsï¼Œæ¯æ¬¡è½¬æ¢éƒ½ä¼šäº§ç”Ÿä¸åŒçš„RDDï¼Œä¾›ä¸‹ä¸€æ¬¡Transformationä½¿ç”¨ï¼› æœ€åä¸€ä¸ªæ˜¯Actionæ“ä½œï¼Œç„¶åè¾“å‡ºåˆ°å¤–éƒ¨æ•°æ®æºæˆ–è€…Scalaé›†åˆæˆ–è€…æ ‡é‡ã€‚ å›ç­”ä¸€ä¸‹ä¸Šé¢çš„å›°æƒ‘ï¼ŒRDDé‡‡ç”¨çš„æƒ°æ€§è°ƒç”¨ï¼Œå­—é¢æ„æ€æ˜¯åªæœ‰åœ¨éœ€è¦çš„æ—¶å€™æ‰ä¼šå»åšï¼Œå³åœ¨Actionçš„æ—¶å€™ï¼Œæ‰ä¼šå»è®¡ç®—ä¹‹å‰çš„Transformationsæ“ä½œã€‚é‚£ä¹ˆæ„å‘³ç€ä¸ä¼šè¾“å‡ºåˆ°å¤–éƒ¨æ•°æ®æºæˆ–è€…Scalaé›†åˆç­‰ï¼Œä¸ä¼šå®é™…è®¡ç®—å‡ºä¸­é—´ç»“æœï¼Œå‡å°‘æ•°æ®IOç­‰æ“ä½œã€‚ æ–°çš„å›°æƒ‘ï¼šæ‰€ä»¥RDDåªæ˜¯ä¸€ä¸ªè½¬æ¢è®°å½•çš„é›†åˆï¼Œé‚£ä¹‹å‰æ€ä¹ˆè¯´æ˜¯åˆ†å¸ƒå¼å¯¹è±¡é›†åˆï¼Œåˆ°åº•ä¼šä¸ä¼šå ç”¨HDFSç£ç›˜æˆ–è€…å†…å­˜å‘¢ï¼Ÿ 1.5.3 RDDç‰¹æ€§ é«˜æ•ˆçš„å®¹é”™æ€§(åˆ†å¸ƒå¼å…±äº«å†…å­˜ã€é”®å€¼å­˜å‚¨ã€å†…å­˜æ•°æ®åº“) ä¸­é—´ç»“æœæŒä¹…åŒ–åˆ°å†…å­˜(Sparkæ˜¯åŸºäºå†…å­˜è®¡ç®—çš„ï¼Œä¸­é—´æ•°æ®æ˜¯æŒä¹…åŒ–åˆ°å†…å­˜ï¼Œè€Œä¸æ˜¯ç£ç›˜ï¼Œé¿å…è¯»å†™IOå¼€é”€ï¼Œä½†å½“Tçº§çš„æ•°æ®æ˜¯å¦‚ä½•å†…å­˜è¶³å¤Ÿå­˜ä¸‹æ•°æ®çš„å‘¢ï¼Ÿ) å­˜æ”¾çš„æ•°æ®å¯ä»¥æ˜¯Javaå¯¹è±¡ï¼Œé¿å…äº†ä¸å¿…è¦çš„å¯¹è±¡åºåˆ—åŒ–å’Œååºåˆ—åŒ–å¼€é”€(è®°å¾—Hadoopä¸­æ˜¯æœ‰è‡ªå·±çš„ä¸€å¥—åºåˆ—åŒ–çš„ï¼Œå’ŒJavaä¸ä¸€æ ·) 1.5.4 RDDä¹‹é—´çš„ä¾èµ–å…³ç³» çª„ä¾èµ–(Narrow Dependency) ä¸€ä¸ªæˆ–å¤šä¸ªçˆ¶RDDåˆ†åŒºå¯¹åº”ä¸€ä¸ªå­RDDåˆ†åŒºï¼Œå³ä¸€å¯¹ä¸€æˆ–è€…å¤šå¯¹ä¸€ï¼› ç”Ÿæˆçª„ä¾èµ–å…³ç³»çš„Transformationsæœ‰Mapï¼ŒFilterï¼ŒUnionç­‰ å®½ä¾èµ–(Wide Dependency) ä¸€ä¸ªçˆ¶RDDåˆ†åŒºå¯¹åº”å¤šä¸ªå­RDDåˆ†åŒºï¼Œå³ä¸€å¯¹å¤šï¼› ç”Ÿæˆå®½ä¾èµ–å…³ç³»çš„Transformationsæœ‰Groupbyï¼ŒSortbykeyç­‰ Joinæ¯”è¾ƒç‰¹æ®Šï¼Œä¸¤è€…éƒ½æœ‰å¯èƒ½æ˜¯ã€‚ Questionsï¼š ä¸æ˜¯å¾ˆç†è§£ä¸åŒTransformationsç”Ÿæˆçš„ä¾èµ–å…³ç³»ä¸åŒ å…¶å®ä¸Šé¢çš„ç†è§£ç‚¹åº”è¯¥æ˜¯åˆ†åŒºï¼Œçˆ¶åˆ†åŒºå’Œå­åˆ†åŒºçš„å…³ç³»ã€‚æ ¹æ®MapReduceçš„è®¡ç®—æ¡†æ¶ï¼ŒGroupbyçš„æ“ä½œè‚¯å®šæ˜¯ä¼šé€ æˆä¸€å¯¹å¤šçš„åˆ†åŒºã€‚åŒæ—¶åœ¨MapReduceä¸­Mapç»™æˆ‘æ„Ÿè§‰ä¹Ÿæ˜¯ä¼šæœ‰ä¸€å¯¹å¤šï¼Œæ˜¯å¦è¿™é‡Œçš„Mapå’ŒMapReduceä¸­çš„Mapä¸å¤ªä¸€æ ·ï¼Ÿ 2. References å­é›¨å¤§æ•°æ®ä¹‹Sparkå…¥é—¨æ•™ç¨‹(Pythonç‰ˆ)","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.cc/tags/Spark/"}]},{"title":"jupyterhubé…ç½®å¤šç”¨æˆ·","date":"2019-01-13T12:42:12.000Z","path":"2019/01/13/jupyterhubé…ç½®å¤šç”¨æˆ·/","text":"æœ€è¿‘ç»„é‡Œç”³è¯·äº†ä¸€å°å¸¦GPUçš„æœåŠ¡å™¨ï¼ŒNvidiaçš„M40ï¼Œè™½ç„¶æ€§èƒ½ä¸€èˆ¬ï¼Œä½†ç©ä¸€ç©ä¹Ÿå¤Ÿç”¨äº†ã€‚å› ä¸ºæ˜¯Centosçš„ç³»ç»Ÿï¼Œç»„é‡Œçš„å°ä¼™ä¼´è§‰å¾—å®‰è£…ä¸å¦‚Ubuntuæ–¹ä¾¿ï¼Œéƒ½ä¸æƒ³é¼“æ£ç¯å¢ƒï¼Œç„¶åå°±ç”±æˆ‘æ¥å®‰è£…äº†ã€‚ä¹‹å‰å·²ç»å®‰è£…äº†CUDAå’Œæ˜¾å¡é©±åŠ¨ï¼Œå› ä¸ºå®‰è£…æ—¶å€™çš„ç¬”è®°æ²¡æ•´ç†å¥½ï¼Œå°±å…ˆæ•´ç†å®‰è£…jupyterå¤šç”¨æˆ·çš„æ•™ç¨‹ã€‚åç»­æœ‰ç©ºçš„æ—¶å€™å†å†™å‰ç½®çš„ç¯å¢ƒæ•™ç¨‹å§ï¼ˆå…¶å®å¦‚æœä¸€å¼€å§‹å°±ç»™ä¸€ä¸ªå¹²å‡€çš„ç¯å¢ƒï¼Œå®‰è£…è¶…ç®€å•çš„ï¼‰ã€‚ 1. ç³»ç»Ÿç¯å¢ƒæˆ‘çš„ç³»ç»Ÿï¼šCentOS Linux release 7.3.1611 (Core) 1234567# æŸ¥çœ‹ç³»ç»Ÿç‰ˆæœ¬lsb_release -a# ä»¥ä¸‹äºŒç§æ–¹æ³•é€‚ç”¨äºRedHat,CentOScat /etc/redhat-releaserpm -q centos-releaserpm -q redhat-release 2. å®‰è£…æ­¥éª¤2.1 å®‰è£…ä¾èµ–åŒ…1234yum install sqlite-devel npm nodejs-legacy zlib-devel openssl-devel# è¿™ä¸ªå¼€å¯äº†ï¼Œå¯¼è‡´æˆ‘åé¢ä¸€ç›´æŠ¥é”™ï¼Œå¤´ç–¼ï¼Œå°±killç›¸å…³è¿›ç¨‹å°±okäº†npm install -g configurable-http-proxy 2.2 å®‰è£…Anaconda312345wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.shbash Anaconda3-4.4.0-Linux-x86_64.sh# å®‰è£…å®Œååº”è¯¥ä¼šæç¤ºä½ æ˜¯å¦è¦å°†anaconda3è·¯å¾„åŠ å…¥åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­å»ï¼Œå½“ç„¶é€‰æ‹©æ˜¯å•¦ï¼Œç„¶åæ›´æ–°ç¯å¢ƒå˜é‡source ~/.bashrc æ£€æµ‹æ˜¯å¦å®‰è£…æˆåŠŸ 1conda list 2.3 å®‰è£…jupyterhub1conda install -c conda-forge jupyterhub æ£€æµ‹æ˜¯å¦å®‰è£…æˆåŠŸ 12jupyterhub -hconfigurable-http-proxy -h 2.4 åˆ›å»ºjupyterhubçš„é…ç½®æ–‡ä»¶123# jupyterhubæ˜¯åœ¨anaconda3/binè·¯å¾„ä¸‹# è¿™æ¡æŒ‡ä»¤ä¼šç”Ÿæˆä¸€ä¸ªjupyterhub_config.pyçš„é…ç½®æ–‡ä»¶jupyterhub --generate-config ä¿®æ”¹é…ç½®æ–‡ä»¶ 123456789101112131415161718# ipå’Œä½¿ç”¨çš„ç«¯å£å·195:c.JupyterHub.ip = '10.105.xx.xxx'219:c.JupyterHub.port = 9999294:c.JupyterHub.statsd_prefix = 'jupyterhub'375:c.Spawner.cmd = ['jupyterhub-singleuser'] # ç®¡ç†ç”¨æˆ·623: #c.Authenticator.admin_users = &#123;'root'&#125; # ç™½åå•ç”¨æˆ·673:c.Authenticator.whitelist =&#123;'user_name_1','user_name_2','user_name_3', ..., 'user_name_x'&#125;708:c.LocalAuthenticator.create_system_users = True722:c.PAMAuthenticator.encoding = 'utf8'# éå¿…è¦749:c.InteractiveShellApp.exec_lines = ['import os;os.environ.update(&#123;\"JAVA_HOME\": \"/usr/local/jdk1.8.0_74\"&#125;)'] æ³¨æ„ï¼šè¿™é‡Œåœ¨ç™½åå•å’Œç®¡ç†ç”¨æˆ·éƒ½æåˆ°çš„è®¿é—®çš„ç”¨æˆ·ï¼Œè¿™é‡Œçš„ç”¨æˆ·å®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬CentosæœåŠ¡å™¨çš„ç”¨æˆ·ï¼Œå¦‚æœæˆ‘ä»¬é…ç½®çš„ç”¨æˆ·æ²¡åœ¨ç³»ç»Ÿå½“ä¸­ï¼Œåˆ™ä¼šè‡ªåŠ¨åˆ›å»ºï¼Œåˆ›å»ºåéœ€è¦æˆ‘ä»¬æ‰‹å·¥çš„æŒ‡å®šç”¨æˆ·å¯†ç ï¼Œç”¨äºç”¨æˆ·åæœŸç™»å½•jupyterhubã€‚ 2.5 å¼€å¯æœåŠ¡1jupyterhub --config=/etc/jupyterhub/jupyterhub.py --no-ssl ç„¶è€Œå¤±è´¥äº† TAT ä¸€ç›´æŠ¥403é”™è¯¯ï¼Œç„¶ågoogleäº†ä¸€ä¸‹è¿™ä¸ªé—®é¢˜ Jupyterhub sevice unavailable error and http :403 forbidden. æ ¹æ®ä¸Šè¿°çš„é—®é¢˜æ‰¾åˆ°äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆ @carlurips aux | grep configurable-http-proxyif you find any processes, shut them down ç„¶åå†å¯åŠ¨ä¸€å¼€å§‹çš„é‚£æ¡æŒ‡ä»¤å°±å¯ä»¥å®Œç¾è¿è¡Œäº†ã€‚ å¯èƒ½æœ‰äº›ç«¥é‹ä¸çŸ¥é“æ€ä¹ˆåœ¨æœ¬åœ°è®¿é—®æœåŠ¡å™¨ä¸Šçš„æœåŠ¡ï¼Œé‚£ä¹ˆæ­¤æ—¶å°±éœ€è¦å°†ç«¯å£è¿›è¡Œè½¬å‘å•¦ï¼Œè¿è¡Œå¦‚ä¸‹æŒ‡ä»¤ 12# ä¸‹é¢çš„æ„æ€æ˜¯å¤§æ¦‚æˆ‘å…ˆç™»å…¥åˆ°fanchangxnè¿™ä¸ªè·³æ¿æœºï¼Œç„¶åå†è¿åˆ°10.105è¿™å°æœåŠ¡å™¨ä¸Šï¼ŒåŒæ—¶å°†è¿™å°æœºå™¨ä¸Šçš„ç«¯å£7864å’Œæœ¬åœ°ç«¯å£7865æŒ‚è½½åˆ°ä¸€èµ·ssh -N -f -L 7865:10.105.xx.xxx:7864 fanchangxun@182.254.xx.xxx -p36000 å®Œç¾ç™»å…¥ æ­¤æ—¶å‘ç°è¦è´¦å·å’Œå¯†ç æ€ä¹ˆåŠï¼Ÿä¼¼ä¹åˆšåˆšåªæ˜¯é…ç½®äº†ç”¨æˆ·åå­—ï¼Œæ²¡æœ‰è¦æ±‚é…ç½®å¯†ç å•Šã€‚ ç”±äºä¹‹å‰æˆ‘ä¸€ç›´æ˜¯åœ¨rootè´¦å·ä¸‹è¿è¡Œçš„ï¼Œè€Œæˆ‘ä»¬æœ¬æ¬¡é…ç½®çš„ç›®çš„æ˜¯å…è®¸å¤šç”¨æˆ·éš”ç¦»è¿è¡Œè‡ªå·±çš„ä»£ç ï¼Œè€Œå…¶ä»–ç”¨æˆ·æ²¡æœ‰æƒé™æŸ¥çœ‹/ä¿®æ”¹/è¿è¡Œã€‚æ‰€ä»¥å…ˆåœ¨æœ¬å°æœºå™¨ä¸‹åˆ›å»ºä¸€äº›ç”¨æˆ·å’Œå¯†ç ï¼ˆéœ€åœ¨rootè´¦å·ä¸‹ï¼‰ 12345678910111213# æ·»åŠ ç”¨æˆ·adduser user1# è®¾ç½®å¯†ç ï¼Œå¤ªç®€å•ç³»ç»Ÿä¸ºæŠ¥ä¸€ä¸ªbad passwordçš„warnningï¼Œå¯ä»¥å¿½ç•¥# å¯¹äºrootç”¨æˆ·æ¥è¯´ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹å…¶ä»–ç”¨æˆ·çš„å¯†ç passwd user1# ç„¶åè¾“å…¥å¯†ç ï¼Œæ¯”å¦‚ pwd12345678# æ£€æŸ¥æ˜¯å¦åˆ›å»ºæˆåŠŸcut -d: -f1 /etc/passwd# å¦‚æœä¸å°å¿ƒæŠŠè´¦å·åˆ›å»ºé”™äº†ï¼Œéœ€è¦åˆ é™¤ä¹Ÿæ˜¯okçš„(æ°¸ä¹…åˆ é™¤)userdel user1 åˆ›å»ºæˆåŠŸåï¼Œç³»ç»Ÿå°±ä¼šåœ¨/homeä¸‹åˆ›å»ºç›¸åº”çš„user1è¿™ä¸ªæ–‡ä»¶å¤¹ï¼Œè¿™ä¸ªå°±æ˜¯ä»¥åuser1ç”¨çš„æ–‡ä»¶å¤¹ï¼Œç„¶åç”¨åˆšåˆšçš„è´¦å·å’Œå¯†ç ç™»å…¥ä¸Šè¿°çš„jupyterå°±okäº†ï¼ˆå‰æå·²ç»æ·»åŠ åˆ°äº†whitelisté‡Œé¢å»ï¼‰ï¼Œåœ¨é‡Œé¢åˆ›å»ºæ–‡ä»¶å¤¹æˆ–æ˜¯notebookï¼Œå°±ä¼šåœ¨å¯¹åº”çš„/home/user1ä¸‹åˆ›å»ºç›¸åº”çš„æ–‡ä»¶å¤¹ç­‰ã€‚ 2.4 å¼€å¯åå°è¿è¡Œå’Œå…³é—­å› ä¸ºè¿™æ ·çš„æœåŠ¡é€šå¸¸éœ€è¦24hè¿è¡Œçš„ï¼Œæ‰€ä»¥éœ€è¦åå°è¿è¡Œï¼Œé€šå¸¸å¯ä»¥ç”¨nohupã€screenéƒ½å¯ä»¥ã€‚ 12345678# åå°è¿è¡Œnohup jupyterhub --config=/data1/jupyterhub/jupyterhub_config.py --no-ssl &gt; /data1/jupyterhub/nohup.out 2&gt;&amp;1 &amp;# æŸ¥çœ‹æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œåªå¯¹å½“å‰ç»ˆç«¯ç”Ÿæ•ˆjobs# åˆ é™¤è¿è¡Œçš„ä»»åŠ¡ï¼Œåé¢ä¸ºä»»åŠ¡å·fg num_of_jobs 12345# ç„¶è€Œjobsåªçœ‹å½“å‰ç»ˆç«¯ç”Ÿæ•ˆçš„ï¼Œå…³é—­ç»ˆç«¯åï¼Œåœ¨å¦ä¸€ä¸ªç»ˆç«¯jobså·²ç»æ— æ³•çœ‹åˆ°åå°è·‘å¾—ç¨‹åºäº†ï¼Œæ­¤æ—¶åˆ©ç”¨psï¼ˆè¿›ç¨‹æŸ¥çœ‹å‘½ä»¤ï¼‰# a:æ˜¾ç¤ºæ‰€æœ‰ç¨‹åº # u:ä»¥ç”¨æˆ·ä¸ºä¸»çš„æ ¼å¼æ¥æ˜¾ç¤º # x:æ˜¾ç¤ºæ‰€æœ‰ç¨‹åºï¼Œä¸ä»¥ç»ˆç«¯æœºæ¥åŒºåˆ†ps aux | egrep node 12# æˆ–è€…æŸ¥çœ‹å“ªä¸ªç«¯å£å·è¢«å ç”¨lsof -i:7864 12# æˆ–è€…netstat -ap | grep 7864 12# æ€æ­»è¿›ç¨‹kill -9 è¿›ç¨‹å· ç„¶è€Œä¸çŸ¥ä¸ºä½•ä¸€ç›´æ€ä¸æ‰è¿™ä¸ªè¿›ç¨‹ï¼Œåªèƒ½å»çœ‹çœ‹ä»–çš„çˆ¶è¿›ç¨‹ï¼Œæ€æ‰çˆ¶è¿›ç¨‹è¯•è¯•çœ‹ å…ˆæ€äº†4773è¿›ç¨‹ï¼Œå†æŠŠ17959è¿›ç¨‹æ€äº†å°±OKäº†ï¼Œå®Œç¾ã€‚ 3. å®‰è£…å¸¸ç”¨æ’ä»¶3.1 nb_extension12345678910111213# ä¸ºjupyterå®‰è£…extensionpip install jupyter_contrib_nbextensionsconda install -c conda-forge jupyter_contrib_nbextensions# å®‰è£…jupyter nb_extensionjupyter contrib nbextension install --user# å¼€å¯/å…³é—­extensionsjupyter nbextension enable &lt;nbextension require path&gt;jupyter nbextension disable &lt;nbextension require path&gt;# e.gjupyter nbextension enable codefolding/main ä»¥ä¸Šæˆ‘åœ¨rootç”¨æˆ·ä¸‹è¿è¡Œå‘ç°å¹¶æ²¡æœ‰æ•ˆæœï¼Œåæ¥å‘ç°éœ€è¦å…ˆåˆ‡åˆ°ç”¨æˆ·è´¦å·ä¸‹è¿è¡Œæ‰è¡Œã€‚ ç„¶åä¼šåœ¨ç”¨æˆ·çš„è·¯å¾„ä¸‹é¢ç”Ÿæˆä¸åŒçš„é…ç½®æ–‡ä»¶ï¼Œç„¶åé‡å¯æœåŠ¡å™¨å³å¯ã€‚ 3.2 notedownä½¿ç”¨notedownæ’ä»¶æ¥è¯»å†™githubæºæ–‡ä»¶ 12# å®‰è£…nodtedownæ’ä»¶pip install https://github.com/mli/notedown/tarball/master ä¿®æ”¹jupyterhub_config.pyæ–‡ä»¶ 12# å°†é…ç½®æ·»åŠ åˆ°æ–‡ä»¶çš„æœ«å°¾c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager' ä½¿ç”¨æ–¹æ³• 1234# ç›¸äº’è½¬åŒ–notedown input.md &gt; output.ipynbnotedown input.ipynb --to markdown --strip &gt; output.mdnotedown input.ipynb --to markdown &gt; output_with_outputs.md åœ¨IPythonä¸­ä½¿ç”¨ 12345678import reimport sysimport argparsefrom IPython.nbformat.v3.rwbase import NotebookReaderfrom IPython.nbformat.v3.nbjson import JSONWriterimport IPython.nbformat.v3.nbbase as nbbase 3. Daily Errors3.1 04/16/2019è«åå…¶å¦™æ— æ³•å¯åŠ¨kernel é”™è¯¯å¦‚ä¸‹ 1The kernel has died, and the automatic restart has failed. It is possible the kernel cannot be restarted. If you are not able to restart the kernel, you will still be able to save the notebook, but running code will no longer work until the notebook is reopened. è§£å†³æ–¹æ¡ˆ å¸è½½é‡è£… ipykernel References 5. References Jupyterhubå®‰è£…æµç¨‹ å¦‚ä½•å»ºç«‹ä¸€å€‹åœ˜éšŠç”¨çš„ Jupyter-Hub CentOS 7ä¸­æ·»åŠ ä¸€ä¸ªæ–°ç”¨æˆ·å¹¶æˆæƒ centosç³»ç»Ÿæ·»åŠ /åˆ é™¤ç”¨æˆ·å’Œç”¨æˆ·ç»„ Installing jupyter_contrib_nbextensions Github-notedown","tags":[{"name":"jupyterhub","slug":"jupyterhub","permalink":"http://chenson.cc/tags/jupyterhub/"}]},{"title":"Mixed-Logistic-Regressionæ¨¡å‹åˆæ¢","date":"2018-12-22T09:05:55.000Z","path":"2018/12/22/Mixed-Logistic-Regressionæ¨¡å‹åˆæ¢/","text":"","tags":[{"name":"MLR","slug":"MLR","permalink":"http://chenson.cc/tags/MLR/"}]},{"title":"å¤§æ•°æ®æ•°ä»“å»ºè®¾ç¬”è®°","date":"2018-12-20T05:02:15.000Z","path":"2018/12/20/å¤§æ•°æ®æ•°ä»“å»ºè®¾ç¬”è®°/","text":"è™½ç„¶ä¸æ˜¯ä»äº‹å¤§æ•°æ®å¼€å‘çš„ï¼Œä½†æˆ‘è§‰å¾—è¿˜æ˜¯æœ‰å¿…è¦å¯¹å¤§æ•°æ®è¿™è¾¹æœ‰ä¸€äº›äº†è§£ï¼Œç®€å•çš„å¯èƒ½æ˜¯å¦‚ä½•ä½¿ç”¨å¤§æ•°æ®çš„ä¸€äº›å·¥å…·ï¼Œæ¯”å¦‚æ˜¯Hiveã€Sparkã€Prestoç­‰ï¼Œä»¥åŠå¦‚ä½•æ—¥æ›´ä¸€å¼ å®½è¡¨ã€‚æ‰€ä»¥å°±æŒ‰ç…§æˆ‘å¹³å¸¸æ¥è§¦å’Œäº†è§£çš„åšäº†ä¸ªç®€å•çš„æ€»ç»“ã€‚ 1. ä¸ºä»€ä¹ˆè¦å»ºè®¾æ•°æ®ä»“åº“ æ•°æ®é›†æˆ ä¸åŒä¸šåŠ¡ç³»ç»Ÿæ•°æ®é›†ä¸­ã€ç»Ÿä¸€ç®¡ç† é¢å‘ä¸»é¢˜ å¯¹ä¸šåŠ¡æ•°æ®åˆ†ä¸»é¢˜å»ºè®¾â€”å‚ä¸äººã€è¡Œä¸ºåŸŸã€é»‘åå• æ—¶é—´ç»´åº¦ æŒ‰å¤©å­˜å‚¨å†å²å¿«ç…§æ•°æ®ï¼Œæ•°æ®å¯è¿½æº¯ ç›¸å¯¹ç¨³å®š æå‡è®¡ç®—èƒ½åŠ› æ•°æ®é¢„æ¸…æ´—å¤„ç†ï¼Œé¢„åŠ å·¥å­˜å‚¨ æ”¯æŒå¤šæ–¹é¢åº”ç”¨ OLAPã€BI 2. æ•°æ®ä»“åº“è§„åˆ’2.1 æ•°æ®ä»“åº“åˆ†å±‚ä»‹ç»ä¸ºä»€ä¹ˆè¦å¯¹æ•°æ®è¿›è¡Œå±‚æ¬¡è®¾è®¡å‘¢ï¼Ÿä¸»è¦æœ‰ä¸€ä¸‹åŸå› ï¼š æ¸…æ™°æ•°æ®ç»“æ„ï¼šæ¯ä¸€ä¸ªæ•°æ®åˆ†å±‚éƒ½æœ‰å®ƒçš„ä½œç”¨åŸŸï¼Œè¿™æ ·æˆ‘ä»¬åœ¨ä½¿ç”¨è¡¨çš„æ—¶å€™èƒ½æ›´æ–¹ä¾¿åœ°å®šä½å’Œç†è§£ã€‚ æ•°æ®è¡€ç¼˜è¿½è¸ªï¼šç®€å•æ¥è®²å¯ä»¥è¿™æ ·ç†è§£ï¼Œæˆ‘ä»¬æœ€ç»ˆç»™ä¸šåŠ¡è¯šä¿¡çš„æ˜¯ä¸€èƒ½ç›´æ¥ä½¿ç”¨çš„å¼ ä¸šåŠ¡è¡¨ï¼Œä½†æ˜¯å®ƒçš„æ¥æºæœ‰å¾ˆå¤šï¼Œå¦‚æœæœ‰ä¸€å¼ æ¥æºè¡¨å‡ºé—®é¢˜äº†ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°å®šä½åˆ°é—®é¢˜ï¼Œå¹¶æ¸…æ¥šå®ƒçš„å±å®³èŒƒå›´ã€‚ å‡å°‘é‡å¤å¼€å‘ï¼šè§„èŒƒæ•°æ®åˆ†å±‚ï¼Œå¼€å‘ä¸€äº›é€šç”¨çš„ä¸­é—´å±‚æ•°æ®ï¼Œèƒ½å¤Ÿå‡å°‘æå¤§çš„é‡å¤è®¡ç®—ã€‚ æŠŠå¤æ‚é—®é¢˜ç®€å•åŒ–ï¼šè®²ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡åˆ†è§£æˆå¤šä¸ªæ­¥éª¤æ¥å®Œæˆï¼Œæ¯ä¸€å±‚åªå¤„ç†å•ä¸€çš„æ­¥éª¤ï¼Œæ¯”è¾ƒç®€å•å’Œå®¹æ˜“ç†è§£ã€‚è€Œä¸”ä¾¿äºç»´æŠ¤æ•°æ®çš„å‡†ç¡®æ€§ï¼Œå½“æ•°æ®å‡ºç°é—®é¢˜ä¹‹åï¼Œå¯ä»¥ä¸ç”¨ä¿®å¤æ‰€æœ‰çš„æ•°æ®ï¼Œåªéœ€è¦ä»æœ‰é—®é¢˜çš„æ­¥éª¤å¼€å§‹ä¿®å¤ã€‚ å±è”½åŸå§‹æ•°æ®çš„å¼‚å¸¸ã€‚ å±è”½ä¸šåŠ¡çš„å½±å“ï¼Œä¸å¿…æ”¹ä¸€æ¬¡ä¸šåŠ¡å°±éœ€è¦é‡æ–°æ¥å…¥æ•°æ® 2.2 ç®€å•ä»‹ç»è¿™é‡Œä¸»è¦ç®€å•ä»‹ç»ä¸Šé¢çš„åˆ†å±‚æ¶æ„ï¼Œå®é™…ä¸Šå¯¹äºå¤§ç‚¹çš„å…¬å¸ï¼Œå¯èƒ½æ•°ä»“åœ¨è®¾è®¡ä¸Šè¿˜æ›´ä¸ºè¯¦ç»†ä¸€äº›ï¼Œä¸ä¼šå¦‚ä¸Šé¢é‚£æ¬ç®€å•ã€‚è¿™é‡Œå¯èƒ½å°±æŒ‰ç…§æˆ‘å¹³å¸¸å·¥ä½œä¸­æ¥è§¦çš„æ¶æ„æ¥ä»‹ç»äº†ã€‚ æºæ•°æ®å±‚ - BDM è¿™éƒ¨åˆ†ä¸€èˆ¬ç”±æ•°æ®ç½‘å…³å»æ‹‰å»æ•°æ®ï¼Œå°½é‡ä¿ç•™æœ€ã¡³çš„æ•°æ®ä¿¡æ¯ï¼Œä¸€èˆ¬ä¸ºæœ€åŸå§‹çš„jsonæ•°æ®ã€‚ æ•°æ®å‡†å¤‡å±‚ - ODS(Operational Data Store) è¿™éƒ¨åˆ†çš„æ•°æ®ä¸€èˆ¬æ˜¯ç”±BDMé‚£è¾¹çš„æ•°æ®è§£ææ¥äº†ï¼Œä¼šå¯¹jsonè¿›è¡Œæ‹†è§£ï¼Œå°†jsonæ‹†è§£æˆç»“æ„åŒ–çš„è¡¨ã€‚åœ¨æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å°½å¯èƒ½ä¼šä¿ç•™jsonä¸­æ‰€æœ‰çš„ä¿¡æ¯ï¼Œæ¯”å¦‚è¡¨åå’Œå­—æ®µå€¼ä¹‹ç±»çš„ã€‚ä¸è¿‡ä¹Ÿæœ‰æ—¶å€™ä¼šå¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæ¯”å¦‚è¯´æ•°æ®è„±æ•å¯èƒ½åœ¨è¿™ä¸€å±‚å°±å¼€å§‹å®Œæˆäº†ã€‚ ä¸ºäº†ä¿ç•™åŸå§‹æ•°æ®ï¼Œé€šå¸¸è¿™è¾¹æ˜¯æ¯å¤©å…¨é‡æ›´æ–°çš„ï¼Œä¹Ÿå°±æ˜¯ä¼šä¿ç•™äº†å†å²å˜æ›´çš„æ•°æ®ï¼ŒæŒ‰å¤©åˆ†åŒºå‚¨å­˜ã€‚è¿™ä¸ªéå¸¸é‡è¦æ–¹ä¾¿ä»¥åå›æº¯æ•°æ®ã€‚ å­˜å‚¨ å°½å¯èƒ½ä¿ç•™åŸå§‹æ•°æ®ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­—æ®µåç§° æ•°æ®å¤„ç† ç»“æ„åŒ–ï¼Œæ‹†è¡¨ï¼ˆjsonå¤æ‚ç»“æ„ï¼‰ è„±æ•ï¼ˆèº«ä»½è¯ã€æ‰‹æœºå·æ‹†å­—æ®µï¼‰ å»å™ªã€å»é‡ç­‰ã€‚ ç¦æ­¢ ç­›é€‰ã€æ±‡æ€»ã€èˆå»å­—æ®µ æ•°æ®æ˜ç»†å±‚ - DWD è¿™ä¸€å±‚ä¼šåœ¨å­˜å‚¨æ–¹é¢å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»å­˜å‚¨ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ•°æ®åšä¸€äº›ç®€å•çš„å¤„ç†ï¼Œæ¯”å¦‚å»å™ªå•Šï¼Œæ ¼å¼åŒ–å¤„ç†å•Šä¹‹ç±»çš„ã€‚ å­˜å‚¨ åˆ†ä¸»é¢˜ã€å…¨é‡æ•°æ® æ•°æ®å¤„ç† å­—æ®µå‘½åè§„èŒƒåŒ–ã€å…³é”®ç»´åº¦æ˜ å°„ æ—¶é—´ç»´åº¦çš„å¤„ç† ç»´åº¦è½¬ä»£ç†é”®ã€ç©ºå€¼å¤„ç†ã€éå¿…è¦å­—æ®µçš„èˆå¼ƒä¸è½¬æ¢ è„æ•°æ®å¤„ç† ä¸šåŠ¡è¡¨åˆå¹¶ å¢é‡è½¬å…¨é‡ è½»åº¦æ±‡æ€»å±‚ - DWS è¿™ä¸€å±‚å¯¹å·²ç»åˆ†ç±»å’Œå¤„ç†å¥½çš„æ•°æ®åšä¸€äº›è½»åº¦çš„æ±‡æ€»ã€‚ å­˜å‚¨ åˆ†ä¸»é¢˜ã€ç»´åº¦æ±‡æ€»æ•°æ® æ•°æ®å¤„ç† ç»´åº¦æ±‡æ€»ã€ä¸åŒé¢—ç²’åº¦çš„ç»´åº¦æ±‡æ€» å„ç±»ä¸šåŠ¡ç»Ÿè®¡åº¦é‡å€¼çš„è®¡ç®— ä¸šåŠ¡å£å¾„é€»è¾‘çš„å®ç° æ•°æ®é›†å¸‚å±‚ - DM ä½œä¸ºä¸€ä¸ªè°ƒåŒ…ä¾ ï¼Œç»å¸¸æ¥è§¦çš„å¯èƒ½å°±æ˜¯è¿™éƒ¨åˆ†äº†ã€‚å› ä¸ºè¿™éƒ¨åˆ†åŸºæœ¬å°±æ˜¯å·²ç»æ±‡æ€»å¥½çš„å®½è¡¨ï¼Œé€šå¸¸éƒ½æ˜¯å¯ä»¥ç›´æ¥å…¥æ¨¡å‹çš„æ•°æ®äº†ï¼Œæˆ–å¯¹è¿™éƒ¨åˆ†æ•°æ®ç®€å¤„ç†ä¸€ä¸‹å°±å¯ä»¥ç›´æ¥ç”¨çš„ã€‚è¿™ä¸€å±‚é€šå¸¸æ˜¯T+1çš„æ—¥æ›´å®½è¡¨ï¼Œå®½è¡¨é€»è¾‘é€šå¸¸ç”±æ¨¡å‹è¿™è¾¹å¼€å‘å¥½æä¾›ç»™å¤§æ•°æ®å¼€å‘ï¼Œæˆ–è€…æ˜¯è¿è¥ã€ä¸šåŠ¡ã€BIé‚£è¾¹æ ¹æ®ä»–ä»¬çš„éœ€æ±‚ï¼Œå°†é€»è¾‘æ•´ç†å¥½ç»™å¤§æ•°æ®å¼€å‘ã€‚ å­˜å‚¨ è·¨ä¸»é¢˜æ¨¡å‹å®ç°ã€å®½è¡¨ æ•°æ®å¤„ç† è·¨ä¸»é¢˜çš„æ±‡æ€»ç»Ÿè®¡è®¡ç®— å±•å¼€ç»´åº¦å½¢æˆå®½è¡¨ ä¸šåŠ¡å£å¾„é€»è¾‘çš„å®ç° 3. æ¶æ„æ€»è§ˆæ•´ä¸ªæ•°ä»“å°±æ˜¯æ ¹æ®ä¸Šé¢çš„åˆ†å±‚æ¥è®¾è®¡çš„ï¼Œä»ç½‘å…³åˆ°ODSåˆ°DWDåˆ°DWSæœ€ååˆ°DMã€‚ 4. DMå±‚å®½è¡¨è®¾è®¡ä¸ç»´æŠ¤12 5. References å¤§æ•°æ®ç¯å¢ƒä¸‹è¯¥å¦‚ä½•ä¼˜é›…åœ°è®¾è®¡æ•°æ®åˆ†å±‚","tags":[{"name":"Big Data","slug":"Big-Data","permalink":"http://chenson.cc/tags/Big-Data/"}]},{"title":"æ‹’ç»å›¾åºŠï¼Œä»æˆ‘åšèµ·","date":"2018-12-01T04:29:20.000Z","path":"2018/12/01/æ‹’ç»å›¾åºŠï¼Œä»æˆ‘åšèµ·/","text":"å‰æ®µæ—¶é—´æƒ³æ•´ç†ä¸€ä¸‹ä»¥å‰çš„åšæ–‡ç¬”è®°ï¼Œå› ä¸ºå¾ˆå¤šéƒ½æ˜¯æ—©æœŸè‡ªå·±è®°å½•çš„ç¬”è®°ï¼Œæ¡ç†æ¯”è¾ƒä¹±ã€‚æƒ³ç¨å¾®æ•´ç†ä¸€ä¸‹ï¼Œæ–¹ä¾¿ä»¥åè‡ªå·±å›é¡¾å¤ä¹ ä»¥åŠä»–äººé˜…è¯»ã€‚ä½†å‘ç°ä¸€ä»¶æ¯”è¾ƒå¤´å¤§çš„äº‹æƒ…å°±æ˜¯ï¼Œä»¥å‰æ’å›¾éƒ½æ˜¯ä¸Šä¼ åˆ°å›¾åºŠä¸Šç„¶ï¼Œè€Œç°åœ¨å¾ˆå¤šå›¾ç‰‡éƒ½å·²ç»æŒ‚æ‰äº†ï¼Œé“¾æ¥ä¸å¯è®¿é—®ã€‚è¿™ç®€ç›´å°±æ˜¯ç¾éš¾æ€§çš„ä¸€å¹•ã€‚ æ—©æœŸä¹‹æ‰€ä»¥ç”¨å›¾åºŠæ˜¯å› ä¸ºæ–¹ä¾¿ï¼Œå› ä¸ºChromeæœ‰ä¸ªæ’ä»¶å¯ä»¥ç›´æ¥ä¸Šä¼ ï¼Œé‡‡ç”¨æœ¬åœ°çš„è¯éœ€è¦æŠŠå›¾ç‰‡ä¿å­˜åˆ°æœ¬åœ°è·¯å¾„ç„¶åå¼•ç”¨å¯¹åº”çš„å›¾ç‰‡ï¼Œè‡³äºå¦‚ä½•åŒæ­¥åˆ°githubä¸Šä¸å½±å“æ˜¾ç¤ºæ•ˆæœä¹Ÿæ˜¯æ¯”è¾ƒéº»çƒ¦çš„ã€‚å¥½åœ¨ç°åœ¨Typoraæ›´æ–°äº†å‡ ä¸ªç‰ˆæœ¬åï¼Œå¯¹äºæ’å…¥å›¾ç‰‡å¯ä»¥æ—¶å€™éå¸¸å‹å¥½ï¼ˆè¿™é‡Œè£‚å¢™æ¨èTyporaï¼Œæœ€å¥½ç”¨çš„MarkDownå·¥å…·ï¼Œæ²¡æœ‰ä¹‹ä¸€ï¼‰ã€‚å…·ä½“ä½¿ç”¨æ­¥éª¤å¦‚ä¸‹ï¼š 1. å®‰è£…æ­¥éª¤ å®‰è£…æ’ä»¶ï¼Œä¿®æ”¹Hexoé…ç½® ä¿®æ”¹ä¸»é¡µé…ç½®æ–‡ä»¶_config.ymlé‡Œé¢çš„post_asset_folder : true åœ¨hexoç›®å½•ä¸‹æ‰§è¡Œå‘½ä»¤npm install hexo-asset-image --saveï¼Œå³å®‰è£…ä¸€ä¸ªå¯ä»¥ä¸Šä¼ æœ¬åœ°å›¾ç‰‡çš„æ’ä»¶ å®‰è£…æˆåŠŸåï¼Œä»¥åå†æ–°å»ºåšæ–‡hexo n â€œXXXâ€çš„æ—¶å€™ï¼Œé™¤äº†ç”Ÿæˆå¯¹åº”çš„.mdæ–‡ä»¶çš„åŒæ—¶ï¼Œä¹Ÿä¼šåœ¨åŒä¸€å±‚ç›®å½•ä¸‹ï¼Œå»ºä¸€ä¸ªåŒåçš„æ–‡ä»¶å¤¹ï¼Œç”¨äºå­˜æ”¾è¿™ç¯‡åšæ–‡å¼•ç”¨çš„æœ¬åœ°å›¾ç‰‡ã€‚ 12345XXX.mdXXX|--img1.jpg|--img2.jpg|--img3.jpg ç”Ÿæˆé™æ€é¡µé¢ï¼Œå¯¹åº”çš„htmlæ˜¯ 1&lt;img src=\"/year/month/day/XXX/img1.jpg\" alt=\"img1\"&gt; ä¿®æ”¹Typoraé…ç½®ï¼Œå…·ä½“å¦‚å›¾ é…ç½®å®Œä¹‹åï¼Œå¯ä»¥åœ¨æˆªå›¾å®Œä¹‹åï¼Œå°†å‰ªåˆ‡æ¿çš„å›¾ç‰‡ç›´æ¥å¤åˆ¶åˆ°å¯¹åº”æ–‡ä»¶ç›®å½•ä¸‹çš„æ–‡ä»¶å¤¹å†…ï¼Œå³æˆ‘ä»¬ä¸Šé¢æ–°å»ºå¥½çš„æ–‡ä»¶å¤¹ã€‚æœ‰æœ¨æœ‰éå¸¸æ–¹ä¾¿ï¼ï¼ï¼å½“ç„¶è¿™é‡Œä¹Ÿå¯ä»¥è®¾ç½®å…¶ä»–è·¯å¾„ï¼Œåªæ˜¯æˆ‘è¿™é‡Œè®¾ç½®çš„æ˜¯è¿™æ ·ï¼Œä¸ºäº†ä¾¿äºå›¾ç‰‡ç®¡ç†ã€‚ å‘å¸ƒæ–‡ç«  æ–‡ç« å†™å¥½ä¹‹åï¼Œæ‰§è¡ŒæŒ‡ä»¤hexo g; hexo dä¹‹åï¼Œå°±ä¼šç”Ÿæˆå¯¹åº”çš„é™æ€é¡µé¢ï¼Œæ”¾åˆ°publicå¯¹åº”çš„æ–‡ä»¶å¤¹ä¸‹ã€‚OKï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å¼€è‡ªå·±çš„ç½‘ç«™çœ‹ä¸€ä¸‹å°±å¯ä»¥å•¦ã€‚è¿™ä¸ªæ—¶å€™ä½ ä¼šæƒŠå–œçš„å‘ç°ï¼Œå›¾ç‰‡å…¨éƒ½æ— æ³•æ˜¾ç¤ºã€‚å“ˆå“ˆå“ˆï¼Œä¸ºä»€ä¹ˆå˜ï¼Ÿ é‚£ä¹ˆçœ‹ä¸€ä¸‹åŸå§‹makrdownç›®å½•ä¸‹çš„ç»“æ„ å†çœ‹ä¸€ä¸‹å¯¹åº”publicä¸‹çš„ç›®å½•ç»“æ„ å‘ç°å›¾ç‰‡å¯¹åº”çš„æ–‡ä»¶å¤¹æ˜¯ä¸ä¸€æ ·çš„ï¼ˆæ­¤æ—¶å¯å›å¤´çœ‹ç”Ÿæˆå¯¹åº”çš„htmlè·¯å¾„åœ°å€ï¼‰åŸå§‹mdæ˜¯å¼•ç”¨åŒåæ–‡ä»¶å¤¹ä¸‹çš„å›¾ç‰‡ï¼Œè€Œhtmlå´å’Œåº”ç”¨çš„å›¾ç‰‡åœ¨åŒä¸€å±‚ç›®å½•ä¸‹ï¼Œæ‰€ä»¥è¯»å–å¤±è´¥äº†ã€‚ ç”Ÿæˆçš„htmlè·¯å¾„ 1&lt;img src=\"/year/month/day/XXX/img1.jpg\" alt=\"img1\"&gt; ä½†æˆ‘æ¯”è¾ƒæƒ³è¦çš„è·¯å¾„æ˜¯ 1&lt;img src=\"/year/month/day/XXX/year-month-day-XXX/img1.jpg\" alt=\"img1\"&gt; åŸå§‹å›¾ç‰‡å’Œmdæ–‡ç« æ‰€åœ¨çš„è·¯å¾„æ˜¯ 12PATH-TO-BLOG/source/_posts/year-month-day-XXX/img1.jpgPATH-TO-BLOG/source/_posts/year-month-day-XXX.md å½“ç„¶æˆ‘ä»¬å¯ä»¥æ„šè ¢çš„è®¾ç½®æœ¬åœ°çš„mdæ–‡ä»¶ï¼ŒæŠŠå›¾ç‰‡å‰é¢çš„XXXå…ˆå»äº†ï¼Œä½†æ˜¯æ¨ä¸Šå»çš„æ—¶å€™ï¼Œå›¾ç‰‡çš„ç›¸å¯¹è·¯å¾„å°±æ˜¯æ­£å¸¸çš„ã€‚è¿™æ ·è™½ç„¶çº¿ä¸Šå¯ä»¥æ­£å¸¸æ˜¾ç¤ºï¼Œä½†æ˜¯åŒæ—¶ä¼šå¯¼è‡´çº¿ä¸‹æ— æ³•æ˜¾ç¤ºå›¾ç‰‡ï¼Œæ‰€ä»¥è¿™æ–¹æ³•çœŸçš„æ˜¯æœ‰ç‚¹è ¢ã€‚ ç½‘ä¸Šæ‰¾äº†ä¼šè§£å†³æ–¹æ¡ˆä¼¼ä¹éƒ½æ²¡æœ‰æˆåŠŸï¼Œå¯èƒ½æ˜¯æˆ‘çš„æ“ä½œé—®é¢˜ï¼Œæ²¡è®¾ç½®å¥½ã€‚äºæ˜¯å°±è‡ªå·±å†™äº†ä¸ªç®€å•è„šæœ¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ è¿™æ®µè„šæœ¬ä¸»è¦å°±æ˜¯åœ¨publicä¸‹å¯¹åº”blogæ‰€åœ¨çš„æ–‡ä»¶ä¸‹æ–°å»ºä¸€ä¸ªæ—¥æœŸ-æ–‡ç« åçš„æ–‡ä»¶å¤¹ï¼Œç„¶åå°†å›¾ç‰‡ç§»åˆ°è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œé‚£ä¹ˆå›¾ç‰‡å’Œæ–‡ç« çš„ç›¸å¯¹è·¯å¾„å°±å’Œmdæ–‡ä»¶é‡Œé¢æ˜¯ä¸€è‡´çš„äº†ï¼Œæ‰€ä»¥åœ¨æ›´æ–°hexoæ¨ä¸Šå»ï¼Œå›¾ç‰‡å°±æ˜¾ç¤ºæ­£å¸¸å•¦ã€‚ä»¥åå†™æ–‡ç« çš„æ—¶å€™åªéœ€è¦åœ¨éƒ¨ç½²hexohexo dçš„ä¹‹å‰ï¼Œè¿è¡Œä¸€ä¸‹ä¸‹é¢ä»£ç å³å¯ã€‚å¦ˆå¦ˆä»¥åå†ä¹Ÿä¸ç”¨æ‹…å¿ƒæˆ‘å›¾ç‰‡æŒ‚æ‰äº†ï¼ï¼ï¼ hexo g python relocated_img.py hexo d è¿è¡Œä»£ç åçš„æ–‡ä»¶å¤¹ç»“æ„ å…·ä½“ä»£ç å¦‚ä¸‹ã€æˆ‘çŸ¥é“å†™äº†è¿™ä¹ˆforå¾ªç¯å¾ˆä¸‘ ==ï¼ã€‘ï¼š 123456789101112131415161718192021222324252627282930313233343536import os_PUBLIC_ = 'public'_PATH_ = os.getcwd()list_year = [year for year in os.listdir(_PUBLIC_) if year.isdigit() and len(year) == 4]_EXT_ = ['.jpg','.jpeg', '.gif','.png']for year in list_year: p_year = os.path.join(_PATH_, _PUBLIC_, year) list_month = [month for month in os.listdir(p_year) if month.isdigit() and len(month) == 2] for month in list_month: p_month = os.path.join(_PATH_, _PUBLIC_, year, month) list_day = [day for day in os.listdir(p_month) if day.isdigit() and len(day) == 2] for day in list_day: p_day = os.path.join(_PATH_, _PUBLIC_, year, month, day) list_blog = [blog for blog in os.listdir(p_day) if not blog.startswith('.')] for blog in list_blog: print('blog :', blog) p_blog = os.path.join(_PATH_, _PUBLIC_, year, month, day, blog) list_file = os.listdir(p_blog) dir_img = '-'.join([year, month, day, blog]) dir_img = os.path.join(p_day, blog, dir_img) if not os.path.exists(dir_img): os.makedirs(dir_img) for file in list_file: if file.endswith(tuple(_EXT_)): p_file = os.path.join(_PATH_, _PUBLIC_, year, month, day, blog, file) print(' &gt;&gt;&gt; img:', file) print(' &gt;&gt;&gt; img_path:', p_file, end='\\n\\n') cmd = f'mv &#123;p_file&#125; &#123;dir_img&#125;' os.system(cmd) 2. Bugsè™½ç„¶è¿™æ–¹æ³•æŒºå¥½ç”¨çš„ï¼Œä½†å‘ç°æœ‰ä¸ªå°bugï¼Œå°±æ˜¯å›¾ç‰‡åœ¨æ”¾åˆ°å¯¹åº”çš„XXXæ–‡ä»¶å¤¹ä¸‹ï¼Œä¼šé‡å¤ä¸€å¼ ã€‚å¦‚æœåœ¨Typoraä¸­è®¾è®¾ç½®æŒ‡å®šçš„custom foldä¸‹å€’ä¸ä¼šï¼Œè®¾ç½®æˆåŒåæ–‡ä»¶å¤¹ä¸‹å°±ä¼šå¤šå‡ºä¸€å¼ ï¼Œç›®å‰æ²¡æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œæš‚ä¸”å…ˆè¿™æ ·å§ã€‚ å½“ç„¶å¦å¤–ä¸€ä¸ªå¯èƒ½çš„é—®é¢˜å°±æ˜¯ä¼šå ç”¨githubçš„ç©ºé—´ï¼Œä½†é‡å°çš„æƒ…å†µä¸‹é—®é¢˜ä¸å¤§ã€‚ 3. References hexoåšå®¢å›¾ç‰‡é—®é¢˜","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chenson.cc/tags/hexo/"},{"name":"Typora","slug":"Typora","permalink":"http://chenson.cc/tags/Typora/"}]},{"title":"èšç±»ç®—æ³•ï¼šK-MeansåŠæ‰©å±•ç®—æ³•K-Modesã€K-Prototypeåˆæ¢","date":"2018-11-27T05:23:30.000Z","path":"2018/11/27/èšç±»ç®—æ³•ï¼šK-MeansåŠæ‰©å±•ç®—æ³•K-Modesã€K-Prototypeåˆæ¢/","text":"ç”±äºæœ€è¿‘æ­£åœ¨å‚ä¸çš„è‡ªåŠ¨åŒ–å»ºæ¨¡å¹³å°éœ€è¦ç”¨åˆ°è¿™ä¸€ç®—æ³•ï¼Œä½†sklearné‡Œé¢çš„èšç±»ç®—æ³•åªæ”¯æŒæ•°å€¼å‹çš„ï¼Œæ— æ³•ç”¨åˆ°ç±»åˆ«å‹çš„ç‰¹å¾ä¸Šï¼Œæ‰€ä»¥å°±ç ”ç©¶äº†K-Modeså’ŒK-Prototypesè¿™ä¸¤ä¸ªç®—æ³•ã€‚å…·ä½“ç®—æ³•çš„æ€æƒ³å’Œæºç å¦‚ä¸‹ã€‚ 1. k-Means Algorightm K-Means K-Means++ K-Means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321class KMeans(BaseEstimator, ClusterMixin, TransformerMixin): \"\"\"K-Means clustering Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : &#123;'k-means++', 'random' or an ndarray&#125; Method for initialization, defaults to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. n_init : int, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, default: 300 Maximum number of iterations of the k-means algorithm for a single run. tol : float, default: 1e-4 Relative tolerance with regards to inertia to declare convergence precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances verbose : int, default 0 Verbosity mode. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. Attributes ---------- cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : Labels of each point inertia_ : float Sum of squared distances of samples to their closest cluster center. Examples -------- &gt;&gt;&gt; from sklearn.cluster import KMeans &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], ... [4, 2], [4, 4], [4, 0]]) &gt;&gt;&gt; kmeans = KMeans(n_clusters=2, random_state=0).fit(X) &gt;&gt;&gt; kmeans.labels_ array([0, 0, 0, 1, 1, 1], dtype=int32) &gt;&gt;&gt; kmeans.predict([[0, 0], [4, 4]]) array([0, 1], dtype=int32) &gt;&gt;&gt; kmeans.cluster_centers_ array([[1., 2.], [4., 2.]]) See also -------- MiniBatchKMeans Alternative online implementation that does incremental updates of the centers positions using mini-batches. For large scale learning (say n_samples &gt; 10k) MiniBatchKMeans is probably much faster than the default batch implementation. Notes ------ The k-means problem is solved using Lloyd's algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, 'How slow is the k-means method?' SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That's why it can be useful to restart it several times. \"\"\" def __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=1e-4, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto'): self.n_clusters = n_clusters self.init = init self.max_iter = max_iter self.tol = tol self.precompute_distances = precompute_distances self.n_init = n_init self.verbose = verbose self.random_state = random_state self.copy_x = copy_x self.n_jobs = n_jobs self.algorithm = algorithm def _check_test_data(self, X): X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES) n_samples, n_features = X.shape expected_n_features = self.cluster_centers_.shape[1] if not n_features == expected_n_features: raise ValueError(\"Incorrect number of features. \" \"Got %d features, expected %d\" % ( n_features, expected_n_features)) return X def fit(self, X, y=None, sample_weight=None): \"\"\"Compute k-means clustering. Parameters ---------- X : array-like or sparse matrix, shape=(n_samples, n_features) Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) \"\"\" random_state = check_random_state(self.random_state) self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \\ k_means( X, n_clusters=self.n_clusters, sample_weight=sample_weight, init=self.init, n_init=self.n_init, max_iter=self.max_iter, verbose=self.verbose, precompute_distances=self.precompute_distances, tol=self.tol, random_state=random_state, copy_x=self.copy_x, n_jobs=self.n_jobs, algorithm=self.algorithm, return_n_iter=True) return self def fit_predict(self, X, y=None, sample_weight=None): \"\"\"Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" return self.fit(X, sample_weight=sample_weight).labels_ def fit_transform(self, X, y=None, sample_weight=None): \"\"\"Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" # Currently, this just skips a copy of the data if it is not in # np.array or CSR format already. # XXX This skips _check_test_data, which may change the dtype; # we should refactor the input validation. return self.fit(X, sample_weight=sample_weight)._transform(X) def transform(self, X): \"\"\"Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by `transform` will typically be dense. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) return self._transform(X) def _transform(self, X): \"\"\"guts of transform method; no input validation\"\"\" return euclidean_distances(X, self.cluster_centers_) def predict(self, X, sample_weight=None): \"\"\"Predict the closest cluster each sample in X belongs to. In the vector quantization literature, `cluster_centers_` is called the code book and each value returned by `predict` is the index of the closest code in the code book. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to predict. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return _labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[0] def score(self, X, y=None, sample_weight=None): \"\"\"Opposite of the value of X on the K-means objective. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- score : float Opposite of the value of X on the K-means objective. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return -_labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[1] k_means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239def k_means(X, n_clusters, sample_weight=None, init='k-means++', precompute_distances='auto', n_init=10, max_iter=300, verbose=False, tol=1e-4, random_state=None, copy_x=True, n_jobs=1, algorithm=\"auto\", return_n_iter=False): \"\"\"K-means clustering algorithm. Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- X : array-like or sparse matrix, shape (n_samples, n_features) The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. verbose : boolean, optional Verbosity mode. tol : float, optional The relative increment in the results before declaring convergence. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. return_n_iter : bool, optional Whether or not to return the number of iterations. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). best_n_iter : int Number of iterations corresponding to the best results. Returned only if `return_n_iter` is set to True. \"\"\" if n_init &lt;= 0: raise ValueError(\"Invalid number of initializations.\" \" n_init=%d must be bigger than zero.\" % n_init) random_state = check_random_state(random_state) if max_iter &lt;= 0: raise ValueError('Number of iterations should be a positive number,' ' got %d instead' % max_iter) # avoid forcing order when copy_x=False order = \"C\" if copy_x else None X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32], order=order, copy=copy_x) # verify that the number of samples given is larger than k if _num_samples(X) &lt; n_clusters: raise ValueError(\"n_samples=%d should be &gt;= n_clusters=%d\" % ( _num_samples(X), n_clusters)) tol = _tolerance(X, tol) # If the distances are precomputed every job will create a matrix of shape # (n_clusters, n_samples). To stop KMeans from eating up memory we only # activate this if the created matrix is guaranteed to be under 100MB. 12 # million entries consume a little under 100MB if they are of type double. if precompute_distances == 'auto': n_samples = X.shape[0] precompute_distances = (n_clusters * n_samples) &lt; 12e6 elif isinstance(precompute_distances, bool): pass else: raise ValueError(\"precompute_distances should be 'auto' or True/False\" \", but a value of %r was passed\" % precompute_distances) # Validate init array if hasattr(init, '__array__'): init = check_array(init, dtype=X.dtype.type, copy=True) _validate_center_shape(X, n_clusters, init) if n_init != 1: warnings.warn( 'Explicit initial center position passed: ' 'performing only one init in k-means instead of n_init=%d' % n_init, RuntimeWarning, stacklevel=2) n_init = 1 # subtract of mean of x for more accurate distance computations if not sp.issparse(X): X_mean = X.mean(axis=0) # The copy was already done above X -= X_mean if hasattr(init, '__array__'): init -= X_mean # precompute squared norms of data points x_squared_norms = row_norms(X, squared=True) best_labels, best_inertia, best_centers = None, None, None if n_clusters == 1: # elkan doesn't make sense for a single cluster, full will produce # the right result. algorithm = \"full\" if algorithm == \"auto\": algorithm = \"full\" if sp.issparse(X) else 'elkan' if algorithm == \"full\": kmeans_single = _kmeans_single_lloyd elif algorithm == \"elkan\": kmeans_single = _kmeans_single_elkan else: raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\" \" %s\" % str(algorithm)) if n_jobs == 1: # For a single thread, less memory is needed if we just store one set # of the best results (as opposed to one set per run per thread). for it in range(n_init): # run a k-means once # å¯é€‰ï¼škmeans_single_lloyd or kmeans_single_elkan labels, inertia, centers, n_iter_ = kmeans_single( X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, precompute_distances=precompute_distances, tol=tol, x_squared_norms=x_squared_norms, random_state=random_state) # determine if these results are the best so far if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia best_n_iter = n_iter_ else: # parallelisation of k-means runs seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(kmeans_single)(X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, tol=tol, precompute_distances=precompute_distances, x_squared_norms=x_squared_norms, # Change seed to ensure variety random_state=seed) for seed in seeds) # Get results with the lowest inertia labels, inertia, centers, n_iters = zip(*results) best = np.argmin(inertia) best_labels = labels[best] best_inertia = inertia[best] best_centers = centers[best] best_n_iter = n_iters[best] if not sp.issparse(X): if not copy_x: X += X_mean best_centers += X_mean distinct_clusters = len(set(best_labels)) if distinct_clusters &lt; n_clusters: warnings.warn(\"Number of distinct clusters (&#123;&#125;) found smaller than \" \"n_clusters (&#123;&#125;). Possibly due to duplicate points \" \"in X.\".format(distinct_clusters, n_clusters), ConvergenceWarning, stacklevel=2) if return_n_iter: return best_centers, best_labels, best_inertia, best_n_iter else: return best_centers, best_labels, best_inertia _kmeans_single_lloyd 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127def _kmeans_single_lloyd(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): \"\"\"A single run of k-means, assumes preparation completed prior. Parameters ---------- X : array-like of floats, shape (n_samples, n_features) The observations to cluster. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,) The weights for each observation in X. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (k, p) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. tol : float, optional The relative increment in the results before declaring convergence. verbose : boolean, optional Verbosity mode x_squared_norms : array Precomputed x_squared_norms. precompute_distances : boolean, default: True Precompute distances (faster but takes more memory). random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). n_iter : int Number of iterations run. \"\"\" random_state = check_random_state(random_state) sample_weight = _check_sample_weight(X, sample_weight) best_labels, best_inertia, best_centers = None, None, None # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) if verbose: print(\"Initialization complete\") # Allocate memory to store the distances for each sample to its # closer center for reallocation in case of ties distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype) # iterations for i in range(max_iter): centers_old = centers.copy() # labels assignment is also called the E-step of EM labels, inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, centers, precompute_distances=precompute_distances, distances=distances) # computation of the means is also called the M-step of EM if sp.issparse(X): centers = _k_means._centers_sparse(X, sample_weight, labels, n_clusters, distances) else: centers = _k_means._centers_dense(X, sample_weight, labels, n_clusters, distances) if verbose: print(\"Iteration %2d, inertia %.3f\" % (i, inertia)) if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia center_shift_total = squared_norm(centers_old - centers) if center_shift_total &lt;= tol: if verbose: print(\"Converged at iteration %d: \" \"center shift %e within tolerance %e\" % (i, center_shift_total, tol)) break if center_shift_total &gt; 0: # rerun E-step in case of non-convergence so that predicted labels # match cluster centers best_labels, best_inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, best_centers, precompute_distances=precompute_distances, distances=distances) return best_labels, best_inertia, best_centers, i + 1 _kmeans_single_elkan 123456789101112131415161718192021222324252627def _kmeans_single_elkan(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): if sp.issparse(X): raise TypeError(\"algorithm='elkan' not supported for sparse input X\") random_state = check_random_state(random_state) if x_squared_norms is None: x_squared_norms = row_norms(X, squared=True) # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) centers = np.ascontiguousarray(centers) if verbose: print('Initialization complete') checked_sample_weight = _check_sample_weight(X, sample_weight) centers, labels, n_iter = k_means_elkan(X, checked_sample_weight, n_clusters, centers, tol=tol, max_iter=max_iter, verbose=verbose) if sample_weight is None: inertia = np.sum((X - centers[labels]) ** 2, dtype=np.float64) else: sq_distances = np.sum((X - centers[labels]) ** 2, axis=1, dtype=np.float64) * checked_sample_weight inertia = np.sum(sq_distances, dtype=np.float64) return labels, inertia, centers, n_iter 2. K-Modes Algorithm step1ï¼šéšæœºç¡®å®škä¸ªèšç±»ä¸­å¿ƒ$C_1$, $C_2$ â€¦ $C_k$ï¼Œ$C_i$æ˜¯é•¿åº¦ä¸ºMçš„å‘é‡ï¼Œ$C_i$ = [$C_{1i}$, $C_{2i}$, â€¦ , $C_{mi}$] step2ï¼šå¯¹äºæ ·æœ¬$x_j$ (j=1,2,â€¦,N)ï¼Œåˆ†åˆ«æ¯”è¾ƒå…¶ä¸kä¸ªä¸­å¿ƒä¹‹é—´çš„è·ç¦» è¿™é‡Œçš„**è·ç¦»ä¸ºä¸åŒå±æ€§å€¼çš„ä¸ªæ•°**ï¼Œå‡å¦‚$x_1$=[1, 2, 1, 3], $C_1$=[1, 2, 3, 4]ï¼Œé‚£ä¹ˆx1ä¸C1ä¹‹é—´çš„è·ç¦»ä¸º2 step3ï¼šå°†$x_j$åˆ’åˆ†åˆ°è·ç¦»æœ€å°çš„ç°‡ï¼Œåœ¨å…¨éƒ¨çš„æ ·æœ¬éƒ½è¢«åˆ’åˆ†å®Œæ¯•ä¹‹åï¼Œé‡æ–°ç¡®å®šç°‡ä¸­å¿ƒï¼Œå‘é‡$C_i$ä¸­çš„æ¯ä¸€ä¸ªåˆ†é‡éƒ½æ›´æ–°ä¸ºç°‡iä¸­çš„ä¼—æ•° step4ï¼šé‡å¤æ­¥éª¤äºŒå’Œä¸‰ï¼Œç›´åˆ°æ€»è·ç¦»ï¼ˆå„ä¸ªç°‡ä¸­æ ·æœ¬ä¸å„è‡ªç°‡ä¸­å¿ƒè·ç¦»ä¹‹å’Œï¼‰ä¸å†é™ä½ï¼Œè¿”å›æœ€åçš„èšç±»ç»“æœ KModes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152class KModes(BaseEstimator, ClusterMixin): \"\"\"k-modes clustering algorithm for categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. cat_dissim : func, default: matching_dissim Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : &#123;'Huang', 'Cao', 'random' or an ndarray&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centroids. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. verbose : int, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, cat_dissim=matching_dissim, init='Cao', n_init=1, verbose=0, random_state=None, n_jobs=1): self.n_clusters = n_clusters # ç°‡çš„ä¸ªæ•° self.max_iter = max_iter # self.cat_dissim = cat_dissim # ç±»åˆ«é—´è·ç¦»çš„è®¡ç®—æ–¹æ³• self.init = init # åˆå§‹ä¸­å¿ƒç‚¹çš„é€‰å–æ–¹æ³• &#123;'Huang', 'Cao', 'random' # or an ndarray&#125; self.n_init = n_init # è¿è¡Œæ¬¡æ•°å–æœ€ä½³å€¼ self.verbose = verbose # self.random_state = random_state # self.n_jobs = n_jobs # if ((isinstance(self.init, str) and self.init == 'Cao') or hasattr(self.init, '__array__')) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method and algorithm are deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, **kwargs): \"\"\"Compute k-modes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] \"\"\" random_state = check_random_state(self.random_state) self._enc_cluster_centroids, self._enc_map, self.labels_,\\ self.cost_, self.n_iter_ = k_modes(X, self.n_clusters, self.max_iter, self.cat_dissim, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def fit_predict(self, X, y=None, **kwargs): \"\"\"Compute cluster centroids and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). \"\"\" return self.fit(X, **kwargs).predict(X, **kwargs) def predict(self, X, **kwargs): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" if self.verbose and self.cat_dissim == ng_dissim: print(\"Ng's dissimilarity measure was used to train this model, \" \"but now that it is predicting the model will fall back to \" \"using simple matching dissimilarity.\") X = check_array(X, dtype=None) X, _ = encode_features(X, enc_map=self._enc_map) return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return decode_centroids(self._enc_cluster_centroids, self._enc_map) else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_modes 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def k_modes(X, n_clusters, max_iter, dissim, init, n_init, verbose, random_state, n_jobs): \"\"\"k-modes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-modes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values X = check_array(X, dtype=None) # Convert the categorical values in X to integers for speed. # Based on the unique values in X, we can make a mapping to achieve this. X, enc_map = encode_features(X) n_points, n_attrs = X.shape assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = unique results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_modes_single)(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best] k_modes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, random_state): \"\"\" X : n_clusters : n_points : n_points, n_attrs = X.shape n_attrs : max_iter : Maximum number of iterations of the k-modes algorithm for a single run. dissim : func, default: matching_dissim, &#123;matching_dissim, ng_dissim&#125; Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : inti_no : verbose : random_state : \"\"\" random_state = check_random_state(random_state) # _____ INIT _____ # åˆå§‹åŒ–ï¼Œé€‰å–ä¸­å¿ƒç‚¹ if verbose: print(\"Init: initializing centroids\") # 1. method huang if isinstance(init, str) and init.lower() == 'huang': centroids = init_huang(X, n_clusters, dissim, random_state) # 2. method cao elif isinstance(init, str) and init.lower() == 'cao': centroids = init_cao(X, n_clusters, dissim) # 3. éšæœºé€‰å– elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = X[seeds] # 4. ä½¿ç”¨æä¾›å¥½çš„array elif hasattr(init, '__array__'): # Make sure init is a 2D array. if len(init.shape) == 1: init = np.atleast_2d(init).T assert init.shape[0] == n_clusters, \\ \"Wrong number of initial centroids in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[0], n_clusters) assert init.shape[1] == n_attrs, \\ \"Wrong number of attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[1], n_attrs) centroids = np.asarray(init, dtype=np.uint16) else: raise NotImplementedError if verbose: print(\"Init: initializing clusters\") # rows : n_clusters, cols : n_points membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # cl_attr_freq is a list of lists with dictionaries that contain the # frequencies of values per cluster and attribute. # rows : n_clusters, cols : n_attrs cl_attr_freq = [[defaultdict(int) for _ in range(n_attrs)] for _ in range(n_clusters)] for ipoint, curpoint in enumerate(X): # Initial assignment to clusters # è¿”å›è·ç¦»æœ€è¿‘çš„ä¸­å¿ƒç‚¹ clust = np.argmin(dissim(centroids, curpoint, X=X, membship=membship)) membship[clust, ipoint] = 1 # Count attribute values per cluster. for iattr, curattr in enumerate(curpoint): cl_attr_freq[clust][iattr][curattr] += 1 # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(n_attrs): if sum(membship[ik]) == 0: # Empty centroid, choose randomly centroids[ik, iattr] = random_state.choice(X[:, iattr]) else: centroids[ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_modes_iter(X, centroids, cl_attr_freq, membship, dissim, random_state) # All points seen in this iteration labels, ncost = _labels_cost(X, centroids, dissim, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, cost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, cost)) return centroids, labels, cost, itr 3. K-prototype Algorithm KPrototypes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160class KPrototypes(kmodes.KModes): \"\"\"k-protoypes clustering algorithm for mixed numerical/categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. num_dissim : func, default: euclidian_dissim Dissimilarity function used by the algorithm for numerical variables. Defaults to the Euclidian dissimilarity function. cat_dissim : func, default: matching_dissim Dissimilarity function used by the kmodes algorithm for categorical variables. Defaults to the matching dissimilarity function. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. init : &#123;'Huang', 'Cao', 'random' or a list of ndarrays&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If a list of ndarrays is passed, it should be of length 2, with shapes (n_clusters, n_features) for numerical and categorical data respectively. These are the initial centroids. gamma : float, default: None Weighing factor that determines relative importance of numerical vs. categorical attributes (see discussion in Huang [1997]). By default, automatically calculated from data. verbose : integer, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. gamma : float The (potentially calculated) weighing factor. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, num_dissim=euclidean_dissim, cat_dissim=matching_dissim, init='Huang', n_init=10, gamma=None, verbose=0, random_state=None, n_jobs=1): super(KPrototypes, self).__init__(n_clusters, max_iter, cat_dissim, init, verbose=verbose, random_state=random_state, n_jobs=n_jobs) self.num_dissim = num_dissim self.gamma = gamma self.n_init = n_init if isinstance(self.init, list) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method is deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, categorical=None): \"\"\"Compute k-prototypes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] categorical : Index of columns that contain categorical data \"\"\" random_state = check_random_state(self.random_state) # If self.gamma is None, gamma will be automatically determined from # the data. The function below returns its value. self._enc_cluster_centroids, self._enc_map, self.labels_, self.cost_,\\ self.n_iter_, self.gamma = k_prototypes(X, categorical, self.n_clusters, self.max_iter, self.num_dissim, self.cat_dissim, self.gamma, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def predict(self, X, categorical=None): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. categorical : Index of columns that contain categorical data Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) Xcat, _ = encode_features(Xcat, enc_map=self._enc_map) return _labels_cost(Xnum, Xcat, self._enc_cluster_centroids, self.num_dissim, self.cat_dissim, self.gamma)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return [ self._enc_cluster_centroids[0], decode_centroids(self._enc_cluster_centroids[1], self._enc_map) ] else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_prototypes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677def k_prototypes(X, categorical, n_clusters, max_iter, num_dissim, cat_dissim, gamma, init, n_init, verbose, random_state, n_jobs): \"\"\"k-prototypes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-prototypes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values if categorical is None or not categorical: raise NotImplementedError( \"No categorical data selected, effectively doing k-means. \" \"Present a list of categorical columns, or use scikit-learn's \" \"KMeans instead.\" ) if isinstance(categorical, int): categorical = [categorical] assert len(categorical) != X.shape[1], \\ \"All columns are categorical, use k-modes instead of k-prototypes.\" assert max(categorical) &lt; X.shape[1], \\ \"Categorical index larger than number of columns.\" ncatattrs = len(categorical) nnumattrs = X.shape[1] - ncatattrs n_points = X.shape[0] assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) # Convert the categorical values in Xcat to integers for speed. # Based on the unique values in Xcat, we can make a mapping to achieve this. Xcat, enc_map = encode_features(Xcat) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = list(_split_num_cat(unique, categorical)) init[1], _ = encode_features(init[1], enc_map) # Estimate a good value for gamma, which determines the weighing of # categorical values in clusters (see Huang [1997]). if gamma is None: gamma = 0.5 * Xnum.std() results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_prototypes_single)(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) # Note: return gamma in case it was automatically determined. return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best], gamma k_prototypes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132def k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, random_state): # For numerical part of initialization, we don't have a guarantee # that there is not an empty cluster, so we need to retry until # there is none. random_state = check_random_state(random_state) init_tries = 0 # åˆå§‹åŒ–å‡ ä¸ªä¸­å¿ƒç‚¹ # å¯èƒ½å­˜åœ¨å¤±è´¥çš„æƒ…å†µ while True: init_tries += 1 # _____ INIT _____ if verbose: print(\"Init: initializing centroids\") if isinstance(init, str) and init.lower() == 'huang': centroids = kmodes.init_huang(Xcat, n_clusters, cat_dissim, random_state) elif isinstance(init, str) and init.lower() == 'cao': centroids = kmodes.init_cao(Xcat, n_clusters, cat_dissim) elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = Xcat[seeds] elif isinstance(init, list): # Make sure inits are 2D arrays. init = [np.atleast_2d(cur_init).T if len(cur_init.shape) == 1 else cur_init for cur_init in init] assert init[0].shape[0] == n_clusters, \\ \"Wrong number of initial numerical centroids in init \" \\ \"(&#123;&#125;, should be &#123;&#125;).\".format(init[0].shape[0], n_clusters) assert init[0].shape[1] == nnumattrs, \\ \"Wrong number of numerical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[0].shape[1], nnumattrs) assert init[1].shape[0] == n_clusters, \\ \"Wrong number of initial categorical centroids in init (&#123;&#125;, \" \\ \"should be &#123;&#125;).\".format(init[1].shape[0], n_clusters) assert init[1].shape[1] == ncatattrs, \\ \"Wrong number of categorical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[1].shape[1], ncatattrs) centroids = [np.asarray(init[0], dtype=np.float64), np.asarray(init[1], dtype=np.uint16)] else: raise NotImplementedError(\"Initialization method not supported.\") if not isinstance(init, list): # Numerical is initialized by drawing from normal distribution, # categorical following the k-modes methods. meanx = np.mean(Xnum, axis=0) stdx = np.std(Xnum, axis=0) centroids = [ meanx + random_state.randn(n_clusters, nnumattrs) * stdx, centroids ] if verbose: print(\"Init: initializing clusters\") # è®¡ç®—å¯¹åº”ç»„å†…çš„æˆå‘˜ membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # Keep track of the sum of attribute values per cluster so that we # can do k-means on the numerical attributes. cl_attr_sum = np.zeros((n_clusters, nnumattrs), dtype=np.float64) # Same for the membership sum per cluster cl_memb_sum = np.zeros(n_clusters, dtype=int) # cl_attr_freq is a list of lists with dictionaries that contain # the frequencies of values per cluster and attribute. cl_attr_freq = [[defaultdict(int) for _ in range(ncatattrs)] for _ in range(n_clusters)] for ipoint in range(n_points): # Initial assignment to clusters # è®¡ç®—åˆå§‹çš„å½’å±ç±»ï¼Œåˆ†åˆ«ç”±ç±»åˆ«çš„å’Œæ•°å€¼å‹æ•°æ®ç»„æˆ # gammaæ˜¯ç±»åˆ«ç‰¹å¾çš„æƒé‡ clust = np.argmin( num_dissim(centroids[0], Xnum[ipoint]) + gamma * cat_dissim(centroids[1], Xcat[ipoint], X=Xcat, membship=membship) ) membship[clust, ipoint] = 1 cl_memb_sum[clust] += 1 # Count attribute values per cluster. for iattr, curattr in enumerate(Xnum[ipoint]): cl_attr_sum[clust, iattr] += curattr for iattr, curattr in enumerate(Xcat[ipoint]): cl_attr_freq[clust][iattr][curattr] += 1 # If no empty clusters, then consider initialization finalized. # å¦‚æœæ¯ä¸€ç»„çš„æˆå‘˜ä¸ªæ•°éƒ½å¤§äº0çš„è¯ï¼Œåˆ™ç®—æ˜¯åˆå§‹åŒ–æˆåŠŸï¼Œå¦åˆ™éœ€è¦é‡æ–°åˆå§‹åŒ– if membship.sum(axis=1).min() &gt; 0: break # TODO: å¦‚æœä¸ä¿®æ”¹éšæœºç§å­æˆ–è€…å…¶ä»–çš„æ•°æ®ï¼Œæ¯æ¬¡è·‘çš„ç»“æœéƒ½æ˜¯ä¸€æ ·çš„ if init_tries == MAX_INIT_TRIES: # Could not get rid of empty clusters. Randomly # initialize instead. init = 'random' elif init_tries == RAISE_INIT_TRIES: raise ValueError( \"Clustering algorithm could not initialize. \" \"Consider assigning the initial clusters manually.\" ) # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(nnumattrs): centroids[0][ik, iattr] = cl_attr_sum[ik, iattr] / cl_memb_sum[ik] for iattr in range(ncatattrs): centroids[1][ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_prototypes_iter(Xnum, Xcat, centroids, cl_attr_sum, cl_memb_sum, cl_attr_freq, membship, num_dissim, cat_dissim, gamma, random_state) # All points seen in this iteration labels, ncost = _labels_cost(Xnum, Xcat, centroids, num_dissim, cat_dissim, gamma, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run: &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, ncost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, ncost)) return centroids, labels, cost, itr","tags":[{"name":"Cluster","slug":"Cluster","permalink":"http://chenson.cc/tags/Cluster/"}]},{"title":"å…³äºPythonçš„Mixinæ¨¡å¼åˆæ¢","date":"2018-11-19T23:25:13.000Z","path":"2018/11/20/å…³äºPythonçš„Mixinæ¨¡å¼åˆæ¢/","text":"1. ç»å…¸ç±» / æ–°å¼ç±» ç»å…¸ç±» (Python2.2ä¹‹å‰çš„ç‰ˆæœ¬) ç»å…¸ç±»æ˜¯ä¸€ç§æ²¡æœ‰ç»§æ‰¿çš„ç±»ï¼Œå®ä¾‹ç±»å‹éƒ½æ˜¯typeç±»å‹(???)ã€‚å¦‚æœç»å…¸ç±»è¢«ä½œä¸ºçˆ¶ç±»ï¼Œå­ç±»åœ¨è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ—¶å°±ä¼šå‡ºé”™ã€‚è¿™æ—¶å€™MROçš„æ–¹æ³•ä¸ºDFSï¼ˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆå­èŠ‚ç‚¹é¡ºåºï¼šä»å·¦åˆ°å³ï¼‰ï¼‰ 1234import inspect # inspect.getmroï¼ˆAï¼‰å¯ä»¥æŸ¥çœ‹ç»å…¸ç±»çš„MROé¡ºåºclass A: def __init__(self): print('This is a classic class.') http://python.jobbole.com/85685/ æ–°å¼ç±» ä¸ºäº†ä½¿ç±»å’Œå†…ç½®ç±»å‹æ›´åŠ ç»Ÿä¸€(???)ï¼Œå¼•å…¥äº†æ–°å¼ç±»ã€‚æ–°å¼ç±»çš„æ¯ä¸ªç±»éƒ½ç»§æ‰¿äºä¸€ä¸ªåŸºç±»ï¼Œå¯ä»¥æ˜¯è‡ªå®šä¹‰ç±»æˆ–è€…å…¶å®ƒç±»ï¼Œé»˜è®¤æ‰¿äºobjectã€‚å­ç±»å¯ä»¥è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°ã€‚ è¿™æ—¶æœ‰ä¸¤ç§MROçš„æ–¹æ³• å¦‚æœæ˜¯ç»å…¸ç±»MROä¸ºDFSï¼ˆæ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆå­èŠ‚ç‚¹é¡ºåºï¼šä»å·¦åˆ°å³ï¼‰ï¼‰ã€‚ å¦‚æœæ˜¯æ–°å¼ç±»MROä¸ºBFSï¼ˆå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆå­èŠ‚ç‚¹é¡ºåºï¼šä»å·¦åˆ°å³ï¼‰ï¼‰ã€‚ 123class A(object): def __init__(self): print('This is a new-style class.') 1234567891011121314151617181920class D(object): pass class E(object): pass class F(object): pass class C(D, F): pass class B(E, D): pass class A(B, C): pass if __name__ == '__main__': print A.__mro__ 2. å¤šé‡ç»§æ‰¿super https://blog.csdn.net/qwertyupoiuytr/article/details/56439134 3. Mix-inMixinè¡¨ç¤ºçš„æ˜¯Mix-inï¼Œè¡¨ç¤ºè¿™ä¸ªç±»æ˜¯ä½œä¸ºåŠŸèƒ½æ·»åŠ åˆ°å­ç±»ä¸­ï¼Œè€Œä¸æ˜¯ä½œä¸ºçˆ¶ç±»ï¼Œå®ƒçš„ä½œç”¨åŒJavaä¸­çš„æ¥å£ã€‚ æ™®é€šç±»å¤šé‡ç»§æ‰¿ä¸‹ï¼Œåªèƒ½æœ‰ä¸€ä¸ªæ™®é€šçˆ¶ç±»å’Œè‹¥å¹²ä¸ªMixinç±»ï¼ˆä¿æŒä¸»å¹²å•ä¸€ï¼‰ Mixinç±»ä¸èƒ½ç»§æ‰¿æ™®é€šç±»ï¼ˆé¿å…é’»çŸ³ç»§æ‰¿ï¼Œé‡å¤è°ƒç”¨ï¼‰ Mixin ç±»åº”è¯¥å•ä¸€èŒè´£ï¼ˆå‚è€ƒ Java çš„ interface è®¾è®¡ï¼ŒMixin å’Œæ­¤æå…¶ç›¸ä¼¼ï¼Œåªä¸è¿‡é™„å¸¦å®ç°è€Œå·²ï¼‰ ä½¿ç”¨Mixinç±»å®ç°å¤šé‡ç»§æ‰¿è¦éå¸¸å°å¿ƒ é¦–å…ˆå¿…é¡»æ˜¯è¡¨ç¤ºä¸€ç§åŠŸèƒ½ï¼Œè€Œä¸æ˜¯æŸä¸ªç‰©å“ å¿…é¡»è´£ä»»å•ä¸€ï¼Œå¦‚æœæœ‰å¤šä¸ªåŠŸèƒ½ï¼Œé‚£ä¹ˆå°±å†™å¤šä¸ªMixinç±» ç„¶åä¸èƒ½ä¾èµ–äºå­ç±»å®ç° å³ä½¿å­ç±»æ²¡æœ‰ç»§æ‰¿è¿™ä¸ªMixinç±»ï¼Œä¹Ÿå¯ä»¥ç…§æ ·å·¥ä½œï¼Œå°±æ˜¯ç¼ºå¤±æŸä¸ªåŠŸèƒ½","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"}]},{"title":"åœ¨çº¿å­¦ä¹ ç®—æ³•FTRLåˆæ¢","date":"2018-11-18T07:36:28.000Z","path":"2018/11/18/åœ¨çº¿å­¦ä¹ ç®—æ³•FTRLåˆæ¢/","text":"","tags":[{"name":"FTRL","slug":"FTRL","permalink":"http://chenson.cc/tags/FTRL/"}]},{"title":"Wide_and_Deepæ¨¡å‹åˆæ¢","date":"2018-11-18T06:01:03.000Z","path":"2018/11/18/Wide-and-Deepæ¨¡å‹åˆæ¢/","text":"1. BackgroundWide and Deepæ˜¯Googleåœ¨16å¹´å·¦å³æå‡ºçš„æ¨¡å‹ï¼Œä¸»è¦åº”ç”¨åœ¨äº†Google Playçš„åº”ç”¨æ¨èç³»ç»Ÿä¸­ï¼ŒåŒæ—¶å›½å†…çš„ä¸€äº›å¤§å‚ä¹Ÿåœ¨å„è‡ªçš„ä¸šåŠ¡ä¸­æœ‰ä½¿ç”¨åˆ°è¿™ä¸€æ¨¡å‹ï¼Œæ¯”å¦‚ç¾å›¢ã€‚è€Œè¯¥ç®—æ³•çš„æ ¸å¿ƒä¸»è¦åœ¨äºç»“åˆäº†çº¿æ€§æ¨¡å‹çš„è®°å¿†èƒ½åŠ›Memorization å’Œæ·±åº¦ç¥ç»ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›Generaizationï¼Œä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–è¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œå³Jointly Trainingã€‚ 2. Overview å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¯¥æ¨¡å‹çš„ä¸»è¦ç»“æ„æ˜¯ä¸­é—´Wide&amp;Deep Modelsé‚£ä¸ªéƒ¨åˆ†ï¼Œå…¶ä¸»è¦ç”±å·¦è¾¹çš„Wide Models å’Œå³è¾¹çš„Deep Modelsç»„æˆã€‚ 3. Wide Partæœ€å·¦ç«¯çš„å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œä¸åšè¿‡å¤šçš„è§£é‡Šã€‚ è¯¥éƒ¨åˆ†çš„æ¨¡å‹ï¼Œè¾“å…¥çš„ç‰¹å¾å¯ä»¥æ˜¯è¿ç»­ç‰¹å¾ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¨€ç–çš„ç¦»æ•£ç‰¹å¾ã€‚ä½†è¯¥æ¨¡å‹å¯ä»¥å¯¹ç¦»æ•£ç‰¹å¾ä¹‹é—´åšç‰¹å¾äº¤å‰ï¼Œå³Cross-Productï¼Œä¸ºäº†å¯ä»¥ä»å†å²æ•°æ®ä¸­å‘ç°ç‰¹å¾ä¹‹é—´çš„ç›¸å…³ä¿¡æ¯ï¼Œç„¶åå¯¹äº¤å‰è¿‡åçš„ç‰¹å¾åšOne-Hotå¤„ç†ã€‚ 4. Deep Partæœ€å³éƒ¨åˆ†æ˜¯ä¸€ä¸ªDNNæ¨¡å‹ï¼Œä¸€ä¸ªå‰é¦ˆçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚ è¯¥éƒ¨åˆ†çš„æ¨¡å‹ï¼Œè¾“å…¥çš„ç‰¹å¾åŒæ ·å¯ä»¥æ˜¯è¿ç»­ç‰¹å¾ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¨€ç–çš„ç¦»æ•£ç‰¹å¾ã€‚ä½†å’Œä¸Šé¢éƒ¨åˆ†çš„ç¦»æ•£ç‰¹å¾å¤„ç†æ–¹å¼ä¸åŒï¼Œè¿™é‡Œå¯¹ç¦»æ•£ç‰¹å¾åšäº†Embeddingï¼Œå³å°†é«˜ç»´ç¨€ç–çš„ç¦»æ•£ç‰¹å¾è½¬åŒ–ä¸ºä½çº¬çš„ç¨ å¯†ç‰¹å¾ã€‚ Embeddingçš„åˆå§‹æƒé‡æ˜¯éšæœºèµ‹äºˆçš„ï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸æ–­å‚ä¸ä¼˜åŒ–ã€‚ æ‰€ä»¥è®­ç»ƒçš„è¿‡ç¨‹å¤§è‡´æ˜¯éœ€è¦åšEmbeddingçš„å‘é‡å…ˆè½¬åŒ–æˆDense Vectorï¼Œç„¶åå’Œå…¶ä»–çš„è¿ç»­å‹æˆ–è€…æ•°å€¼å‹çš„ç‰¹å¾åˆå¹¶ä¸€èµ·ï¼Œå–‚ç»™æ¥ä¸‹å»çš„å‡ å±‚DNNã€‚ å…¶ä¸­éšå«å±‚çš„è®¡ç®—æ–¹æ³•ä¸º a^{l + 1} = f(W^{(l)}a^{(l)} + b^{(l)})è¿™é‡Œ læ˜¯éšå«å±‚æ•°ï¼Œ fæ˜¯æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ä½¿ç”¨ReLUsä½œä¸ºæ¿€æ´»å‡½æ•° 5. Wide&amp;Deep Partå¦‚ä¸Šå›¾ä¸­é—´éƒ¨åˆ†æ‰€ç¤ºï¼Œä¸¤ä¸ªæ¨¡å‹æœ€åæ˜¯åˆå¹¶åˆ°ä¸€èµ·è®­ç»ƒçš„ï¼Œå¹¶å°†ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœçš„åŠ æƒå’Œä½œä¸ºæœ€ç»ˆçš„é¢„æµ‹ç»“æœï¼ˆæœ€ç»ˆå–‚ç»™äº†åŒä¸€ä¸ªLRæ¨¡å‹å»è®­ç»ƒï¼‰ï¼Œå…·ä½“å¦‚ä¸‹å…¬å¼æ‰€ç¤ºã€‚ P(Y=1 | x) = \\sigma(w^T_{wide}[x, \\phi(x)] + w^T_{deep}a^{(l_f)} + b)è¿™é‡Œ$\\sigma(Â·)$æ˜¯sigmoidæ¿€æ´»å‡½æ•°ã€‚ åœ¨è®ºæ–‡ä¸­ä½œè€…ä¹Ÿç‰¹åˆ«çš„æåˆ°ï¼Œè¿™é‡Œçš„Joinly Training å’ŒEnsembleæ¨¡å‹æ˜¯ä¸ä¸€æ ·çš„ã€‚å¯¹äºEnsembleæ¨¡å‹ï¼Œå„ä¸ªå­æ¨¡å‹æ˜¯å•ç‹¬è®­ç»ƒçš„ï¼Œå„è‡ªçš„é¢„æµ‹ç»“æœä¸å½±å“å…¶ä»–çš„å­æ¨¡å‹ï¼Œåªæ˜¯æœ€åå¤§å®¶çš„é¢„æµ‹ç»“æœé€šè¿‡æŸä¸€ç§æ–¹å¼ï¼Œæ¯”å¦‚æŠ•ç¥¨ï¼ŒåŠ æƒç­‰ï¼Œæœ€ç»ˆå¾—å‡ºä¸€ä¸ªé¢„æµ‹ç»“æœã€‚ Jointly Trainingçš„é¢„æµ‹ç»“æœï¼Œæœ€ç»ˆä¼šé€šè¿‡Back-Propagating the Gradientsï¼Œä¼ æ’­åˆ°Wide Partå’ŒDeep Partã€‚è¿™é‡Œä¸¤éƒ¨åˆ†æ¨¡å‹é‡‡ç”¨äº†ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼š Wide Modelsé‡‡ç”¨äº†FTRL + L1æ­£åˆ™ Deep Modelsé‡‡ç”¨äº†AdaGrad 6. Conclusionåœ¨æœ€ç»ˆçš„æµ‹è¯•éƒ¨åˆ†ï¼Œæˆ‘è‡ªå·±å‡†å¤‡äº†ä¸€ä»½æ•°æ®ç”¨äºæµ‹è¯•è¯¥æ¨¡å‹ã€‚ Wide Part Deep Part Wide &amp; Deep Part ä»ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹å‡ºï¼Œä»Wide Modelsåˆ°Deep Modelsï¼Œè®­ç»ƒæ•ˆæœæå‡è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ï¼Œä½†ä»Deep Modelsåˆ°Wide&amp;Deep Modelsè®­ç»ƒç»“æœéä½†æ²¡æœ‰æå‡ï¼Œè¿˜æœ‰äº†ä¸€ä¸ä¸çš„ä¸‹é™ï¼Œä¸”è¿™ä¸ªæˆ‘ä¹Ÿå·²ç»æµ‹è¯•äº†å¾ˆå¤šæµ‹ï¼Œä¸»è¦ä¿®æ”¹äº†Embeddingä¸­å’ŒWide Modelsä¸­çš„ä¸€äº›å‚æ•°ï¼Œåè€…å§‹ç»ˆæ²¡æœ‰è¶…è¿‡å‰è€…çš„æ•ˆæœã€‚ä½†åœ¨ç½‘ä¸Šæ‰¾çš„ä¸€äº›æµ‹è¯•æ•°æ®ä¸­ï¼ŒWide&amp;Deep Modelsè¿˜æ˜¯æœ‰ä¸€ä¸ä¸çš„æå‡ã€‚åŸå› æˆ‘åˆ†æäº†ä¸€äº›å¯èƒ½ç”±å¦‚ä¸‹é€ æˆ æ•°æ®é›†æœ¬èº«å¯èƒ½ä¸å¤ªé€‚åˆè¿™ä¸ªç®—æ³• æ•°æ®å¤„ç†æ–¹å¼ï¼Œå› ä¸ºæœ‰éƒ¨åˆ†æ•°æ®æœ‰ç©ºå€¼ï¼Œæˆ‘åŸºæœ¬åªæ˜¯ç®€å•çš„å¡«å……äº†ä¸€ä¸‹ Wide Modelsè®­ç»ƒæ•°æ® è®­ç»ƒå‚æ•° 7. References æ·±åº¦å­¦ä¹ åœ¨ç¾å›¢ç‚¹è¯„æ¨èå¹³å°æ’åºä¸­çš„è¿ç”¨ Wide &amp; Deep Learning for Recommender Systems ç®€å•æ˜“å­¦çš„æ·±åº¦å­¦ä¹ ç®—æ³•â€”â€”Wide &amp; Deep Learning","tags":[{"name":"Wide&Deep","slug":"Wide-Deep","permalink":"http://chenson.cc/tags/Wide-Deep/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://chenson.cc/tags/Pytorch/"}]},{"title":"Neo4jæ•°æ®å¯¼å…¥ç¬”è®°","date":"2018-08-31T15:23:32.000Z","path":"2018/08/31/Neo4jæ•°æ®å¯¼å…¥ç¬”è®°/","text":"1. å¯¼å…¥æ•°æ®å¯¼å…¥æ•°æ®å¤§æ¦‚æœ‰ä»¥ä¸‹å‡ ç§æ–¹å¼ Cypherçš„CREATEè¯­å¥ Cypherçš„LOAD CSVè¯­å¥ å®˜æ–¹çš„Java APIå¯¼å…¥ - Batch Inserter å®˜æ–¹æä¾›çš„neo4j-importå·¥å…· ç½‘å‹ç¼–å†™çš„Batch-importå·¥å…· 2. å…·ä½“æ“ä½œ2.1 CREATE2.2 LOAD CSV2.3 Batch Insert2.4 neo4j-import éœ€è¦åœæ­¢Neo4j åªèƒ½ç”Ÿæˆæ–°çš„æ•°æ®åº“ï¼Œå³æ— æ³•åœ¨å·²å­˜åœ¨çš„æ•°æ®åº“ä¸­æ’å…¥æ•°æ® 12345bin/neo4j-import --into retail.db --id-type string \\ --nodes:Customer customers.csv --nodes products.csv \\ --nodes orders_header.csv,orders1.csv,orders2.csv \\ --relationships:CONTAINS order_details.csv \\ --relationships:ORDERED customer_orders_header.csv,orders1.csv,orders2.csv å‚æ•°è¯´æ˜ --into retail.dbæ˜¾ç„¶æ˜¯ç›®æ ‡æ•°æ®åº“ï¼Œå…¶ä¸­ä¸èƒ½åŒ…å«ç°æœ‰æ•°æ®åº“ã€‚ é‡å¤--nodeså’Œ--relationshipså‚æ•°æ˜¯åŒä¸€å®ä½“çš„å¤šä¸ªï¼ˆå¯èƒ½åˆ†è£‚çš„ï¼‰csvæ–‡ä»¶çš„ç»„ï¼Œå³å…·æœ‰ç›¸åŒçš„åˆ—ç»“æ„ã€‚ æ¯ç»„çš„æ‰€æœ‰æ–‡ä»¶éƒ½è¢«è§†ä¸ºå¯ä»¥è¿æ¥æˆä¸€ä¸ªå¤§æ–‡ä»¶ã€‚ä¸€ä¸ªæ ‡é¢˜è¡Œçš„ç»„çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶æ˜¯å¿…éœ€çš„ï¼Œå®ƒç”šè‡³å¯èƒ½è¢«åŒ…å«åœ¨å…¶ä¸­å¯èƒ½æ¯”ä¸€ä¸ªå¤šGBçš„æ–‡æœ¬æ–‡ä»¶æ›´æ˜“äºå¤„ç†å’Œç¼–è¾‘ä¸€ä¸ªå•è¡Œæ–‡ä»¶ã€‚ä¹Ÿæ”¯æŒå‹ç¼©æ–‡ä»¶ã€‚ customers.csvç›´æ¥ä½œä¸ºå¸¦æœ‰:Customeræ ‡ç­¾çš„èŠ‚ç‚¹å¯¼å…¥ï¼Œå±æ€§ç›´æ¥ä»æ–‡ä»¶ä¸­è·å–ã€‚å¯¹äºä»:LABELåˆ—ä¸­è·å–èŠ‚ç‚¹æ ‡ç­¾çš„äº§å“ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è®¢å•èŠ‚ç‚¹å–è‡ª3ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ ‡é¢˜å’Œä¸¤ä¸ªå†…å®¹æ–‡ä»¶ã€‚è¾“å…¥:CONTAINSçš„order_details.csvè®¢å•é¡¹å…³ç³»æ˜¯é€šè¿‡å…¶ID æ¥åˆ›å»ºçš„ï¼ŒåŒ…å«ä¸æ‰€åŒ…å«äº§å“çš„è®¢å•ã€‚è®¢å•é€šè¿‡å†æ¬¡ä½¿ç”¨è®¢å•csvæ–‡ä»¶è¿æ¥åˆ°å®¢æˆ·ï¼Œä½†è¿™æ¬¡ä½¿ç”¨ä¸åŒçš„æ ‡å¤´ï¼Œå…¶ä¸­ï¼šIGNOREæ˜¯ä¸ç›¸å…³çš„åˆ—è¿™â€“id-type stringè¡¨ç¤ºæ‰€æœ‰:IDåˆ—éƒ½åŒ…å«å­—æ¯æ•°å­—å€¼ï¼ˆå¯¹ä»…æ•°å­—IDè¿›è¡Œä¼˜åŒ–ï¼‰ã€‚åˆ—åç”¨äºèŠ‚ç‚¹å’Œå…³ç³»çš„å±æ€§åç§°ï¼Œç‰¹å®šåˆ—æœ‰ä¸€äº›é¢å¤–çš„æ ‡è®° name:ID - å…¨å±€idåˆ—ï¼Œé€šè¿‡è¯¥åˆ—æŸ¥æ‰¾èŠ‚ç‚¹ä»¥ä¾¿ä»¥åé‡æ–°è¿æ¥ï¼Œå¦‚æœä¿ç•™å±æ€§åç§°ï¼Œå®ƒå°†ä¸ä¼šè¢«å­˜å‚¨ï¼ˆä¸´æ—¶ï¼‰ï¼Œè¿™å°±æ˜¯â€“id-typeæ‰€æŒ‡çš„å¦‚æœä½ æœ‰è·¨å®ä½“çš„é‡å¤idï¼Œä½ å¿…é¡»åœ¨æ‹¬å·ä¸­æä¾›å®ä½“ï¼ˆid-groupï¼‰ :ID(Order)å¦‚æœæ‚¨çš„IDæ˜¯å…¨çƒå”¯ä¸€çš„ï¼Œæ‚¨å¯ä»¥å°†å…¶å…³é—­ :LABEL - èŠ‚ç‚¹çš„æ ‡ç­¾åˆ—ï¼Œå¤šä¸ªæ ‡ç­¾å¯ä»¥ç”¨åˆ†éš”ç¬¦åˆ†éš” :START_IDï¼Œ:END_ID- å…³ç³»æ–‡ä»¶åˆ—ï¼Œå¼•ç”¨èŠ‚ç‚¹IDï¼Œç”¨äºid-groupsä½¿ç”¨:END_ID(Order) :TYPE - å…³ç³»å‹åˆ—æ‰€æœ‰å…¶ä»–åˆ—éƒ½è¢«è§†ä¸ºå±æ€§ï¼Œä½†å¦‚æœä¸ºç©ºæˆ–åœ¨æ³¨é‡Šæ—¶è·³è¿‡ï¼šIGNORE ç±»å‹è½¬æ¢å¯ä»¥é€šè¿‡åé¢æ·»åŠ çš„åç§°ï¼Œä¾‹å¦‚é€šè¿‡:INTï¼Œ:BOOLEANç­‰ https://neo4j.com/docs/operations-manual/current/tools/import/file-header-format/?_ga=2.242662919.407071438.1562662054-675208984.1561960585#import-tool-header-format-header-files 1234neo4j-admin import --database=graph_test.db \\ --nodes=\"import/movies3-header.csv,import/movies3.csv\" \\ --nodes=\"import/actors3-header.csv,import/actors3.csv\" \\ --relationships=\"import/roles3-header.csv,import/roles3.csv\" å¯¼å…¥æ•°æ®æ•°æ®movies.csvï¼ˆheaderæœªåˆ†ç¦»ï¼‰ 1234movieId:ID,title,year:int,:LABELtt0133093,&quot;The Matrix&quot;,1999,Moviett0234215,&quot;The Matrix Reloaded&quot;,2003,Movie;Sequeltt0242653,&quot;The Matrix Revolutions&quot;,2003,Movie;Sequel movies3-header.csv 1movieId:ID,title,year:int,:LABEL å› ä¸ºIDæ­¤æ—¶æ˜¯å…¨å±€çš„ï¼Œæ‰€ä»¥å¦‚æœå…¶ä»–nodeséƒ½æ˜¯ä»0å¼€å§‹è‡ªå¢çš„IDï¼Œé‚£ä¹ˆå°±ä¼šé‡å¤äº†ã€‚æ‰€ä»¥å¯ä»¥æŒ‡å®šä¸€ä¸ªå‘½åç©ºé—´ 1movieId:ID(Movie-ID),title,year:int,:LABEL movies3.csv 123tt0133093,&quot;The Matrix&quot;,1999,Moviett0234215,&quot;The Matrix Reloaded&quot;,2003,Movie;Sequeltt0242653,&quot;The Matrix Revolutions&quot;,2003,Movie;Sequel å¯¹åº”åˆ°Neo4jé‡Œé¢çš„æ•°æ®æ ¼å¼æ˜¯ 12345678910&gt;&gt;&gt; MATCH (n:Movie) RETURN n LIMIT 3â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••â”‚\"n\" â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡â”‚&#123;\"title\":\"The Matrix\",\"year\":1999,\"movieId\":\"tt0133093\"&#125; â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚&#123;\"title\":\"The Matrix Reloaded\",\"year\":2003,\"movieId\":\"tt0234215\"&#125; â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚&#123;\"title\":\"The Matrix Revolutions\",\"year\":2003,\"movieId\":\"tt0242653\"&#125;â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ actors3-header.csv 1personId:ID,name,:LABEL actors3.csv 123keanu,&quot;Keanu Reeves&quot;,Actorlaurence,&quot;Laurence Fishburne&quot;,Actorcarrieanne,&quot;Carrie-Anne Moss&quot;,Actor å¯¹åº”åˆ°Neo4jé‡Œé¢çš„æ•°æ®æ ¼å¼æ˜¯ 1234567891011&gt;&gt;&gt; MATCH (n:Actor) RETURN n LIMIT 3â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••â”‚\"n\" â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡â”‚&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125; â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚&#123;\"name\":\"Laurence Fishburne\",\"personId\":\"laurence\"&#125;â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚&#123;\"name\":\"Carrie-Anne Moss\",\"personId\":\"carrieanne\"&#125;â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ roles3.csv ï¼ˆheaderæœªåˆ†ç¦»ï¼‰ 12345678910:START_ID,role,:END_ID,:TYPEkeanu,&quot;Neo&quot;,tt0133093,ACTED_INkeanu,&quot;Neo&quot;,tt0234215,ACTED_INkeanu,&quot;Neo&quot;,tt0242653,ACTED_INlaurence,&quot;Morpheus&quot;,tt0133093,ACTED_INlaurence,&quot;Morpheus&quot;,tt0234215,ACTED_INlaurence,&quot;Morpheus&quot;,tt0242653,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0133093,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0234215,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0242653,ACTED_IN roles3-header.csv 1:START_ID,role,:END_ID,:TYPE å¦‚æœä¸Šé¢ä½¿ç”¨äº†IDçš„å‘½åç©ºé—´çš„è¯ éœ€è¦åœ¨å…³ç³»ä¸­æŒ‡æ˜ï¼Œå¦åˆ™å…³ç³»ä¼šå‡ºç°é”™ä¹± 1:START_ID(Actor-ID),role,:END_ID(Movie-ID),:TYPE roles3.csv 123456789keanu,&quot;Neo&quot;,tt0133093,ACTED_INkeanu,&quot;Neo&quot;,tt0234215,ACTED_INkeanu,&quot;Neo&quot;,tt0242653,ACTED_INlaurence,&quot;Morpheus&quot;,tt0133093,ACTED_INlaurence,&quot;Morpheus&quot;,tt0234215,ACTED_INlaurence,&quot;Morpheus&quot;,tt0242653,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0133093,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0234215,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0242653,ACTED_IN 1234567891011121314&gt;&gt;&gt; MATCH p=()-[r:ACTED_IN]-&gt;() RETURN p LIMIT 3â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••â”‚\"p\" â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡â”‚[&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125;,&#123;\"role\":\"Neo\"&#125;,&#123;\"title\":\"Tâ”‚â”‚he Matrix Revolutions\",\"year\":2003,\"movieId\":\"tt0242653\"&#125;] â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚[&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125;,&#123;\"role\":\"Neo\"&#125;,&#123;\"title\":\"Tâ”‚â”‚he Matrix Reloaded\",\"year\":2003,\"movieId\":\"tt0234215\"&#125;] â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚[&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125;,&#123;\"role\":\"Neo\"&#125;,&#123;\"title\":\"Tâ”‚â”‚he Matrix\",\"year\":1999,\"movieId\":\"tt0133093\"&#125;] â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ å…³äºä¸Šé¢çš„æ•°æ®çš„headeræ ¼å¼ï¼Œå¯ä»¥ï¼Œå†’å·ï¼šå‰é¢æ˜¯å­—æ®µåï¼Œåé¢è·Ÿç€çš„æ˜¯å­—æ®µçš„æ•°æ®ç±»å‹ï¼Œè¿™è¾¹æ¯”è¾ƒç‰¹æ®Šçš„æœ‰IDå’ŒLABLE,å…¶ä»–çš„å¯ä»¥è·Ÿstirngï¼Œintzä¹‹ç±»çš„ å¯¼å…¥æˆåŠŸ å¦‚è¿™è¾¹å¯¼å…¥åˆ°çš„æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®åº“ï¼Œéœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶ conf/neo4j.conf 123# The name of the database to mount# dbms.active_database=graph.dbdbms.active_database=graph_test.db 2.5 Batch-importå¯¼å…¥è‡ªå·±å‡†å¤‡çš„æ•°æ® 1234567Exception in thread \"Thread-4\" java.lang.RuntimeException: org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.DuplicateInputIdException: Id '12370' is defined more than once in group 'global id space' at org.neo4j.unsafe.impl.batchimport.staging.AbstractStep.issuePanic(AbstractStep.java:150) at org.neo4j.unsafe.impl.batchimport.staging.AbstractStep.issuePanic(AbstractStep.java:142) at org.neo4j.unsafe.impl.batchimport.staging.LonelyProcessingStep.lambda$receive$0(LonelyProcessingStep.java:58) at java.lang.Thread.run(Thread.java:745)Caused by: org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.DuplicateInputIdException: Id '12370' is defined more than once in group 'global id space' at è¿™è¾¹æœ‰é‡å¤æ•°æ®ï¼Œæ£€æŸ¥ä¸€ä¸‹ç”¨æˆ·ID 123456789101112&gt;&gt;&gt; df_cus.customer_id.value_counts().head()12370 212431 212394 212429 212457 2Name: customer_id, dtype: int64&gt;&gt;&gt; df_cus.query(&apos;customer_id == 12370&apos;) customer_id country sex age20 12370 Austria female 2221 12370 Cyprus male 32 é‡Œé¢æœ‰é‡å¤çš„æ•°æ®ï¼Œè§£å†³æ–¹æ¡ˆæ˜¯å¿½ç•¥æœ‰é‡å¤çš„æ•°æ®æˆ–è€…ä¿®æ”¹åº•å±‚æ•°æ® 12345neo4j-admin import --ignore-duplicate-nodes\\ --database=graph_online_retail.db \\ --nodes=\"import/online_retail/node_stocks.csv\" \\ --nodes=\"import/online_retail/node_customers.csv\" \\ --relationships=\"import/online_retail/relationships.csv\" 3. References neo4j(äºŒ).ä½¿ç”¨neo4j-importå¯¼å…¥æ•°æ®åŠå…³ç³» Importing CSV Data into Neo4j Neo4j | æ•°æ®å¯¼å…¥ä¹‹neo4j-adminå’Œneo4j-importçš„åŒºåˆ«","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.cc/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"}]},{"title":"å›¾æ•°æ®åº“-å…³ç³»ç½‘ç»œè®¾è®¡","date":"2018-08-18T06:45:01.000Z","path":"2018/08/18/å›¾æ•°æ®åº“-å…³ç³»ç½‘ç»œè®¾è®¡/","text":"1. ç®€å•ä»‹ç»ç›®å‰çŸ¥è¯†å›¾è°±å¯ä»¥åˆ†æˆä¸¤å¤§ç±»ï¼šä¸€ç±»æ˜¯å¼€æ”¾é¢†åŸŸçš„çŸ¥è¯†å›¾è°±ï¼Œæ¯”å¦‚è°·æ­Œæœç´¢å¼•æ“æ‰€å»ºç«‹çš„çŸ¥è¯†å›¾è°±ã€‚å¦ä¸€ç±»æ˜¯å‚ç›´é¢†åŸŸçš„çŸ¥è¯†å›¾è°±ï¼Œæ ¹æ®å­—é¢æ„æ€æ¥çœ‹ï¼Œå°±æ˜¯ç‰¹å®šæŸä¸€ä¸ªé¢†åŸŸï¼Œæ¯”å¦‚é‡‘èé£æ§é¢†åŸŸã€‚ä»Šå¤©ä»‹ç»ä¸€ä¸‹å…³ç³»ç½‘ç»œåœ¨é‡‘èé£æ§é¢†åŸŸçš„åº”ç”¨ã€‚ ä¸»è¦åº”ç”¨å¯ä»¥åˆ†æˆä¸¤å¤§ç±»ã€‚ ç¬¬ä¸€ç±» ç½‘ç»œç‰¹å¾çš„ç›´æ¥æå–ï¼Œæå–å‡ºä¸­å¿ƒåº¦æˆ–è€…ä¸€åº¦/äºŒåº¦å…³è”ç‰¹å¾ä¾›ä¸Šå±‚è§„åˆ™ç³»ç»Ÿæˆ–è€…é£é™©è¯„ä¼°æ¨¡å‹ä½¿ç”¨ã€‚ æ¯”å¦‚è¯´åœ¨é“¶è¡Œè´·æ¬¾ç”³è¯·çš„ä¸šåŠ¡åœºæ™¯ä¸‹ï¼Œåœ¨è´·æ¬¾ç”³è¯·äººä¸­ï¼Œç¬¬ä¸€åº¦æ˜¯è”ç³»äººï¼Œåˆ™ä¸ç›®æ ‡ç”³è¯·å…±äº«ç›¸åŒè”ç³»äººçš„ç”³è¯·æ•°ç›®æ˜¯ä¸€åº¦å…³è”åº¦ï¼Œå¦‚æœè¿™äº›ç”³è¯·è¿˜ä¸å¦å¤–ä¸€äº›ç”³è¯·å…±äº«ç›¸åŒçš„ä½å€æˆ–å…¶ä»–ç‰¹å¾ï¼Œè¿™äº›ç”³è¯·å°±å’Œç›®æ ‡ç”³è¯·å½¢æˆäºŒåº¦å…³è”ã€‚ä¸€äº›ç®€å•çš„æŒ‡æ ‡ï¼Œæ¯”å¦‚ä¸€åº¦å…³è”èŠ‚ç‚¹æˆ–è€…äºŒåº¦å…³è”èŠ‚ç‚¹æ˜¯å¦è§¦é»‘/è§¦é»‘ä¸ªæ•°ï¼Œåœ¨å®é™…çš„åæ¬ºè¯ˆå®è·µä¸­æ•ˆæœæ˜¯éå¸¸æ˜¾è‘—çš„ã€‚å¦‚æœç‰¹å¾ä¸æ¶‰åŠæ·±åº¦çš„å…³ç³»ï¼Œä¼ ç»Ÿçš„å…³ç³»å‹æ•°æ®åº“å°±èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ï¼Œå› ä¸ºè®¡ç®—é‡æ˜¯éšç€åº¦æ•°çš„å¢åŠ å‘ˆæŒ‡æ•°å¢åŠ ã€‚ ç¬¬äºŒç±» ç½‘ç»œä¿¡æ¯çš„æ·±åº¦æŒ–æ˜ã€‚æ·±åº¦æŒ–æ˜é€šå¸¸å§‹äºå¯¹è¿é€šå­å›¾çš„è®¡ç®—ï¼Œå¯¹äºç¤¾äº¤å±æ€§è¾ƒå¼±çš„é‡‘èåº”ç”¨ï¼Œè¾ƒå¤§çš„è¿é€šå­å›¾å¯èƒ½å¯¹æ¬ºè¯ˆç½‘ç»œæœ‰æ­ç¤ºä½œç”¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿˜å¯ä»¥è¿›è¡Œè¿›ä¸€æ­¥å±•å¼€ç¤¾åŒºçš„å‘ç°ï¼Œè¿™é‡Œé¢ç¤¾åŒºçš„å‘ç°ä¸ç­‰äºè¿é€šå­å›¾ï¼Œæ±‡èšæ€§æ˜¯ä¸€ä¸ªæ›´ä¸¥æ ¼çš„æŒ‡æ ‡ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œé€šè¿‡æ¬ºè¯ˆæ¯”ä¾‹ä¼ æ’­ï¼Œæˆ–è€…è¯´æŸ“è‰²ï¼Œå°†å·²çŸ¥çš„æ¬ºè¯ˆæ ‡æ³¨æ‰©æ’’å¼€æ¥ï¼Œä»è€Œè·å¾—æ›´å¤šçš„æ¬ºè¯ˆæ ‡æ³¨ï¼Œä¹Ÿæ˜¯å…³ç³»å›¾è°±çš„ä¸€ä¸ªé‡è¦åº”ç”¨ã€‚ 2. åº”ç”¨ - åŸºäºé€šè¯è¯¦å•çš„å…³ç³»ç½‘ç»œè®¾è®¡ä»¥ç”¨æˆ·çš„é€šè¯å…³ç³»ç½‘ç»œä¸ºä¾‹ï¼ŒèŠ‚ç‚¹ã€å±æ€§å’Œå…³ç³»è®¾è®¡å¦‚ä¸‹ï¼š 2.1 èŠ‚ç‚¹åŠå±æ€§ User (ä¸šåŠ¡åœºæ™¯å†…çš„ç”¨æˆ·) æ‰‹æœºå· æ˜¯å¦æ³¨å†Œ æ˜¯å¦ç”³è¯· æ˜¯å¦é€šè¿‡ (æœ€è¿‘ä¸€æ¬¡) æ˜¯å¦è§¦é»‘ (å†å²) æ˜¯å¦ä¸ºåç”¨æˆ· (æœ‰D7|M1|M2é€¾æœŸè®°å½•ç­‰) æ˜¯å¦ä¸ºå¥½ç”¨æˆ· (æ— é€¾æœŸè®°å½•) æ˜¯å¦å¤„äºé€¾æœŸä¸­ (ä»»åœ¨é€¾æœŸä¸­) è¯„åˆ† (ä¸šåŠ¡åœºæ™¯å†…åˆ†è¯„åˆ†ï¼Œå¯ä»¥æ˜¯åæ¬ºè¯ˆåˆ†æˆ–è€…ä¿¡ç”¨åˆ†ç­‰) ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œä»¥ä¸Šå…³ç³»(ä¸åŒ…æ‹¬æ‰‹æœºå·)å¯è¡¨ç¤ºä¸º 1101000576ï¼Œå…·ä½“ä»£è¡¨æ„æ€å¦‚ä¸‹ï¼š | Register | Apply | Pass | Balck | Bad | Good | Undue | Score || â€”â€”â€”â€” | â€”â€”- | â€”â€” | â€”â€”- | â€”â€” | â€”â€” | â€”â€”- | â€”â€”- || 1 | 1 | 0 | 1 | 0 | 0 | 0 | 576 | å…¶ä¸­æ‰‹æœºå·å¯ä»¥å°†åè¿›åˆ¶æ•°è½¬æ¢æˆ62è¿›åˆ¶è¡¨ç¤º(å³0-9,a-z,A-Z)ï¼Œ1ï¸ä»¥èŠ‚çœå†…å­˜ã€‚ æ‰€ä»¥ä¸€ä¸ªèŠ‚ç‚¹çš„å±æ€§å¯è¡¨ç¤ºä¸º(P_num, Status)ã€‚ Phone (éç”¨æˆ·) (P_num, Status) è¿™é‡ŒStatuså°±åªæœ‰ä¸€ä¸ªåˆ†æ•°ï¼Œè‡³äºå¦‚ä½•è®¡ç®—ä¸€ä¸ªéä¸šåŠ¡åœºæ™¯å†…çš„åˆ†æ•°å‘¢ï¼Œå¯ä»¥é‡‡ç”¨åœ¨ä¸šåŠ¡åœºæ™¯æ•°æ®é‡Œï¼Œè¯¥éç”¨æˆ·çš„è”ç³»äººåˆ†æ•°çš„åŠ æƒè¯„åˆ†åˆ†ã€‚æ¯”å¦‚è¯´Phone2è”ç³»äº†ä¸šåŠ¡åœºæ™¯å†…ä¸¤ä¸ªæ³¨å†Œç”¨æˆ·User1å’ŒUser2ï¼Œå…¶åˆ†æ•°åˆ†åˆ«ä¸º550ï¼Œ580ï¼Œä¸”ä¹‹é—´çš„æƒé‡ä¸ºW1å’ŒW2ã€‚åˆ™è¯¥éæ³¨å†Œç”¨æˆ·çš„è¯„åˆ†ä¸º W1 * User_1 + W2 * User2 é—®é¢˜ï¼šæ˜¯å¦åº”è¯¥è€ƒè™‘éä¸šåŠ¡åœºæ™¯å†…çš„ç”¨æˆ·åˆ†æ•°ï¼Œå¦‚è€ƒè™‘äº†ï¼Œå¥½åƒå­˜åœ¨é€’å½’è°ƒç”¨çš„ç°è±¡ã€‚ 2.2 å…³ç³» E - ç´§æ€¥è”ç³»äºº B - é“¶è¡Œé¢„ç•™æ‰‹æœºå· D - å­˜åœ¨é€šè¯è®°å½• W - æƒé‡ 2.3 ä¸€åº¦å…³ç³» æ ¹æ®ä¸Šå›¾æ˜¾ç¤º Userå’ŒUser1ä¹‹é—´å­˜åœ¨E/B|D|W1 ä¹‹é—´çš„å…³ç³»ï¼Œåˆ™è¯´æ˜User1æ˜¯Userçš„ç´§æ€¥è”ç³»äººæˆ–è€…é“¶è¡Œé¢„ç•™æ‰‹æœºå·ï¼Œä¸”ä¸¤äººä¹‹é—´å­˜åœ¨é€šè¯è®°å½•ã€‚åŒæ—¶å¯ä»¥çœ‹åˆ°Userå’ŒUser1ä¹‹é—´å…³ç³»æ˜¯å•å‘çš„ï¼Œæ‰€ä»¥U-&gt;U1å’ŒU1-&gt;Uä¹‹é—´çš„æƒé‡æ˜¯ä¸ä¸€æ ·çš„ã€‚ 2.4 äºŒåº¦å…³ç³» æ ¹æ®ä¸Šå›¾æ˜¾ç¤º Userä¸User3ã€User4ã€User5å’ŒPhone2éƒ½æ˜¯äºŒåº¦è”ç³»äººã€‚å…¶ä¸­Userä¸User3æ„æˆå¼ºå¯¹å¼ºå…³ç³»ï¼ŒUserä¸Phone2æ„æˆå¼ºå¯¹å¼±å…³ç³»ï¼ŒUserä¸User4/User5æ„æˆå¼±å¯¹å¼±å…³ç³»ã€‚ åœ¨è¯¥ä¸šåŠ¡åœºæ™¯ä¸‹ï¼Œå¼ºå…³ç³»ä»£è¡¨äº†æ˜¯å¦ä¸ºå¯¹æ–¹ç´§æ€¥è”ç³»äººæˆ–è€…é“¶è¡Œé¢„ç•™æ‰‹æœºå·ï¼Œå¯ä½“ç°å‡ºä¸¤è€…ä¹‹é—´çš„å…³ç³»(è¿™ä¸ªä¸æ˜¯å¿…é¡»çš„)ã€‚ 2.5 è¯„åˆ†è¿™ä¸ªè¯„åˆ†åœ¨å¾ˆå¤šç¬¬ä¸‰æ–¹æä¾›çš„æ•°æ®ä¸­ä¼šæœ‰çœ‹åˆ°ç±»ä¼¼çš„ 3. ç‰¹å¾è¡ç”Ÿ3.1 ç›´æ¥æå–3.2æ·±åº¦æŒ–æ˜4. å»ºè®®åŠæ€è€ƒåœ¨ä¸Šè¿°çš„è®¾è®¡å½“ä¸­ï¼Œè¿˜æ˜¯å­˜åœ¨å¾ˆå¤šè®¾è®¡ä¸Šçš„ç¼ºç‚¹å¯ä»¥æ”¹å–„çš„ï¼Œå…·ä½“å¦‚ä¸‹ 4.1 æ•°æ®å‡†å¤‡ä¸Šçš„å»ºè®® æ˜¯å¦å¯ä»¥è€ƒè™‘åªå¯¼å…¥æ‰‹æœºå· å¯¹é€šè¯è¯¦å•è¿›è¡Œæ¸…æ´—ï¼Œå‡å°‘æ•°æ®é‡ã€‚æ¯”å¦‚è¥é”€ã€ç”µå­æ¬ºè¯ˆå·ç ç­‰ã€‚ å¯¼å…¥é€šè®¯å½•ä¼šå¸¦æ¥å¤§é‡çš„å†—ä½™èŠ‚ç‚¹(å½“ç”¨æˆ·åŸºæ•°å¾ˆåºå¤§çš„æ—¶å€™)ï¼Œå¯è€ƒè™‘ä¸å¯¼å…¥é€šè®¯å½• å¦‚æœä¸šåŠ¡åœºæ™¯å†…å…è®¸ï¼Œæ¯”å¦‚æ—¶å€™Userå’ŒPhoneä¹‹é—´å…¶å®ä¸€ä¸€å¯¹åº”çš„è¯ï¼Œæˆ‘ä»¬å³å¯è€ƒè™‘åˆ é™¤UserèŠ‚ç‚¹ï¼Œç®€åŒ–å›¾çš„ç»“æ„ å‡å°‘å†…å­˜ï¼Œå³æ‰‹æœºå·ã€çŠ¶æ€å’Œå…³ç³»ä»¥æœ€èŠ‚çœå†…å­˜çš„æ–¹å¼å­˜æ”¾ã€‚ å¢åŠ æ¯æ¡å…³ç³»çš„æƒé‡ 4.2 ç»†åŒ–å…³ç³»ç§ç±» call_in - æœ‰æ¥ç”µè®°å½• call_out - æœ‰å»ç”µè®°å½• msg_in - æœ‰æ”¶çŸ­ä¿¡è®°å½• msg_out - æœ‰å‘çŸ­ä¿¡è®°å½• emergency_call - ç´§æ€¥è”ç³»äººå·ç  contact (keep) - é€šè®¯å½•è®°å½• user_phone - ç”¨æˆ·æ³¨å†Œç”µè¯ 4.3 è€ƒè™‘å…¶ä»–æ•°æ®æºæ¥æ­å»ºç½‘ç»œ IPåœ°å€ MACåœ°å€ LBSåœ°å€ èº«ä»½è¯å‰6ä½å¯¹åº”åœ°å€/å½’å±åœ° å®¶åº­ä½å€ å·¥ä½œåœ°å€ æ‰‹æœºå·å¯¹åº”åœ°å€/å½’å±åœ° ç­‰ç­‰ 5. References å¦‚ä½•æ„å»ºçŸ¥è¯†å›¾è°± å¤§æ•°æ®å’Œäººå·¥æ™ºèƒ½åœ¨P2Pä¸­çš„åº”ç”¨ è¿™æ˜¯ä¸€ä»½é€šä¿—æ˜“æ‡‚çš„çŸ¥è¯†å›¾è°±æŠ€æœ¯ä¸åº”ç”¨æŒ‡å—","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.cc/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"},{"name":"å…³ç³»ç½‘ç»œ","slug":"å…³ç³»ç½‘ç»œ","permalink":"http://chenson.cc/tags/å…³ç³»ç½‘ç»œ/"}]},{"title":"å›¾æ•°æ®åº“-Neo4j-å¸¸ç”¨ç®—æ³•","date":"2018-08-18T06:03:23.000Z","path":"2018/08/18/å›¾æ•°æ®åº“-Neo4j-å¸¸ç”¨ç®—æ³•/","text":"æœ¬æ¬¡ä¸»è¦å­¦ä¹ å›¾æ•°æ®åº“ä¸­å¸¸ç”¨åˆ°çš„ä¸€äº›ç®—æ³•ï¼Œä»¥åŠå¦‚ä½•åœ¨Neo4jä¸­è°ƒç”¨ï¼Œæ‰€ä»¥è¿™ä¸€ç¯‡åå®æˆ˜ï¼Œæ¯ä¸ªç®—æ³•çš„åŸç†å°±ç®€å•çš„æä¸€ä¸‹ã€‚ 1. å›¾æ•°æ®åº“ä¸­å¸¸ç”¨çš„ç®—æ³• PathFinding &amp; Search ä¸€èˆ¬ç”¨æ¥å‘ç°Nodesä¹‹é—´çš„æœ€çŸ­è·¯å¾„ï¼Œå¸¸ç”¨ç®—æ³•æœ‰å¦‚ä¸‹å‡ ç§ Google Search Results Dijkstra - è¾¹ä¸èƒ½ä¸ºè´Ÿå€¼ Folyd - è¾¹å¯ä»¥ä¸ºè´Ÿå€¼ï¼Œæœ‰å‘å›¾ã€æ— å‘å›¾ Bellman-Ford SPFA Centrality ä¸€èˆ¬ç”¨æ¥è®¡ç®—è¿™ä¸ªå›¾ä¸­èŠ‚ç‚¹çš„ä¸­å¿ƒæ€§ï¼Œç”¨æ¥å‘ç°æ¯”è¾ƒé‡è¦çš„é‚£äº›Nodesã€‚è¿™äº›ä¸­å¿ƒæ€§å¯ä»¥æœ‰å¾ˆå¤šç§ï¼Œæ¯”å¦‚ Degree Centrality - åº¦ä¸­å¿ƒæ€§ Weighted Degree Centrality - åŠ æƒåº¦ä¸­å¿ƒæ€§ Betweenness Centrality - ä»‹æ•°ä¸­å¿ƒæ€§ Closeness Centrality - ç´§åº¦ä¸­å¿ƒæ€§ Community Detection åŸºäºç¤¾åŒºå‘ç°ç®—æ³•å’Œå›¾åˆ†æNeo4jè§£è¯»ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹ ç”¨äºå‘ç°è¿™ä¸ªå›¾ä¸­å±€éƒ¨è”ç³»æ¯”è¾ƒç´§å¯†çš„Nodesï¼Œç±»ä¼¼æˆ‘ä»¬å­¦è¿‡çš„èšç±»ç®—æ³•ã€‚ Strongly Connected Components Weakly Connected Components (Union Find) Label Propagation Lovain Modularity Triangle Count and Average Clustering Coefficient 2. è·¯å¾„æœç´¢ç®—æ³• Shortest Path 1234567MATCH (start:Loc&#123;name:&quot;A&quot;&#125;), (end:Loc&#123;name:&quot;F&quot;&#125;)CALL algo.shortestPath.stream(start, end, &quot;cost&quot;)YIELD nodeId, costMATCH (other:Loc) WHERE id(other) = nodeIdRETURN other.name AS name, cost Single Source Shortest Path 123456MATCH (n:Loc &#123;name:&quot;A&quot;&#125;)CALL algo.shortestPath.deltaStepping.stream(n, &quot;cost&quot;, 3.0YIELD nodeId, distanceMATCH (destination) WHERE id(destination) = nodeIdRETURN destination.name AS destination, distance All Pairs Shortest Path 1234567891011CALL algo.allShortestPaths.stream(&quot;cost&quot;,&#123;nodeQuery:&quot;Loc&quot;,defaultValue:1.0&#125;)YIELD sourceNodeId, targetNodeId, distanceWITH sourceNodeId, targetNodeId, distanceWHERE algo.isFinite(distance) = trueMATCH (source:Loc) WHERE id(source) = sourceNodeIdMATCH (target:Loc) WHERE id(target) = targetNodeIdWITH source, target, distance WHERE source &lt;&gt; targetRETURN source.name AS source, target.name AS target, distanceORDER BY distance DESCLIMIT 10 Minimum Weight Spanning Tree 12345MATCH (n:Place &#123;id:&quot;D&quot;&#125;)CALL algo.spanningTree.minimum(&quot;Place&quot;, &quot;LINK&quot;, &quot;cost&quot;, id(n), &#123;write:true, writeProperty:&quot;MINST&quot;&#125;)YIELD loadMillis, computeMillis, writeMillis, effectiveNodeCountRETURN loadMillis, computeMillis, writeMillis, effectiveNodeCount; CASE 123456789101112131415MERGE (a:Loc &#123;name:&quot;A&quot;&#125;)MERGE (b:Loc &#123;name:&quot;B&quot;&#125;)MERGE (c:Loc &#123;name:&quot;C&quot;&#125;)MERGE (d:Loc &#123;name:&quot;D&quot;&#125;)MERGE (e:Loc &#123;name:&quot;E&quot;&#125;)MERGE (f:Loc &#123;name:&quot;F&quot;&#125;)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(b)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(c)MERGE (a)-[:ROAD &#123;cost:100&#125;]-&gt;(d)MERGE (b)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:80&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:30&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:80&#125;]-&gt;(f)MERGE (e)-[:ROAD &#123;cost:40&#125;]-&gt;(f); 3. ä¸­å¿ƒæ€§ç®—æ³• PageRank 123456CALL algo.pageRank.stream(&quot;Page&quot;, &quot;LINKS&quot;,&#123;iterations:20&#125;)YIELD nodeId, scoreMATCH (node) WHERE id(node) = nodeIdRETURN node.name AS page,scoreORDER BY score DESC Degree Centrality Betweenness Centrality 12345CALL algo.betweenness.stream(&quot;User&quot;, &quot;MANAGES&quot;, &#123;direction:&quot;out&quot;&#125;)YIELD nodeId, centralityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user,centralityORDER BY centrality DESC; Closeness Centrality 123456CALL algo.closeness.stream(&quot;Node&quot;, &quot;LINK&quot;)YIELD nodeId, centralityMATCH (n:Node) WHERE id(n) = nodeIdRETURN n.id AS node, centralityORDER BY centrality DESCLIMIT 20; CASE 12345678910111213141516171819202122MERGE (home:Page &#123;name:&quot;Home&quot;&#125;)MERGE (about:Page &#123;name:&quot;About&quot;&#125;)MERGE (product:Page &#123;name:&quot;Product&quot;&#125;)MERGE (links:Page &#123;name:&quot;Links&quot;&#125;)MERGE (a:Page &#123;name:&quot;Site A&quot;&#125;)MERGE (b:Page &#123;name:&quot;Site B&quot;&#125;)MERGE (c:Page &#123;name:&quot;Site C&quot;&#125;)MERGE (d:Page &#123;name:&quot;Site D&quot;&#125;)MERGE (home)-[:LINKS]-&gt;(about)MERGE (about)-[:LINKS]-&gt;(home)MERGE (product)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(product)MERGE (links)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(links)MERGE (links)-[:LINKS]-&gt;(a)MERGE (a)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(b)MERGE (b)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(c)MERGE (c)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(d)MERGE (d)-[:LINKS]-&gt;(home) 4. ç¤¾åŒºå‘ç°ç®—æ³• Strongly Connected Components 1234CALL algo.scc.stream(&quot;User&quot;,&quot;FOLLOWS&quot;)YIELD nodeId, partitionMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS name, partition Weakly Connected Components (Union Find) 1234CALL algo.unionFind.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId,setIdMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS user, setId Label Propagation 12CALL algo.labelPropagation.stream(&quot;User&quot;, &quot;FOLLOWS&quot;, &#123;direction: &quot;OUTGOING&quot;, iterations: 10&#125;) Lovain Modularity 12345CALL algo.louvain.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId, communityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user, communityORDER BY community; Triangle Count and Average Clustering Coefficient 123456CALL algo.triangle.stream(&quot;Person&quot;,&quot;KNOWS&quot;)YIELD nodeA,nodeB,nodeCMATCH (a:Person) WHERE id(a) = nodeAMATCH (b:Person) WHERE id(b) = nodeBMATCH (c:Person) WHERE id(c) = nodeCRETURN a.id AS nodeA, b.id AS nodeB, c.id AS node 5. References Neo4j in deep å®˜æ–¹æ–‡æ¡£ï¼šComprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.cc/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"}]},{"title":"å›¾æ•°æ®åº“-Neo4j-åˆæ¢","date":"2018-08-17T05:15:46.000Z","path":"2018/08/17/å›¾æ•°æ®åº“-Neo4j-åˆæ¢/","text":"æœ¬æ¬¡åˆæ¢ä¸»è¦å­¦ä¹ å¦‚ä½•å®‰è£…Neo4jï¼Œä»¥åŠCypherçš„åŸºæœ¬è¯­æ³•ã€‚ 1. å®‰è£…Neo4j Desktopç‰ˆæœ¬ neo4j-desktop Serverç‰ˆæœ¬ï¼ˆCommunityç‰ˆ) æ¯”è¾ƒå»ºè®®å®‰è£…è¿™ä¸ªç‰ˆæœ¬ï¼Œå› ä¸ºDesktopç‰ˆæœ¬çš„è€æ˜¯é—ªé€€ï¼Œä¸”è¦æ¿€æ´»ä¹‹ç±»çš„ã€‚ ä¸‹è½½Neo4jæ•°æ®åº“ neo4j-server-community ä¸‹è½½å¸¸ç”¨ç®—æ³•çš„æ’ä»¶ graph-algorithms neo4j-graph-algorithms apoc-procedures neo4j-apoc-procedures å°†ä¸‹è½½ä¸‹æ¥çš„ç®—æ³•æ’ä»¶æ”¾å…¥åˆ°$NEO4J_HOME/pluginsæ–‡ä»¶å¤¹ä¸‹ Serviceç‰ˆä¿®æ”¹é…ç½®æ–‡ä»¶$NEO4J_HOME/conf/neo4j.conf 1234567891011# è§£å†³ç™»å…¥çš„æ—¶å€™æŠ¥æ²¡æœ‰æˆæƒçš„é”™è¯¯dbms.security.auth_enabled=false# æ·»åŠ ä¸‹è½½çš„ç®—æ³•æ’ä»¶dbms.security.procedures.unrestricted=apoc.*,algo.*apoc.import.file.enabled=true#å¢åŠ é¡µç¼“å­˜åˆ°è‡³å°‘4Gï¼Œæ¨è20G:dbms.memory.pagecache.size=4g#JVMå †ä¿å­˜ç•™å†…å­˜ä»1Gèµ·ï¼Œæœ€å¤§4G:dbms.memory.heap.initial_size=1gdbms.memory.heap.max_size=4g å¯åŠ¨/åœæ­¢ (æŠŠserveræ‰€åœ¨çš„è·¯å¾„æ·»åŠ åˆ°ç³»ç»Ÿçš„PATH) 1234567# å»ºè®®å°†neo4jæ‰€åœ¨çš„è·¯å¾„æ¡ä»¶åˆ°ç³»ç»Ÿ$PATHå½“ä¸­ï¼Œ# export NEO4J_HOME=\"path-to-neo4j\"$NEO4J_HOME/bin/neo4j start$NEO4J_HOME/bin/neo4j console$NEO4J_HOME/bin/neo4j stop$NEO4J_HOME/bin/neo4j start -u neo4j -p neo4j$NEO4J_HOME/bin/cypher-shell 1CALL dbms.procedures() // æŸ¥çœ‹neo4jå¯ç”¨çš„è¿›ç¨‹ï¼ŒåŒ…æ‹¬åˆšåˆšå®‰è£…çš„æ’ä»¶ 2. CypheråŸºæœ¬è¯­æ³• NodesåŸºæœ¬è¯­æ³• åœ¨Cypheré‡Œé¢é€šè¿‡ä¸€å¯¹å°æ‹¬å·ä»£è¡¨ä¸€ä¸ªèŠ‚ç‚¹ () ä»£è¡¨åŒ¹é…ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ (node1) ä»£è¡¨åŒ¹é…ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¹¶ç»™å®ƒèµ·äº†ä¸€ä¸ªåˆ«å (:Lable) ä»£è¡¨æŸ¥è¯¢ä¸€ä¸ªç±»å‹çš„æ•°æ® (person:Lable) ä»£è¡¨æŸ¥è¯¢ä¸€ä¸ªç±»å‹çš„æ•°æ®ï¼Œå¹¶ç»™å®ƒèµ·äº†ä¸€ä¸ªåˆ«å (person:Lable {name:â€å°ç‹â€}) æŸ¥è¯¢æŸä¸ªç±»å‹ä¸‹ï¼ŒèŠ‚ç‚¹å±æ€§æ»¡è¶³æŸä¸ªå€¼çš„æ•°æ® (person:Lable {name:â€å°ç‹â€,age:23}) èŠ‚ç‚¹çš„å±æ€§å¯ä»¥åŒæ—¶å­˜åœ¨å¤šä¸ªï¼Œæ˜¯ä¸€ä¸ªANDçš„å…³ç³» RelationshipåŸºæœ¬è¯­æ³• ç³»ç”¨ä¸€å¯¹-ç»„æˆï¼Œå…³ç³»åˆ†æœ‰æ–¹å‘çš„è¿›å’Œå‡ºï¼Œå¦‚æœæ˜¯æ— æ–¹å‘å°±æ˜¯è¿›å’Œå‡ºéƒ½æŸ¥è¯¢ â€”&gt; æŒ‡å‘ä¸€ä¸ªèŠ‚ç‚¹ -[role]-&gt; ç»™å…³ç³»åŠ ä¸ªåˆ«å -[:acted_in]-&gt; è®¿é—®æŸä¸€ç±»å…³ç³» -[role:acted_in]-&gt; è®¿é—®æŸä¸€ç±»å…³ç³»ï¼Œå¹¶åŠ äº†åˆ«å -[role:acted_in {roles:[â€œneoâ€,â€Hadoopâ€œ]}]-&gt; åˆ›å»º/åˆ é™¤èŠ‚ç‚¹ 1234567891011121314151617181920212223// æ’å…¥ä¸€ä¸ªArtistç±»åˆ«çš„èŠ‚ç‚¹ï¼Œè€Œä¸”è¿™ä¸ªèŠ‚ç‚¹æœ‰ä¸€ä¸ªå±æ€§ä¸ºNameï¼Œå€¼ä¸ºLady GagaCREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;&#125;)// åˆ›å»ºå¹¶è¿”å›CREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;, Gemder:&quot;Femal&quot;&#125;) return a// ä¸€æ¬¡æ€§åˆ›å»ºå¤šä¸ªCREATE (a:Album &#123; Name: &quot;Killers&quot;&#125;), (b:Album &#123; Name: &quot;Fear of the Dark&quot;&#125;) RETURN a, bCREATE (a:Album &#123; Name: &quot;Piece of Mind&quot;&#125;) CREATE (b:Album &#123; Name: &quot;Somewhere in Time&quot;&#125;) RETURN a, b// åˆ é™¤èŠ‚ç‚¹ï¼Œå¦‚æœè¿™ä¸ªèŠ‚ç‚¹å’Œå…¶ä»–èŠ‚ç‚¹æœ‰è¿æ¥çš„è¯ï¼Œä¸èƒ½å•å•åˆ é™¤è¿™ä¸ªèŠ‚ç‚¹MATCH (a:Album &#123;Name: &quot;Killers&quot;&#125;) DELETE a// ä¸€æ¬¡æ€§åˆ é™¤å¤šä¸ªèŠ‚ç‚¹MATCH (a:Artist &#123;Name: &quot;Iron Maiden&quot;&#125;), (b:Album &#123;Name: &quot;Powerslave&quot;&#125;) DELETE a, b // åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹MATCH (n) DELETE n åˆ›å»º/åˆ é™¤å…³ç³» 123456789101112131415161718192021222324252627282930// å¯¹Lady Gagaå’Œä¸“è¾‘PieceOfMindä¹‹é—´åˆ›å»ºä¸€ä¸ªreleasedçš„å…³ç³»MATCH (a:Artist), (b:Album)WHERE a.Name = &quot;Lady Gaga&quot; AND b.Name = &quot;Piece of Mind&quot;CREATE (a)-[r:RELEASED]-&gt;(b)RETURN rMATCH (a:Artist), (b:Album), (p:Person)WHERE a.Name = &quot;Strapping Young Lad&quot; AND b.Name = &quot;Heavy as a Really Heavy Thing&quot; AND p.Name = &quot;Devin Townsend&quot; CREATE (p)-[pr:PRODUCED]-&gt;(b), (p)-[pf:PERFORMED_ON]-&gt;(b), (p)-[pl:PLAYS_IN]-&gt;(a)RETURN a, b, p // åˆ é™¤æŒ‡å®šçš„å…³ç³»MATCH (:Artist)-[r:RELEASED]-(:Album) DELETE r MATCH (:Artist &#123;Name: &quot;Strapping Young Lad&quot;&#125;)-[r:RELEASED]-(:Album &#123;Name: &quot;Heavy as a Really Heavy Thing&quot;&#125;) DELETE r // åˆ é™¤æ‰€æœ‰çš„å…³ç³»MATCH ()-[r:RELEASED]-() DELETE r // æ¸…é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³» MATCH (n)OPTIONAL MATCH(n)-[r]-()DELETE n,r // åˆ é™¤æ•´ä¸ªæ•°æ®åº“MATCH (n) DETACH DELETE n åˆ›å»º/åˆ é™¤çº¦æŸ åŒSQLä¸€æ ·ï¼ŒNeo4jæ•°æ®åº“æ”¯æŒå¯¹Nodeæˆ–relationshipçš„å±æ€§çš„UNIQUEçº¦æŸ 123CREATE CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUEDROP CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUE åˆ›å»º/åˆ é™¤ç´¢å¼• 123456CREATE INDEX ON :Album(Name) // View the schema:schemaDROP INDEX ON :Album(Name) æ›´æ–°ä¸€ä¸ªèŠ‚ç‚¹/è¾¹ 12MATCH (n:Person &#123; name: &quot;Andres&quot; &#125;)SET n.name = &quot;Taylor&quot;; ç­›é€‰è¿‡æ»¤ 123456789// WHEREMATCH (p1: Person)-[r:friend]-&gt;(p2: Person) WHERE p1.name=~&quot;K.+&quot; or p2.age=24 or &quot;neo&quot; in r.rels RETURN p1, r, p2 // NOT MATCH (p:Person)-[:ACTED_IN]-&gt;(m)WHERE NOT (p)-[:DIRECTED]-&gt;()RETURN p, m ç»“æœé›†è¿”å› 12345MATCH (p:Person)RETURN p, p.name AS name, upper(p.name), coalesce(p.nickname,&quot;n/a&quot;) AS nickname, &#123; name: p.name, label:head(labels(p))&#125; AS person MATCH (n) RETURN DISTINCT n.name; èšåˆå‡½æ•° Cypheræ”¯æŒcount, sum, avg, min, max èšåˆçš„æ—¶å€™nullä¼šè¢«è·³è¿‡ count è¯­æ³• æ”¯æŒ count( distinct role ) 123456MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations// æ”¶é›†èšåˆç»“æœMATCH (m:Movie)&lt;-[:ACTED_IN]-(a:Person)RETURN m.title AS movie, collect(a.name) AS cast, count(*) AS actors æ’åºå’Œåˆ†é¡µ 123MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a, count(*) AS appearancesORDER BY appearances DESC SKIP 3 LIMIT 10; Union è”åˆ 12345MATCH (actor:Person)-[r:ACTED_IN]-&gt;(movie:Movie)RETURN actor.name AS name, type(r) AS acted_in, movie.title AS titleUNION ï¼ˆALLï¼‰MATCH (director:Person)-[r:DIRECTED]-&gt;(movie:Movie)RETURN director.name AS name, type(r) AS acted_in, movie.title AS title Withè¯­å¥ withè¯­å¥ç»™Cypheræä¾›äº†å¼ºå¤§çš„pipelineèƒ½åŠ›ï¼Œå¯ä»¥ä¸€ä¸ªæˆ–è€…queryçš„è¾“å‡ºï¼Œæˆ–è€…ä¸‹ä¸€ä¸ªqueryçš„è¾“å…¥ å’Œreturnè¯­å¥éå¸¸ç±»ä¼¼ï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œwithçš„æ¯ä¸€ä¸ªç»“æœï¼Œå¿…é¡»ä½¿ç”¨åˆ«åæ ‡è¯†ã€‚ ä½¿ç”¨withæˆ‘ä»¬å¯ä»¥åœ¨æŸ¥è¯¢ç»“æœé‡Œé¢åœ¨ç»§ç»­åµŒå¥—æŸ¥è¯¢ã€‚ 1234MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)WITH p, count(*) AS appearances, COLLECT(m.Title) AS moviesWHERE appearances &gt; 1RETURN p.name, appearances, movies æœ‰ç‚¹ç±»ä¼¼SQLä¸­çš„havingï¼Œè¿™é‡Œæ˜¯with + whereä¸¤ä¸ªä¸€èµ·æ¥å®ç°çš„ã€‚ æŸ¥è¯¢æœ€çŸ­è·¯å¾„ 12MATCH (ms:Person &#123; name: &quot;Node A&quot; &#125;),(cs:Person &#123; name:&quot;Node B&quot; &#125;), p = shortestPath((ms)-[r:Follow]-(cs)) RETURN p; åŠ è½½æ•°æ® Cypher Neo4j Couldnâ€™t load the external resource neo4jåˆæ¢ åŠ è½½å­˜åœ¨æœ¬åœ°serverä¸Šçš„æ•°æ®ï¼Œä¼šåœ¨è·¯å¾„å‰é¢è‡ªåŠ¨åŠ ä¸ªå‰ç¼€ /path-to-neo4j/neo4j-community-3.4.5/importï¼Œå³Serverå¯¹åº”æ‰€åœ¨çš„è·¯å¾„ä¸‹çš„import 12345678910111213141516// åŠ è½½addressLOAD CSV WITH HEADERS FROM &quot;file:///data/addresses.csv&quot; AS csvLineCREATE (p:Person &#123;id: toInt(csvLine.id), email: csvLine.address &#125;)// åŠ è½½emailLOAD CSV WITH HEADERS FROM &quot;file:///data/emails.csv&quot; AS csvLineCREATE (e:Email &#123;id: toInt(csvLine.id), time: csvLine.time, content: csvLine.content &#125;) // åˆ›å»ºæ”¶å‘å…³ç³»USING PERIODIC COMMIT 500 // åˆ†æ®µåŠ è½½LOAD CSV WITH HEADERS FROM &quot;file:///data/relations.csv&quot; AS csvLineMATCH (p1:Person &#123;id: toInt(csvLine.fromId)&#125;),(e:Email &#123; id: toInt(csvLine.emailId)&#125;),(p2:Person&#123; id: toInt(csvLine.toId)&#125;)CREATE UNIQUE (p1)-[:FROM]-&gt;(e)CREATE(e)-[:TO]-&gt;(p2) å¦‚æœéœ€è¦å¯¼å…¥å…¶ä»–åœ°æ–¹çš„ï¼Œå¯ä»¥ä½¿ç”¨ 123456789LOAD CSV FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine[0], Name: csvLine[1]&#125;)// ä½¿ç”¨csvä¸­çš„header LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine.Id, Name: csvLine.Track, Length: csvLine.Length&#125;) // è‡ªå®šä¹‰csvæ–‡ä»¶ä¸­çš„åˆ†éš”ç¬¦LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLine FIELDTERMINATOR &quot;;&quot; ä½¿ç”¨ neo4j-import å¯¼å…¥æ•°æ® ä½¿ç”¨neo4j-importå¯¼å…¥æ•°æ® ä½¿ç”¨æ¡ä»¶ éœ€è¦å…ˆå…³é—­neo4j æ— æ³•å†åŸæœ‰çš„æ•°æ®åº“æ·»åŠ ï¼Œåªèƒ½é‡æ–°ç”Ÿæˆä¸€ä¸ªæ•°æ®åº“ å¯¼å…¥æ–‡ä»¶æ ¼å¼ä¸ºcsv å‚æ•° â€”intoï¼šæ•°æ®åº“åç§° â€”bad-toleranceï¼šèƒ½å®¹å¿çš„é”™è¯¯æ•°æ®æ¡æ•°ï¼ˆå³è¶…è¿‡æŒ‡å®šæ¡æ•°ç¨‹åºç›´æ¥æŒ‚æ‰ï¼‰ï¼Œé»˜è®¤1000 â€”multiline-fieldsï¼šæ˜¯å¦å…è®¸å¤šè¡Œæ’å…¥ï¼ˆå³æœ‰äº›æ¢è¡Œçš„æ•°æ®ä¹Ÿå¯è¯»å–ï¼‰ â€”nodesï¼šæ’å…¥èŠ‚ç‚¹ â€”relationshipsï¼šæ’å…¥å…³ç³» æ›´å¤šå‚æ•°å¯å…è®¸å‘½ä»¤bin/neo4j-import 1bin/neo4j-import --multiline-fields=true --bad-tolerance=1000000 --into graph.db --id-type string --nodes:person node.csv --relationships:related relation_header.csv,relation.csv è¿è¡Œå®Œåï¼Œå°†ç”Ÿæˆçš„graph.dbæ”¾å…¥data/databasesï¼Œè¦†ç›–åŸæœ‰æ•°æ®åº“ï¼Œå¯åŠ¨è¿è¡Œå³å¯ 3. References Neo4jçš„ç®€å•æ­å»ºä¸ä½¿ç”¨ Neo4j Tutorial Neo4jçš„æŸ¥è¯¢è¯­æ³•ç¬”è®° å®˜æ–¹æ–‡æ¡£ï¼šComprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.cc/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-RNNå¸¸è§ç½‘ç»œ","date":"2018-05-25T11:10:06.000Z","path":"2018/05/25/æ·±åº¦å­¦ä¹ ç¬”è®°-RNNå¸¸è§ç½‘ç»œ/","text":"è¯´å®è¯ï¼Œåªæ˜¯å¤§æ¦‚äº†è§£ä¸€ä¸‹è¿™äº›å¸¸è§çš„RNNæ¨¡å‹ï¼Œä½†æ˜¯è¿™äº›æ¨¡å‹ä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ï¼Œä¸ºä»€ä¹ˆèƒ½å¤ŸæŠ“å–ç½‘ç»œä¸­å¯èƒ½æœ‰ç”¨çš„é•¿çŸ­æœŸä¿¡æ¯å…¶å®æˆ‘è¿˜æ˜¯æ¯”è¾ƒæ‡µçš„ã€‚è¿™ä¸€å—ä»¥åä¹Ÿæ˜¯éœ€è¦å»æ·±å…¥ç†è§£çš„ä¸€éƒ¨åˆ†ï¼Œå…ˆmarkä¸€ä¸‹ã€‚ 1. å¾ªç¯ç¥ç»ç½‘ç»œ - RNN (Recurrent Neural Network)2. é—¨æ§å¾ªç¯å•å…ƒ - GRN (Gated Recurrent Neural Networks)æŒ‰æ—¶é—´é¡ºåºä¸Šæ¥çœ‹ï¼ŒGRNæ˜¯åœ¨LSTMä¹‹åæå‡ºæ¥çš„ã€‚ä½†æ˜¯è¿™ä¸ªç›¸å¯¹æ¥è¯´å¯èƒ½æ²¡é‚£ä¹ˆå¤æ‚ï¼Œæ‰€ä»¥å…ˆæ€»ç»“è¿™ä¸ªï¼Œç„¶åå†æ€»ç»“LSTMã€‚ é—¨æ§å¾ªç¯å•å…ƒï¼Œé‡ç‚¹æ˜¯åœ¨äºé—¨çš„ï¼Œå³Gatedã€‚è¿™é‡ŒæŒ‡çš„æ˜¯ $\\sigma$ = sigmod func. é‚£ä¹ˆä¸ºä»€ä¹ˆè¦å‡ºç°è¿™äº›é—¨å‘¢ï¼Œç›¸æ¯”åŸå…ˆçš„RNNæ¨¡å‹ï¼Œè¿™äº›é—¨çš„å‡ºç°èƒ½å¤Ÿè§£å†³ä»€ä¹ˆæ ·çš„é—®é¢˜ï¼Ÿ ä¹‹å‰è®¨è®ºè¿‡ï¼ŒRNNåœ¨(BPTT)è®¡ç®—çš„æ—¶å€™ï¼Œæ¢¯åº¦å®¹æ˜“å‡ºç°è¡°å‡æˆ–çˆ†ç‚¸ï¼Œç‰¹åˆ«æ˜¯æ¢¯åº¦è¡°å‡çš„é—®é¢˜ã€‚ç”±äºå‡ºç°äº†æ¢¯åº¦è¡°å‡ï¼Œé‚£ä¹ˆå¯¹äºä¸€äº›æ—¶é—´åºåˆ—ä¸Šï¼Œè·ç¦»è¾ƒé•¿çš„æ•°æ®ï¼Œæ¨¡å‹å°±æ¯”è¾ƒéš¾æ•æ‰åˆ°è¿™ä¸¤ä¸ªæ—¶é—´ç‚¹ä¸Šçš„å…³ç³»ã€‚ æ¨¡å‹è®¾è®¡å¦‚ä¸‹ï¼š é‡ç½®é—¨ - R (reset) æœ‰åŠ©äºæ•æ‰æ—¶åºæ•°æ®ä¸­çŸ­æœŸçš„ä¾èµ–å…³ç³»ã€‚ æ ¹æ®$sigmod$å‡½æ•°çš„æ€§è´¨ï¼Œå€¼ä»‹äº0-1ä¹‹é—´ï¼Œå¯èƒ½ä¸¢å¼ƒè¿‡å»çš„ä¸€äº›ä¸ç›¸å…³çš„ä¿¡æ¯ï¼š R == 0ï¼Œä¸¢å¼ƒäº†ä¸Šä¸€è½®çš„éšå«å±‚çš„ä¿¡æ¯ï¼› R == 1ï¼Œåˆ™ä¿ç•™ç›¸å…³çš„ä¿¡æ¯ã€‚ æ ¹æ®ä¸Šé¢çš„æ¨¡å‹å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é‡ç½®é—¨å…¶å®æ˜¯æœ‰ä¸¤ä¸ªè¾“å…¥çš„ï¼Œåˆ†åˆ«æ˜¯ï¼š $X_t$ $H_{t-1}$ å³å½“å‰å’Œä¸Šä¸€è½®çš„ä¿¡æ¯ï¼Œåˆ™æ˜¯çŸ­æœŸå†…çš„ä¿¡æ¯ã€‚ R_t = \\sigma(X_t W_{xr} + H_{t-1}W_{hr} + b_r) æ›´æ–°é—¨ æœ‰åŠ©äºæ•æ‰æ—¶åºæ•°æ®ä¸­é•¿æœŸçš„ä¾èµ–çŠ¶æ€ã€‚ ç”¨äºæ›´æ–°å½“å‰éšå«å±‚çš„çŠ¶æ€ï¼Œä¸ºäº†æ•æ‰åˆ°ä¹‹å‰æ¯”è¾ƒä¹…ã€é•¿æœŸä¹‹å‰çš„çŠ¶æ€ Z_t = \\sigma(X_t W_{xz} + H_{t-1}W_{hz} + b_z) $Z_t$ == 0ï¼Œåˆ™ä¸ä¿ç•™ä¹‹å‰çš„ä¿¡æ¯ï¼› $Z_t$ == 1ï¼Œä¿ç•™ä¹‹å‰çš„ä¿¡æ¯ã€‚ å€™é€‰éšå«çŠ¶æ€ \\tilde H= tanh(X_tW_{xh} + R_t \\bigodot H_{t-1}W_{hh} + b_h å½“å‰éšå«çŠ¶æ€ H_t = Z_t\\bigodot H_{t-1} + (1 - Z_t) \\bigodot \\tilde H_t 3. é•¿çŸ­æœŸè®°å¿† - LSTM (Long and Short-Term Memory)ç»“æ„æ¯”ä¹‹å‰çš„GRUæ›´ä¸ºå¤æ‚ä¸€äº›ï¼ŒLSTMçš„éšå«å±‚åŒ…å«éšå«å±‚å˜é‡Hå’Œè®°å¿†ç»†èƒCï¼Œå®ƒä»¬çš„è®¾è®¡å’Œå½¢çŠ¶ç›¸åŒï¼ˆå¯ä»¥çœ‹åˆ°åé¢çš„å‡ ä¸ªfuncè®¾è®¡ä¸Šå…¶å®æ˜¯ä¸€æ ·çš„ï¼Œjiã€‚ è¾“å…¥é—¨ çŸ­æœŸè®°å¿† I_t= \\sigma(X_t W_{xi} + H_{t-1}W_{hi} + b_i) é—å¿˜é—¨ é•¿æœŸè®°å¿† F_t= \\sigma(X_t W_{xf} + H_{t-1}W_{hf} + b_f) è¾“å‡ºé—¨ O_t= \\sigma(X_t W_{xo} + H_{t-1}W_{ho} + b_o) å€™é€‰è®°å¿†ç»†èƒ Memory Cell \\tilde C_t = \\tanh(X_t W_{xc} + H_{t-1}W_{hc} + b_c) è®°å¿†ç»†èƒ C_t = F_t \\bigodot C_{t-1} + I_t \\bigodot \\tilde C_t å½“å‰éšå«çŠ¶æ€ 4. æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ5. åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ6. References åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ -å¾ªç¯ç¥ç»ç½‘ç»œ RNN LSTMä¸GRUæ·±åº¦å­¦ä¹ æ¨¡å‹å­¦ä¹ ç¬”è®°","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"RNN","slug":"RNN","permalink":"http://chenson.cc/tags/RNN/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-RNN","date":"2018-05-14T03:35:41.000Z","path":"2018/05/14/æ·±åº¦å­¦ä¹ ç¬”è®°-RNN/","text":"1. ä¸€äº›é¢„å¤‡çŸ¥è¯†1.1 Markov Chain 2. å¾ªç¯ç¥ç»ç½‘ç»œ - RNN (Recurrent Neural Network)3. é—¨æ§å¾ªç¯å•å…ƒ - GRN (Gated Recurrent Neural Networks)4. é•¿çŸ­æœŸè®°å¿† - LSTM (Long and Short-Term Memory)","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.cc/tags/CNN/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-CNNå¸¸è§ç½‘ç»œ","date":"2018-04-27T02:32:41.000Z","path":"2018/04/27/æ·±åº¦å­¦ä¹ ç¬”è®°-CNNå¸¸è§ç½‘ç»œ/","text":"ä¸Šæ¬¡ç¬”è®°ä¸»è¦è®°å½•äº†å·ç§¯ç¥ç»ç½‘ç»œæ€»å·ç§¯Blockä¸­çš„åŸºæœ¬æ¦‚å¿µï¼Œæœ¬æ¬¡ç¬”è®°æ‰“ç®—æ•´ç†ä¸€ä¸‹å¸¸è§çš„CNNï¼Œä»å¼€å±±é¼»ç¥–çš„LeNetåˆ°ç›®å‰æ¯”è¾ƒæµè¡Œçš„ResNetå’ŒDenseNetç­‰ã€‚ 1. LeNetLeNetæ˜¯æ—©èµ·ç”¨æ¥è¯†åˆ«æ‰‹å†™æ•°å­—å›¾åƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œç®—æ˜¯å·ç§¯ç¥ç»ç½‘ç»œçš„å¼€å±±é¼»ç¥–ã€‚ LeNetä¸»è¦åˆ†ä¸ºå·ç§¯å±‚å—å’Œå…¨è¿æ¥å±‚å—ä¸¤å¤§éƒ¨åˆ†ã€‚ 1.1 å·ç§¯å±‚å—åŸºæœ¬å•ä½æ˜¯å·ç§¯å±‚åæ¥æœ€å¤§æ± åŒ–å±‚ï¼ˆå®é™…ä¸Šé‡Œé¢è¿˜å¯ä»¥é¢å¤–çš„æ·»åŠ ä¸€äº›å…¶ä»–çš„æ¨¡å—ï¼‰ï¼Œç„¶åå·ç§¯å±‚å—å°±ç”±è¿™ä¿©åŸºæœ¬å•ä½é‡å¤çš„å †å æ„æˆã€‚ å·ç§¯å±‚ ç”¨æ¥è¯†åˆ«å›¾åƒé‡Œçš„ç©ºé—´æ¨¡å¼ï¼Œè¯†åˆ«ä¸€äº›å±€éƒ¨çš„locallyç‰¹å¾ï¼Œæ¯”å¦‚çº¿æ¡å’Œç‰©ä½“çš„å±€éƒ¨ã€‚ å·ç§¯å±‚çš„è¾“å…¥ä¸º4Dæ•°æ®ï¼Œå½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°ï¼Œé€šé“ï¼Œé«˜ï¼Œå®½)ï¼Œè¾“å‡ºä½¿ç”¨äº†Sigmoidæ¿€æ´»å‡½æ•°ã€‚ æœ€å¤§æ± åŒ–å±‚ ç”¨æ¥é™ä½å·ç§¯å±‚å¯¹ä½ç½®çš„æ•æ„Ÿæ€§ï¼Œæ± åŒ–å±‚æ³¨æ„è®¾ç½®å¥½pool_sizeå’Œstrideså¤§å°ã€‚ 1.2 å…¨è¿æ¥å±‚å—åœ¨Convå’ŒDenseäº¤ç•Œçš„éƒ¨åˆ†ï¼Œéœ€è¦å°†è¾“å‡ºçš„æ•°æ®flattenæ‰å¯ä»¥ç»™Denseç”¨ã€‚æ¿€æ´»å‡½æ•°ä½¿ç”¨Sigmod 1.3 MxNetå®ç°ä»£ç 12345678910111213141516171819202122232425import d2lzh as d2limport mxnet as mxfrom mxnet import autograd, gluon, init, ndfrom mxnet.gluon import loss as gloss, nnimport time# å®šä¹‰ç½‘ç»œnet = nn.Sequential()net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'), nn.MaxPool2D(pool_size=2, strides=2), nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'), nn.MaxPool2D(pool_size=2, strides=2), # Denseä¼šé»˜è®¤å°†(æ‰¹é‡å¤§å°,é€šé“,é«˜,å®½)å½¢çŠ¶çš„è¾“å…¥è½¬æ¢æˆ(æ‰¹é‡å¤§å°,é€šé“ * é«˜ * å®½)å½¢ # çŠ¶çš„è¾“å…¥ nn.Dense(120, activation='sigmoid'), nn.Dense(84, activation='sigmoid'), nn.Dense(10))# è®­ç»ƒéƒ¨åˆ†(æœ‰äº›ä»£ç ä¸å®Œæ•´ï¼Œè¯¦æƒ…è§æ²ç¥Githubä»£ç )lr, num_epochs = 0.9, 5# å…¶å®è¿™é‡Œæƒ³å±•ç¤ºçš„æ˜¯netæƒé‡çš„åˆå§‹åŒ–æ—¶ï¼Œä½¿ç”¨çš„æ˜¯Xavieréšæœºåˆå§‹åŒ–net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())trainer = gluon.Trainer(net.collect_params(), 'sgd', &#123;'learning_rate': lr&#125;)train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs) è¿è¡Œç»“æœ ä¸‹å›¾æœ‰ä¿©ç»“æœï¼Œä¸€ä¸ªæ˜¯æ²ç¥ç¬”è®°é‡Œé¢ç”¨K80è·‘çš„ç»“æœï¼Œä¸€ä¸ªæ˜¯æˆ‘ç¬”è®°æœ¬è·‘çš„ç»“æœï¼Œé…ç½®æ˜¯2015å¹´çš„ä¸­é…RMBPï¼Œcpuæ˜¯i5 8Gï¼Œå¤§æ¦‚é€Ÿåº¦å·®äº†10å€å·¦å³ã€‚ è°ƒå‚ç»ƒä¹  è¿™éƒ¨åˆ†å‘¢ï¼Œå…¶å®å°±æ˜¯æ‰¾ä¸€ä¸‹åœ¨æ·±åº¦å­¦ä¹ ä¸­è°ƒå‚çš„ä¸€äº›æ„Ÿè§‰ï¼Œæ¯•ç«Ÿæˆ‘åªæ˜¯ä¸€æšç‚¼ä¸¹å·¥ç¨‹å¸ˆã€‚ æ”¹ä¸ºæ™®é€šçš„éšæœºåˆå§‹åŒ–ç½‘ç»œæƒé‡å¯ä»¥çœ‹åˆ°ä¸‹å›¾åŸºæœ¬ä¸æ”¶æ•›ã€‚ æ¥ç€å°è¯•ä¸€ä¸‹æŠŠlrç»™è°ƒå°ä¸€äº›ï¼Œè°ƒåˆ°0.05ï¼Œä¼¼ä¹ä¹Ÿæ²¡å•¥æå‡ï¼Œç»§ç»­è°ƒå°åˆ°0.005ï¼Œä¸”å¢åŠ num_epochsåˆ°20ï¼ŒåŸºæœ¬ä¹Ÿæ²¡æå‡ã€‚ç„¶åæƒ³æƒ³æˆ‘æ˜¯æ”¹äº†ä¸€ä¸ªè¾ƒä¸ºåˆé€‚çš„æƒé‡ï¼Œæ‰€ä»¥åº”è¯¥æŠŠlrè°ƒé«˜ä¸€äº›ï¼Œæ–¹ä¾¿å¿«é€Ÿè°ƒæ•´åˆ°åˆé€‚çš„æƒé‡ï¼Œå°±æŠŠlræé«˜åˆ°1.5ï¼Œç”¨çš„éšæœºåˆå§‹åŒ–ï¼Œä¸€ç›´ç­‰åˆ°epocheåˆ°äº†10å·¦å³çš„æ—¶å€™accæ‰å¼€å§‹æå‡ï¼Œå¯ä»¥è¯´æ”¶æ•›å¾—éå¸¸æ…¢äº†ã€‚çœ‹æ¥è¿™ä¸ªæƒé‡çš„åˆå§‹åŒ–éå¸¸é‡è¦ï¼Œéœ€è¦å•ç‹¬è°ƒç ”ä¸€ä¸‹ï¼Œåˆ°æ—¶å€™çœ‹å•¥åŸå› ã€‚ å¾€å°è°ƒ å¾€å¤§è°ƒ å…¶å®lrè°ƒå¤ªå°äº†(0.05/0.005, åŸå…ˆæ˜¯0.9)ï¼Œè™½ç„¶æ”¹å›å»Xavieråˆå§‹åŒ–æƒé‡ä¹ŸåŸºæœ¬æ˜¯ä¸Šé¢æ•ˆæœï¼Œä¼°è®¡æ˜¯å¤ªå°äº†ä¸æ”¶æ•›ï¼Ÿè‹¥æ˜¯è°ƒåˆ°0.5çš„è¯ï¼Œå¯ä»¥æ”¶æ•›ï¼Œä½†ç›¸æ¯”é€Ÿåº¦ä¸Šä¼šæ”¶æ•›å¾—æ…¢ä¸€äº›ï¼Œæ‰€ä»¥é€‰å®šä¸€ä¸ªåˆé€‚çš„lrä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚ è°ƒæ•´å·ç§¯çª—å£çš„å¤§å° è°ƒæ•´è¾“å‡ºé€šé“æ•° è°ƒæ•´æ¿€æ´»å‡½æ•° è°ƒæ•´å…¨è¿æ¥å±‚è¾“å‡ºä¸ªæ•° 2. AlexNetï¼ˆ2013ï¼‰ æ€»å…±å…­ä¸ªé˜¶æ®µ kernelï¼Œstridesç›¸å¯¹å¤§å¾ˆå¤š Xavieræ˜¯æ€ä¹ˆåˆå§‹åŒ–çš„ï¼Ÿä¸é»˜è®¤çš„åŒºåˆ«ï¼Ÿ LeNet V.S AlexNet 3. VGGä½¿ç”¨é‡å¤å…ƒç´ çš„éå¸¸æ·±çš„ç½‘ç»œ ä½¿ç”¨å¾ˆå¤šç›¸å¯¹åŠ å¤šçš„kernelï¼Œæ¯”å¦‚3x3çš„å·ç§¯ï¼Œç„¶åæ¥ä¸Šä¸€ä¸ªæ± åŒ–å±‚ï¼Œä¹‹åå†å°†è¿™ä¸ªæ¨¡å—é‡å¤å¾ˆå¤šæ¬¡ã€‚ VGG Block ç›¸åŒå åŠ  VGG Stack ç›¸ä¼¼å åŠ  VGG11, VGG13, VGG16, VGG19 â€¦ â€¦ 4. Bath Normæ‰¹é‡å½’ä¸€åŒ–ï¼Œå°†æ•°æ®å‡å€¼å˜ä¸º0ï¼Œæ–¹å·®å˜ä¸º1ï¼Œä½¿æ•°å€¼æ›´åŠ ç¨³å®šã€‚ $\\lambda$ $\\beta$ å…¨è¿æ¥2Dæ•°æ® æ¯ä¸ªæ ·æœ¬ä¹‹é—´ï¼Œå‡å€¼å˜ä¸º0ï¼Œæ–¹å·®å˜ä¸º1 å¦‚æœæ˜¯å·ç§¯3Dæ•°æ® å¯¹æ¯ä¸ªchannelåšå˜æ¢ï¼Œä½¿å¾—å‡å€¼å˜ä¸º0ï¼Œæ–¹å·®å˜ä¸º1 Q&amp;A: batchNorm åŠ çš„ä½ç½®ï¼Ÿ ä¸ºä»€ä¹ˆæ˜¯Conv2Dçš„åé¢ï¼ŒActivationçš„å‰é¢ï¼Œè€Œä¸æ˜¯Conv2Dçš„å‰é¢ï¼Ÿ æ¯ä¸ªbatchä¹‹é—´çš„å½’ä¸€åŒ–ä¸ä¸€æ ·ï¼Ÿ è®­ç»ƒçš„æ—¶å€™okï¼Œæµ‹è¯•çš„æ—¶å€™å¦‚ä½•åšå½’ä¸€åŒ–ï¼Ÿ ä¿ç•™è®­ç»ƒæ—¶å€™ç®—å‡ºæ¥çš„ç»“æœåšå½’ä¸€åŒ– mean, variance, $\\lambda$ , $\\beta$ 5. Network in Networ (NiN) AlexNet å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚åˆ†åˆ«åŠ æ·±åŠ å®½ä»è€Œå¾—åˆ°æ·±åº¦ç½‘ç»œ NiN ä¸²è”æ•°ä¸ªå·ç§¯å±‚å—å’Œå…¨è¿æ¥å±‚å—æ¥æ„å»ºæ·±åº¦ç½‘ç»œ Q&amp;Aï¼š å¦‚ä½•è§£å†³Convå’ŒDenseä¹‹é—´è¾“å…¥ç»´åº¦çš„é—®é¢˜ï¼Œæ¯”å¦‚Convçš„è¾“å…¥ä¸º4Dï¼Œä½†æ˜¯Denseå¯èƒ½æ— æ³•è¾“å‡ºå¦‚æ­¤ç»´åº¦çš„æ•°æ® æŠŠDenseå±‚æ¢æˆkernelä¸º1x1çš„Conv 4Dæ•°æ®åˆ°2Dæ•°æ®é€šå¸¸ä¼šç¼–ç¨‹éå¸¸å¤§ 6. GoogLeNet(2014) Inception","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.cc/tags/CNN/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-CNN","date":"2018-04-20T06:45:37.000Z","path":"2018/04/20/æ·±åº¦å­¦ä¹ ç¬”è®°-CNN/","text":"æœ€è¿‘æ­£åœ¨å­¦ä¹ ææ²æ²ç¥çš„åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ï¼Œç›®å‰å¬åˆ°äº†ç¬¬äº”è¯¾ï¼ŒçœŸçš„æ˜¯éå¸¸å¥½çš„è¯¾ç¨‹ã€‚å…¶å®åœ¨å­¦ä¹ ä¹‹å‰ä¹Ÿæœ‰æ–­æ–­ç»­ç»­çš„å­¦è¿‡ä¸€äº›æ·±åº¦å­¦ä¹ ç›¸å…³çš„ä¸œè¥¿ï¼Œä¹Ÿä¼šç”¨PyTorchè·‘ä¸€äº›ç®€å•çš„æ¨¡å‹ï¼Œä½†ä»…é™äºæ­¤ï¼Œå¯¹ç†è®ºçŸ¥è¯†ç†è§£è¿˜æ˜¯æ¯”è¾ƒæµ…è–„çš„ï¼Œåªæ˜¯ä¸€ä¸ªè°ƒåŒ…ä¾ ï¼Œæ‰€ä»¥è¿™æ¬¡å­¦ä¹ æ‰“ç®—è®¤çœŸå­¦ä¹ ä¸€ä¸‹ç›¸å…³çš„ç†è®ºçŸ¥è¯†ã€‚ åœ¨å­¦ä¹‹å‰ä¹Ÿæœ‰å¬äº†ä¸€ä¸‹NGåˆšå‘å¸ƒçš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹ï¼Œä½†å¯¹æ¯”äº†ä¸€ä¸‹ï¼Œè¿˜æ˜¯è§‰å¾—æ²ç¥çš„æ¯”è¾ƒé€‚åˆæˆ‘ï¼Œæ¯•ç«Ÿæ•™å­¦ç”¨çš„æ˜¯ä¸­æ–‡ï¼Œæ–¹ä¾¿ç†è§£ï¼Œä¸”ç†è®ºçŸ¥è¯†æ·±åº¦åˆé€‚ï¼Œæ‰€ä»¥æ‰“ç®—å…ˆå¬å®Œè¿™é—¨è¯¾ï¼Œä»¥åç†è®ºæ–¹é¢æœ‰éœ€è¦çš„ï¼Œå†å¬NGç›¸å…³çš„è¯¾ç¨‹ã€‚ 1. å·ç§¯ç¥ç»ç½‘ç»œ CNN - (Convolutional Neural Network)1.1 ç¥ç»ç½‘ç»œ - Neural Network å•ä¸ªç¥ç»å…ƒ ç¥ç»ç½‘ç»œ 1. 2 å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„åŸºæœ¬ç»“æ„å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªblock è¾“å…¥å±‚ å·ç§¯å±‚+æ¿€æ´»å‡½æ•°+æ± åŒ–å±‚ è¾“å‡ºå±‚ è¾“å…¥å±‚ â€”-&gt; å·ç§¯å±‚ â€”-&gt; æ¿€æ´»å‡½æ•° â€”-&gt; æ± åŒ–å±‚ â€”-&gt; å…¨è¿æ¥å±‚ ä¸­é—´ä¸‰ä¸ªéƒ¨åˆ†å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªblockï¼Œå¤æ‚çš„æ¨¡å‹å¯ä»¥é‡å¤å¤šä¸ªè¿™æ ·çš„blockï¼Œé‡Œé¢å‚æ•°å¯ä»¥æ˜¯ä¸€æ ·æˆ–è€…ç›¸ä¼¼çš„ã€‚ 1. 3 å·ç§¯ - Convolutionalç›¸å¯¹æ™®é€šçš„ç¥ç»ç½‘ç»œï¼Œå·ç§¯ç¥ç»ç½‘ç»œåœ¨ä¸­çš„æ ¸å¿ƒå°±æ˜¯åœ¨äºå·ç§¯è®¡ç®—å±‚äº†ï¼Œå·ç§¯ç¥ç»ç½‘ç»œåŒ…å«äº†ä¸€ä¸ªç”±å·ç§¯å±‚å’Œå­é‡‡æ ·å±‚æ„æˆçš„ç‰¹å¾æŠ½å–å™¨ã€‚å¯¹äºå¦‚ä½•ç†è§£å·ç§¯ï¼Œè£‚å¢™æ¨èçŸ¥ä¹ä¸Šçš„è¿™ç¯‡æ–‡ç« é€šä¿—ç†è§£ã€å·ç§¯ã€â€”â€”ä»å‚…é‡Œå¶å˜æ¢åˆ°æ»¤æ³¢å™¨ï¼Œä»ä¿¡å·å¤„ç†è¿™ä¸ªæœ¬è´¨çš„è§’åº¦æ¥ç†è§£å·ç§¯ï¼Œéå¸¸æ£’ğŸ‘ã€‚ å·ç§¯çš„æ„æ€å°±æ˜¯ï¼Œç¥ç»ç½‘ç»œæ˜¯å¯¹å›¾ç‰‡ä¸Šçš„ä¸€å°å—åŒºåŸŸè¿›è¡Œå¤„ç†ï¼Œè¿™ç§åšæ³•åŠ å¼ºäº†å›¾ç‰‡ä¿¡æ¯çš„è¿ç»­æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥çœ‹åˆ°å›¾ç‰‡ä¸Šçš„å›¾å½¢ï¼Œè€Œéæ˜¯ä¸€ä¸ªä¸ªç¦»æ•£çš„ç‚¹ã€‚ä¸‹å›¾æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„çš„å·ç§¯è¿ç®—ï¼Œä¸­é—´çš„æ ¸æ˜¯å·ç§¯ä¸­çš„kernelã€‚ æ ¹æ®å›¾ç‰‡ç¤ºæ„ï¼Œæˆ‘ä»¬å°†è¾“å…¥å±‚è“è‰²çš„æ¡†æ¡†éƒ¨åˆ†ä¸kernelç›¸ä¹˜ï¼Œæœ‰ï¼š 0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19å½“ç„¶æ­¤æ—¶åªè¿ç®—äº†ä¸€å°éƒ¨åˆ†ï¼Œå·¦è¾¹çš„è¾“å…¥æ•°ç»„ä¸­ï¼Œè“è‰²çš„å››æ ¼å°æ¡†æ¡†å¯ä»¥é€æ­¥çš„å¾€å³è¾¹ä¸€æ ¼ä¸€æ ¼çš„æŒªåŠ¨ï¼Œç„¶ååˆ°è¾¾æœ€å³è¾¹åï¼Œåˆå¾€ä¸‹ç§»åŠ¨ä¸€ä¸ªæ ¼å­ï¼Œä»å·¦å¾€å³é‡å¤ä¸Šé¢çš„è¿ç®—ï¼Œå³æœ‰æœ€å³è¾¹çš„è¾“å‡ºçŸ©é˜µ 0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19 \\\\ 1 \\times 0 + 2 \\times 1 + 4 \\times 2 + 5 \\times 3 = 25 \\\\ 3 \\times 0 + 4 \\times 1 + 6 \\times 2 + 7 \\times 3 = 37 \\\\ 4 \\times 0 + 5 \\times 1 + 7 \\times 2 + 8 \\times 3 = 43ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ–¹ä¾¿ç†è§£çš„åŠ¨å›¾ ä»¥ä¸Šå°±æ˜¯å·ç§¯æœ€åŸºæœ¬çš„è¿ç®—è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸‹å‡ ä¸ªå‡è®¾ kernel size çš„å¤§å° åœ¨ä¸Šè¿°æˆ‘ä»¬å‡è®¾äº†kernelçš„sizeæ˜¯ $2 \\times 2$ çš„ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥æ˜¯å…¶ä»–ç»´åº¦çš„kernelï¼Œæ¯”å¦‚è¯´ $3 \\times 3$ ä¹‹ç±»çš„ï¼Œæˆ–è€…æ˜¯ $4 \\times 5$ è¿™æ ·çš„éæ­£æ–¹å½¢çŸ©é˜µ å‘¨å›´çš„å¡«å……å€¼ padding å…¶å®è¿™ä¸€éƒ¨åˆ†ä¸Šå›¾ä¸­å¹¶æ²¡æœ‰ä½“ç°å‡ºæ¥è¿™å‡è®¾ï¼Œpaddingçš„ä¸­æ–‡æ„æ€æ˜¯å¡«å……ï¼Œé¡¾æ˜æ€è®®å°±æ˜¯åœ¨è¾“å…¥çš„äºŒç»´çŸ©é˜µçš„é«˜å’Œå®½ä¸¤ä¾§å¡«å……å…ƒç´ ï¼ˆé€šå¸¸æ˜¯0ï¼‰ã€‚æ¯”å¦‚ä¸‹å›¾ è¿™é‡Œå‡è®¾paddingä¸º0ï¼Œå³åœ¨ç»™å®šè¾“å…¥çš„äºŒç»´çŸ©é˜µçš„é«˜å’Œå®½ä¸¤ä¾§åˆ†åˆ«å¡«å……äº†ä¸€åœˆ0å…ƒç´ ï¼ˆæ˜¯å¦å¯ä»¥æ˜¯ä¸¤åœˆæˆ–è€…å¤šåœˆï¼Ÿï¼‰ã€‚ ç›¸æ¯”ä¸Šä¸€å¼ å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¾“å‡ºçš„äºŒç»´çŸ©é˜µä¹Ÿå˜å¤§äº†ä¸€åœˆã€‚ å…¶å®å¯¹äºç»™å®šçš„è¾“å…¥çš„äºŒç»´çŸ©é˜µã€kernelå¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºè¾“å‡ºçŸ©é˜µçš„å¤§å°çš„ æ²¡æœ‰è®¾å®špadding (n_h - k_h + 1)/1 \\times (n_w - k_w + 1)/1è¿™é‡Œhä»£è¡¨çš„æ˜¯é«˜åº¦ã€wä»£è¡¨äº†å®½åº¦ï¼š è¾“å‡ºå®½åº¦ = è¾“å…¥å®½åº¦ - kernelå®½åº¦ + 1 è¾“å…¥é«˜åº¦ = è¾“å…¥é«˜åº¦ - kernelé«˜åº¦ + 1 è®¾å®šäº†padding (n_h - k_h + p_h + 1)/1 \\times (n_w - k_w + p_w + 1)/1 è¾“å‡ºå®½åº¦ = è¾“å…¥å®½åº¦ - kernelå®½åº¦ + å¡«å……çš„å®½åº¦ + 1 è¾“å…¥é«˜åº¦ = è¾“å…¥é«˜åº¦ - kernelé«˜åº¦ + å¡«å……çš„é«˜åº¦ + 1 è¿™é‡Œå›ç­”ä¸€ä¸‹ä¸Šé¢æˆ‘æ‹¬å·å†…åŠ ç²—çš„é—®é¢˜ï¼Œå½“ç»™å‘¨å›´å¡«å……ä¸€åœˆ0çš„æ—¶å€™ï¼Œpaddingçš„å€¼ä¸º $p_w \\&amp; p_h = 1$ï¼Œå½“ä¸¤åœˆ0çš„æ—¶å€™ä¸º $ p_w \\&amp; p_h=2 $ã€‚é€šå¸¸å¯¹äºpaddingçš„å–å€¼ä¸ºï¼š p_w = k_w - 1 \\\\ p_h = k_h - 1è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†è¾“å‡ºçš„çŸ©é˜µå˜å¤§ï¼Œå°½é‡ä¿æŒå’Œå’Œè¾“å…¥å¤§å°æ¯”è¾ƒç›¸è¿‘ï¼Œé¿å…å¤šå±‚å·ç§¯çš„æ—¶å€™ï¼ŒçŸ©é˜µè¶Šæ¥è¶Šå°ï¼Œç›¸å½“äºä¿¡æ¯è¶Šæ¥è¶Šå°‘ã€‚ å¦‚æœéœ€è¦ä¿æŒè¾“å…¥å’Œè¾“å‡ºç›¸åŒçš„å¤§å°ï¼Œåˆ™å¯ä»¥é‡‡ç”¨å¦‚ä¸‹è®¾å®šï¼š kernelå¤§å°ä¸ºå¥‡æ•°ï¼Œåœ¨çŸ©é˜µçš„é«˜å®½ä¸¤ä¾§éƒ½å¡«å……på€¼ kernelå¤§å°ä¸ºå¶æ•°ï¼Œåœ¨çŸ©é˜µçš„é«˜å®½ä¸€ä¾§ä¸­å¡«å……på€¼ æ»‘åŠ¨çš„æ­¥é•¿ stride è¿™é‡Œæˆ‘ä»¬å‡è®¾è¾“å…¥æ¡†æ¡†ä»å·¦å¾€å³å’Œä»ä¸Šåˆ°ä¸‹çš„ç§»åŠ¨æ­¥ä¼æ˜¯1ä¸ªå°æ ¼å­ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥æ˜¯2ä¸ªå°æ ¼å­ï¼Œ3ä¸ªå°æ ¼å­ç­‰ï¼Œè¿™ä¸ªå‚æ•°åœ¨CNNä¸­å«åšstrideã€‚ä¸‹å›¾strideçš„å¤§å°ä¸º(2, 3)ï¼Œå³å‘å³æ»‘åŠ¨çš„æ—¶å€™ï¼Œæ¯æ¬¡2ä¸ªæ ¼å­ï¼Œå‘ä¸‹æ»‘åŠ¨çš„æ—¶å€™ï¼Œæ¯æ¬¡3ä¸ªæ ¼å­ã€‚ æ³¨æ„åˆ°äº†åœ¨paddingéƒ¨åˆ†æœ‰ä¸€ä¸ªå¸¸æ•°é¡¹ä¸º1äº†å—ï¼Œå…¶å®é‚£ä¸ªå°±æ˜¯æˆ‘ä»¬è®¾å®šçš„strideçš„æ»‘åŠ¨å¤§å°ï¼Œå¦‚æœstrideä¸ä¸º(1, 1)çš„æ—¶å€™ï¼Œæ­¤æ—¶è¾“å…¥å¤§å°è®¡ç®—å…¬å¼ä¸ºï¼š (n_h - k_h + p_h + s_h)/s_h \\times (n_w - k_w + p_w + s_w)/s_w 1.4 æ¿€æ´»å‡½æ•°å…ˆè¯´ç»“è®ºï¼Œæ¿€æ´»å‡½æ•°ä¸»è¦çš„ä½œç”¨æ˜¯åŠ å…¥éçº¿æ€§å› ç´ ï¼Œå› ä¸ºçº¿æ€§çš„è¡¨è¾¾èƒ½åŠ›ä¸å¤Ÿï¼ˆå¦‚ä½•ä½“ç°ï¼Ÿï¼‰ã€‚ å…¶å®ä»ä¸Šé¢å·ç§¯çš„è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œè¿‡ç¨‹å¤§è‡´å¯ä»¥ç†è§£ä¸ºï¼š ä»ä¸€ä¸ªè¾“å…¥çš„çŸ©é˜µä¸­æŠ½å–å‡ºä¸€ä¸ªå±€éƒ¨çŸ©é˜µï¼Œç„¶åè·Ÿkernelçº¿æ€§ç›¸ä¹˜ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªç»“æœå€¼ã€‚é€šè¿‡ä»å·¦å‘å³ï¼Œä»ä¸Šè‡³ä¸‹çš„æ»‘åŠ¨ï¼Œéå†äº†æ•´ä¸ªè¾“å…¥çŸ©é˜µçš„ã€‚ ä¸Šè¿°è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªå¾ˆé‡è¦çš„æ¦‚å¿µæ˜¯çº¿æ€§ç›¸ä¹˜ï¼Œå·ç§¯çš„æ“ä½œå…¶å®æ˜¯çº¿æ€§çš„ã€‚æ­¤æ—¶åŠ å…¥æ¿€æ´»å‡½æ•°å¯ä»¥ç»™æ¨¡å‹åŠ å…¥éçº¿æ€§å› ç´ ï¼Œä¸”æ¿€æ´»å‡½æ•°éœ€è¦å…·å¤‡ä»¥ä¸‹å‡ ä¸ªæ€§è´¨ éçº¿æ€§ çº¿æ€§æ¿€æ´»å±‚å¯¹äºæ·±å±‚ç¥ç»ç½‘ç»œæ²¡æœ‰ä½œç”¨ï¼Œå› ä¸ºå…¶ä½œç”¨ä»¥åä»ç„¶æ˜¯è¾“å…¥çš„å„ç§çº¿æ€§å˜æ¢ è¿ç»­å¯å¾® å› ä¸ºç¥ç»ç½‘ç»œä½¿ç”¨çš„æ¢¯åº¦ä¸‹é™æ³• èŒƒå›´æœ€å¥½ä¸é¥±å’Œï¼ˆä¸æ˜¯å¾ˆç†è§£ï¼‰ å½“æœ‰é¥±å’Œçš„åŒºé—´æ®µæ—¶ï¼Œè‹¥ç³»ç»Ÿä¼˜åŒ–è¿›å…¥åˆ°è¯¥æ®µï¼Œæ¢¯åº¦è¿‘ä¼¼ä¸º0ï¼Œç½‘ç»œçš„å­¦ä¹ å°±ä¼šåœæ­¢ã€‚ å•è°ƒæ€§ å½“æ¿€æ´»å‡½æ•°æ˜¯å•è°ƒæ—¶ï¼Œå•å±‚ç¥ç»ç½‘ç»œçš„è¯¯å·®å‡½æ•°æ˜¯å‡¸çš„ï¼Œå¥½ä¼˜åŒ– åœ¨åŸç‚¹å¤„è¿‘ä¼¼çº¿æ€§ æ ·å½“æƒå€¼åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„éšæœºå€¼æ—¶ï¼Œç½‘ç»œå¯ä»¥å­¦ä¹ çš„è¾ƒå¿«ï¼Œä¸ç”¨å¯ä»¥è°ƒèŠ‚ç½‘ç»œçš„åˆå§‹å€¼ å¸¸ç”¨çš„å‡ ä¸ªæ¿€æ´»å‡½æ•°æœ‰ï¼š Sigmoid åœ¨CNNå·ç§¯éƒ¨åˆ†ä¸­åŸºæœ¬è¢«æ·˜æ±°ï¼Œå› ä¸ºé¥±å’Œæ—¶æ¢¯åº¦å€¼éå¸¸å°ï¼Œå½“å±‚æ•°æ¯”è¾ƒå¤šçš„æ—¶å€™ï¼Œç”¨BPç®—æ³•æ–¹å‘ä¼ æ’­çš„æ—¶å€™ï¼Œé è¿‘å‰é¢å±‚éƒ¨åˆ†åŸºæœ¬å¾—ä¸åˆ°æ›´æ–°ï¼Œå³æ¢¯åº¦è€—æ•£ã€‚ä¸”è¾“å‡ºçš„å€¼ä¸æ˜¯ä»¥0ä¸ºä¸­å¿ƒã€‚ Tanh å¦‚ä¸‹å›¾ï¼ŒåŸºæœ¬ç¼ºç‚¹åŒSigmoidå‡½æ•° ReLu ï¼ˆæœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼‰ Alexåœ¨2012æå‡ºAlexNetæ—¶å€™æå‡ºçš„ å½“x&gt;0çš„æ—¶å€™ï¼Œæ¢¯åº¦æ’ä¸º1ï¼Œä¸å­˜åœ¨æ¢¯åº¦è€—æ•£çš„é—®é¢˜ï¼Œæ”¶æ•›å¿«ã€‚ å½“x&lt;0çš„æ—¶å€™ï¼Œè¾“å‡ºä¸º0ï¼Œå¢åŠ ç½‘ç»œçš„ç¨€ç–æ€§ï¼Œè¿™æ ·æå–å‡ºçš„ç‰¹å¾å…·æœ‰ä»£è¡¨æ€§ï¼Œæ³›åŒ–æ€§å¼ºã€‚ï¼ˆå¦‚ä½•ä¿è¯x&lt;0çš„éƒ¨åˆ†å€¼æ˜¯æ— æ„ä¹‰çš„ï¼Ÿï¼‰ ç¼ºç‚¹å°±æ˜¯å…·æœ‰æ­»äº¡ç‰¹æ€§ï¼ˆæœ‰ç‚¹æ„Ÿè§‰ï¼Œä½†ä¸æ˜¯å¾ˆç†è§£ï¼‰ï¼Œéœ€è¦æ³¨æ„è°ƒæ•´Learning Rateæ¥é¿å…æ­»äº¡èŠ‚ç‚¹è¿‡å¤šã€‚ Leaky ReLu æ”¹æ•£äº†ReLUçš„æ­»äº¡ç‰¹æ€§ï¼ŒåŒæ—¶ä¹ŸæŸå¤±äº†ä¸€éƒ¨åˆ†ç¨€ç–æ€§ï¼Œå¢åŠ ä¸€ä¸ªè¶…å‚ã€‚ 1.5 æ± åŒ–å±‚ - Poolingå…ˆè¯´ç»“è®ºï¼šæ± åŒ–å±‚çš„å‡ ä¸ªä¸»è¦ä½œç”¨ï¼š ç¼“è§£äº†å·ç§¯å±‚å¯¹ä½ç½®çš„è¿‡åº¦æ•æ„Ÿæ€§ èƒ½å¤Ÿå‹ç¼©å›¾ç‰‡ï¼Œä½¿ç‰¹å¾å›¾å˜å°ï¼ŒåŒæ—¶ä¿ç•™äº†æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæå–ä¸»è¦ç‰¹å¾ æ± åŒ–å¤§è‡´å°±æ˜¯ä¸€ä¸ªç­›é€‰è¿‡æ»¤çš„è¿‡ç¨‹ï¼Œèƒ½è¿‡å°†å·¦è¾¹çš„è¾“å‡ºå±‚ä¸­æœ‰ç”¨çš„ä¿¡æ¯ç­›é€‰å‡ºç»™åˆ°è¾“å‡ºå±‚ã€‚è®¡ç®—æ–¹å¼å’Œkernelçš„è®¡ç®—æ–¹å¼æ¯”è¾ƒç›¸ä¼¼ï¼Œä¹Ÿæ˜¯ç»™å®šä¸€ä¸ª(w, h)å¤§å°çš„poolingçŸ©é˜µï¼Œä»å·¦å‘å³ï¼Œä»ä¸Šè‡³ä¸‹ç¼“æ…¢æ»‘åŠ¨è®¡ç®—è¾“å‡ºä¸€ä¸ªå€¼ç»™åˆ°è¾“å‡ºå±‚ã€‚ä¸kernelä¸åŒçš„æ—¶å€™ï¼Œè¿™ä¸åšçŸ©é˜µè¿ç®—ï¼Œåªæ˜¯è®¡ç®—è¾“å…¥çŸ©é˜µä¸­å½“å‰å±€éƒ¨çŸ©é˜µ(w, h)ä¸­çš„æœ€å¤§å€¼æˆ–æ˜¯å‡å€¼ï¼Œå¯¹åº”ä½¿ç”¨çš„å°±æ˜¯æœ€å¤§æ± åŒ–æˆ–æ˜¯å‡å€¼æ± åŒ–ã€‚ï¼ˆæ»‘åŠ¨çš„å¤§å°æ—¶å€™å¦‚ä½•è®¾å®šï¼Ÿï¼‰ å›ç­”ä¸€ä¸‹ä¸Šè¿°æ‹¬å·ä¸­çš„é—®é¢˜ï¼Œæ± åŒ–å±‚åŒæ ·ä¹Ÿæœ‰paddingå’Œstrideçš„æ¦‚å¿µï¼Œæ–¹å¼å’Œå·ç§¯ä¸­æ¯”è¾ƒç›¸ä¼¼ï¼Œå°±ä¸ä¸€ä¸€è§£é‡Šäº†ã€‚ 1.6 å…¶ä»–ä¸€äº›æ¦‚å¿µ æ„Ÿå—é‡ - Receptive Field ä¸­æ–‡åçœŸçš„æ˜¯â€¦ â€¦ å¤ªå¤ªå¤ªéš¾ç†è§£äº†ï¼Œç›´æ¥çœ‹è‹±æ–‡åReceptive Fieldæ¯”è¾ƒç›´è§‚ä¸€äº›ã€‚ åŸºæœ¬å®šä¹‰å°±æ˜¯ å·ç§¯ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚è¾“å‡ºçš„ç‰¹å¾å›¾ï¼ˆFeature apï¼‰ä¸Šçš„åƒç´ ç‚¹åœ¨åŸå›¾åƒä¸Šæ˜ å°„çš„åŒºåŸŸå¤§å°ã€‚ ç¬¬ä¸€å±‚å·ç§¯ è¾“å…¥ä¸º10x10çš„å›¾ç‰‡ï¼Œç»è¿‡3x3çš„kernelï¼Œè¾“å‡ºä¸º8x8çš„å¤§å°ï¼ˆè¿™é‡Œåªè€ƒè™‘å•å±‚ï¼‰ ä¸”åœ¨output1ä¸­çš„æ¯ä¸€ä¸ªåƒç´ ç‚¹ï¼Œéƒ½å—åˆ°åŸå§‹å›¾åƒå¯¹åº”çš„3x3åŒºåŸŸå†…çš„å½±å“ï¼Œç¬¬ä¸€åœºçš„æ„Ÿå—é‡ä¸º3ï¼Œç”¨å­—æ¯è¡¨ç¤ºRF1=3 ç¬¬äºŒå±‚å·ç§¯ åœ¨ç¬¬ä¸€å±‚å·ç§¯çš„è¾“å‡ºoutput1ä¸‹ï¼Œç»è¿‡ç¬¬äºŒå±‚3x3çš„kernelå·ç§¯ï¼Œè¾“å‡ºçš„å¤§å°ä¸º6x6 å¦‚æœä»output2å¾€å›æ¨çš„è¯ï¼Œoutput2ä¸Šçš„ä¸€ä¸ªåƒç´ ç‚¹ï¼Œå—åˆ°output1ä¸Šä¸‰ä¸ªåƒç´ ç‚¹çš„å½±å“ï¼Œè€Œè¿™ä¸‰ä¸ªåƒç´ ç‚¹åˆæ€»å…±å—åˆ°è¾“å…¥å±‚äº”ä¸ªåƒç´ ç‚¹çš„å½±å“ï¼Œæ‰€ä»¥ç¬¬äºŒå±‚çš„æ„Ÿå—é‡RF2=5 ç¬¬ä¸‰å±‚å·ç§¯ æ­¤æ—¶kernel3ä¾æ—§ä¸º3x3ï¼Œæ ¹æ®ä¸Šé¢æ¨ï¼ŒRF3=7 ç¬¬å››æ¬¡å·ç§¯ ç¬¬äº”æ¬¡æ± åŒ–è¿ç®— é€šé“ - Channel å¤šè¾“å…¥é€šé“ ä¸Šé¢æˆ‘ä»¬è®¨è®ºçš„éƒ½æ˜¯äºŒç»´çš„æ•°æ®çŸ©é˜µï¼Œå…¶å®åœ¨å›¾åƒå¤„ç†è¿‡ç¨‹ä¸­ï¼Œå›¾ç‰‡é€šå¸¸éƒ½æ˜¯ä¸‰ç»´çš„ã€‚é™¤äº†å›¾ç‰‡çš„é«˜åº¦å’Œå®½åº¦å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä»é¢œè‰²çš„è§’åº¦ï¼Œåˆ†ä¸ºRGBä¸‰ä¸ªé¢œè‰²é€šé“ã€‚å³ä¸€ä¸ªä¸‰ç»´çš„çŸ©é˜µï¼Œè¿™é‡Œçš„ç¬¬ä¸‰ç»´ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºChannelï¼Œå¯ä»¥è¯´çš„ç›¸äº’ç‹¬ç«‹çš„(è®¡ç®—ç”¨åˆ°çš„kernelå¯ä»¥æ˜¯ç‹¬ç«‹çš„)ã€‚ ä¾æ—§æ˜¯ä»¥ä¸Šé¢çš„ä¾‹å­ä¸ºä¾‹ï¼Œä»Channelä¸€ç»´å‡çº§åˆ°äºŒç»´ï¼Œè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š channel_1:1 \\times 1 + 2 \\times 2 + 3 \\times 4 + 5 \\times 4 = 37 \\\\ channel_2:0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 4 = 19 \\\\ output = 37 + 19 = 56 å¤šè¾“å‡ºé€šé“ ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œè¾“å‡ºé€šé“ä¸º1ï¼Œå³æœ€ç»ˆå¯¹æ¯ä¸ªç‹¬ç«‹channelè¿ç®—çš„é€šé“è¾“å‡ºçš„ç»“æœåšç´¯åŠ è¿ç®—ã€‚ å¦‚æœæ˜¯éœ€è¦è¾“å‡ºå¤šé€šé“å‘¢ï¼Ÿç›¸ä¼¼çš„ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—ç‹¬ç«‹channelçš„æ—¶å€™ï¼Œå°±è¾“å‡ºä¸€ä¸ªå¤šé€šé“çš„çš„è¾“å‡ºï¼Œç„¶ååœ¨ä¸åŒchannelä¹‹é—´ï¼Œç»§ç»­åšç´¯åŠ è¿ç®—ã€‚ $1 \\times 1$ å·ç§¯å±‚ è¿™ä¸ªå·ç§¯å±‚å¾ˆæœ‰æ„æ€ï¼Œæ²ç¥åœ¨ä¸Šè¯¾çš„è¿‡ç¨‹ä¸­å¤šæ¬¡æåˆ°äº†è¯¥å·ç§¯ã€‚ å½“kernelçš„sizeä¸º(1, 1)çš„æ—¶å€™ï¼Œè¯´æ˜æ­¤æ—¶kernelä¸å¯¹è¾“å…¥å±‚åšé‡‡æ ·ç„¶åè®¡ç®—ï¼Œå³å¤±å»äº†å·ç§¯å±‚å¯ä»¥è¯†åˆ«é«˜å’Œå®½ç»´åº¦ä¸Šç›¸é‚»å…ƒç´ æ„æˆçš„æ¨¡å¼çš„åŠŸèƒ½ã€‚ å¯ä»¥è¯´æ­¤æ—¶ä¸€å¼€å§‹çš„äºŒç»´å±‚é¢ä¸Šï¼Œkernelæ˜¯æ²¡å•¥ç”¨çš„ï¼Œä½†æ˜¯åœ¨ç¬¬ä¸‰ç»´channelçš„è§’åº¦è¿˜æ˜¯æœ‰ç”¨çš„ï¼Œå³æˆ‘ä»¬å¯ä»¥ç”¨1x1çš„çŸ©é˜µåšç»´åº¦å˜æ¢ã€‚ ä¸Šå›¾ä¸­ï¼Œè¾“å…¥é€šé“çš„ç»´åº¦ä¸º(3, 3, 3)ï¼Œ kernelçš„ç»´åº¦ä¸º(2, 1, 1), æ­¤æ—¶çš„è¾“å‡ºé€šé“ä¸º(2, 3, 3)ã€‚ å…·ä½“è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ 1234567def corr2d_multi_in_out_1_times_1(Xï¼Œ K)ï¼šc_i, h, w = X.shape # 3, 3, 3 c_o = K.shape[0] # 2X = X.reshape((c_o, h * w)) # (2, 3 * 3)K = K.reshape((c_o, c_i)) # (2, 3)Y = nd.dot(K, X) # å…¨è¿æ¥å±‚çš„çŸ©é˜µä¹˜æ³•return Y.reshape((c_o, h, w)) # (2, 3, 3) 2. PyTorchä»£ç å®ç°å¯¹ä¸èµ·æ²ç¥ï¼Œæˆ‘è¿˜æ˜¯ç”¨çš„PyTorchï¼Œè€Œä¸æ˜¯MxNetã€‚ ğŸ¤¦â€â™‚ï¸ ğŸ¤¦â€â™‚ï¸ ğŸ¤¦â€â™‚ï¸ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class CNN(nn.Module): def __init__(self, in_dim, n_class): super(CNN, self).__init__() # è¡¨ç¤ºå°†ä¸€ä¸ªæœ‰åºçš„æ¨¡å—å†™åœ¨ä¸€èµ· # ç›¸å½“äºç¥ç»ç½‘ç»œçš„å±‚æŒ‰é¡ºåºæ”¾ä¸€èµ·æ–¹ä¾¿ç»“æ„æ˜¾ç¤º self.conv = nn.Sequential( # å·ç§¯å±‚ï¼Œæœ‰äº”ä¸ªå‚æ•°ï¼š # in_channels: è¡¨ç¤ºçš„æ˜¯è¾“å…¥å·ç§¯å±‚çš„å›¾ç‰‡åšåº¦ # out_channels: æ¯”å¶å¥¥æ•°çš„æ˜¯è¦è¾“å‡ºçš„åšåº¦ # kernel_size: è¡¨ç¤ºçš„æ˜¯å·ç§¯æ ¸çš„å¤§å°ï¼Œä¸€ä¸ªæ•°å­—çš„è¯è¡¨ç¤ºé•¿å®½ç›¸ç­‰çš„å·ç§¯æ ¸ # stride: è¡¨ç¤ºå·ç§¯æ ¸æ»‘åŠ¨çš„æ­¥é•¿ # padding: è¡¨ç¤ºåœ¨å›¾ç‰‡èµ°ä½å¡«å……0çš„å¤šå°‘ï¼Œpadding=0è¡¨ç¤ºä¸å¡«å……ï¼Œpadding=1å››å‘¨éƒ½å¡«å……1ç»´ nn.Conv2d(in_dim, 6, 3, stride=1, padding=1), # æ¿€æ´»å‡½æ•°ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªå‚æ•°inplace # Falseï¼Œè¡¨ç¤ºæ–°åˆ›å»ºä¸€ä¸ªå¯¹è±¡å¯¹å…¶ä¿®æ”¹ # Trueï¼Œè¡¨ç¤ºç›´æ¥å¯¹è¿™ä¸ªå¯¹è±¡è¿›è¡Œä¿®æ”¹ nn.ReLU(True), # æœ€å¤§æ± åŒ–å±‚ï¼Œä¹Ÿæœ‰å¹³å‡æ± åŒ–å±‚ç­‰ï¼Œé‡Œé¢çš„å‚æ•°æœ‰: # kernel_size: è¡¨ç¤ºæ± åŒ–çš„çª—å£çš„å¤§å°ï¼Œå’Œå·ç§¯é‡Œé¢çš„kernel_sizeæ˜¯ä¸€æ ·çš„ # stride: ä¹Ÿå’Œå·ç§¯å±‚é‡Œé¢ä¸€æ ·ï¼Œéœ€è¦è‡ªå·±è®¾ç½®æ»‘åŠ¨æ­¥é•¿ # padding å’Œå·ç§¯å±‚ä¸€æ ·ï¼Œé»˜è®¤æ˜¯0 nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5, stride=1, padding=0), nn.ReLU(True), nn.MaxPool2d(2, 2) ) # å…¨è¿æ¥å±‚ self.fc = nn.Sequential( nn.Linear(400, 120), nn.Linear(120, 84), nn.Linear(84, n_class) ) def forward(self, x): out = self.conv(x) # batch_size out = out.view(out.size(0), -1) out = self.fc(out) return out # å®šä¹‰losså’Œoptimizercriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 # å¼€å§‹è®­ç»ƒ for epoch in range(num_epoches): print('epoch &#123;&#125;'.format(epoch + 1)) print('*' * 10) running_loss = 0.0 running_acc = 0.0 for i, data in enumerate(train_loader, 1): img, label = data if use_gpu: img = img.cuda() label = label.cuda() img = Variable(img) label = Variable(label) # å‘å‰ä¼ æ’­ out = model(img) loss = criterion(out, label) running_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() accuracy = (pred == label).float().mean() running_acc += num_correct.data[0] # å‘åä¼ æ’­ optimizer.zero_grad() loss.backward() optimizer.step() # ========================= Log ====================== step = epoch * len(train_loader) + i # (1) Log the scalar values info = &#123;'loss' : loss.data[0], 'accuracy' : accuracy.data[0]&#125; for tag, value in info.items(): logger.scalar_summary(tag, value, step) # (2) Log values and gradients of the parameters (histogram) for tag, value in model.named_parameters(): tag = tag.replace('.', '/') logger.histo_summary(tag, to_np(value), step) logger.histo_summary(tag + '/grad', to_np(value.grad), step) # (3) Log the images info = &#123;'images': to_np(img.view(-1, 28, 28)[:10])&#125; for tag, images in info.items(): logger.image_summary(tag, images, step) if i % 300 == 0: print('[&#123;&#125;/&#123;&#125;] Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, num_epoches, running_loss / (batch_size * i), running_acc / (batch_size * i))) print('Finish &#123;&#125; epoch, Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, running_loss / (len(train_dataset)), running_acc / (len( train_dataset)))) model.eval() eval_loss = 0 eval_acc = 0 for data in test_loader: img, label = data if use_gpu: img = Variable(img, volatile=True).cuda() label = Variable(label, volatile=True).cuda() else: img = Variable(img, volatile=True) label = Variable(label, volatile=True) out = model(img) loss = criterion(out, label) eval_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() eval_acc += num_correct.data[0] print('Test Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format(eval_loss / (len( test_dataset)), eval_acc / (len(test_dataset)))) print()# ä¿å­˜æ¨¡å‹torch.save(model.state_dict(), './model/cnn.pth') 3. References è«çƒ¦ - ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œ CNN (Convolutional Neural Network) ä»€ä¹ˆæ˜¯æ„Ÿå—é‡ é€šä¿—ç†è§£ã€å·ç§¯ã€â€”â€”ä»å‚…é‡Œå¶å˜æ¢åˆ°æ»¤æ³¢å™¨ ä¸€æ–‡è¯»æ‡‚å·ç§¯ç¥ç»ç½‘ç»œCNN ä¸€å¥è¯CNNï¼šå¦‚ä½•ç†è§£paddingçš„ä½œç”¨å’Œç®—æ³• CNNå…¥é—¨è®²è§£ï¼šä»€ä¹ˆæ˜¯æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.cc/tags/CNN/"}]},{"title":"Macç³»ç»Ÿä¸‹matplotlibæ˜¾ç¤ºä¸­æ–‡","date":"2018-04-10T07:08:20.000Z","path":"2018/04/10/Macç³»ç»Ÿä¸‹matplotlibæ˜¾ç¤ºä¸­æ–‡/","text":"1. Macç³»ç»Ÿä¸‹è®©matplotlibæ˜¾ç¤ºä¸­æ–‡ç°åœ¨çš„æ—¶é—´æ˜¯2018.04.10ï¼Œæ›´æ–°ä¸€ä¸‹Macç³»ç»Ÿä¸‹çš„è§£å†³æ–¹æ³• æˆ‘çš„ç¯å¢ƒï¼šanaconda3 + Python 3.6.3 æ·»åŠ å­—ä½“ æ·»åŠ  SimHei å­—ä½“ï¼ˆsimhei.ttfæ–‡ä»¶ï¼‰åˆ° ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf ä¸­ï¼› ä¸‹è½½åœ°å€ï¼šé»‘ä½“å­—ä½“simhei.ttf æˆ‘ç”¨çš„æ˜¯ anaconda3 ä¸‹çš„ python ç¯å¢ƒï¼Œè¿™ä¸ªåœ°å€å¯¹åº”ä½ æ­£åœ¨ä½¿ç”¨çš„ python å®‰è£…åœ°å€ â€‹ ä¿®æ”¹matplotlibé…ç½®æ–‡ä»¶ 12cd ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-datavi matplotlibrc # ç¼–è¾‘é…ç½®æ–‡ä»¶ æ‰¾åˆ° font.sans-serif æ·»åŠ  SimHei åˆ°å­—ä½“åˆ—è¡¨ ï¼ˆå¦‚å›¾ï¼Œå¤§çº¦åœ¨211è¡Œï¼‰ åŒæ—¶ä¿®æ”¹ axes.unicode_minusï¼Œå°† True æ”¹ä¸º Falseï¼Œä½œç”¨å°±æ˜¯è§£å†³è´Ÿå·â€™-â€˜æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜ï¼ˆå¤§çº¦åœ¨330è¡Œï¼‰ åˆ é™¤ç¼“å­˜æ–‡ä»¶ Mac ç³»ç»Ÿä¸‹åˆ é™¤ ~/.matplotlib/ ä¸‹çš„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ 12rm -rf ~/.matplotlib/*.cacherm -rf ~/.matplotlib/fontList.json Linuxã€CentOS åˆ é™¤ ~/.cache/matplotlibç›®å½•ä¸‹çš„ä¸¤ä¸ªç¼“å­˜æ–‡ä»¶ï¼ˆåŒä¸Šï¼‰ æ³¨æ„ rm -rf å‘½ä»¤ï¼Œç¡®è®¤è·¯å¾„æ²¡é”™åœ¨ç”¨ â€‹ ç”»å›¾æµ‹è¯• æœªä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œéœ€è¦æ·»åŠ å¦‚ä¸‹ä»£ç ï¼š 1234567891011121314#coding:utf-8 import matplotlib #æŒ‡å®šé»˜è®¤å­—ä½“ matplotlib.rcParams['font.sans-serif'] = ['SimHei'] matplotlib.rcParams['font.family']='sans-serif' #è§£å†³è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜ matplotlib.rcParams['axes.unicode_minus'] = False from matplotlib.font_manager import _rebuild_rebuild()plt.plot([-1,2,-5,3]) plt.title(u'ä¸­æ–‡',fontproperties=myfont) plt.show() â€‹ å¦‚æœå·²ç»ä¿®æ”¹äº† matplotlib é…ç½®æ–‡ä»¶ï¼Œåˆ™ä¸éœ€è¦ä¸Šè¿°ä»£ç ï¼Œç›´æ¥ç”»å›¾å³å¯ã€‚ â€‹ 2. References å½»åº•è§£å†³matplotlibä¸­æ–‡ä¹±ç é—®é¢˜ matplotlibå›¾ä¾‹ä¸­æ–‡ä¹±ç ?","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"http://chenson.cc/tags/Jupyter/"}]},{"title":"PyTorchå­¦ä¹ ç¬”è®°","date":"2018-03-22T07:52:02.000Z","path":"2018/03/22/PyTorchå­¦ä¹ ç¬”è®°/","text":"è¶ç€å…¬å¸æœ€è¿‘ä¸æ˜¯å¾ˆå¿™ï¼Œè®¤çœŸå­¦ä¹ ä¸€ä¸‹è¿™ä¸¤å¹´æ¯”è¾ƒç«çš„PyTorchï¼Œè™½ç„¶æ²¡æœ‰å®æ‰“å®çš„ã€è®¤è®¤çœŸçœŸçš„å­¦è¿‡æ·±åº¦å­¦ä¹ ï¼Œä½†ä¹Ÿç¨å¾®æ‡‚ä¸€ç‚¹ã€‚æ‰“ç®—å…ˆç®€å•å­¦ä¹ ä¸€ä¸‹åŸºæœ¬æ“ä½œï¼Œæ­ä¸ªç®€å•çš„DNNã€CNNä¹‹ç±»çš„ã€‚çœ‹çœ‹åé¢æœ‰æ²¡æœ‰æ—¶é—´ï¼Œç„¶åå†ä¸Šä¸ªå´æ©è¾¾æˆ–è€…æé£é£çš„è¯¾ç¨‹ï¼Œè¡¥å……ä¸€ä¸‹ç†è®ºçŸ¥è¯†ã€‚æ‰€ä»¥è¿™ç¯‡æ–‡ç« å°±è®°å½•ä¸€äº›ç®€å•çš„æ¦‚å¿µå’Œä¸Šæ‰‹è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„é—®é¢˜ã€‚ 1. æŸå¤±å‡½æ•°1.1 äºŒåˆ†ç±»é—®é¢˜æŸå¤±å‡½æ•°ç”¨ criterion = nn.BCELoss() æ­¤æ—¶ï¼Œyçš„ç±»å‹åº”è¯¥ä¸ºfloatç±»å‹ï¼Œ ä¸”y_predç”¨çš„æ—¶å€™éœ€è¦åœ¨è¯¥å±‚å‰é¢åŠ ä¸Š Sigmoid å‡½æ•° 123456import torch.nn.functional as Floss_fn = torch.nn.BCELoss(reduce=False, size_average=False)y_true = Variable(torch.randn(3, 4)) # é™åˆ¶äº†targetç±»å‹ä¸ºfloaty_pred = Variable(torch.FloatTensor(3, 4).random_(2))loss = loss_fn(F.sigmoid(y_pred), y_true) 1.2 å¤šåˆ†ç±»çš„é—®é¢˜ (&gt;= 2ï¼ŒäºŒåˆ†ç±»ä¸ºç‰¹æ®Šçš„å¤šåˆ†ç±»)æŸå¤±å‡½æ•°ç”¨ nn.CrossEntropyLoss, å¤šåˆ†ç±»ç”¨çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨è¿™ä¸ª loss å‰é¢ä¸éœ€è¦åŠ  Softmax å±‚ 123456789# ç±»åˆ«ä¹‹é—´çš„æ ·æœ¬å‡è¡¡é—®é¢˜weight = torch.Tensor([1, 2, 1, 1, 10])loss_fn = torch.nn.CrossEntropyLoss(reduce=False, size_average=False, weight=weight) # è€ƒè™‘åˆ°æ ·æœ¬ä¸å‡è¡¡çš„é—®é¢˜y_pred = Variable(torch.randn(3, 5)) y_true = Variable(torch.FloatTensor(3).random_(5)) # é™åˆ¶äº†targetç±»å‹ä¸ºlongloss = loss_fn(y_pred, y_true) 1.3 å…¶ä»–å¸¸è§çš„æŸå¤±å‡½æ•°2. ä¼˜åŒ–å™¨ä¼˜åŒ–å™¨çš„é€‰æ‹© 3. é‡åˆ°çš„é—®é¢˜ Learnning Rate ä¹‹å‰å› ä¸ºæ¥è§¦æ·±åº¦å­¦ä¹ æ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥å¯¹æ·±åº¦å­¦ä¹ çš„è°ƒå‚ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Œæ‰€ä»¥é‡åˆ°äº†è¿™ä¸ªå‘ã€‚ä¹‹å‰åœ¨æ­ä¸€ä¸ªæ¯”è¾ƒç®€å•çš„DNNçš„æ—¶å€™ï¼Œå‘ç°å‡½æ•°ä¸€ç›´ä¸ä¼šæ”¶æ•›ï¼Œæ‰¾äº†å¥½ä¹…å‘ç°æ˜¯Learning rateè®¾ç½®å¤ªå¤§äº†ï¼Œæ”¹å°äº†å°±æ¯”è¾ƒå¥½äº†ï¼Œä»0.05æ”¹åˆ°0.001æ•ˆæœå°±æ¯”è¾ƒå¥½ã€‚ ç„¶åæ›´æ”¹ç½‘ç»œç»“æ„çš„æ—¶å€™ï¼Œä¹Ÿæ˜¯é‡åˆ°ç±»ä¼¼çš„é—®é¢˜ï¼Œè¾“å‡ºç»å¸¸éƒ½æ˜¯ä¸ºåŒä¸€ä¸ªå€¼ï¼Œå¯ä»¥å°è¯•ä¸€ä¸‹æŠŠå­¦ä¹ ç‡å¢å¤§æˆ–è€…è°ƒå°è¯•è¯•çœ‹ï¼Œå¤šè¯•å‡ ä¸ªå€¼ã€‚ ä¼˜åŒ–å‡½æ•° æŸå¤±å‡½æ•° 4. å¯è§†åŒ–123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import torch.nn.functional as f# å»ºé€ æ•°æ®é›†data = torch.ones((100, 2))x0 = torch.normal(2 * data, 1)y0 = torch.zeros(100) # y0æ˜¯æ ‡ç­¾ shape(100,),æ˜¯ä¸€ç»´x1 = torch.normal(-2 * data, 1)y1 = torch.ones(100) # y1ä¹Ÿæ˜¯æ ‡ç­¾ shape(100,)ï¼Œæ˜¯ä¸€ç»´x = torch.cat((x0, x1), 0).type(torch.FloatTensor)y = torch.cat((y0, y1), 0).type(torch.LongTensor)x, y = Variable(x), Variable(y) # è®­ç»ƒç¥ç»ç½‘ç»œåªèƒ½æ¥å—å˜é‡è¾“å…¥ï¼Œæ•…è¦æŠŠx, yè½¬åŒ–ä¸ºå˜é‡plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], # è¿™ä¸¤ä¸ªå‚æ•°åˆ†åˆ«ä»£è¡¨x,yè½´åæ ‡ c=y.data.numpy(), s=100, cmap='RdYlGn') # cä¸ºcolor,yæœ‰ä¸¤ç§æ ‡ç­¾ï¼Œä»£è¡¨ä¸¤ç§é¢œè‰²çš„ç‚¹ï¼Œ'RdYlGn'çº¢è‰²å’Œç»¿è‰²plt.show()# å»ºé€ ç¥ç»ç½‘ç»œæ¨¡å‹class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) self.out = torch.nn.Linear(n_hidden, n_output) def forward(self, x): x = f.relu(self.hidden(x)) y = self.out(x) return y# è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹å¹¶å°†è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–net = Net(n_feature=2, n_hidden=10, n_output=2)optimizer = torch.optim.SGD(net.parameters(), lr=0.02)loss_func = torch.nn.CrossEntropyLoss()plt.ion()for i in range(100): out = net(x) loss = loss_func(out, y) optimizer.zero_grad() loss.backward() optimizer.step() # ç»˜å›¾ if i % 10 == 0: plt.cla() # torch.max(a,1) è¿”å›æ¯ä¸€è¡Œä¸­æœ€å¤§å€¼çš„é‚£ä¸ªå…ƒç´ ï¼Œä¸”è¿”å›å…¶ç´¢å¼•ï¼ˆè¿”å›æœ€å¤§å…ƒç´ åœ¨è¿™ä¸€è¡Œçš„åˆ—ç´¢å¼• # f.softmax(outï¼‰æ˜¯å°†outçš„å†…å®¹ä»¥æ¦‚ç‡è¡¨ç¤ºã€‚ # torch.max()è¿”å›çš„æ˜¯ä¸¤ä¸ªVariableï¼Œç¬¬ä¸€ä¸ªVariableå­˜çš„æ˜¯æœ€å¤§å€¼ï¼Œç¬¬äºŒä¸ªå­˜çš„æ˜¯å…¶å¯¹åº”çš„ä½ç½®ç´¢å¼•indexã€‚è¿™é‡Œæˆ‘ä»¬æƒ³è¦å¾—åˆ°çš„æ˜¯ç´¢å¼•ï¼Œæ‰€ä»¥åé¢ç”¨[1]ã€‚ prediction = torch.max(f.softmax(out), 1)[1] pred_y = prediction.data.numpy().squeeze() target_y = y.data.numpy() plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[ :, 1], c=pred_y, s=100, cmap='RdYlGn') accuracy = sum(pred_y == target_y) / len(y_train) print('&gt;&gt;&gt; accuracy :', accuracy) plt.text(1.5, -4, 'accuracy=%.2f' % accuracy, fontdict=&#123;'size': 10, 'color': 'red'&#125;) plt.pause(0.1)plt.ioff()plt.show() 5. Utils in PyTorch12345678910111213141516171819202122232425262728293031323334def _reshape(df_meta, features, label='14d', shape=(6, 4, 32)): \"\"\"å°†ä¸€ç»´çš„æ•°æ®è½¬æ¢æˆå¤šç»´çš„\"\"\" df = df_meta.reset_index(drop=True) matrix, label = [], [] for i in range(df.shape[0]): x = df.iloc[i, :][features] y = df.iloc[i, :][features] m = np.array(x).reshape(shape) matrix += [m] label += [y] return np.array(matrix), np.array(label)def data2tensor(X, y=None, x_dtype='float32', y_dtype='float32'): \"\"\"å°†è®­ç»ƒç”¨çš„Xå’Œyåˆå¹¶æˆä¸€ä¸ªTonsorDataset\"\"\" X2 = torch.from_numpy(np.array(X).astype(dtype=x_dtype)) if y is None: return TensorDataset(X2) else: y2 = torch.from_numpy(np.array(y).astype(dtype=y_dtype)) return TensorDataset(X2, y2)def df2var(df, dtype='float32'): \"\"\"æŠŠdfè½¬æˆPyTorchä¸­çš„variable\"\"\" from torch.autograd import Variable return Variable(torch.from_numpy(np.array(df).astype(dtype=dtype)))def adjust_learning_rate(optimizer, epoch): \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\" lr = args.lr * (0.1 ** (epoch // 30)) for param_group in optimizer.param_groups: param_group['lr'] = lr 6. References Pytorch Loss Function æ€»ç»“ PyTorchä¸­çš„Loss Fucntion","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://chenson.cc/tags/PyTorch/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"}]},{"title":"Dockerå¸¸ç”¨æŒ‡ä»¤","date":"2018-03-13T05:12:33.000Z","path":"2018/03/13/Dockerå¸¸ç”¨æŒ‡ä»¤/","text":"1. dockeré•œåƒæ„å»º12cd analyze-serverdocker build -t analyze-server . 2. åˆ›å»º12345docker create analyze-server # åˆ›å»ºå®¹å™¨ï¼Œä½†å¤„äºåœæ­¢çŠ¶æ€docker run analyze-server # åˆ›å»ºå¹¶å¯åŠ¨å®¹å™¨# å¯åŠ¨å®¹å™¨ï¼Œå¹¶æŒ‚è½½æ–‡ä»¶docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server 3. æŸ¥çœ‹12docker ps # æŸ¥çœ‹å½“å‰è¿è¡Œçš„å®¹å™¨docker ps -a # æŸ¥çœ‹æ‰€æœ‰çš„å®¹å™¨ï¼ŒåŒ…æ‹¬åœæ­¢çš„ 4. å¯åŠ¨1docker start [NAME]/[CONTAINER ID] # é€šè¿‡docker startæ¥å¯åŠ¨ä¹‹å‰å·²ç»åœæ­¢çš„docker_runé•œåƒ 12docker exec -it analyze /bin/bash # åœ¨è¿è¡Œçš„å®¹å™¨ä¸­æ‰§è¡Œåé¢çš„æŒ‡ä»¤docker exec -it -u root spark-notebook /bin/bash # ä»¥rootç”¨æˆ·è¿è¡ŒæŒ‡ä»¤ äº¤äº’å‹å®¹å™¨ï¼šè¿è¡Œåœ¨å‰å°ï¼Œå®¹å™¨ä¸­ä½¿ç”¨exitå‘½ä»¤æˆ–è€…è°ƒç”¨docker stopã€docker killå‘½ä»¤ï¼Œå®¹å™¨åœæ­¢ã€‚ docker -it --name analyze -i -t -i æ‰“å¼€å®¹å™¨çš„æ ‡å‡†è¾“å…¥ -t å‘Šè¯‰dockerä¸ºå®¹å™¨åˆ›å»ºä¸€ä¸ªå‘½ä»¤è¡Œç»ˆç«¯ --name analyze æŒ‡å®šå®¹å™¨åç§°ï¼Œå¯ä»¥ä¸å¡«(éšæœº) analyze-server å‘Šè¯‰æˆ‘ä»¬ä½¿ç”¨ä»€ä¹ˆé•œåƒæ¥å¯åŠ¨å®¹å™¨ /bin/bash å‘Šè¯‰dockerè¦åœ¨å®¹å™¨é‡Œé¢æ‰§è¡Œæ­¤å‘½ä»¤ åå°å‹å®¹å™¨ï¼šè¿è¡Œåœ¨åå°ï¼Œåˆ›å»ºåä¸ç»ˆç«¯æ— å…³ï¼Œåªæœ‰è°ƒç”¨docker stopã€docker killå‘½ä»¤æ‰èƒ½ä½¿å®¹å™¨åœæ­¢ã€‚ 1docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server -d ä½¿å®¹å™¨åœ¨åå°è¿è¡Œã€‚ -c å¯ä»¥è°ƒæ•´å®¹å™¨çš„CPUä¼˜å…ˆçº§ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„å®¹å™¨æ‹¥æœ‰ç›¸åŒçš„CPUä¼˜å…ˆçº§å’ŒCPUè°ƒåº¦å‘¨æœŸï¼Œä½†ä½ å¯ä»¥é€šè¿‡Dockeræ¥é€šçŸ¥å†…æ ¸ç»™äºˆæŸä¸ªæˆ–æŸå‡ ä¸ªå®¹å™¨æ›´å¤šçš„CPUè®¡ç®—å‘¨æœŸã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬ä½¿ç”¨-cæˆ–è€…â€“cpu-shares =0å¯åŠ¨äº†C0ã€C1ã€C2ä¸‰ä¸ªå®¹å™¨ï¼Œä½¿ç”¨-c/â€“cpu-shares=512å¯åŠ¨äº†C3å®¹å™¨ã€‚è¿™æ—¶ï¼ŒC0ã€C1ã€C2å¯ä»¥100%çš„ä½¿ç”¨CPUèµ„æºï¼ˆ1024ï¼‰ï¼Œä½†C3åªèƒ½ä½¿ç”¨50%çš„CPUèµ„æºï¼ˆ512ï¼‰ã€‚å¦‚æœè¿™ä¸ªä¸»æœºçš„æ“ä½œç³»ç»Ÿæ˜¯æ—¶åºè°ƒåº¦ç±»å‹çš„ï¼Œæ¯ä¸ªCPUæ—¶é—´ç‰‡æ˜¯100å¾®ç§’ï¼Œé‚£ä¹ˆC0ã€C1ã€C2å°†å®Œå…¨ä½¿ç”¨æ‰è¿™100å¾®ç§’ï¼Œè€ŒC3åªèƒ½ä½¿ç”¨50å¾®ç§’ã€‚ -c åçš„å‘½ä»¤æ˜¯å¾ªç¯ï¼Œä»è€Œä¿æŒå®¹å™¨çš„è¿è¡Œ 5. åœæ­¢12docker stop [NAME]/[CONTAINER ID] # é€€å‡ºå®¹å™¨docker kill [NAME]/[CONTAINER ID] # å¼ºåˆ¶åœæ­¢ä¸€ä¸ªå®¹å™¨ 6. åˆ é™¤12345docker rm [NAME]/[CONTAINER ID] # ä¸èƒ½å¤Ÿåˆ é™¤ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„å®¹å™¨ï¼Œä¼šæŠ¥é”™ã€‚éœ€è¦å…ˆåœæ­¢å®¹å™¨docker rm 'docker ps -a -q' # ä¸€æ¬¡æ€§åˆ é™¤æ‰€æœ‰å®¹å™¨ # -aæ ‡å¿—åˆ—å‡ºæ‰€æœ‰å®¹å™¨ï¼Œ # -qæ ‡å¿—åªåˆ—å‡ºå®¹å™¨çš„IDï¼Œç„¶åä¼ é€’ç»™rmå‘½ä»¤ï¼Œä¾æ¬¡åˆ é™¤å®¹å™¨ 7. References Docker ç®€æ˜“æ•™ç¨‹ Docker å‘½ä»¤å¤§å…¨","tags":[{"name":"docker","slug":"docker","permalink":"http://chenson.cc/tags/docker/"}]},{"title":"Hadoop2.7.4å®Œå…¨åˆ†å¸ƒå¼é›†ç¾¤æ­å»ºå’Œæµ‹è¯•","date":"2017-10-10T05:24:58.000Z","path":"2017/10/10/Hadoop2-7-4å®Œå…¨åˆ†å¸ƒå¼é›†ç¾¤æ­å»ºå’Œæµ‹è¯•/","text":"1. ç¯å¢ƒé…ç½®1.1 ç¯å¢ƒè¯´æ˜ 1.2 ä¿®æ”¹æœºå™¨åç§°å’Œhostsç­‰ vi /etc/sysconfig/network 1234HOSTNAME=hadoop-masterHOSTNAME=hadoop-salve1HOSTNAME=hadoop-salve2HOSTNAME=hadoop-salve3 æ‰§è¡Œ reboot åç”Ÿæ•ˆ sudo vi /etc/hostname 1234# ç›¸åº”ä¿®æ”¹ä¸‰å°æœºå™¨hadoop-masterhadoop-slave1hadoop-slave2 sudo vi /etc/hosts 1234567127.0.0.1 localhost localhost.localdomain VM-0-6-ubuntu# Hadoop Cluster# ã€æ³¨æ„ã€‘ï¼šç”¨å†…ç½‘IPï¼Œè‹¥ç”¨å…¬ç½‘IPï¼Œåˆ™æ— æ³•å¯åŠ¨masterä¸Šçš„9000ç›‘å¬ç«¯å£172.17.6 hadoop-master172.17.11 hadoop-salve1 172.17.7 hadoop-salve2 1.5. SSHæ— å¯†ç éªŒè¯é…ç½® å®‰è£… ssh 123sudo apt-get install openssh-serverps -e | grep &quot;ssh&quot;ssh localhost ç”Ÿæˆå¯†é’¥ pair 12345678910# æŸ¥çœ‹æƒé™ls -aldrwxr-x--x 2 root root 4096 Dec 23 2015 .ssh# ç»™ç”¨æˆ·æƒé™# sudo chown ubuntu .sshchmod 700 .ssh # ç”Ÿæˆå¯†é’¥ssh-keygen -t rsa åœ¨ master ä¸Šå¯¼å…¥ authorized_keys 123456789# é‡è¦sudo chmod 700 .ssh sudo chmod 640 .ssh/authorized_keyssudo chown $USER .sshsudo chown $USER .ssh/authorized_keyscat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# åŒæ—¶ç»™å…¶ä»–çš„ salver åœ¨å„ä¸ªæœºå™¨éªŒè¯ä¸€ä¸‹ 2. æ‰€éœ€è½¯ä»¶2.1 JDKè½¯ä»¶tutorial 1 å®‰è£… JDK 123456sudo add-apt-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installersudo apt-get install openjdk-8-jdksudo apt-get install openjdk-8-jre é…ç½®ç¯å¢ƒå˜é‡ vi /etc/profile 123456789101112131415# JAVA# 1. AWS EC2 Ubuntu 16export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64# 2. è…¾è®¯äº‘ Ubuntu 14export JAVA_HOME=/usr/lib/jvm/java-8-oracle# 3. MAC OSexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Homeexport JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar æ‰§è¡Œå‘½ä»¤ä½¿ä¹‹ç”Ÿæ•ˆ source /etc/profile 2.2 Hadoop è½¯ä»¶ ä¸‹è½½è½¯ä»¶ 123wget http://apache.uberglobalmirror.com/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gztar xvf hadoop-2.7.4.tar.gz é…ç½® vi ~/.bashrc è®¾ç½® Hadoop çš„ç¯å¢ƒå˜é‡ 1234export HADOOP_HOMEexport PATHexport HADOOP_CONF_DIRexport YARN_CONF_DIR 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# ~/.bashrc: executed by bash(1) for non-login shells.# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)# for examples# If not running interactively, don't do anythingcase $- in *i*) ;; *) return;;esac# don't put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don't overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern \"**\" used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)[ -x /usr/bin/lesspipe ] &amp;&amp; eval \"$(SHELL=/bin/sh lesspipe)\"# set variable identifying the chroot you work in (used in the prompt below)if [ -z \"$&#123;debian_chroot:-&#125;\" ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we \"want\" color)case \"$TERM\" in xterm-color|*-256color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the prompt#force_color_prompt=yesif [ -n \"$force_color_prompt\" ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it's compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ \"$color_prompt\" = yes ]; then PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ 'else PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h:\\w\\$ 'fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase \"$TERM\" inxterm*|rxvt*) PS1=\"\\[\\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h: \\w\\a\\]$PS1\" ;;*) ;;esac# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' alias grep='grep --color=auto' alias fgrep='fgrep --color=auto' alias egrep='egrep --color=auto'fi# colored GCC warnings and errors#export GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'# some more ls aliasesalias ll='ls -alF'alias la='ls -A'alias l='ls -CF'# Add an \"alert\" alias for long running commands. Use like so:# sleep 10; alertalias alert='notify-send --urgency=low -i \"$([ $? = 0 ] &amp;&amp; echo terminal || echo error)\" \"$(history|tail -n1|sed -e '\\''s/^\\s*[0-9]\\+\\s*//;s/[;&amp;|]\\s*alert$//'\\'')\"'# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don't need to enable# this, if it's already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi# HADOOPexport HADOOP_HOME=/home/ubuntu/workdir/hadoop-2.7.4export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=$CLASSPATH:$HADOOP_HOME/bin# JAVAexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib# LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/home/ubuntu/workdir/hadoop-2.7.4/lib/native 1export CLASSPATH=$CLASSPATH:/home/ubuntu/workdir/hadoop-2.7.4/etc/hadoop:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/contrib/capacity-scheduler/*.jar åœ¨ $HADOOP_CONF_DIR ä¸­è®¾ç½® JAVA_HOME 123vi /home/hadoop-2.7.4/etc/hadoop/hadoop-env.sh è®¾ç½®JAVA_HOMEvi /home/hadoop-2.7.4/etc/hadoop/mapred-env.sh è®¾ç½®JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 vi slaves è¿™é‡Œé¢å¡«å†™çš„å…¨æ˜¯slaves 12hadoop-slave1hadoop-slave2 vi $HADOOP_CONF_DIR/core-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!-- æ³¨æ„è¿™é‡Œçš„åŒºåˆ« ä¸€ä¸ªé€‚ç”¨äºå•æœº ä¸€ä¸ªé€‚ç”¨äºé›†ç¾¤ --&gt; &lt;!-- å•æœº --&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;!-- é›†ç¾¤ master --&gt; &lt;value&gt;hdfs://hadoop-master:9000&lt;/value&gt; &lt;description&gt;è®¾å®šnamenodeçš„ä¸»æœºååŠç«¯å£(å»ºè®®ä¸è¦æ›´æ”¹ç«¯å£å·)&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;description&gt; è®¾ç½®ç¼“å­˜å¤§å° &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop-2.7.4/tmp&lt;/value&gt; &lt;description&gt; å­˜æ”¾ä¸´æ—¶æ–‡ä»¶çš„ç›®å½• &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/hdfs-site.xml 1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/name&lt;/value&gt; &lt;description&gt; namenode ç”¨æ¥æŒç»­å­˜æ”¾å‘½åç©ºé—´å’Œäº¤æ¢æ—¥å¿—çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/data&lt;/value&gt; &lt;description&gt; DataNode åœ¨æœ¬åœ°å­˜æ”¾å—æ–‡ä»¶çš„ç›®å½•åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš” &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt; è®¾å®š HDFS å­˜å‚¨æ–‡ä»¶çš„å‰¯æœ¬ä¸ªæ•°ï¼Œé»˜è®¤ä¸º3 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; vi $HADOOP_CONF_DIR/mapred-site.xml (å¤åˆ¶mapred-site.xml.template,å†ä¿®æ”¹æ–‡ä»¶å) 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;hadoop-master:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop-master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://hadoop-master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;!-- master node çš„åå­— --&gt; &lt;!-- å•æœºå’Œé›†ç¾¤çš„åŒºåˆ«ï¼Ÿï¼Ÿ --&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop-master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop-master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;hadoop-master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop-master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345dfs.nameservices â€”â€“ HDFS NNçš„é€»è¾‘åç§°ï¼Œä½¿ç”¨ä¸Šé¢è®¾ç½®çš„myhdfsdfs.ha.namenodes.myhdfs â€”â€“ ç»™å®šæœåŠ¡é€»è¾‘åç§°myhdfsçš„èŠ‚ç‚¹åˆ—è¡¨dfs.namenode.rpc-address.myhdfs.nn1 â€”â€“ myhdfsä¸­nn1èŠ‚ç‚¹å¯¹å¤–æœåŠ¡çš„RPCåœ°å€dfs.namenode.http-address.myhdfs.nn1 â€”â€“ myhdfsä¸­nn1èŠ‚ç‚¹å¯¹å¤–æœåŠ¡çš„httpåœ°å€dfs.namenode.shared.edits.dir â€”â€“ è®¾ç½®ä¸€ç»„ journalNode çš„ URI åœ°å€ï¼Œactive NN å°† edit log å†™å…¥è¿™äº› 12345åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•/home/hadoop-2.7.4/hdfs ç”¨æ¥å­˜æ”¾é›†ç¾¤æ•°æ®ã€‚åœ¨ä¸»èŠ‚ç‚¹nodeä¸Šåˆ›å»ºç›®å½•/home/hadoop-2.7.4/hdfs/name ç”¨æ¥å­˜æ”¾æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®ã€‚åœ¨æ¯ä¸ªä»èŠ‚ç‚¹ä¸Šåˆ›å»ºç›®å½•/home/hadoop-2.7.4/hdfs/data ç”¨æ¥å­˜æ”¾çœŸæ­£çš„æ•°æ®ã€‚æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„æ—¥å¿—ç›®å½•ä¸º/home/hadoop-2.7.4/logsæ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„ä¸´æ—¶ç›®å½•ä¸º/home/hadoop-2.7.4/tmp 1ä¸Šé¢çš„é…ç½®åªéœ€è¦åœ¨masterä¸­é…å¥½, ç„¶åå¤åˆ¶åˆ°å…¶ä»–çš„slavesèŠ‚ç‚¹ä¸­ä¸­å» æ ¼å¼åŒ– namenode å’Œ datanodeï¼ˆåœ¨ master ä¸Šæ‰§è¡Œå°±å¯ä»¥äº† ä¸éœ€è¦åœ¨ slaves ä¸Šæ‰§è¡Œï¼‰ 12hdfs namenode -formathdfs datanode -format åˆ†åˆ«åœ¨ master å’Œ slaves ä¸­ç”¨ jps æŸ¥çœ‹è¿›ç¨‹","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"}]},{"title":"åŒæ­¥å¼‚æ­¥IO","date":"2017-09-29T03:54:40.000Z","path":"2017/09/29/åŒæ­¥å¼‚æ­¥IO/","text":"1. å¤§è‡´åŒºåˆ«æ‰€è°“çš„åŒæ­¥å’Œå¼‚æ­¥çš„åŒºåˆ«ï¼ŒæŒ‡çš„æ˜¯Applicationå’Œkernelä¹‹é—´çš„äº¤äº’æ–¹å¼ã€‚ å¦‚æœApplicationä¸éœ€è¦ç­‰å¾…kernelçš„å›åº”ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯å¼‚æ­¥çš„ï¼ˆkernelä¼šå‘ä¿¡å·é€šçŸ¥ï¼‰ã€‚ å¦‚æœApplicationæäº¤å®ŒIOè¯·æ±‚åï¼Œéœ€è¦ç­‰å¾…â€œå›æ‰§â€ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯åŒæ­¥çš„ã€‚ è€Œé˜»å¡å’Œéé˜»å¡ï¼ŒæŒ‡çš„æ˜¯Applicationæ˜¯å¦ç­‰å¾…IOæ“ä½œçš„å®Œæˆã€‚ å¦‚æœApplicationå¿…é¡»ç­‰åˆ°IOæ“ä½œå®é™…å®Œæˆä»¥åå†æ‰§è¡Œæ¥ä¸‹é¢çš„æ“ä½œï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯é˜»å¡çš„ã€‚ï¼ˆä»å‘å‡ºä¿¡å·å¼€å§‹ï¼Œä¼šä¸€ç›´blockå¯¹åº”çš„è¿›ç¨‹çŸ¥é“æ“ä½œå®Œæˆï¼Œï¼‰ åä¹‹ï¼Œå¦‚æœä¸éœ€è¦ç­‰å¾…IOæ“ä½œçš„å®Œæˆå°±å¼€å§‹æ‰§è¡Œå…¶ä»–çš„æ“ä½œï¼Œé‚£ä¹ˆå°±æ˜¯éé˜»å¡çš„ã€‚ï¼ˆåœ¨kernelè¿˜å‡†å¤‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°±ç«‹åˆ»è¿”å›äº†ã€‚ï¼‰ å«è½¦ä¹‹åï¼Œå°±ä¸€ç›´åœ¨è·¯å£ç­‰ç€ï¼Œè½¦æ¥äº†è‡ªå·±ä¸Šå» â€”â€”åŒæ­¥ï¼Œé˜»å¡ å«è½¦ä¹‹åï¼Œä¸€è¾¹ç­‰ç€ä¸€è¾¹çœ‹ç¾å¥³ï¼Œè½¦æ¥äº†è‡ªå·±ä¸Šå» â€”â€”åŒæ­¥ï¼Œéé˜»å¡ å«è½¦ä¹‹åï¼Œå…‰é¡¾ç€çœ‹ç¾å¥³ï¼Œå¸æœºåˆ°äº†ä¹‹åæ‰“ç”µè¯ç»™ä½  â€”â€”å¼‚æ­¥ï¼Œéé˜»å¡ å³æ˜¯ï¼šåŒæ­¥å°±æ˜¯ä½ è¦è‡ªå·±æ£€æŸ¥è½¦æ¥äº†æ²¡æœ‰ï¼›å¼‚æ­¥å°±æ˜¯è½¦æ¥äº†å¸æœºè”ç³»ä½ ã€‚é˜»å¡å°±æ˜¯ç­‰è½¦çš„æ—¶å€™è€å®ç­‰ç€ï¼Œåˆ«å¹²åˆ«çš„ï¼ˆè¢«é˜»å¡ï¼‰ï¼›éé˜»å¡å°±æ˜¯ç­‰è½¦çš„æ—¶å€™ä½ å¯ä»¥åšå…¶ä»–äº‹æƒ…ã€‚ 2. Blocking IO 3. Nonblocking IO 4. IO multiplexing / Event driven IOä½¿ç”¨select å’Œepollï¼Œå•ä¸ªprocesså¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªç½‘ç»œè¿æ¥çš„IOã€‚åŸºæœ¬åŸç†å°±æ˜¯select/epolè¿™ä¸ªfunctionä¼šä¸æ–­çš„è½®è¯¢æ‰€è´Ÿè´£çš„æ‰€æœ‰çš„socektï¼Œå½“æŸä¸ªsocketæœ‰æ•°æ®åˆ°è¾¾äº†ï¼Œå°±é€šçŸ¥ç”¨æˆ·è¿›ç¨‹ã€‚å®ƒçš„æµç¨‹å¦‚ä¸‹å›¾ï¼š è¿™ä¸ªå›¾å’Œblocking IOçš„å›¾å…¶å®å¹¶æ²¡æœ‰å¤ªå¤§çš„ä¸åŒï¼Œäº‹å®ä¸Šï¼Œè¿˜æ›´å·®ä¸€äº›ã€‚å› ä¸ºè¿™é‡Œéœ€è¦ä½¿ç”¨ä¸¤ä¸ªsystem call (select å’Œ recvfrom)ï¼Œè€Œblocking IOåªè°ƒç”¨äº†ä¸€ä¸ªsystem call (recvfrom)ã€‚ä½†æ˜¯ï¼Œç”¨selectçš„ä¼˜åŠ¿åœ¨äºå®ƒå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªconnectionã€‚ï¼ˆå¤šè¯´ä¸€å¥ã€‚æ‰€ä»¥ï¼Œå¦‚æœå¤„ç†çš„è¿æ¥æ•°ä¸æ˜¯å¾ˆé«˜çš„è¯ï¼Œä½¿ç”¨select/epollçš„web serverä¸ä¸€å®šæ¯”ä½¿ç”¨multi-threading + blocking IOçš„web serveræ€§èƒ½æ›´å¥½ï¼Œå¯èƒ½å»¶è¿Ÿè¿˜æ›´å¤§ã€‚select/epollçš„ä¼˜åŠ¿å¹¶ä¸æ˜¯å¯¹äºå•ä¸ªè¿æ¥èƒ½å¤„ç†å¾—æ›´å¿«ï¼Œè€Œæ˜¯åœ¨äºèƒ½å¤„ç†æ›´å¤šçš„è¿æ¥ã€‚ï¼‰åœ¨IO multiplexing Modelä¸­ï¼Œå®é™…ä¸­ï¼Œå¯¹äºæ¯ä¸€ä¸ªsocketï¼Œä¸€èˆ¬éƒ½è®¾ç½®æˆä¸ºnon-blockingï¼Œä½†æ˜¯ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæ•´ä¸ªç”¨æˆ·çš„processå…¶å®æ˜¯ä¸€ç›´è¢«blockçš„ã€‚åªä¸è¿‡processæ˜¯è¢«selectè¿™ä¸ªå‡½æ•°blockï¼Œè€Œä¸æ˜¯è¢«socket IOç»™blockã€‚ 5. Asynchronous IO ç”¨æˆ·è¿›ç¨‹å‘èµ·readæ“ä½œä¹‹åï¼Œå°±å¯ä»¥ç«‹åˆ»åšå…¶ä»–çš„äº‹æƒ…ã€‚è€Œå¦ä¸€æ–¹é¢ï¼Œä»kernelçš„è§’åº¦ï¼Œå½“å®ƒæ”¶åˆ°ä¸€ä¸ªasynchronous readä¹‹åï¼Œé¦–å…ˆå®ƒå°±ä¼šç«‹åˆ»è¿”å›ä¸€ä¸ªä¿¡å·ï¼Œæ‰€ä»¥ä¸ä¼šå¯¹ç”¨æˆ·è¿›ç¨‹äº§ç”Ÿä»»ä½•çš„blockã€‚ ç„¶åï¼Œkernelä¼šç­‰å¾…æ•°æ®å‡†å¤‡æ™šé¤ï¼Œå°†æ•°æ®æ‹·è´åˆ°ç”¨æˆ·å†…å­˜ï¼Œå½“è¿™ä¸€åˆ‡éƒ½å®Œæˆåï¼Œkernelä¼šç»™ç”¨æˆ·è¿›ç¨‹å‘é€ä¸€ä¸ªsignalï¼Œå‘Šè¯‰å®ƒreadæ“ä½œå·²ç»å®Œæˆäº†ã€‚","tags":[{"name":"åŒæ­¥å¼‚æ­¥IO","slug":"åŒæ­¥å¼‚æ­¥IO","permalink":"http://chenson.cc/tags/åŒæ­¥å¼‚æ­¥IO/"}]},{"title":"å¤šçº¿ç¨‹ä¸å¤šè¿›ç¨‹ç®€å•ç†è§£","date":"2017-09-22T06:28:08.000Z","path":"2017/09/22/å¤šçº¿ç¨‹ä¸å¤šè¿›ç¨‹ç®€å•ç†è§£/","text":"1. å¤šè¿›ç¨‹ç›´è§‚æ¥çœ‹ï¼Œå°±æ˜¯ä¸€ä¸ªä¸ªpidï¼Œè¿›ç¨‹æ˜¯ç¨‹åºåœ¨è®¡ç®—æœºä¸Šçš„ä¸€æ¬¡æ‰§è¡Œæ´»åŠ¨ã€‚ åˆ›å»ºå­è¿›ç¨‹çš„è°ƒç”¨æ˜¯ fork() fork() çš„åŠŸèƒ½å°±æ˜¯äº§ç”Ÿå­è¿›ç¨‹ï¼Œè°ƒç”¨ä¸€æ¬¡ï¼Œè¿”å›ä¸¤æ¬¡ã€‚ ä¸€æ¬¡è¿”å›0ï¼Œä¸€æ¬¡è¿”å›å­è¿›ç¨‹çš„pid å­è¿›ç¨‹æ°¸è¿œè¿”å› 0ï¼Œè€Œçˆ¶è¿›ç¨‹è¿”å›å­è¿›ç¨‹çš„IDã€‚è¿™æ ·åšçš„ç†ç”±æ˜¯ï¼Œä¸€ä¸ªçˆ¶è¿›ç¨‹å¯ä»¥forkå‡ºå¾ˆå¤šå­è¿›ç¨‹ï¼Œæ‰€ä»¥ï¼Œçˆ¶è¿›ç¨‹è¦è®°ä¸‹æ¯ä¸ªå­è¿›ç¨‹çš„IDï¼Œè€Œå­è¿›ç¨‹åªéœ€è¦è°ƒç”¨ getppid() å°±å¯ä»¥æ‹¿åˆ°çˆ¶è¿›ç¨‹çš„ID å¤±è´¥è¿”å›-1 12345678910111213141516171819202122232425#include &lt;unistd.h&gt; #include &lt;sys/types.h&gt; #include &lt;stdio.h&gt; void print_exit() &#123; printf(\"the exit pid:%d/n\",getpid() ); &#125; main()&#123; pit_t, pid; atexit(print_exit); //æ³¨å†Œè¯¥è¿›ç¨‹é€€å‡ºæ—¶çš„å›è°ƒå‡½æ•° pid = fork(); if (pid &lt; 0) printf(\"error in fork!\"); else if (pid == 0) // å­è¿›ç¨‹ printf(\"i am the child process, my process id is %d/n\", getpid()); else // çˆ¶è¿›ç¨‹ &#123; printf(\"i am the parent process, my process id is %d/n\", getpid()); sleep(2); wait(); &#125; &#125; 2. å¤šçº¿ç¨‹è¿›ç¨‹æ˜¯ç”±è‹¥å¹²çº¿ç¨‹ç»„æˆçš„ï¼Œä¸€ä¸ªè¿›ç¨‹è‡³å°‘æœ‰ä¸€ä¸ªçº¿ç¨‹ã€‚ çº¿ç¨‹å°±æ˜¯æŠŠä¸€ä¸ªè¿›ç¨‹åˆ†æˆå¾ˆå¤šç‰‡ï¼Œæ¯ä¸€ç‰‡éƒ½æ˜¯å¯ä»¥ç‹¬ç«‹çš„æµç¨‹ linuxæä¾›çš„å¤šçº¿ç¨‹çš„ç³»ç»Ÿè°ƒç”¨ï¼š 123456int pthread_create(pthread_t *restrict tidp, const pthread_attr_t *restrict attr, void *(*start_rtn)(void), void *restrict arg);Returns: 0 if OK, error number on failure ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæŒ‡å‘çº¿ç¨‹æ ‡è¯†ç¬¦çš„æŒ‡é’ˆã€‚ç¬¬äºŒä¸ªå‚æ•°ç”¨æ¥è®¾ç½®çº¿ç¨‹å±æ€§ã€‚ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯çº¿ç¨‹è¿è¡Œå‡½æ•°çš„èµ·å§‹åœ°å€ã€‚æœ€åä¸€ä¸ªå‚æ•°æ˜¯è¿è¡Œå‡½æ•°çš„å‚æ•°ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include&lt;stdio.h&gt;#include&lt;string.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;pthread.h&gt; void* task1(void*);void* task2(void*);void usr();int p1,p2;int main()&#123; usr(); getchar(); return 1;&#125; void usr()&#123; pthread_t pid1, pid2; pthread_attr_t attr; void *p; int ret=0; pthread_attr_init(&amp;attr); // åˆå§‹åŒ–çº¿ç¨‹å±æ€§ç»“æ„ pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_DETACHED); // è®¾ç½®attrç»“æ„ä¸ºåˆ†ç¦» pthread_create(&amp;pid1, &amp;attr, task1, NULL); // åˆ›å»ºçº¿ç¨‹ï¼Œè¿”å›çº¿ç¨‹å·ç»™pid1,çº¿ç¨‹å±æ€§è®¾ç½®ä¸ºattrçš„å±æ€§ï¼Œçº¿ç¨‹å‡½æ•°å…¥å£ä¸ºtask1ï¼Œå‚æ•°ä¸ºNULL pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE); pthread_create(&amp;pid2, &amp;attr, task2, NULL); // å‰å°å·¥ä½œ ret=pthread_join(pid2, &amp;p); // ç­‰å¾…pid2è¿”å›ï¼Œè¿”å›å€¼èµ‹ç»™p printf(\"after pthread2:ret=%d,p=%d/n\", ret,(int)p); &#125;void* task1(void *arg1)&#123; printf(\"task1/n\"); // è‰°è‹¦è€Œæ— æ³•é¢„æ–™çš„å·¥ä½œï¼Œè®¾ç½®ä¸ºåˆ†ç¦»çº¿ç¨‹ï¼Œä»»å…¶è‡ªç”Ÿè‡ªç­ pthread_exit( (void *)1);&#125;void* task2(void *arg2)&#123; int i=0; printf(\"thread2 begin./n\"); // ç»§ç»­é€å¤–å–çš„å·¥ä½œ pthread_exit((void *)2);&#125; 3. çº¿ç¨‹ VS è¿›ç¨‹ è¿›ç¨‹å¼€é”€å¤§ï¼Œçº¿ç¨‹å¼€é”€å°ã€‚ çº¿ç¨‹å®‰å…¨ï¼šæ¦‚å¿µæ¯”è¾ƒç›´è§‚ã€‚ä¸€èˆ¬è¯´æ¥ï¼Œä¸€ä¸ªå‡½æ•°è¢«ç§°ä¸ºçº¿ç¨‹å®‰å…¨çš„ï¼Œå½“ä¸”ä»…å½“è¢«å¤šä¸ªå¹¶å‘çº¿ç¨‹åå¤è°ƒç”¨æ—¶ï¼Œå®ƒä¼šä¸€ç›´äº§ç”Ÿæ­£ç¡®çš„ç»“æœã€‚ çº¿ç¨‹å®‰å…¨çš„æ¡ä»¶ï¼šä¸»è¦éœ€è¦è€ƒè™‘çš„æ˜¯çº¿ç¨‹ä¹‹é—´çš„å…±äº«å˜é‡ã€‚å±äºåŒä¸€è¿›ç¨‹çš„ä¸åŒçº¿ç¨‹ä¼šå…±äº«è¿›ç¨‹å†…å­˜ç©ºé—´ä¸­çš„å…¨å±€åŒºå’Œå †ï¼Œè€Œç§æœ‰çš„çº¿ç¨‹ç©ºé—´åˆ™ä¸»è¦åŒ…æ‹¬æ ˆå’Œå¯„å­˜å™¨ã€‚å› æ­¤ï¼Œå¯¹äºåŒä¸€è¿›ç¨‹çš„ä¸åŒçº¿ç¨‹æ¥è¯´ï¼Œæ¯ä¸ªçº¿ç¨‹çš„å±€éƒ¨å˜é‡éƒ½æ˜¯ç§æœ‰çš„ï¼Œè€Œå…¨å±€å˜é‡ã€å±€éƒ¨é™æ€å˜é‡ã€åˆ†é…äºå †çš„å˜é‡éƒ½æ˜¯å…±äº«çš„ã€‚åœ¨å¯¹è¿™äº›å…±äº«å˜é‡è¿›è¡Œè®¿é—®æ—¶ï¼Œå¦‚æœè¦ä¿è¯çº¿ç¨‹å®‰å…¨ï¼Œåˆ™å¿…é¡»é€šè¿‡åŠ é”çš„æ–¹å¼ å¤šè¿›ç¨‹æ¨¡å¼æœ€å¤§çš„ä¼˜ç‚¹å°±æ˜¯ç¨³å®šæ€§é«˜ï¼Œå› ä¸ºä¸€ä¸ªå­è¿›ç¨‹å´©æºƒäº†ï¼Œä¸ä¼šå½±å“ä¸»è¿›ç¨‹å’Œå…¶ä»–å­è¿›ç¨‹ã€‚ å¤šè¿›ç¨‹æ¨¡å¼çš„ç¼ºç‚¹æ˜¯åˆ›å»ºè¿›ç¨‹çš„ä»£ä»·å¤§ï¼Œåœ¨Unix/Linuxç³»ç»Ÿä¸‹ï¼Œç”¨forkè°ƒç”¨è¿˜è¡Œï¼Œåœ¨Windowsä¸‹åˆ›å»ºè¿›ç¨‹å¼€é”€å·¨å¤§ã€‚å¦å¤–ï¼Œæ“ä½œç³»ç»Ÿèƒ½åŒæ—¶è¿è¡Œçš„è¿›ç¨‹æ•°ä¹Ÿæ˜¯æœ‰é™çš„ï¼Œåœ¨å†…å­˜å’ŒCPUçš„é™åˆ¶ä¸‹ï¼Œå¦‚æœæœ‰å‡ åƒä¸ªè¿›ç¨‹åŒæ—¶è¿è¡Œï¼Œæ“ä½œç³»ç»Ÿè¿è°ƒåº¦éƒ½ä¼šæˆé—®é¢˜ å¤šçº¿ç¨‹æ¨¡å¼é€šå¸¸æ¯”å¤šè¿›ç¨‹å¿«ä¸€ç‚¹ï¼Œä½†æ˜¯ä¹Ÿå¿«ä¸åˆ°å“ªå»ï¼Œè€Œä¸”ï¼Œå¤šçº¿ç¨‹æ¨¡å¼è‡´å‘½çš„ç¼ºç‚¹å°±æ˜¯ä»»ä½•ä¸€ä¸ªçº¿ç¨‹æŒ‚æ‰éƒ½å¯èƒ½ç›´æ¥é€ æˆæ•´ä¸ªè¿›ç¨‹å´©æºƒï¼Œå› ä¸ºæ‰€æœ‰çº¿ç¨‹å…±äº«è¿›ç¨‹çš„å†…å­˜ã€‚åœ¨Windowsä¸Šï¼Œå¦‚æœä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œçš„ä»£ç å‡ºäº†é—®é¢˜ï¼Œä½ ç»å¸¸å¯ä»¥çœ‹åˆ°è¿™æ ·çš„æç¤ºï¼šâ€œè¯¥ç¨‹åºæ‰§è¡Œäº†éæ³•æ“ä½œï¼Œå³å°†å…³é—­â€ï¼Œå…¶å®å¾€å¾€æ˜¯æŸä¸ªçº¿ç¨‹å‡ºäº†é—®é¢˜ï¼Œä½†æ˜¯æ“ä½œç³»ç»Ÿä¼šå¼ºåˆ¶ç»“æŸæ•´ä¸ªè¿›ç¨‹ 4. è¿›ç¨‹é—´é€šä¿¡4.1 å•æœºå¤šè¿›ç¨‹é€šä¿¡IPCï¼ˆInter Process Communicationï¼‰åŒ…æ‹¬ï¼šç®¡é“ã€æ–‡ä»¶ã€å’Œæ¶ˆæ¯ä¼ é€’ ç®¡é“ï¼ˆPipeï¼‰åŠæœ‰åç®¡é“ï¼ˆnamed pipeï¼‰ï¼šç®¡é“å¯ç”¨äºå…·æœ‰äº²ç¼˜å…³ç³»è¿›ç¨‹é—´çš„é€šä¿¡ï¼Œæœ‰åç®¡é“å…‹æœäº†ç®¡é“æ²¡æœ‰åå­—çš„é™åˆ¶ï¼Œå› æ­¤ï¼Œé™¤å…·æœ‰ç®¡é“æ‰€å…·æœ‰çš„åŠŸèƒ½å¤–ï¼Œå®ƒè¿˜å…è®¸æ— äº²ç¼˜å…³ç³»è¿›ç¨‹é—´çš„é€šä¿¡ï¼› ä¿¡å·ï¼ˆSignalï¼‰ï¼šä¿¡å·æ˜¯æ¯”è¾ƒå¤æ‚çš„é€šä¿¡æ–¹å¼ï¼Œç”¨äºé€šçŸ¥æ¥å—è¿›ç¨‹æœ‰æŸç§äº‹ä»¶å‘ç”Ÿï¼Œé™¤äº†ç”¨äºè¿›ç¨‹é—´é€šä¿¡å¤–ï¼Œè¿›ç¨‹è¿˜å¯ä»¥å‘é€ä¿¡å·ç»™è¿›ç¨‹æœ¬èº«ï¼›linuxé™¤äº†æ”¯æŒUnixæ—©æœŸä¿¡å·è¯­ä¹‰å‡½æ•°sigalå¤–ï¼Œè¿˜æ”¯æŒè¯­ä¹‰ç¬¦åˆPosix.1æ ‡å‡†çš„ä¿¡å·å‡½æ•°sigactionï¼ˆå®é™…ä¸Šï¼Œè¯¥å‡½æ•°æ˜¯åŸºäºBSDçš„ï¼ŒBSDä¸ºäº†å®ç°å¯é ä¿¡å·æœºåˆ¶ï¼Œåˆèƒ½å¤Ÿç»Ÿä¸€å¯¹å¤–æ¥å£ï¼Œç”¨sigactionå‡½æ•°é‡æ–°å®ç°äº†signalå‡½æ•°ï¼‰ï¼› æŠ¥æ–‡ï¼ˆMessageï¼‰é˜Ÿåˆ—ï¼ˆæ¶ˆæ¯é˜Ÿåˆ—ï¼‰ï¼šæ¶ˆæ¯é˜Ÿåˆ—æ˜¯æ¶ˆæ¯çš„é“¾æ¥è¡¨ï¼ŒåŒ…æ‹¬Posixæ¶ˆæ¯é˜Ÿåˆ—system Væ¶ˆæ¯é˜Ÿåˆ—ã€‚æœ‰è¶³å¤Ÿæƒé™çš„è¿›ç¨‹å¯ä»¥å‘é˜Ÿåˆ—ä¸­æ·»åŠ æ¶ˆæ¯ï¼Œè¢«èµ‹äºˆè¯»æƒé™çš„è¿›ç¨‹åˆ™å¯ä»¥è¯»èµ°é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯ã€‚æ¶ˆæ¯é˜Ÿåˆ—å…‹æœäº†ä¿¡å·æ‰¿è½½ä¿¡æ¯é‡å°‘ï¼Œç®¡é“åªèƒ½æ‰¿è½½æ— æ ¼å¼å­—èŠ‚æµä»¥åŠç¼“å†²åŒºå¤§å°å—é™ç­‰ç¼ºç‚¹ã€‚ å…±äº«å†…å­˜ï¼šä½¿å¾—å¤šä¸ªè¿›ç¨‹å¯ä»¥è®¿é—®åŒä¸€å—å†…å­˜ç©ºé—´ï¼Œæ˜¯æœ€å¿«çš„å¯ç”¨IPCå½¢å¼ã€‚æ˜¯é’ˆå¯¹å…¶ä»–é€šä¿¡æœºåˆ¶è¿è¡Œæ•ˆç‡è¾ƒä½è€Œè®¾è®¡çš„ã€‚å¾€å¾€ä¸å…¶å®ƒé€šä¿¡æœºåˆ¶ï¼Œå¦‚ä¿¡å·é‡ç»“åˆä½¿ç”¨ï¼Œæ¥è¾¾åˆ°è¿›ç¨‹é—´çš„åŒæ­¥åŠäº’æ–¥ã€‚ ä¿¡å·é‡ï¼ˆsemaphoreï¼‰ï¼šä¸»è¦ä½œä¸ºè¿›ç¨‹é—´ä»¥åŠåŒä¸€è¿›ç¨‹ä¸åŒçº¿ç¨‹ä¹‹é—´çš„åŒæ­¥æ‰‹æ®µã€‚ å¥—æ¥å£ï¼ˆSocketï¼‰ï¼šæ›´ä¸ºä¸€èˆ¬çš„è¿›ç¨‹é—´é€šä¿¡æœºåˆ¶ï¼Œå¯ç”¨äºä¸åŒæœºå™¨ä¹‹é—´çš„è¿›ç¨‹é—´é€šä¿¡ã€‚èµ·åˆæ˜¯ç”±Unixç³»ç»Ÿçš„BSDåˆ†æ”¯å¼€å‘å‡ºæ¥çš„ï¼Œä½†ç°åœ¨ä¸€èˆ¬å¯ä»¥ç§»æ¤åˆ°å…¶å®ƒç±»Unixç³»ç»Ÿä¸Šï¼šLinuxå’ŒSystem Vçš„å˜ç§éƒ½æ”¯æŒå¥—æ¥å­—ã€‚ 4.2 åˆ†å¸ƒå¼è¿›ç¨‹é€šä¿¡5. çº¿ç¨‹é€šä¿¡å¤šæ•°çš„å¤šçº¿ç¨‹éƒ½æ˜¯åœ¨åŒä¸€ä¸ªè¿›ç¨‹ä¸‹çš„ï¼Œå®ƒä»¬å…±äº«è¯¥è¿›ç¨‹çš„å…¨å±€å˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å…¨å±€å˜é‡æ¥å®ç°çº¿ç¨‹é—´é€šä¿¡ã€‚å¦‚æœæ˜¯ä¸åŒçš„è¿›ç¨‹ä¸‹çš„2ä¸ªçº¿ç¨‹é—´é€šä¿¡ï¼Œç›´æ¥å‚è€ƒè¿›ç¨‹é—´é€šä¿¡ã€‚ 6. References å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹çš„åŒºåˆ«ï¼ˆå°ç»“ï¼‰","tags":[{"name":"çº¿ç¨‹","slug":"çº¿ç¨‹","permalink":"http://chenson.cc/tags/çº¿ç¨‹/"},{"name":"è¿›ç¨‹","slug":"è¿›ç¨‹","permalink":"http://chenson.cc/tags/è¿›ç¨‹/"}]},{"title":"Machine Learning - PCA","date":"2017-06-17T12:59:32.000Z","path":"2017/06/17/Machine-Learning-PCA/","text":"1. PCAPCA(Principal Components Analysis)æ˜¯ä¸»æˆæˆåˆ†åˆ†æï¼Œä¹‹å‰ä¹Ÿå«åšPricipal Factor Analysisã€‚é¡¾åæ€ä¹‰å°±æ˜¯åˆ†ææ•°æ®é‡Œé¢çš„ä¸»è¦éƒ¨åˆ†ï¼Œæ˜¯æœ€å¸¸ç”¨çš„ä¸€ç§é™ç»´æ–¹æ³•ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆéœ€è¦é™ç»´å‘¢ï¼Ÿ å› ä¸ºåœ¨çœŸå®çš„è®­ç»ƒæ•°æ®ä¸­ï¼Œæ€»æ˜¯ä¼šå­˜åœ¨å„ç§å†—ä½™çš„æ•°æ® æ¯”å¦‚è¯´æ‹¿åˆ°ä¸€ä¸ªæ±½è½¦çš„æ ·æœ¬ï¼Œé‡Œé¢æ—¢æœ‰ä»¥â€œåƒç±³/æ¯å°æ—¶â€åº¦é‡çš„æœ€å¤§é€Ÿåº¦ç‰¹å¾ï¼Œä¹Ÿæœ‰â€œè‹±é‡Œ/å°æ—¶â€çš„æœ€å¤§é€Ÿåº¦ç‰¹å¾ï¼Œæ˜¾ç„¶è¿™ä¸¤ä¸ªç‰¹å¾æœ‰ä¸€ä¸ªå¤šä½™ æ‹¿åˆ°ä¸€ä¸ªæ•°å­¦ç³»çš„æœ¬ç§‘ç”ŸæœŸæœ«è€ƒè¯•æˆç»©å•ï¼Œé‡Œé¢æœ‰ä¸‰åˆ—ï¼Œä¸€åˆ—æ˜¯å¯¹æ•°å­¦çš„å…´è¶£ç¨‹åº¦ï¼Œä¸€åˆ—æ˜¯å¤ä¹ æ—¶é—´ï¼Œè¿˜æœ‰ä¸€åˆ—æ˜¯è€ƒè¯•æˆç»©ã€‚æˆ‘ä»¬çŸ¥é“è¦å­¦å¥½æ•°å­¦ï¼Œéœ€è¦æœ‰æµ“åšçš„å…´è¶£ï¼Œæ‰€ä»¥ç¬¬äºŒé¡¹ä¸ç¬¬ä¸€é¡¹å¼ºç›¸å…³ï¼Œç¬¬ä¸‰é¡¹å’Œç¬¬äºŒé¡¹ä¹Ÿæ˜¯å¼ºç›¸å…³ã€‚é‚£æ˜¯ä¸æ˜¯å¯ä»¥åˆå¹¶ç¬¬ä¸€é¡¹å’Œç¬¬äºŒé¡¹å‘¢ï¼Ÿ è¿™ä¸ªä¸ç¬¬äºŒä¸ªæœ‰ç‚¹ç±»ä¼¼ï¼Œå‡è®¾åœ¨IRä¸­æˆ‘ä»¬å»ºç«‹çš„æ–‡æ¡£-è¯é¡¹çŸ©é˜µä¸­ï¼Œæœ‰ä¸¤ä¸ªè¯é¡¹ä¸ºâ€œlearnâ€å’Œâ€œstudyâ€ï¼Œåœ¨ä¼ ç»Ÿçš„å‘é‡ç©ºé—´æ¨¡å‹ä¸­ï¼Œè®¤ä¸ºä¸¤è€…ç‹¬ç«‹ã€‚ç„¶è€Œä»è¯­ä¹‰çš„è§’åº¦æ¥è®²ï¼Œä¸¤è€…æ˜¯ç›¸ä¼¼çš„ï¼Œè€Œä¸”ä¸¤è€…å‡ºç°é¢‘ç‡ä¹Ÿç±»ä¼¼ï¼Œæ˜¯ä¸æ˜¯å¯ä»¥åˆæˆä¸ºä¸€ä¸ªç‰¹å¾å‘¢ï¼Ÿ å› ä¸ºè¿™äº›å†—ä½™çš„æ•°æ®ï¼Œå¸¸å¸¸ä¼šå¯¼è‡´æˆ‘ä»¬çš„æ¨¡å‹è¿‡åº¦æ‹Ÿåˆã€‚è¿™ä¸ªæ—¶å€™ï¼Œå°±éœ€è¦ä¸€ç§ç‰¹å¾é™ç»´çš„æ–¹æ³•æ¥å‡å°‘ç‰¹å¾æ•°ï¼Œå‡å°‘å™ªéŸ³å’Œå†—ä½™ï¼Œå‡å°‘è¿‡åº¦æ‹Ÿåˆçš„å¯èƒ½æ€§ã€‚è€ŒPCAå°±æ˜¯é™ç»´çš„ç®—æ³•ä¹‹ä¸€ï¼Œå°†åŸå…ˆçš„æ•°æ®ä»nç»´æ˜ å°„åˆ°kç»´ä¸Š(k&lt;n)ã€‚è¿™é‡Œkç»´æ˜¯å…¨æ–°çš„æ­£äº¤ç‰¹å¾ï¼Œè¿™kç»´ç‰¹å¾æˆä¸ºä¸»å…ƒï¼Œæ˜¯é‡æ–°æ„é€ å‡ºå‡ºæ¥çš„kç»´ç‰¹å¾ï¼Œè€Œä¸æ˜¯ç®€å•çš„ä»nç»´å½“ä¸­ç§»å»äº†(n-k)ç»´ï¼Œç„¶åå‰©ä¸‹ä¸»è¦çš„kç»´ç‰¹å¾ã€‚ é‚£ä¹ˆè¯·æ€è€ƒä¸€ä¸ªé—®é¢˜ï¼šå¯¹äºæ­£äº¤å±æ€§ç©ºé—´ä¸­çš„æ ·æœ¬ç‚¹ï¼Œå¦‚ä½•ç”¨ä¸€ä¸ªè¶…å¹³é¢(ç›´çº¿çš„é«˜ç»´æ¨å¹¿)ï¼Œå¯¹æ‰€æœ‰çš„æ ·æœ¬è¿›è¡Œæ°å½“çš„è¡¨è¾¾ï¼Ÿé‚£ä¹ˆéœ€è¦æ‰¾åˆ°æ€ä¹ˆæ ·çš„ä¸€ä¸ªè¶…å¹³é¢æ¥åˆ†å‰²å‘¢ï¼Ÿä¸€èˆ¬éœ€è¦å…·å¤‡å¦‚ä¸‹ç‰¹å¾ï¼š æœ€è¿‘é‡æ„æ€§ï¼šæ ·æœ¬ç‚¹åˆ°è¿™ä¸ªè¶…å¹³é¢çš„è·ç¦»éƒ½è¶³å¤Ÿè¿‘ æœ€å¤§å¯åˆ†æ€§ï¼šæ ·æœ¬ç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½çš„è¦åˆ†å¼€ï¼Œè€Œä¸æ˜¯é‡å ä¸€èµ· 2. è®¡ç®—è¿‡ç¨‹é‚£ä¹ˆå¦‚ä½•æ¥æ„å»ºå‘¢ï¼Ÿä¸¾ä¸ªæ —å­ï¼š ç°åœ¨æœ‰è¿™ä¹ˆä¸€ç»„æ•°æ®ï¼Œxå’Œyæ˜¯ä¸¤ä¸ªç‰¹å¾ ç¬¬ä¸€æ­¥ï¼šæ±‚å‡ºæ‰€æœ‰ç»´åº¦çš„å¹³å‡å€¼ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œxçš„å¹³å‡å€¼æ˜¯1.81ï¼Œyçš„å¹³å‡å€¼æ˜¯1.91 ç¬¬äºŒæ­¥ï¼šä¸­å¿ƒåŒ–ï¼Œå³$\\sum x_i=0$.å°†æ‰€æœ‰çš„æ ·æœ¬éƒ½å‡å»è¿™ä¿©ä¸ªå¹³å‡å€¼ã€‚æ¯”å¦‚ç¬¬ä¸€ä¸ªæ ·æœ¬(2.5, 2.4) - (1.81, 1.91) = (0.69, 0.49) ç¬¬ä¸‰æ­¥ï¼šæ±‚ç‰¹å¾çš„åæ–¹å·®çŸ©é˜µå¦‚æœæœ‰x, y, zä¸‰ä¸ªç‰¹å¾ï¼Œåˆ†åˆ«éœ€è¦æ±‚cov(x, x), cov(x, y), cov(x, z), cov(y, y), cov(y, z), cov(z, z)è¿™å‡ ä¸ªã€‚ å½“åæ–¹å·®å¤§äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºxå’Œyè‹¥æœ‰ä¸€ä¸ªå¢åŠ ï¼Œå¦ä¸€ä¸ªä¹Ÿä¼šå¢åŠ ã€‚ å½“åæ–¹å·®ä¸‹äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºä¸€ä¸ªå¢åŠ ï¼Œå¦ä¸€ä¸ªä¼šå‡å°‘ å½“åæ–¹å·®ç­‰äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºæ¥è€…ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ã€‚ åæ–¹å·®çš„ç»å¯¹å€¼è¶Šå¤§ï¼Œä¸¤è€…å¯¹å½¼æ­¤çš„å½±å“ä¹Ÿå°±è¶Šå¤§ å…·ä½“å¦‚ä¸‹å›¾ è€Œæˆ‘ä»¬ä¸Šé¢çš„ä¾‹å­ä¸­åªæœ‰xå’Œyä¸¤ä¸ªå˜é‡ï¼Œå³ ç¬¬å››æ­¥ï¼šæ±‚åæ–¹å·®çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå¾—åˆ° ä¸Šé¢æ˜¯ä¸¤ä¸ªç‰¹å¾å€¼ï¼Œä¸‹é¢æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ æ¯”å¦‚ç‰¹å¾å€¼0.0490833989å¯¹åº”çš„ç‰¹å¾å‘é‡ä¸º(-0.735178656, 0.677873399)Tï¼Œè¿™é‡Œçš„ç‰¹å¾å‘é‡éƒ½å½’ä¸€åŒ–ä¸ºå•ä½å•ä½å‘é‡ æ±‚åæ–¹å·®çš„æ­¥éª¤ï¼š ç¬¬äº”æ­¥ï¼šå°†ç‰¹å¾å€¼æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ï¼Œé€‰æ‹©å…¶ä¸­ä¸ªæœ€å¤§çš„kä¸ªï¼Œç„¶åå°†å…¶å¯¹åº”çš„kä¸ªç‰¹å¾å‘é‡åˆ†åˆ«ä½œä¸ºåˆ—å‘é‡ç»„æˆç‰¹å¾å‘é‡çŸ©é˜µã€‚ ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç‰¹å¾å€¼åªæœ‰ä¸¤ä¸ªï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©å…¶ä¸­æœ€å¤§çš„é‚£ä¸ªã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬é€‰æ‹©1.28402771ï¼Œå¯¹åº”çš„ç‰¹å¾å‘é‡æ˜¯ (-0.677873399, -0.735178656)T. ç¬¬å…­æ­¥ï¼šå°†æ ·æœ¬æŠ•å½±åˆ°é€‰å–çš„ç‰¹å¾å‘é‡ä¸Šå»ã€‚ å‡è®¾æ ·æœ¬æ•°é‡ä¸ºmï¼Œç‰¹å¾æ•°é‡ä¸ºnï¼Œå‡å»å‡å€¼åçš„æ ·æœ¬çŸ©é˜µä¸ºDataAdjust(m*n)ï¼Œåæ–¹å·®çŸ©é˜µæ˜¯n*mï¼Œé€‰å–çš„kä¸ªç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µä¸ºEigenVectors(n*k)ã€‚é‚£ä¹ˆæŠ•å½±åçš„æ•°æ®FinalDataä¸º FinalData(m\\ast k) = DataAdjust(m\\ast n) \\times EigenVectors(n \\ast k)æ‰€ä»¥ä¸Šé¢çš„ä¾‹å­ä¸­ FinalData(10\\ast 1) = DataAdjust(10\\ast 2) \\times EigenVectors(-0.677873399, -0.735178656)^Tå¾—åˆ°çš„ç»“æœæ˜¯ è¿™æ ·å°±å°†åŸå§‹çš„æ ·æœ¬æ•°æ®ä»nç»´ç‰¹å¾å˜æˆäº†kç»´ç‰¹å¾ï¼Œè¿™kç»´å°±æ˜¯åŸå§‹ç‰¹å¾åœ¨kç»´ä¸Šçš„æŠ•å½±ã€‚ ä¸‹é¢çš„å›¾æè¿°äº†ä¸Šé¢çš„è¿‡ç¨‹ï¼š åŸå…ˆæ‰€æœ‰æ•°æ®æ˜¯åˆ†å¸ƒåœ¨x-yè¿™ä¸ªåæ ‡ç³»ä¸Šï¼Œ+è¡¨ç¤ºçš„æ˜¯æ ·æœ¬æ•°æ®ã€‚å·®ä¸å¤šæ˜¯å¯¹è§’çº¿ä¸Šçš„ä¸¤æ¡çº¿åˆ†åˆ«ä»£è¡¨äº†ä¸¤ä¸ªæ­£äº¤çš„ç‰¹å¾å‘é‡ã€‚ç”±äºåæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œå› æ­¤å…¶ç‰¹å¾å‘é‡æ­£äº¤ã€‚ç„¶åæˆ‘ä»¬å°†åŸæœ‰çš„æ ·æœ¬æ•°æ®æŠ•å½±åˆ°è¿™ä¸ªæ–°çš„åæ ‡ç³»ä¸­å»ï¼Œå¾—åˆ°è½¬æ¢åçš„æ•°æ®ï¼š ä»ä¸Šé¢çš„å›¾æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œåœ¨æ–°çš„åæ ‡ç³»ä¸­ï¼Œxè½´åŸºæœ¬å°±å¯ä»¥è¡¨ç¤ºåŸæœ‰çš„æ ·æœ¬æ•°æ®ç‰¹å¾ã€‚è€Œæ•´ä¸ªè¿‡ç¨‹ï¼Œçœ‹èµ·æ¥æœ‰ç‚¹åƒæ˜¯å°†åæ ‡ç³»åšäº†æ—‹è½¬ã€‚å¦‚æœå½“k=1çš„æ—¶å€™ï¼Œä¸Šé¢æ•°æ®å°±åªä¼šå˜æˆä¸€ç»´çš„æ•°æ®ã€‚è€Œå½“åŸæœ‰çš„æ•°æ®ç»´åº¦å¾ˆé«˜çš„æ—¶å€™ï¼Œæœ‰æ—¶å€™ä¸ºäº†å¯è§†åŒ–æ“ä½œï¼Œæˆ‘ä»¬å°±ä¼šå¯¹å…¶è¡ŒPCAé™ç»´ã€‚ 3. PCAç†è®ºåŸºç¡€ä¹‹å‰æˆ‘ä»¬æåˆ°äº†ï¼Œæˆ‘ä»¬å¸Œæœ›æ–°çš„è¶…å¹³é¢å…·å¤‡æœ€å¤§å¯åˆ†æ€§å’Œæœ€è¿‘é‡æ„æ€§è¿™ä¸¤ä¸ªç‰¹å¾ã€‚å®é™…ä¸Šæ ¹æ®è¿™ä¸¤ä¸ªç‰¹å¾ï¼Œèƒ½å¤Ÿåˆ†åˆ«å¾—åˆ°ä¸»æˆåˆ†åˆ†æçš„ä¸¤ç§ç­‰ä»·æ¨å¯¼ã€‚ æœ€å¤§å¯åˆ†æ€§ï¼šæœ€å¤§æ–¹å·®ç†è®º æ ·æœ¬ç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½å°½é‡åˆ†å¼€ï¼Œå³æŠ•å½±åçš„æ ·æœ¬ä¹‹é—´çš„æ–¹å·®è¦æœ€å¤§åŒ–ã€‚å‡è®¾ç°åœ¨æœ‰äº”ä¸ªæ ·æœ¬åˆ†å¸ƒåœ¨x-yåæ ‡ç³»ä¸‹ï¼Œå¦‚ä¸‹å›¾ æŠ•å½±çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š çº¢è‰²ç‚¹è¡¨ç¤ºæŸä¸€ä¸ªæ ·æœ¬ç‚¹$x^{(i)}$ï¼Œè“è‰²ç‚¹è¡¨ç¤º $x^{(i)}$åœ¨è¶…å¹³é¢uä¸Šçš„æŠ•å½±ï¼Œåœ¨è¿™é‡Œuæ˜¯ç›´çº¿çš„æ–œç‡ï¼Œä¹Ÿæ˜¯ç›´çº¿çš„æ–¹å‘å‘é‡ï¼Œä¸”æ˜¯å•ä½å‘é‡ã€‚è“è‰²çš„ç‚¹$x^{(i)}$åœ¨uä¸Šçš„æŠ•å½±å¸¦ä½ ï¼Œç¦»åŸç‚¹çš„è·ç¦»æ˜¯($x^{(i)}$, u)ï¼Œå³$(x^{(i)})^Tu$æˆ–è€…$u^Tx^{(i)}$ï¼Œè€Œè“è‰²ç‚¹åˆ°åŸç‚¹çš„è·ç¦»ï¼Œå°±æ˜¯åœ¨è“è‰²åæ ‡è½´ä¸Šçš„åæ ‡ã€‚ç”±äºè¿™äº›æ ·æœ¬ç‚¹çš„æ¯ä¸€ç»´çš„ç‰¹å¾å‡å€¼éƒ½æ˜¯0ï¼ˆä¹‹å‰å½’ä¸€åŒ–è¿‡ï¼‰ï¼Œå› æ­¤æŠ•å½±åˆ°uä¸Šçš„æ ·æœ¬ç‚¹çš„å‡å€¼ä»ç„¶æ˜¯0. å¦‚æœæˆ‘ä»¬å°†è¿™è¿™äº”ä¸ªæ ·æœ¬æŠ•å½±åˆ°æŸä¸€ä¸ªç»´åº¦ä¸Šï¼Œå³ä»äºŒç»´æŠ•å½±åˆ°ä¸€ç»´ä¸Šã€‚è¿™é‡Œä¸ºäº†æ¯”è¾ƒä¸åŒæ•ˆæœï¼Œåˆ†åˆ«é€‰äº†ä¸€æ¡è¿‡åŸç‚¹çš„ç›´çº¿è¡¨ç¤ºã€‚å¦‚ä¸‹å›¾ ä»ä¸Šé¢ä¸¤å¹…å›¾å¯ä»¥çœ‹å‡ºï¼Œå·¦è¾¹çš„å›¾ä¸Šçš„æ ·æœ¬é—´çš„è·ç¦»æ¯”å³è¾¹çš„è¦å¤§ï¼Œå³å·¦è¾¹æŠ•å½±åçš„æ ·æœ¬ç‚¹ä¹‹é—´çš„æ–¹å·®æœ€å¤§ï¼Œä¸ºï¼š æœ€åçš„ä¸€ä¸ªç­‰å¼ä¸­ï¼Œä¸­é—´é‚£éƒ¨åˆ†æ°å¥½æ˜¯æ±‚åæ–¹å·®çš„å…¬å¼(è¿™é‡Œç”¨mï¼Œè€Œä¸æ˜¯m-1)ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨$\\lambda$è¡¨ç¤ºå·¦è¾¹éƒ¨åˆ†ï¼Œå³æ ·æœ¬ç‚¹ä¹‹é—´çš„æ–¹å·®: \\lambda = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2ç”¨$\\sum$è¡¨ç¤ºä¸­é—´éƒ¨åˆ†ï¼š \\sum = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2é‚£ä¹ˆä¸Šé¢çš„å¼å­å¯ä»¥æ”¹å†™ä¸ºï¼š \\lambda = u^T\\sum uä¹‹å‰æåˆ°äº†uæ˜¯è¶…å¹³é¢çš„å•ä½å‘é‡ï¼Œå³æœ‰$u^Tu = 1$ï¼Œå°†ä¸Šé¢å·¦å³ä¸¤è¾¹çš„å¼å­éƒ½ä¹˜ä»¥uï¼Œå¯å¾—ï¼š u\\lambda = \\lambda u= uu^T\\sum u = \\sum uå³$\\sum u = \\lambda u$ï¼Œæ‰€ä»¥$\\lambda$å°±æ˜¯$\\sum$çš„ç‰¹å¾å€¼ï¼Œuæ˜¯ç‰¹å¾å‘é‡ã€‚æœ€ä½³çš„æŠ•å½±ç›´çº¿æ˜¯ç‰¹å¾å€¼$\\lambda$æœ€å¤§æ—¶å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œå…¶æ¬¡æ˜¯$\\lambda$ç¬¬äºŒå¤§å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œä¾æ¬¡ç±»æ¨ã€‚ å› æ­¤åªéœ€è¦å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå¾—åˆ°çš„å‰kå¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å°±æ˜¯æœ€ä½³çš„kç»´æ–°ç‰¹å¾ï¼Œè€Œä¸”è¿™kç»´æ–°ç‰¹å¾æ˜¯æ­£äº¤çš„ã€‚å¾—åˆ°å‰kä¸ªuä»¥åï¼Œå°±å¯å°†æ ·ä¾‹$x^{(i)}$è¡¨ç¤ºä¸ºä¸‹é¢çš„æ–°æ ·æœ¬ï¼š å…¶ä¸­çš„ç¬¬jç»´å°±æ˜¯$x^{(i)}$åœ¨$u_j$ä¸Šçš„æŠ•å½±ã€‚é€šè¿‡é€‰å–æœ€å¤§çš„kä¸ªuï¼Œä½¿å¾—æ–¹å·®è¾ƒå°çš„ç‰¹å¾ï¼ˆå¦‚å™ªå£°ï¼‰è¢«ä¸¢å¼ƒã€‚ æœ€è¿‘é‡æ„æ€§ï¼šæœ€å°å¹³æ–¹è¯¯å·®ç†è®º å‡è®¾ç°åœ¨é€‰çš„è¶…å¹³é¢æ˜¯L(è¿™ä¸ªä¾‹å­ä¸­æ˜¯ç›´çº¿)ï¼Œé‚£ä¹ˆæŸä¸€æ ·æœ¬$x_k$åˆ°Lçš„å‚ç›´è·ç¦»ä¸ºdâ€™ï¼Œé‚£ä¹ˆæœ‰æ‰€æœ‰ç‚¹åˆ°è¯¥ç›´çº¿çš„è·ç¦»ä¸ºï¼š \\sum_{k=1}^n||(x_k' - x_k)||^2ä¸Šé¢è¿™ä¸ªå…¬å¼ç§°ä½œæœ€å°å¹³æ–¹è¯¯å·®(Least Squared Erroe)","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"PCA","slug":"PCA","permalink":"http://chenson.cc/tags/PCA/"}]},{"title":"Machine Learning - æ”¯æŒå‘é‡æœº","date":"2017-06-16T09:21:11.000Z","path":"2017/06/16/Machine-Learning-æ”¯æŒå‘é‡æœº/","text":"1. Margins Logistic Regression â€‹åœ¨logistic regressionä¸­, å¦‚æœæˆ‘ä»¬è¦åˆ¤æ–­ä¸€ä¸ªç‚¹æ˜¯å±äº0è¿˜æ˜¯1æ¦‚ç‡æ˜¯Î¸ p(y=1 | x; \\theta) \\\\ h_\\theta(x) = g(\\theta^T x) logistic regressionå’ŒSVMçš„åŒºåˆ« ç”±ä¸Šå›¾å¯çŸ¥, Cæ˜¯éå¸¸é è¿‘Boundaryçš„, è€ŒAæ˜¯ç¦»Boundaryæœ€è¿œçš„ç‚¹. æ‰€ä»¥æˆ‘ä»¬éå¸¸æœ‰ä¿¡å¿ƒçš„è¯´Aæ˜¯å±äº+, è€ŒCæ¯”è¾ƒæœ‰å¯èƒ½å±äº+. æ‰€ä»¥å¯¹äºé‚£äº›è·ç¦»è¾¹ç•Œæ¯”è¾ƒè¿‘çš„ç‚¹æ‰æ˜¯æˆ‘ä»¬éœ€è¦é‡ç‚¹è€ƒè™‘çš„. è€Œè¿™ä¹Ÿæ­£æ˜¯logistic regressionå’ŒSVMä¹‹é—´çš„åŒºåˆ«. logistic regressionè€ƒè™‘å…¨å±€(å¦‚ä½•è€ƒè™‘å…¨å±€ï¼Ÿansï¼šRMSEæœ€å°å€¼)çº¢çº¿ SVMè€ƒè™‘å±€éƒ¨(æœ€å°é—´éš”è¦å¤§äºå¤šå°‘)ç»¿çº¿ 2. SVM ç¬¦å·è¯´æ˜ åœ¨Logsticä¸­, å¯¹äºäºŒåˆ†ç±»é—®é¢˜, y âˆˆ {0, 1} åœ¨SVMä¸­, å¯¹äºäºŒåˆ†ç±»é—®é¢˜, y âˆˆ {-1, +1}, ä¸”é‡æ–°å®šä¹‰å…¬å¼ h_{w,b} (x) = g(w^Tx + b) \\\\ g(z) = 1 \\space (z\\geq 0) \\\\ g(z) = -1 \\space (z < 0) å‡½æ•°é—´éš” Functional Margins ç¬¬ä¸€ä¸ªå…¬å¼è¡¨ç¤ºçš„æ˜¯æŸä¸ªæ ·æœ¬çš„å‡½æ•°é—´éš”. ç¬¬äºŒä¸ªå…¬å¼è¡¨ç¤ºçš„æ˜¯å…¨å±€çš„å‡½æ•°é—´éš”, ä¹Ÿå°±æ˜¯å‡½æ•°é—´éš”ä¸ºæ‰€æœ‰æ ·æœ¬ä¸­å‡½æ•°é—´éš”æœ€å°çš„é‚£ä¸ªå‡½æ•°é—´éš”å†³å®š. ä¹‹å‰ä½¿ç”¨æ­£è´Ÿ1æ¥è¡¨ç¤º y, æ‰€ä»¥è®¡ç®—å‡ºæ¥çš„è·ç¦»éƒ½æ˜¯ä¸€ä¸ªéè´Ÿæ•°, ä¸”è¿™ä¸ªæ•°å€¼çš„å¤§å°è¡¨ç¤ºäº†å¯¹äºé¢„æµ‹ç»“æœçš„confidence. y å€¼è¶Šæ¥è¿‘1, è¡¨ç¤ºå¯¹è¿™ä¸ªåˆ¤æ–­è¶Šè‚¯å®š. \\hat {\\gamma}^{(i)} = y^{(i)} (w^Tx^{(i)} + b) \\\\ \\hat \\gamma = \\min_{i=1,...,m} \\hat \\gamma^{(i)}å‡½æ•°é—´éš”è¶Šå¤§, ä»£è¡¨äº†æˆ‘ä»¬å¯¹äºåˆ†ç±»çš„ç»“æœéå¸¸çš„è‚¯å®š, æ‰€ä»¥å¸Œæœ›å‡½æ•°é—´éš”è¶Šå¤§è¶Šå¥½, ä½†éœ€è¦å¯¹è¿™ä¸ªé—´éš”åŠ ä¸Šä¸€äº›é™åˆ¶æ¡ä»¶(åé¢å…·ä½“è®²)æ‰è¡Œ. å› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ä¸æ”¹å˜è¿™ä¸ªè¶…å¹³é¢çš„æƒ…å†µä¸‹, åªè¦æˆæ¯”ä¾‹å¢åŠ wå’Œb, å°±èƒ½è®©å‡½æ•°é—´éš”ä»»æ„çš„å¤§. å‡ ä½•è·ç¦» Geometrix Margins ä¸‹å›¾ä¸­, å¦‚æœæˆ‘ä»¬çŸ¥é“Bç‚¹æ‰€åœ¨çš„è¶…å¹³é¢(separating hyperplane)çš„è§£æå¼, ä»»ä½•å…¶ä»–ç‚¹åˆ°è¯¥é¢çš„è·ç¦»éƒ½å¯ä»¥ç”¨ä¸Šé¢å®šä¹‰è¿‡çš„å‡½æ•°é—´éš”æ¥è¡¨ç¤º. å†³ç­–è¾¹ç•Œï¼šw^Tx + b = 0wæ˜¯è¶…å¹³é¢çš„æ³•å‘é‡, å‚ç›´äºå†³ç­–è¾¹ç•Œ, ä¹Ÿå°±æ˜¯è¿™ä¸ªè¶…å¹³é¢. è‹¥Bæ˜¯Aåœ¨åˆ†å‰²é¢ä¸Šçš„æŠ•å½± (ABå‚ç›´äºè¶…å¹³é¢), é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è®¡ç®—Aåˆ°è¶…å¹³é¢çš„è·ç¦» Î³ . å‡è®¾Aç‚¹æ˜¯$x_i$, é‚£ä¹ˆBç‚¹ä¸ºï¼š \\overrightarrow {OB} = \\overrightarrow {OA} - \\overrightarrow {BA} \\\\ x^{(i)} - \\gamma^{(i)} Â· \\frac w {||w||}å› ä¸ºBç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Š, æ‰€ä»¥æˆ‘ä»¬å°†è¿™ä¸ªç‚¹å¸¦å›è¶…å¹³é¢å¾—åˆ°ï¼š w^T(x^{(i)} - \\gamma^{(i)} Â· \\frac w {||w||}) + b = 0é€šè¿‡ä¸Šé¢çš„å¼å­, å¯ä»¥è§£å‡ºÎ³ï¼š w^Tx^{(i)} - \\gamma^{(i)} Â· \\frac {w^Tw} {||w||} + b = 0 \\\\ w^Tx^{(i)} + b = \\gamma^{(i)} ||w|| \\\\ \\gamma^{(i)} = (\\frac w {||w||})^Tx^{(i)} + \\frac b {||w||}åŠ ä¸Šå‰é¢çš„$y^{(i)}$, äºæ˜¯æˆ‘ä»¬å°±èƒ½å¾—åˆ°äº†å‡ ä½•é—´éš”ï¼š \\gamma^{(i)} = y^{(i)}(\\frac {w^T} {||w||}x^{(i)} + \\frac b {||w||})é€šè¿‡ä¸Šé¢çš„å¼å­, å‘ç°å½“||w|| = 1æ—¶, å‡ ä½•é—´éš”å°±æ˜¯å‡½æ•°é—´éš”. è¿™ä¸ªæ—¶å€™, å¦‚æœä»»æ„æ”¾å¤§||w||, å‡ ä½•é—´éš”æ˜¯ä¸ä¼šæ”¹å˜çš„. å› ä¸º||w||ä¹Ÿä¼šéšç€è¢«æ”¾å¤§. å‡ ä½•é—´éš”ä¸å‡½æ•°é—´éš”çš„å…³ç³»ä¸ºï¼š \\gamma^{(i)} = \\frac {\\hat \\gamma^{(i)}} {||w||}å¯¹äºæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬, å‡ ä½•é—´éš”ä¸ºï¼š \\gamma = \\min_{i=1,...,m} \\gamma^{(i)} 2. é—´éš”æœ€å¤§åŒ–æ ¹æ®ä¸Šä¸€èŠ‚, æˆ‘ä»¬å¯ä»¥æ±‚å‡ºå‡ ä½•é—´éš”Î³. å¦‚æœç°åœ¨éœ€è¦æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢S, ä½¿å¾—ç¦»è¶…å¹³é¢æœ€è¿‘çš„ç‚¹çš„å‡ ä½•é—´éš”è¶Šå¤§è¶Šå¥½, å¯ä»¥ç”¨ä¸‹åˆ—ä¼˜åŒ–é—®é¢˜è¡¨ç¤ºï¼š \\max_{w,b} \\gamma \\\\ s.t. \\space y_i(\\frac w {||w||}Â·x_i + \\frac b {||w||}) \\geq \\gamma, \\space i = 1, 2, ..., Nå³æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¶…å¹³é¢(w, b)å…³äºè®­ç»ƒæ•°æ®é›†çš„å‡ ä½•é—´éš”Î³, çº¦æŸæ¡ä»¶è¡¨ç¤ºè¶…å¹³é¢(w,b)å…³äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ç‚¹çš„å‡ ä½•é—´éš”è‡³å°‘æ˜¯Î³. è€ƒè™‘åˆ°å‡ ä½•é—´éš”ä¸å‡½æ•°é—´éš”çš„å…³ç³»å¼, å¯ä»¥å°†è¿™ä¸ªé—®é¢˜æ”¹å†™æˆå‡½æ•°é—´éš”æ¥è¡¨ç¤º, å³ï¼š \\max_{w,b} \\frac {\\hat \\gamma} {||w||} \\\\ s.t. \\space y_i(wÂ·x_i + b) \\geq \\hat \\gamma, \\space i = 1, 2, ..., Nä¸Šé¢å¼å­ä¸­, å‡½æ•°é—´éš”çš„å–å€¼å¹¶ä¸ä¼šå½±å“åˆ°æœ€ä¼˜åŒ–é—®é¢˜çš„è§£. äº‹å®ä¸Š, å‡è®¾å°†wå’ŒbæŒ‰æ¯”ä¾‹æ”¹å˜ä¸ºÎ»wå’ŒÎ»b, è¿™æ—¶å‡½æ•°é—´éš”ä¹Ÿä¼šè¢«å½“å¤§Î»å€, æ‰€ä»¥è¿™ä¸ªå¯¹ä¸Šé¢çš„æœ€ä¼˜åŒ–é—®é¢˜çš„ä¸ç­‰å¼çº¦æŸæ˜¯æ²¡æœ‰å½±å“çš„, å¯¹ç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–ä¹Ÿæ²¡æœ‰å½±å“. è¿™æ ·, å¯ä»¥å»å‡½æ•°é—´éš”Î³=1, ä»£å…¥åˆ°ä¸Šé¢çš„æœ€ä¼˜åŒ–é—®é¢˜, æ³¨æ„åˆ°æœ€å¤§åŒ– $\\frac 1 {||w||}$å’Œæœ€å°åŒ– $\\frac 1 2$ $||w||^2$æ˜¯ç­‰ä»·çš„, ä¹Ÿæ˜¯å°±å¾—åˆ°å¦é—¨çš„çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœºå­¦ä¹ çš„æœ€ä¼˜åŒ–é—®é¢˜ï¼š \\min_{w,b} \\frac 1 2 ||w||^2 \\\\ s.t. \\space y_i(wÂ·x_i + b) - 1 \\geq 0è¿™ä¸ªæ—¶å€™æˆ‘ä»¬çš„é—®é¢˜å°±è½¬åŒ–æˆäº†åœ¨çº¿æ€§çº¦æŸä¸‹çš„äºŒæ¬¡è§„åˆ’, å¯ä»¥ä½¿ç”¨äºŒæ¬¡è§„åˆ’çš„è½¯ä»¶æ¥è§£å†³è¿™ä¸ªä¼˜åŒ–é—®é¢˜,, ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„æœ€ä¼˜é—´éš”åˆ†ç±»å™¨.å®é™…ä¸Š, æˆ‘ä»¬æœ‰æ›´å¥½çš„åŠæ³•å»è§£è¿™ä¸ªä¼˜åŒ–é—®é¢˜. ä½†åœ¨è¿™ä¹‹å‰, æˆ‘ä»¬éœ€è¦è¡¥å……ä¸€ä¸‹å…¶ä»–çš„ç›¸å…³çŸ¥è¯†. 3. æ‹‰æ ¼æœ—æ—¥å¯¹å¶åœ¨çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ä¸­, å¸¸å¸¸åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§(Lagrange dulity)è®²åŸå§‹é—®é¢˜è½¬æ¢ä¸ºå¯¹å¶é—®é¢˜, é€šè¿‡è§£åº¦å¶é—®é¢˜è€Œå¾—åˆ°çš„åŸå§‹é—®é¢˜çš„è§£. è¯¥æ–¹æ³•åº”ç”¨åœ¨è®¸å¤šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸­, ä¾‹å¦‚æœ€å¤§ç†µæ¨¡å‹å’Œæ”¯æŒå‘é‡æœº. åŸå§‹é—®é¢˜ \\min_w f(w) \\\\ s.t. \\space h_i(w) = 0, \\space i = 1,2,...,lä¸‹é¢æ˜¯çº¦æŸæ¡ä»¶, ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•, å°†é—®é¢˜è½¬æ¢ä¸ºï¼š L(w, b) = f(w) + \\sum_{i=1}^l\\beta_ih_i(w)è¿™é‡Œé¢, Î²iä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜å­(Lagrange Multipliers). ç„¶åä»¤åå¯¼ä¸º0æ¥è§£å¾—wå’ŒÎ²ï¼š \\frac {âˆ‚L} {âˆ‚w_i} = 0 \\\\ \\frac {âˆ‚L} {âˆ‚\\beta_i} = 0æ›´åŠ å¹¿æ³›çš„çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ï¼š \\min_w f(w) \\\\ s.t. \\space g_i(x) â‰¤ 0, \\space i = 1, 2, ...,k \\\\ \\space \\space \\space \\space \\space \\space \\space h_i(w) = 0, \\space i = 1,2,...,læ‰€ä»¥æˆ‘ä»¬å®šä¹‰å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å…¬å¼(Generalized Lagrangian)ä¸ºï¼š L(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^k \\alpha_ig_i(w) + \\sum_{i=1}^l \\beta_ih_i(w)å…¶ä¸­, Î±i,Î²iä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜å­(Lagrange Multipliers). ç°åœ¨æˆ‘ä»¬å®šä¹‰ï¼š \\theta_p(w) = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} L (w, \\alpha, \\beta) â€‹ å…¶ä¸­ä¸‹è¡¨Pä»£è¡¨äº†â€œprimalâ€. è‹¥ä¸¤ä¸ªçº¦æŸæ¡ä»¶å½“ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå¾—ä¸åˆ°æ»¡è¶³çš„æ—¶å€™, åˆ™å¯ä»¤Î±iä¸ºæ— ç©·å¤§æˆ–Î²iä¸ºæ— ç©·å¤§ä½¿å¾—ï¼š \\theta_P = f(w) \\space \\space \\space (wæ»¡è¶³åŸå§‹é—®é¢˜çš„çº¦æŸ)\\\\ \\theta_p = \\infty \\space \\space \\space (å…¶ä»–)\\â€‹ å¯¹äºæ»¡è¶³åŸå§‹çº¦æŸçš„wæ¥è¯´, Î¸pä¸åŸå§‹é—®é¢˜ä¸­çš„ç›®æ ‡å‡½æ•°ç›¸åŒ. å¯¹äºè¿ååŸå§‹çº¦æŸé—®é¢˜çš„wæ¥è¯´Î¸p = âˆ â€‹ å¦‚æœè€ƒè™‘æœ€å°åŒ–ï¼š â€‹ \\min_w\\theta_p(w) = \\min_w \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0 } L(w, \\alpha, \\beta)â€‹ å®ƒæ˜¯ä¸åŸå§‹æœ€ä¼˜åŒ–é—®é¢˜ç›¸ç­‰ä»·çš„, å³å®ƒä»¬æœ‰ç›¸åŒçš„è§£. å¯¹å¶é—®é¢˜ ç°åœ¨çœ‹å¦å¤–ä¸€ä¸ªé—®é¢˜ï¼š \\theta_D(\\alpha, \\beta) = \\min_w L(w, \\alpha, \\beta)å…¶ä¸­ä¸‹è¡¨Dä»£è¡¨äº†å¯¹å¶(dual). åœ¨åŸå§‹é—®é¢˜ä¸­, æˆ‘ä»¬å…ˆæœ€å¤§åŒ–å…³äºÎ±å’ŒÎ²çš„å‡½æ•°, å†æœ€å°åŒ–å…³äºwçš„å‡½æ•°ï¼› è€Œå¯¹å¶é—®é¢˜ä¸­, æˆ‘ä»¬å…ˆæœ€å°åŒ–å…³äºwçš„å‡½æ•°, åœ¨æœ€å¤§åŒ–å…³äºÎ±å’ŒÎ²çš„å‡½æ•°ï¼š \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\theta_D(\\alpha, \\beta) = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\min_w L(w, \\alpha, \\beta)å®ƒä»¬å”¯ä¸€çš„åŒºåˆ«å°±åœ¨äºminå’Œmaxçš„é¡ºåºä¸åŒ. æˆ‘ä»¬ä»¤: d^{\\*} = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\min_w L(w, \\alpha, \\beta) \\leq \\min_w \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} L(w, \\alpha, \\beta) = p^{\\*}ä¹Ÿå°±æ˜¯è¯´, åœ¨æŸç§æƒ…å†µä¸‹, ä¼šæœ‰d = p. è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±å¯æŠŠæ±‚åŸå§‹é—®é¢˜è½¬åŒ–æˆæ±‚å¯¹å¶é—®é¢˜. å‡è®¾få’Œgéƒ½æ˜¯å‡¸å‡½æ•°(convex function), hæ˜¯ä»¿å°„çš„, å¹¶ä¸”å­˜åœ¨wæ˜¯å¯¹æ‰€æœ‰çš„i, èƒ½å¤Ÿä½¿å¾—: g_i(w) < 0ä¸Šè¿°å‡è®¾æ¡ä»¶ä¸‹, ä¸€å®šå­˜åœ¨w*, Î±*å’ŒÎ²*ä½¿å¾—w*æ˜¯åŸå§‹é—®é¢˜çš„è§£. Î±*å’ŒÎ²*æ˜¯å¯¹å¶é—®é¢˜çš„è§£. å¹¶ä¸”è¿˜æœ‰ p = d = L(w, Î±\\, Î²*). w*, Î±*å’ŒÎ²*æ»¡è¶³KKTæ¡ä»¶(Karush-Kuhn-Tucker conditions)ï¼š å¦‚æœå­˜åœ¨æ»¡è¶³KKTæ¡ä»¶çš„w*,Î±*,Î²*, åˆ™åŸå§‹æ¡ä»¶ä¸å¯¹å¶é—®é¢˜ä¸€å®šæœ‰è§£. å…¬å¼(5)åˆç§°ä¹‹ä¸ºKKTå¯¹å¶äº’è¡¥æ¡ä»¶, è¿™ä¸ªæ¡ä»¶è¡¨æ˜å¦‚æœ Î±* &gt; 0, é‚£ä¹ˆå°±æœ‰g(w*) = 0. å³çº¦æŸæ¡ä»¶ g(w*) &lt;= 0 æ¿€æ´», wå¤„äºå¯è¡ŒåŸŸçš„è¾¹ç•Œä¸Š. è€Œå…¶ä»–è°“è¯­å¯è¡ŒåŸŸå†…éƒ¨g(w*) &lt; 0çš„ç‚¹éƒ½ä¸èµ·çº¦æŸä½œç”¨, å¯¹åº”çš„Î±* = 0. ä¸ºä»€ä¹ˆè¦å¼•å…¥å¯¹å¶ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ ç”¨æ™®é€šçš„åŸºäºæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦å’Œçš„å‡å€¼çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆBGDï¼‰æ˜¯è¡Œä¸é€šçš„ï¼ŒåŸå› åœ¨äºæˆ‘ä»¬çš„æŸå¤±å‡½æ•°é‡Œé¢æœ‰é™å®šï¼Œåªæœ‰è¯¯åˆ†ç±»çš„Mé›†åˆé‡Œé¢çš„æ ·æœ¬æ‰èƒ½å‚ä¸æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç”¨æœ€æ™®é€šçš„æ‰¹é‡æ¢¯åº¦ä¸‹é™(ä¸ºä»€ä¹ˆï¼Ÿï¼Ÿ), åªèƒ½é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æˆ–è€…å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMBGDï¼‰ã€‚ å¯¹å¶çš„ä¼˜åŠ¿ å¯¹å¶å½¢å¼å°†æƒé‡å‘é‡wè½¬åŒ–æˆå®ä¾‹$x_i$å’Œ$y_i$çš„çº¿æ€§ç»„åˆå½¢å¼ã€‚ä¸”å¯¹å¶æ˜¯ä»¥å†…ç§¯çš„å½¢å¼å‡ºç°çš„ï¼Œå¯ä»¥é¢„å…ˆä½¿ç”¨GramçŸ©é˜µå‚¨å­˜ï¼Œç”¨ç©ºé—´æ¢æ—¶é—´çš„æ–¹æ³•æé«˜è®¡ç®—æ•ˆç‡ã€‚åŸå§‹å½¢å¼æ¯æ¬¡åˆ¤æ–­è¯¯åˆ†ç±»ç‚¹æ—¶éƒ½éœ€è¦è¿›è¡Œå‘é‡ç‚¹ä¹˜è¿ç®—ã€‚ è¿™é‡ŒåŒæ—¶ä¹Ÿä¸ºåé¢å¼•å…¥æ ¸å‡½æ•°åšä¼ç¬”ï¼Œå› ä¸ºæ„ŸçŸ¥æœºæ˜¯ç¥ç»ç½‘ç»œå’Œæ”¯æŒå‘é‡æœºçš„åŸºç¡€ã€‚ 4. æœ€ä¼˜é—´éš”åˆ†ç±»å™¨æ ¹æ®ä¸Šé¢çš„å†…å®¹, å›é¡¾SVMçš„é—®é¢˜ \\min_{\\gamma,w,b} \\frac 1 2 ||w||^2 \\\\ s.t. \\space y^{(i)}(w^Tx^{(i)} + b) \\geq 1 \\æ ¹æ®æ‹‰æ ¼æœ—æ—¥å¯¹å¶é—®é¢˜, ä¿®æ”¹çº¦æŸæ¡ä»¶ä¸º g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0é‚£ä¹ˆå°±å’Œæ‹‰æ ¼æœ—æ—¥å…¬å¼æ˜¯ä¸€æ ·çš„äº†. æ ¹æ®ä¸Šé¢çš„KTTæ¡ä»¶å¯çŸ¥åªæœ‰å‡½æ•°é—´éš”æ˜¯1, çº¿æ€§çº¦æŸå¼å‰é¢çš„ç³»æ•°å¤§äº0, g(w)=0, å…¶ä»–çš„ä¸åœ¨çº¿ä¸Šçš„ç‚¹(g(w)&lt;0), æå€¼ä¸ä¼šåœ¨ä»–ä»¬æ‰€åœ¨çš„èŒƒå›´å†…å–å¾—, å› æ­¤å‰é¢çš„ç³»æ•°ç­‰äº0. è€ƒè™‘ä¸‹å›¾, æœ€å¤§é—´éš”åˆ†ç±»è¶…å¹³é¢ä¸ºå®çº¿ï¼š å…¶ä¸­ä¸€ä¸ªæ­£æ ·æœ¬å’Œä¸¤ä¸ªè´Ÿæ ·æœ¬æ­£å¥½åœ¨å¹³è¡Œäºåˆ†ç±»è¶…å¹³é¢çš„è™šçº¿ä¸Š, åªæœ‰è¿™ä¸‰ä¸ªæ ·æœ¬å¯¹åº”çš„ Î±i&lt;0, å…¶ä»–æ ·æœ¬å¯¹åº”çš„ Î±i=0. è¿™ä¸‰ä¸ªæ ·æœ¬å°±å«åšæ”¯æŒå‘é‡æœº. ä»è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹å‡º, æ”¯æŒå‘é‡çš„ä¸ªæ•°è¿œè¿œå°äºé›†è®­é›†çš„å¤§å°. ç°åœ¨æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]æ‰€ä»¥æ¥ä¸‹å»çš„ä»»åŠ¡å°±æ˜¯æ±‚è§£å¯¹å¶é—®é¢˜, æ ¹æ®ä¸Šé¢çš„çŸ¥è¯†, æœ‰ï¼š d* = \\max_{\\alpha:\\alpha_i \\geq 0} \\theta_D(\\alpha) = \\max_{\\alpha:\\alpha_i \\geq 0} \\min_{w, b} L(w, b,\\alpha)é¦–å…ˆ, æ±‚L(w,b,Î±)å…³äºw, b çš„æœ€å°å€¼. ä»¤åå¯¼æ•°ä¸º0ï¼š \\frac {âˆ‚L} {âˆ‚w} = w - \\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)} = 0 \\\\ \\frac {âˆ‚L} {âˆ‚b} = 0 - \\sum_{i=1}^m\\alpha_iy^{(i)}= 0å¯å¾—ï¼š w = \\sum_{i=1}^m\\alpha_i y^{(i)} x^{(i)} \\\\ \\frac {âˆ‚} {âˆ‚b} L(w, b, \\alpha) = \\sum_{i=1}^m\\alpha_i y^{(i)} = 0å°†æ±‚å¾—çš„wå¸¦å›æ‹‰æ ¼æœ—æ—¥å‡½æ•°L(w,b,Î±), å¯å¾—åˆ°ï¼š L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1] è®¡ç®—å‡ºäº†min L(w,b,Î±), ä¾¿å¯ä»¥ç»§ç»­è¿›è¡Œmaxæ“ä½œ, å³ï¼š å¯ä»¥è¯æ˜è¯¥ä¼˜åŒ–é—®é¢˜æ»¡è¶³KKTæ¡ä»¶, æ±‚å¾—$Î±^{*}_i$ä¹‹å(åé¢å°†å¦‚ä½•æ±‚è§£), å¯é€šè¿‡: w = \\sum_{i=1}^m \\alpha_i y^{(i)}x^æ±‚å¾—w*, æœ€åé€šè¿‡ä¸‹é¢å¼å­æ±‚å¾—b* b^{\\*} = -\\frac {\\max_{i:y^{(i)}=-1}w^{\\*T}x^{(i)} + \\min_{i:y^{(i)} = 1}w^{\\*T}x^{(i)}}{2}å½“æ±‚å‡ºæ‰€æœ‰çš„å‚æ•°, å°±å¯ä»¥é€šè¿‡$w^T$x + b æ¥è¿›è¡Œåˆ†ç±»äº†ï¼š w^T + b = (\\sum_i^m \\alpha_iy_ix_i)^T x + b = \\sum_i^m \\alpha_iy_iâŸ¨x_i, xâŸ© + bé€šè¿‡ä¸Šé¢å¼å­å‘ç°, ç°åœ¨æ–°æ¥ä¸€ä¸ªæ–°æ•°æ®, åªéœ€è¦è®¡ç®—å®ƒä¸è®­ç»ƒæ ·æœ¬çš„å†…ç§¯å³å¯. å¹¶é€šè¿‡å‰é¢çš„KKTæ¡ä»¶æˆ‘ä»¬çŸ¥é“, åªæœ‰é™¤äº†æ”¯æŒå‘é‡çš„é‚£äº›åŸæœ¬, éƒ½æœ‰$Î±_i$ = 0. æ‰€ä»¥, æˆ‘ä»¬åªéœ€è¦å°†æ–°æ ·æœ¬ä¸æ”¯æŒå‘é‡æœºåšå†…ç§¯è¿ç®—, å³å¯æ±‚å‡º$w^T$x + b Example å‡è®¾è¿™é‡Œæœ‰ä¸‰ä¸ªæ ·æœ¬ç‚¹ï¼Œæ­£æ ·æœ¬ç‚¹x1=(3,3)^T, x2=(4,3)^T, è´Ÿæ ·æœ¬ç‚¹æ˜¯x3=(1,1)^Tï¼Œè¯•ç”¨æ„ŸçŸ¥æœºå­¦ä¹ ç®—æ³•å¯¹å¶å½¢å¼æ±‚æ„ŸçŸ¥æœºæ¨¡å‹ã€‚ ANSWER å–Î±i = 0ï¼Œè¿™é‡Œi=1ï¼Œ2ï¼Œ3ï¼Œb=0ï¼Œn=1 è®¡ç®—GramçŸ©é˜µ G= \\begin {bmatrix} ||x_1||^2 & x_1Â·x2 & x_1Â·x3 \\\\ x2Â· x1 & ||x2||^2 & x_2Â·x3 \\\\ ||x_3Â· x1 & x_3Â·x2 & ||x_3||^2 \\end{bmatrix} = \\begin {bmatrix} 18 & 21 & 6 \\\\ 21 & 25 & 7 \\\\ 6 & 7 & 2 \\end{bmatrix} è¯¯åˆ†æ¡ä»¶ y_i(\\sum_{j=1}^N\\alpha_jy_jx_j Â·x_i + b) \\leq 0å‚æ•°æ›´æ–° \\alpha_i \\leftarrow \\alpha_i + 1 \\\\ b \\leftarrow b + y_iâ€‹ è¿­ä»£è¿‡ç¨‹å¦‚ä¸‹ï¼Œç»“æœåˆ—äºä¸‹è¡¨ | k | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 || :â€”: | :â€”: | :â€”: | :â€”: | :â€”: | :â€”: | :â€”: | :â€”: | :â€”â€”: || | | x1 | x3 | x3 | x3 | x1 | x3 | x3 || Î±1 | 0 | 1 | 1 | 1 | 2 | 2 | 2 | 2 || Î±2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 || Î±3 | 0 | 0 | 1 | 2 | 2 | 3 | 4 | 5 || b | 0 | 1 | 0 | -1 | 0 | -1 | -2 | -3 | w, båˆ†åˆ«ä¸ºï¼š w = 2x_1 + 0x_2 - 5x_3 = (1, 1)^T \\\\ b = -3åˆ†ç¦»è¶…å¹³é¢ï¼š x^{(1)} + x^{(2)} - 3 = 0 5. Kernelsåœ¨ä¹‹å‰çš„çº¿æ€§å›å½’çš„ç« èŠ‚ä¸­ï¼Œæœ‰æåˆ°è¿‡polynomial regressionã€‚å‡è®¾xæ˜¯æˆ¿å­çš„é¢ç§¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªç‰¹å¾$x$, $x^2$, $x^3$æ¥æ„é€ ä¸€ä¸ªä¸‰æ¬¡å¤šé¡¹å¼ã€‚ è¿™é‡Œé¢æœ‰ä¸¤ä¸ªæ¦‚å¿µè¦åŒºåˆ†ä¸€ä¸‹ã€‚xä¸ºåŸå…ˆæˆ¿å­çš„é¢ç§¯ï¼Œæ˜¯å±æ€§(attributes)ã€‚é€šè¿‡è¿™ä¸ªå±æ€§xæ˜ å°„å‡ºæ¥çš„$x$, $x^2$, $x^3$å«åšç‰¹å¾(features)ã€‚åœ¨è¿™é‡Œä½¿ç”¨Ï•æ¥è¡¨ç¤ºä»å±æ€§åˆ°ç‰¹å¾çš„ç‰¹å¾æ˜ å°„(featuer mapping)ã€‚æ¯”å¦‚ï¼š Ï•(x) = \\begin{bmatrix} x \\\\ x^2 \\\\ x^3 \\end{bmatrix}é‚£ä¹ˆåœ¨SVMä¸­ï¼Œå¦‚ä½•ä½¿ç”¨è¿™ç§ç‰¹å¾æ˜ å°„å‘¢ï¼Ÿ é€šè¿‡ä¸Šé¢çŸ¥è¯†ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ‰€æœ‰å‡ºç°$âŸ¨x^{(i)}, x^{(j)}âŸ©$ æ›¿æ¢ä¸º $âŸ¨Ï•^{(i)}, Ï•^{(j)}âŸ©$ çœ‹ä¸Šå»å¥½åƒæˆ‘ä»¬æ—¢åœ¨SVMä¸­ä½¿ç”¨äº†ç‰¹å¾æ˜ å°„, åˆè§£å†³äº†æ•°æ®åœ¨ä½ç»´ç©ºé—´ä¸­çº¿æ€§ä¸å¯åˆ†çš„æƒ…å†µ. ä½†æ˜¯, è¿™é‡Œæœ‰ä¸ªé—®é¢˜. å¦‚æœæˆ‘ä»¬é€šè¿‡ç‰¹å¾æ˜ å°„å¾—åˆ°çš„$Ï•(x)$æ˜¯ä¸€ä¸ªå¾ˆé«˜ç»´ç”šè‡³æ˜¯æ— ç©·ç»´çš„, é‚£ä¹ˆè®¡ç®—$âŸ¨Ï•(x^{(i)}),Ï•(x^{(j)})âŸ©$å°±ä¸æ˜¯é‚£ä¹ˆç°å®äº†, è®¡ç®—æ—¶é—´ä¼šå¾ˆä¹…. è¿™é‡Œæˆ‘ä»¬å°±è¦å¼•å‡ºä¸€ä¸ªå«kernelsçš„æ¦‚å¿µ, å‡è®¾ Q: zåœ¨è¿™é‡Œä»£è¡¨çš„ä»€ä¹ˆï¼Ÿ K(x, z) = (x^Tz)^2 \\space \\space \\space \\space \\space \\space \\space \\space x, z \\in R^bå±•å¼€K(x, z): K(x, z) = (\\sum_{i=1}^n x_iz_i) (\\sum_{j=1}^n x_iz_i) \\\\ = \\sum_{i=1}^n \\sum_{j=1}^n x_ix_j z_i z_j \\\\ = \\sum_{i,j=1}^n (x_ix_j) (z_iz_j)å±•å¼€åæˆ‘ä»¬å‘ç°ï¼ŒK(x, z)è¿˜å¯ä»¥å†™æˆ$K(x, z) = Ï•(x)^T Ï•(z)$ï¼Œå…¶ä¸­ï¼š Ï•(x) = \\begin{bmatrix} x _1x_1 \\\\ x _1x_2 \\\\x _1x_3 \\\\ x _2x_1 \\\\ x _2x_2 \\\\ x _2x_3 \\\\ x _3x_1 \\\\ x _3x_2 \\\\ x _3x_3 \\\\ \\end{bmatrix}åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ˜ å°„åç‰¹å¾çš„å†…ç§¯å’ŒåŸå§‹ç‰¹å¾çš„å†…ç§¯çš„å¹³æ–¹æ˜¯ç­‰ä»·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬åªéœ€è¦è®¡ç®—åŸå§‹ç‰¹å¾çš„å†…ç§¯å†è¿›è¡Œå¹³æ–¹å°±å¯ä»¥äº†ï¼Œå¹¶ä¸éœ€è¦å…ˆå¾—åˆ°æ˜ å°„åå†è®¡ç®—æ˜ å°„åç‰¹å¾çš„å†…ç§¯ã€‚è®¡ç®—åŸå§‹ç‰¹å¾å†…ç§¯çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n)ï¼Œè€Œè®¡ç®—æ˜ å°„ç‰¹å¾çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n^2)ã€‚ å†çœ‹å¦å¤–ä¸€ä¸ªkernels K(x, z) = (x^T z + c)^2 \\\\ = \\sum_{i,j=1}^n (x_ix_j) (z_iz_j) + \\sum_{i=1}^n (\\sqrt {scx_i})( \\sqrt {scx_j} ) + c^2åŒæ ·æˆ‘ä»¬ä¹Ÿå¯ä»¥æ”¹å†™ä¸Šé¢çš„å¼å­ æ‰€ä»¥å¹¿æ³›çš„æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ K(x, z) = (x^T z + c)^dè¿™ä¸ªkernelå°†nç»´çš„ç‰¹å¾æ˜ å°„ä¸º(d, n+d)ç»´ï¼Œå³è¿™é‡Œé¢å¯¹åº”çš„å¤šé¡¹å¼$x_{i1}$, $x_{i2}$, â€¦, $x_{ik}$æœ€å¤šåˆ°dç»´ã€‚å°½ç®¡ç©ºé—´ç»´åº¦ä¸ºO(n^d)ï¼Œä½†è®¡ç®—æ—¶é—´ä»ç„¶åªæ˜¯O(n)ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸éœ€è¦å°†æ˜ å°„åçš„ç‰¹å¾å…¨éƒ¨è®¡ç®—å‡ºæ¥å†è®¡ç®—å†…ç§¯ã€‚ ä½†å› ä¸ºè®¡ç®—çš„æ˜¯å†…ç§¯ï¼Œæœ‰IRä¸­çš„ä½™å¼¦ç›¸ä¼¼åº¦å¯å­©å­ï¼Œå¦‚æœxå’Œzçš„å‘é‡å¤¹è§’è¶Šå°ï¼Œé‚£ä¹ˆæ ¸å‡½æ•°çš„å€¼å°±è¶Šå¤§ã€‚åä¹‹å°±è¶Šå°ã€‚å› æ­¤æ ¸å‡½æ•°å€¼æ˜¯Ï•(x)å’ŒÏ•(z)ç›¸ä¼¼åº¦ã€‚ å†çœ‹å¦å¤–ä¸€ä¸ªå¾ˆå‡½æ•° K(x, z) = exp(\\frac {||x - z||^2} {2\\sigma^2})åœ¨è¿™ä¸ªæ ¸å‡½æ•°ä¸­ï¼Œå¦‚æœxå’Œzå¾ˆç›¸è¿‘ï¼Œåˆ™(||x-z||~= 0)ï¼Œé‚£ä¹ˆæ ¸å‡½æ•°å€¼ä¸º1. å¦‚æœç›¸å·®å¾ˆå¤§ï¼Œåˆ™(||x-z||&gt;&gt; 0), é‚£ä¹ˆå¾ˆå‡½æ•°çš„å€¼çº¦ç­‰äº0. ç”±äºè¿™ä¸ªæ ¸å‡½æ•°ç±»ä¼¼äºé«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤æˆä¸ºé«˜æ–¯æ ¸å‡½æ•°ï¼Œä¹Ÿå«åšå¾„å‘åŸºå‡½æ•°(Radial Basis Function ç®€ç§°RBF)ï¼Œå®ƒèƒ½å¤ŸæŠŠåŸå§‹ç‰¹å¾æ˜ å°„åˆ°æ— ç©·ç»´ã€‚ ç±»ä¼¼äºé«˜æ–¯å¾ˆå‡½æ•°ï¼Œæ¯”è¾ƒxå’Œzçš„ç›¸ä¼¼åº¦ï¼Œå¹¶æ˜ å°„åˆ°0~1ä¹‹é—´ã€‚logistic Regressionä¸­ï¼Œ sigmoidå‡½æ•°ä¹Ÿå¯ä»¥ï¼Œæ‰€ä»¥è¿˜æœ‰sigmoidæ ¸å‡½æ•°ã€‚ K(x, z) = tanh(\\beta Â· xz + b)ä¸‹é¢æœ‰å¼ å›¾è¯´æ˜åœ¨ä½ç»´çº¿æ€§ä¸å¯åˆ†æ—¶ï¼Œæ˜ å°„åˆ°é«˜ç»´åå°±å¯åˆ†äº†ï¼Œä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°ã€‚ åœ¨SVMä¸­ï¼Œå¯¹äºè®­ç»ƒæ ·æœ¬å­¦ä¹ å‡ºwå’Œbå‚æ•°åï¼Œå¯¹äºæ–°æ¥çš„æ ·æœ¬æˆ‘ä»¬åªéœ€è¦è®¡ç®—$w^Tx + b$æ¥åˆ¤æ–­ã€‚é‚£ä¹ˆåœ¨ä½¿ç”¨äº†æ ¸å‡½æ•°ä¹‹åï¼Œåˆ™éœ€è¦ç›¸åº”çš„æ”¹ä¸º$w^TÏ•(x) + b$ é‚£ä¹ˆæ˜¯éœ€è¦å…ˆè®¡ç®—å¥½Ï•(x)å†è¿›è¡Œé¢„æµ‹å‘¢ï¼Ÿå®é™…ä¸Šä¸éœ€è¦çš„, ä¹‹å‰è®¡ç®—è¿‡ w^Tx + b = (\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)})^T x + b \\\\ = \\sum_{i=1}^m \\alpha_i y^{(i)} âŸ¨x^{(i)}, xâŸ© + bæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦å°† âŸ¨x^{(i)}, xâŸ© \\\\ æ›¿æ¢ä¸º \\\\ K(x^{(i)}, x)6. æ ¸å‡½æ•°çš„æœ‰æ•ˆæ€§åˆ¤æ–­æ ¸å‡½æ•°çš„æœ‰æ•ˆæ€§ï¼Œå³åˆ¤æ–­æ˜¯å¦å­˜åœ¨Ï•, ä½¿å¾—ä¸‹é¢å¼å­æˆç«‹ K(x,z)=âŸ¨Ï•(x)Ï•(z)âŸ©å‡è®¾æˆ‘ä»¬æœ‰æ ¸Kå’Œmä¸ªè®­ç»ƒæ ·æœ¬{x(1),x(2),â€¦,x(m)}, å®šä¹‰ä¸€ä¸ª (mÃ—m) çš„çŸ©é˜µK K_{ij} = K(x^{(i)}, x^{(j)})å¦‚æœæ­¤æ—¶Kæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„kerneï¼Œé‚£ä¹ˆåˆ™æœ‰ï¼š K_{ij} = k(x^{(i)}, x^{(j)}) = Ï•(x^{(i)})Ï•(x^{(i)}) \\\\ = Ï•(x^{(j)})^T Ï•(x^{(i)}) = K(x^{(j)}, x^{(i)}) = K_{ji}å³Kæ˜¯å¯¹ç§°çŸ©é˜µã€‚ç°åœ¨æˆ‘ä»¬ç”¨Ï•k(x)ä¸æ¾³æ˜¯å‘é‡Ï•(x)çš„ç¬¬kä¸ªå…ƒç´ ï¼Œå¯¹ä»»æ„çš„å‘é‡zéƒ½æœ‰ï¼š ä»ä¸Šé¢çš„è¯æ˜æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼Œå¦‚æœKæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„kernelï¼Œé‚£ä¹ˆå¯¹äºåœ¨è®­ç»ƒé›†ä¸Šçš„æ ¸çŸ©é˜µKä¸€ç‚¹æ˜¯åŠæ­£å®šçš„ã€‚äº‹å®ä¸Šï¼Œè¿™ä¸ä»…ä»…æ˜¯ä¸ªå¿…è¦æ¡ä»¶ï¼Œä¹Ÿæ˜¯å……åˆ†æ¡ä»¶ã€‚æœ‰æ•ˆæ ¸ä¹Ÿå«åšMercer Kernel 7. Reference http://zhihaozhang.github.io/2014/05/11/svm3/ http://www.cnblogs.com/bourneli/p/4199990.html http://www.cnblogs.com/90zeng/p/Lagrange_duality.html","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.cc/tags/SVM/"}]},{"title":"Machine Learning - Ensemble Learning","date":"2017-06-13T04:27:45.000Z","path":"2017/06/13/Machine-Learning-Ensemble-Learning/","text":"1. é›†æˆå­¦ä¹  (Ensemble Learning)åœ¨å¯¹æ–°çš„æ•°æ®å®ä¾‹è¿›è¡Œåˆ†ç±»çš„æ—¶å€™ï¼Œé›†æˆå­¦ä¹ é€šè¿‡è®­ç»ƒå¥½å¤šä¸ªå­¦ä¹ å™¨ï¼ŒæŠŠè¿™äº›åˆ†ç±»å™¨çš„çš„åˆ†ç±»ç»“æœè¿›è¡ŒæŸç§ç»„åˆ (æ¯”å¦‚æŠ•ç¥¨) å†³å®šåˆ†ç±»ç»“æœï¼Œä»¥å–å¾—æ›´å¥½çš„ç»“æœï¼Œå°±æ˜¯æˆ‘ä»¬ç”Ÿæ´»ä¸­é‚£å¥è¯â€œä¸‰ä¸ªè‡­çš®åŒ é¡¶ä¸ªè¯¸è‘›äº®â€ï¼Œé€šè¿‡ä½¿ç”¨å¤šä¸ªå†³ç­–è€…å…±åŒå†³ç­–ä¸€ä¸ªå®ä¾‹çš„åˆ†ç±»ä»è€Œæé«˜åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ åŒè´¨é›†æˆ (Homogeneous) éœ€è¦æ˜¯åŒç§ç±»å‹ï¼Œæ¯”å¦‚å…¨éƒ¨æ˜¯å†³ç­–æ ‘æˆ–ç¥ç»ç½‘ç»œç­‰ï¼Œæ¯ä¸ªä¸ªä½“å­¦ä¹ å™¨ç§°ä¹‹ä¸ºåŸºå­¦ä¹ å™¨ (base learner)ï¼Œç›¸åº”çš„å­¦ä¹ ç®—æ³•ç§°ä¸ºåŸºå­¦ä¹ ç®—æ³• (base learning algorithm) å¼‚è´¨é›†æˆ (Heterogenous) å¯ä»¥ç”±ä¸åŒçš„å­¦æ ¡ç®—æ³•ç”Ÿæˆï¼Œè¿™æ—¶å€™å°±ä¸å†æœ‰åŸºå­¦ä¹ ç®—æ³•ï¼Œæ¯ä¸ªä¸ªä½“å­¦ä¹ å™¨ç§°ä¸ºç»„ä»¶å­¦ä¹ å™¨ (component learner) ä»¥ä¸‹æˆ‘ä»¬é‡ç‚¹è®¨è®ºåŒè´¨é›†æˆã€‚ 2. åˆ†ç±»å™¨çš„é€‰æ‹© å·®å¼‚æ€§ é—®é¢˜ï¼šå¦‚ä½•é€‰æ‹©/æ„å»ºå·®å¼‚æ€§çš„åŸºåˆ†ç±»å™¨ï¼Ÿ(ans: section 3) ç²¾åº¦ &gt; 0.5 ç²¾åº¦ç•¥é«˜äº50%çš„åˆ†ç±»å™¨ç§°ä¹‹ä¸ºå¼±å­¦ä¹ å™¨ (weak learner) é—®é¢˜ï¼šå¦‚ä½•æŠ•ç¥¨é€‰æ‹©å‡ºæœ€ä½³çš„é¢„æµ‹ï¼Ÿ(ans: section 4) å¦‚ä½•ç»„åˆ å‡è®¾åœ¨äºŒåˆ†ç±»å™¨ä¸­ï¼Œè¿™é‡Œæœ‰ä¸‰ä¸ªåˆ†ç±»å™¨åœ¨æµ‹è¯•ä¸‰ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªåˆ†ç±»å™¨çš„æ­£ç¡®ç‡éƒ½æ˜¯66%ï¼Œé‚£ä¹ˆç»„åˆå‡ºçš„ç»“æœå¤§è‡´å¯ä»¥åˆ†æˆä»¥ä¸‹å‡ ç§ é›†æˆæ€§èƒ½æå‡ | | test1 | test2 | test3 || :â€”: | :â€”-: | :â€”-: | :â€”-: || h1 | right | right | wrong || h2 | wrong | right | right || h3 | right | wrong | right | å¦‚æœåªæ˜¯ç®€å•æŠ•ç¥¨æ³•çš„è¯ï¼Œåœ¨æ¯ä¸ªtestä¸­ï¼Œæœ‰ä¸¤ä¸ªåˆ†ç±»å™¨æ˜¯å¯¹çš„ï¼Œä¸€ä¸ªæ˜¯é”™çš„ï¼Œé‚£ä¹ˆæŠ•ç¥¨å‡ºæ¥çš„ç»“æœæ˜¯å¯¹çš„ï¼Œæœ€ç»ˆç²¾åº¦å¯ä»¥è¾¾åˆ°100%ã€‚ é›†æˆä¸èµ·ä½œç”¨ | | test1 | test2 | test3 || :â€”: | :â€”-: | :â€”-: | :â€”-: || h1 | right | right | wrong || h2 | right | right | wrong || h3 | right | right | wrong | ä¸‰ä¸ªåˆ†ç±»å™¨å¯¹ä¸‰ä¸ªtestè¿›è¡Œé¢„æµ‹ï¼Œæ°å¥½ä¸‰ä¸ªåˆ†ç±»å™¨å¯¹test3çš„æƒ…å†µåˆ†ç±»éƒ½æ˜¯é”™çš„ï¼Œè€Œå¯¹test1å’Œtest2çš„ç»“æœéƒ½é¢„æµ‹æ­£ç¡®ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„æµ‹è¯•ç»“æœæ²¡æœ‰å½±å“ï¼Œéƒ½æ˜¯66%ã€‚ é›†æˆèµ·è´Ÿä½œç”¨ | | test1 | test2 | test3 || â€”â€” | â€”â€”- | â€”â€”- | â€”â€”- || h1 | right | wrong | wrong || h2 | wrong | right | wrong || h3 | wrong | wrong | right | å’Œç¬¬ä¸€ç§æƒ…å†µç›¸åï¼Œæœ€ç»ˆçš„æµ‹è¯•ç»“æœæ˜¯33% é€šè¿‡ä¸Šé¢çš„ä¾‹å­ï¼Œå¯ä»¥åæ˜ å‡ºä¸€ä¸ªé—®é¢˜ï¼Œåœ¨åŒè´¨é›†æˆä¸­ï¼Œå¦‚ä½•æ„å»ºå¤šä¸ªè¯¯å·®æ˜¯ç›¸äº’ç‹¬ç«‹çš„åŸºå­¦ä¹ å™¨ã€‚å› ä¸ºè¿™äº›åŸºå­¦ä¹ å™¨æ˜¯ç”¨çš„åŒä¸€ç§ç®—æ³•ï¼ŒåŸºæœ¬ä¹Ÿæ˜¯ä½¿ç”¨åŒä¸€ç»„æ•°æ®ï¼Œè§£å†³çš„ä¹Ÿæ˜¯åŒä¸€ä¸ªé—®é¢˜ã€‚ æ ¹æ®ä¸ªä½“å­¦æœŸçš„ç”Ÿæˆæ–¹å¼ï¼Œé›†æˆå­¦ä¹ æ–¹æ³•å¤§è‡´å¯ä»¥åˆ†æˆä¸¤å¤§ç±»ï¼š Boosting ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œå¿…é¡»ä¸²è¡Œç”Ÿæˆçš„åºåˆ—åŒ–æ–¹æ³• Bagging å’Œ Random Forest ä¸ªä½“å­¦ä¹ å™¨ä¹‹å‰ä¸å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œå¯åŒæ—¶ç”Ÿæˆçš„å¹¶è¡ŒåŒ–æ–¹æ³• 3. æ„å»ºå·®å¼‚æ€§åŸºåˆ†ç±»å™¨åœ¨åŒä¸€ä¸ªæ•°æ®é›†ä¸Šï¼Œæ„å»ºä¸åŒçš„ï¼Œå…·æœ‰å·®å¼‚æ€§çš„åˆ†ç±»å™¨ (å¥½è€Œä¸åŒ)ï¼Œå°±æ˜¯é€šè¿‡æŠ½æ ·æŠ€æœ¯è·å–å¤šä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œç”Ÿæˆå¤šä¸ªå·®å¼‚æ€§åˆ†ç±»å™¨ã€‚ç›®å‰ä¸»è¦çš„æ–¹æ³•æœ‰ï¼šBagging å’Œ Boostingã€‚ 3.1 Boostingæå‡æ–¹æ³•æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œé€šè¿‡æ”¹å˜æ ·æœ¬åˆ†å¸ƒï¼Œä½¿å¾—åˆ†ç±»å™¨èšé›†åœ¨é‚£äº›å¾ˆéš¾åˆ†çš„æ ·æœ¬ä¸Šï¼Œå¯¹é‚£äº›å®¹æ˜“é”™åˆ†çš„æ•°æ®åŠ å¼ºå­¦ä¹ ï¼Œå¢åŠ é”™åˆ†æ•°æ®çš„æƒé‡ï¼Œè¿™æ ·é”™åˆ†çš„æ•°æ®å†ä¸‹ä¸€è½®çš„è¿­ä»£å°±æœ‰æ›´å¤§çš„ä½œç”¨ (å¯¹é”™åˆ†æ•°æ®è¿›è¡Œæƒ©ç½š)ã€‚ å…·ä½“æ¥è¯´ï¼Œå…ˆä»åˆå§‹è®­ç»ƒé›†è®­ç»ƒå‡ºä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼Œå†æ ¹æ®è¿™ä¸ªåŸºå­¦ä¹ å™¨çš„è¡¨ç°å¯¹è®­ç»ƒæ ·æœ¬åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—å…ˆå‰åŸºå­¦ä¹ å™¨åšé”™çš„è®­ç»ƒæ ·æœ¬åœ¨åç»­å—åˆ°æ›´å¤šçš„å…³æ³¨ï¼Œç„¶ååŸºäºè°ƒæ•´åçš„æ ·æœ¬åˆ†å¸ƒæ¥è®­ç»ƒä¸‹ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›å¦‚æ­¤é‡å¤è¿›è¡Œï¼Œä¸€ç›´åˆ°åŸºå­¦ä¹ å™¨çš„æ•°ç›®åˆ°è¾¾äº‹å…ˆæŒ‡å®šçš„Tå€¼ï¼Œç„¶åå°†æ‰€æœ‰çš„åŸºå­¦ä¹ å™¨ç›¸ç»“åˆã€‚(é—®é¢˜ï¼šå¦‚ä½•è°ƒæ•´åˆ†å¸ƒï¼Ÿ) æ•°æ®çš„æƒé‡æœ‰ä¸¤ä¸ªä½œç”¨ï¼Œä¸€æ–¹é¢æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æƒå€¼ä½œä¸ºæŠ½æ ·åˆ†å¸ƒï¼Œè¿›è¡Œå¯¹æ•°æ®çš„æŠ½æ ·ï¼Œå¦ä¸€æ–¹é¢åˆ†ç±»å™¨å¯ä»¥ä½¿ç”¨æƒå€¼å­¦ä¹ æœ‰åˆ©äºé«˜æƒé‡æ ·æœ¬çš„åˆ†ç±»å™¨ã€‚æŠŠä¸€ä¸ªå¼±åˆ†ç±»å™¨æå‡ä¸ºä¸€ä¸ªå¼ºåˆ†ç±»å™¨ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒAdaBoostç®—æ³• (è¥¿ç“œä¹¦P173) Example å‡è®¾ç°åœ¨æœ‰ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨å¦‚ä¸‹è¡¨ | | é¢„æµ‹ + | é¢„æµ‹ - | count || :â€”â€”â€”-: | :â€”: | :â€”: | :â€”-: || å®é™… + | 24 | 16 | 40 || å®é™… - | 9 | 51 | 60 || count | 33 | 67 | 100 | é”™è¯¯ç‡è®¡ç®— \\epsilon = \\frac {(9 + 16)} {100} = 0.25 é”™åˆ†æ ·æœ¬çš„æƒå€¼æ›´æ–° w_e = \\frac 1 {2*\\epsilon} = 2 æ­£ç¡®æ ·æœ¬çš„æƒå€¼æ›´æ–° w_r = \\frac 1 {2*(1 - \\epsilon)} = \\frac 2 3 æ‰€ä»¥è®²æƒé‡ä¹˜ä»¥ç›¸åº”çš„æ•°æ®é‡ï¼Œå¾—åˆ°æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œå¦‚ä¸‹ é¢„æµ‹ + é¢„æµ‹ - count å®é™… + 24 * 2/3 = 16 16 * 2 = 32 16+32=48 å®é™… - 9 * 2 = 18 51 * 2/3 = 34 18 + 34 = 52 count 16 + 18 = 34 32 + 34 = 66 100 3.2 Baggingé€šè¿‡å¯¹åŸæ•°æ®é›†è¿›è¡Œæœ‰æ”¾å›çš„é‡‡æ · (bootstrap sampling) æ„å»ºå‡ºå¤§å°å’ŒåŸæ•°æ®é›†å¤§å°ä¸€æ ·çš„æ–°æ•°æ®é›†D1ï¼ŒD2ï¼ŒD3â€¦..ï¼Œç„¶åç”¨è¿™äº›æ–°çš„æ•°æ®é›†è®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨H1ï¼ŒH2ï¼ŒH3â€¦.ã€‚å› ä¸ºæ˜¯æœ‰æ”¾å›çš„é‡‡æ ·æ‰€ä»¥ä¸€äº›æ ·æœ¬å¯èƒ½ä¼šå‡ºç°å¤šæ¬¡ï¼Œè€Œå…¶ä»–æ ·æœ¬ä¼šè¢«å¿½ç•¥ï¼Œç†è®ºä¸Šåˆå§‹è®­ç»ƒé›†ä¸­çº¦æœ‰63.2%çš„æ ·æœ¬ä¼šå‡ºç°åœ¨é‡‡æ ·é›†ä¸­ã€‚(è¥¿ç“œä¹¦P27) è‡ªåŠ©é‡‡æ ·æ³• (Bootstrap sampling) å‡è®¾ç»™å®šä¸€ä¸ªåŒ…å«äº†mä¸ªæ ·æœ¬çš„æ•°æ®é›†Dï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæ–°çš„æ•°æ®D1ï¼Œå¤§å°å’ŒDä¸€æ ·çš„ã€‚é‚£ä¹ˆæ¯æ¬¡æˆ‘ä»¬ä»Dä¸­å–å‡ºä¸€ä¸ªæ ·æœ¬å…¥åˆ°D1ä¸­ï¼Œä¹‹åæŠŠæ ·æœ¬æ”¾å›Dä¸­ï¼Œé‡æ–°é‡‡æ ·ï¼Œæ€»å…±é‡‡æ ·mæ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸ºmçš„D1ã€‚å› ä¸ºæˆ‘ä»¬æŠŠæ ·æœ¬åˆé‡æ–°æ”¾å…¥Dä¸­ï¼Œä¸‹æ¬¡è¿˜æœ‰å¯èƒ½æŠ½å–åˆ°ï¼Œå¯¼è‡´åœ¨D1ä¸­ï¼Œæœ‰äº›æ ·æœ¬ä¼šå‡ºç°å¤šæ¬¡ï¼Œè€Œæœ‰äº›æ ·æœ¬åˆ™ä¸€æ¬¡ä¹Ÿä¸ä¼šå‡ºç°ã€‚æ‰€ä»¥æ ¹æ®å¦‚ä¸‹å…¬å¼ï¼š lim_{m -> \\infty} (1 - \\frac 1 m)^m -> \\frac 1 e \\approx 0.368æœ€ç»ˆåœ¨åˆå§‹æ ·æœ¬æ•°æ®é›†Dä¸­ï¼Œæœ‰36.8%çš„æ ·æœ¬æ˜¯ä¸€æ¬¡ä¹Ÿæ²¡æœ‰å‡ºç°çš„ï¼Œè€Œ63.2%é‡å¤å‡ºç°äº†ï¼Œè¿™æ ·æˆ‘ä»¬å°±æ”¹å˜äº†åˆå§‹æ ·æœ¬æ•°æ®é›†çš„åˆ†å¸ƒã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†D1ä½œä¸ºè®­ç»ƒé›†ï¼ŒD\\D1ï¼Œå³å‰©ä¸‹çš„36.8%æœªå‡ºç°åœ¨D1ä¸­çš„ä½œä¸ºæµ‹è¯•é›†ã€‚è¿™æ ·çš„æµ‹è¯•ç»“æœç§°ä¹‹ä¸ºåŒ…å¤–ä¼°è®¡ (out-of-bag-estimate) Baggingé€šè¿‡é™ä½åŸºåˆ†ç±»å™¨æ–¹å·®æ”¹å–„äº†æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤Baggingçš„æ€§èƒ½ä¾èµ–äºåŸºåˆ†ç±»å™¨çš„ç¨³å®šæ€§ï¼Œå¦‚æœåŸºåˆ†ç±»å™¨æ˜¯ä¸ç¨³å®šçš„ï¼ŒBaggingæœ‰åŠ©äºå‡ä½è®­ç»ƒæ•°æ®çš„éšæœºæ‰°åŠ¨å¯¼è‡´çš„è¯¯å·®ï¼Œä½†æ˜¯å¦‚æœåŸºåˆ†ç±»å™¨æ˜¯ç¨³å®šçš„ï¼Œå³å¯¹æ•°æ®å˜åŒ–ä¸æ•æ„Ÿï¼Œé‚£ä¹ˆBaggingæ–¹æ³•å°±å¾—ä¸åˆ°æ€§èƒ½çš„æå‡ï¼Œç”šè‡³ä¼šå‡ä½ï¼Œå› ä¸ºæ–°æ•°æ®é›†åªæœ‰63%ã€‚ 3.3 Random Forestéšæœºæ£®æ—æ˜¯Baggingçš„ä¸€ä¸ªæ‹“å±•å˜ä½“ï¼ŒåŸºäºBaggingæ¡†æ¶è¿›ä¸€æ­¥é™ä½äº†äº†æ¨¡å‹çš„æ–¹å·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨çš„å†³ç­–æ ‘ä¸ä¼ ç»Ÿçš„å†³ç­–æ ‘ç•¥æœ‰ä¸åŒã€‚ ä¼ ç»Ÿçš„å†³ç­–æ ‘ï¼Œæ¯”å¦‚C45æˆ–è€…CARTï¼Œæ¯å½“éœ€è¦åˆ’åˆ†ä¸€ä¸ªå±æ€§çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šå°†æ‰€æœ‰çš„å±æ€§çš„éƒ½è¿›è¡Œåˆ’åˆ†ä¸€éï¼Œç„¶åè®¡ç®—å…¶åˆ’åˆ†ä¹‹åä¸åˆ’åˆ†ä¹‹å‰ç†µçš„åå·®ï¼Œæˆ–è€…è®¡ç®—GiniæŒ‡æ•°ç­‰ï¼Œç„¶åä»ä¸­é€‰å–å‡ºæœ€ä½³å±æ€§è¿›è¡Œåˆ’åˆ†ã€‚ è€Œéšæœºæ£®æ—ï¼Œå¯¹åŸºå†³ç­–æ ‘çš„æ¯ä¸ªç»“ç‚¹ï¼Œå…ˆä»è¯¥èŠ‚ç‚¹çš„å±æ€§é›†åˆä¸­éšæœºé€‰æ‹©ä¸€ä¸ªåŒ…å«kä¸ªå±æ€§çš„å­é›†ï¼Œç„¶åå†ä»è¿™ä¸ªå­é›†ä¸­é€‰å–ä¸€ä¸ªæœ€ä¼˜å±æ€§ç”¨äºåˆ’åˆ† (ä»ç‰¹å¾çš„ä¸åŒå­é›†æ¥æ„å»ºæ ‘)ã€‚è¿™é‡Œçš„å‚æ•°kæ§åˆ¶äº†éšæœºæ€§çš„å¼•å…¥ç¨‹åº¦ï¼š è‹¥k=dï¼Œåˆ™åŸºå†³ç­–æ ‘å’Œä¼ ç»Ÿå†³ç­–æ ‘ç›¸åŒã€‚ è‹¥k=1ï¼Œåˆ™æ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªå±æ€§ç”¨äºåˆ’åˆ† æ¨èå€¼k=log_2 d éšæœºæ£®æ—é™¤äº†åŸºäºBaggingä¸­å¯¹äºæ ·æœ¬çš„æ‰°åŠ¨ï¼Œè¿˜åŒæ—¶å¯¹æ•°æ®é›†ä¸­çš„å±æ€§åˆ’åˆ†è¿›è¡Œäº†æ‰°åŠ¨ï¼Œè¿™æ ·å¯ä»¥å¢åŠ åŸºå­¦ä¹ å™¨çš„å¤šæ ·æ€§å’Œå·®å¼‚æ€§ï¼Œä½¿å¾—æœ€ç»ˆé›†æˆçš„æ³›åŒ–æ€§å¾—åˆ°æå‡ã€‚ 3.4 å¢å¼ºå¤šæ ·æ€§ æ•°æ®æ ·æœ¬æ‰°åŠ¨ è¾“å…¥å±æ€§æ‰°åŠ¨ è¾“å‡ºè¡¨ç¤ºæ‰°åŠ¨ ç®—æ³•å‚æ•°æ‰°åŠ¨ 4. ç»„åˆç­–ç•¥4.1 å½’å›é¢„æµ‹ (æ•°å€¼é¢„æµ‹) ç®€å•å¹³å‡å€¼æ³• å°±æ˜¯å–å„ä¸ªåˆ†ç±»å™¨ç»“æœçš„å¹³å‡å€¼ï¼ˆä¼šä¸ä¼šé™ä½è¿™é‡Œé¢æœ€å¥½çš„é‚£ä¸ªåˆ†ç±»å™¨çš„ç²¾åº¦ï¼‰ H(x) = \\frac 1 T \\sum_{i=1}^T h_i(x) åŠ æƒå¹³å‡æ³• ç»™ä¸åŒçš„åˆ†ç¦»å™¨èµ‹äºˆä¸ä¸€æ ·çš„æƒé‡å€¼ï¼Œç„¶åæ±‚å’Œ H(x) = \\sum_{i=1}^Tw_i Â· h_i(x) 4.2 åˆ†ç±»é¢„æµ‹ (ç±»åˆ«é¢„æµ‹) ç®€å•æŠ•ç¥¨æ³• æ¯ä¸ªåˆ†ç±»å™¨çš„æƒé‡å¤§å°éƒ½ä¸€æ ·ï¼Œå°‘æ•°æœä»å¤šæ•° ç»å¯¹å¤šæ•°æŠ•ç¥¨æ³•ç¥¨æ•°è¿‡åŠï¼Œå¦åˆ™æ‹’ç»é¢„æµ‹ ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•ç¥¨æ•°æœ€å¤šçš„é‚£ä¸ª åŠ æƒæŠ•ç¥¨æ³• ç»™æ¯ä¸ªåˆ†ç±»å™¨èµ‹äºˆä¸€ä¸ªæƒé‡ï¼Œç„¶åæ ¹æ®æƒé‡æ¥æŠ•ç¥¨ï¼Œå¾—åˆ°ç¥¨æ•°é«˜çš„æœ€ä¸ºè¾“å‡ºç»“æœ æ¦‚ç‡æŠ•ç¥¨æ³•ï¼ˆå’Œç®€å•åˆ†ç±»æœ‰ä»€ä¹ˆä¸åŒï¼Ÿï¼‰ æœ‰çš„åˆ†ç±»å™¨çš„è¾“å‡ºæ˜¯æœ‰æ¦‚ç‡ä¿¡æ¯çš„ï¼Œæ¯”å¦‚ åˆ†ç±»å™¨Aè¾“å‡ºç»“æœ1çš„æ¦‚ç‡ä¸º75% åˆ†ç±»å™¨Bè¾“å‡ºç»“æœ0çš„æ¦‚ç‡ä¸º80% åˆ†ç±»å™¨Cè¾“å‡ºç»“æœ1çš„æ¦‚ç‡ä¸º52% æœ€ç»ˆè¾“å‡ºçš„ç»“æœä¸ºï¼Ÿï¼Ÿï¼Ÿï¼ˆæ¦‚ç‡ç›¸åŠ è¿˜æ˜¯æ¦‚ç‡çš„å¹³å‡å€¼ï¼Œç›¸åŠ å¯èƒ½æ€§é«˜ï¼‰ 4.3 å­¦ä¹ æ³•5. References è¥¿ç“œä¹¦ç¬¬å…«ç«  - å‘¨å¿—å éå‚è€ƒï¼Œæ¨èé˜…è¯» å°èœé¸Ÿå¯¹å‘¨å¿—åå¤§ç¥gcForestçš„ç†è§£ gcForestç®—æ³•ç†è§£","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"Ensemble","slug":"Ensemble","permalink":"http://chenson.cc/tags/Ensemble/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸‰ï¼‰","date":"2017-06-09T06:08:52.000Z","path":"2017/06/09/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸‰ï¼‰/","text":"1. ä¸ºä»€ä¹ˆè¦ä½¿ç”¨Secondary Sortåœ¨Hadoopä¸­ï¼Œä»Mapåˆ°Reduceçš„è¿‡ç¨‹ä¸­ï¼Œkeyæ˜¯ä¸æ–­è¢«sortçš„ã€‚æ‰€ä»¥ä»mapå‡ºæ¥çš„æ—¶å€™å†™å…¥åˆ°ä¸€ä¸ªintermediate output fileçš„æ—¶å€™ï¼Œkeyæ˜¯æœ‰åºçš„ã€‚ç„¶åreduceä¸æ–­çš„ä»ä¸åŒçš„clusteré‡Œé¢fetché‡Œé¢çš„key-value pairsçš„æ—¶å€™ï¼Œä»ç„¶å¤šæ¬¡sortè¿™äº›pairsã€‚æ‰€ä»¥æœ€ç»ˆè¿›å…¥åˆ°reducerçš„keyæ˜¯æœ‰åºçš„ï¼Œä½†æ˜¯valueæ˜¯æ— åºçš„ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦å¯¹valueä¹Ÿè¿›è¡Œæ’åºå‘¢ï¼ŸGoogleçš„MRå†…ç½®äº†å‡½æ•°å¯¹valueä¹Ÿå¯ä»¥æ’åºï¼Œä½†æ˜¯Hadoopä¸è¡Œï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±å»å®šåˆ¶partitionerç­‰å»å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚ ä¸¾ä¸ªæ —å­ï¼š è¾“å…¥æ–‡ä»¶æ ¼å¼å¦‚ä¸‹ 12345678910112015,1,242015,3,542015,1,32015,2,-432015,4,52015,3,462014,2,642015,1,42015,1,212015,2,352015,2,20 â€‹ æœŸæœ›çš„è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼ˆvalueæ˜¯æœ‰åºçš„ï¼‰ 123452014-2 642015-1 3ï¼Œ4ï¼Œ21ï¼Œ242015-2 -43ï¼Œ0ï¼Œ352015-3 46ï¼Œ562015-4 5 â€‹ Hadoopé»˜è®¤çš„è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼ˆvalueæ˜¯æ— åºçš„ï¼‰ 123452014-2 642015-1 21ï¼Œ4ï¼Œ3ï¼Œ242015-2 0ï¼Œ35ï¼Œ-432015-3 56ï¼Œ462015-4 5 2. è§£å†³æ–¹æ¡ˆ ä¼ ç»Ÿçš„è§£å†³æ–¹æ³• å°±æ˜¯è¿›å…¥åˆ°åŒä¸€ä¸ªreducerçš„æ—¶å€™ï¼Œè¿™äº›åŒä¸€ä¸ªå¯ä»¥çš„valuesæ˜¯åœ¨ä¸€ä¸ªlisté‡Œé¢çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å…ˆæŠŠè¿™ä¸ªlisté‡Œé¢çš„valueå­˜åˆ°å†…å­˜ä¸­å»ï¼Œç„¶ååœ¨å†…å­˜ä¸­å°†è¿™äº›valueæ’åºã€‚è¿™ä¸ªæ–¹æ³•åªé€‚ç”¨äºæ•°æ®é‡è¾ƒå°çš„æ—¶å€™ï¼Œå½“æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™ï¼Œå†…å­˜å¹¶ä¸èƒ½åŒæ—¶å­˜å…¥è¿™äº›valuesï¼Œç¨‹åºå°±æŠ¥é”™æ— æ³•æ­£å¸¸è¿è¡Œã€‚ åˆ©ç”¨Hadoopç‰¹ç‚¹çš„æ–¹æ³• æ—¢ç„¶Hadoopå¯ä»¥å¯¹Keyè¿›è¡Œæ’åºï¼Œé‚£ä¹ˆå¯ä»¥åˆ©ç”¨è¿™ç‚¹ï¼Œå°†ä¹‹å‰çš„key-value pairs ç»„æˆä¸€ä¸ªæ–°çš„keyï¼Œè®©Hadoopå¯¹è¿™äº›Keyè¿›è¡Œæ’åºï¼Œæ —å­å¦‚ä¸‹ 12345// åŸå…ˆçš„key-value pair(2015,1) 21// æ–°çš„composite-key-value pair((2015,1),21) 21 è¿™é‡Œçš„(2015,1)æ˜¯æˆ‘ä»¬åŸå…ˆå°±æœ‰çš„keyï¼Œä¸ºäº†åŒºåˆ†æ–°çš„keyï¼Œå°†è¿™ä¸ªåŸæœ‰çš„key (k, v1) ,ç§°ä¸ºnatural keyï¼Œå°†æ–°çš„key ((k, v1), v2)ç§°ä¸ºcomposite keyã€‚å°†v2ç§°ä¸ºnatural valueã€‚å…·ä½“å¦‚ä¸‹å›¾ â€‹ ä¸ºäº†å®ç°Hadoopå¯¹æ–°çš„composite keyè¿›è¡Œæ’åºï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰partitionerå’Œgroupingæ¥ç¡®ä¿è¿™äº›composite keyä¸­natural keyç›¸åŒçš„ä¼šè¢«åˆ†é…åˆ°åŒä¸€ä¸ªreducerã€‚å› ä¸ºåœ¨composite keyä¸­ï¼Œå³ä½¿(k, v1)ç›¸åŒï¼Œåªè¦v2ä¸åŒï¼Œé»˜è®¤çš„partitionerå°±ä¼šè®¤ä¸ºè¿™æ˜¯ä¿©ä¸ªä¸åŒçš„keyï¼Œå°±å¾ˆæœ‰å¯èƒ½è®²è¿™ä¸ªcomposite keyåˆ†é…åˆ°ä¸åŒçš„reduceré‡Œå»ã€‚ â€‹ 3.å®ç°è¿‡ç¨‹ partitioner: å°†natural keyç›¸åŒçš„å‘é€åˆ°åŒä¸€ä¸ªreduceré‡Œå» åœ¨åŒä¸€ä¸ª[]é‡Œé¢è¯´æ˜æ˜¯ä¸€ä¸ªreducerï¼Œvalueä»»ç„¶æ˜¯æ— åºçš„ 123456789[((2014-2,64),64)][((2015-1,24),24), ((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,56),56), ((2015-3,46),46)][((2015-4,5),5)] â€‹ grouping comparator: å°†natural keyç›¸åŒçš„ä½œä¸ºä¸€ä¸ªgroupæ’åº 123456789[((2014-2,64),64)][((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21), ((2015-1,24),24)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,46),46), ((2015-3,56),56)][((2015-4,5),5)] â€‹ è¿›å…¥åˆ°Reducerçš„æ ¼å¼ (æœ‰ç‚¹é‡Œä¸ç†è§£æ­¤æ—¶composite keyé‡Œé¢çš„natural valueæ˜¯å¦‚ä½•ç¡®å®šçš„) 123456789((2014-2,64), (64))((2015-1,24), (2ï¼Œ4ï¼Œ21ï¼Œ24))((2015-2,35), (-43,0,35))((2015-3,46), (46,56))((2015-4,5), (5)) â€‹ æœ€ç»ˆçš„è¾“å‡º 12342014-2 642015-1 3,4,21,242015-2 -43,0,352015-4 5 æ•´ä¸ªæµç¨‹å›¾å¦‚ä¸‹ï¼ˆå›¾å†…æ•°æ®ä¸ä¸Šé¢æ•°æ®ä¸ç¬¦åˆï¼‰ 4. ä»£ç  composite key å°†æ—§çš„Keyï¼ˆnatural keyï¼‰å’ŒValueç»„åˆæˆæ–°çš„Keyï¼ˆcomposite keyï¼‰çš„ä»£ç  12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.secondarySort;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class Entry implements WritableComparable&lt;Entry&gt; &#123; private String yearMonth; private int count; public Entry() &#123; &#125; @Override public int compareTo(Entry entry) &#123; int result = this.yearMonth.compareTo(entry.getYearMonth()); if (result == 0) &#123; result = compare(count, entry.getCount()); &#125; return result; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(yearMonth); dataOutput.writeInt(count); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.yearMonth = dataInput.readUTF(); this.count = dataInput.readInt(); &#125; public String getYearMonth() &#123; return yearMonth; &#125; public void setYearMonth(String yearMonth) &#123; this.yearMonth = yearMonth; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public static int compare(int a, int b) &#123; return a &lt; b ? -1 : (a &gt; b ? 1 : 0); &#125; @Override public String toString() &#123; return yearMonth; &#125;&#125; â€‹ Partitioner 1234567891011package com.secondarySort; import org.apache.hadoop.mapreduce.Partitioner; public class EntryPartitioner extends Partitioner&lt;Entry, Integer&gt; &#123; @Override public int getPartition(Entry entry, Integer integer, int numberPartitions) &#123; return Math.abs((entry.getYearMonth().hashCode() % numberPartitions)); &#125;&#125; â€‹ Grouping Compartor 1234567891011121314151617package com.secondarySort; import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class EntryGroupingComparator extends WritableComparator &#123; public EntryGroupingComparator() &#123; super(Entry.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; Entry a1 = (Entry) a; Entry b1 = (Entry) b; return a1.getYearMonth().compareTo(b1.getYearMonth()); &#125;&#125; â€‹ Mapper 1234567891011121314151617181920212223public class SecondarySortMapper extends Mapper&lt;LongWritable, Text, Entry, Text&gt; &#123; private Entry entry = new Entry(); private Text value = new Text(); @Override protected void map(LongWritable key, Text lines, Context context) throws IOException, InterruptedException &#123; String line = lines.toString(); String[] tokens = line.split(\",\"); // YYYY = tokens[0] // MM = tokens[1] // count = tokens[2] String yearMonth = tokens[0] + \"-\" + tokens[1]; int count = Integer.parseInt(tokens[2]); entry.setYearMonth(yearMonth); entry.setCount(count); value.set(tokens[2]); context.write(entry, value); &#125;&#125; â€‹ Reducer 123456789101112public class SecondarySortReducer extends Reducer&lt;Entry, Text, Entry, Text&gt; &#123; @Override protected void reduce(Entry key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder builder = new StringBuilder(); for (Text value : values) &#123; builder.append(value.toString()); builder.append(\",\"); &#125; context.write(key, new Text(builder.toString())); &#125;&#125; â€‹ Deriver 123456789101112131415Configuration conf = new Configuration();Job job = Job.getInstance(conf);job.setJarByClass(Iteblog.class);job.setJobName(\"SecondarySort\"); FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setOutputKeyClass(Entry.class);job.setOutputValueClass(Text.class); job.setMapperClass(SecondarySortMapper.class);job.setReducerClass(SecondarySortReducer.class);job.setPartitionerClass(EntryPartitioner.class);job.setGroupingComparatorClass(EntryGroupingComparator.class); 5. å¸¸ç”¨çš„Secondary Sortä»£ç  IntPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; int first, second; // public IntPair() &#123;&#125; // // public IntPair(int left, int right) &#123; // set(left, right); // &#125; public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; public String toString()&#123; return \"(\" + first + \",\" + second + \")\"; &#125; @Override public void readFields(DataInput arg0) throws IOException &#123; // TODO Auto-generated method stub first = arg0.readInt(); second = arg0.readInt(); &#125; @Override public void write(DataOutput arg0) throws IOException &#123; // TODO Auto-generated method stub arg0.writeInt(first); arg0.writeInt(second); &#125; // å…³é”®ï¼šè‡ªå®šä¹‰ç±»å‹çš„æ¯”è¾ƒæ–¹æ³• @Override public int compareTo(IntPair arg0) &#123; // TODO Auto-generated method stub if (first != arg0.first) &#123; return first &lt; arg0.first ? -1 : 1; &#125; else if (second != arg0.second) &#123; return second &lt; arg0.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; public int hashCode() &#123; return first * 157 + second; &#125; public boolean equals(Object right) &#123; if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125;&#125;public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; &#123; // ç±»å‹è¦å’ŒMapperè¾“å‡ºçš„ä¸€æ · @Override public int getPartition(IntPair arg0, IntWritable arg1, int arg2) &#123; // TODO Auto-generated method stub return Math.abs((arg0.getFirst() * 127) % arg2); &#125;&#125;/* * ç¬¬ä¸€ç§æ–¹æ³•ï¼Œå®ç°æ¥å£RawComparator æ•°æ®ç±»å‹çš„æ¯”è¾ƒåœ¨MapReduceä¸­å¼åŠå…¶é‡è¦çš„, * Mapreduceä¸­æœ‰ä¸€ä¸ªæ’åºé˜¶æ®µï¼Œkeyå’Œå…¶ä»–çš„keyç›¸æ¯”è¾ƒã€‚ é’ˆå¯¹æ­¤ï¼ŒHadoop æä¾›çš„ä¸€ä¸ªä¼˜åŒ–æ˜¯ RawComparator * * public static class GroupingComparator implements RawComparator&lt;IntPair&gt;&#123; * * @Override public int compare(IntPair arg0, IntPair arg1) &#123; // TODO * Auto-generated method stub int l = arg0.getFirst(); int r = * arg0.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; * * @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int * s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, * Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; &#125; */// æ–¹æ³•äºŒpublic static class GroupingComparator extends WritableComparator &#123; protected GroupingComparator() &#123; super(IntPair.class, true);// è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•° &#125; public int compare(WritableComparable w1, WritableComparable w2) &#123; IntPair i1 = (IntPair) w1; IntPair i2 = (IntPair) w2; int l = i1.getFirst(); int r = i2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125;&#125; â€‹ TextPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package TextPair;// cc TextPair A Writable implementation that stores a pair of Text objects// cc TextPairComparator A RawComparator for comparing TextPair byte representations// cc TextPairFirstComparator A custom RawComparator for comparing the first field of TextPair byte representations// vv TextPairimport java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.io.WritableUtils;public class TextPair implements WritableComparable&lt;TextPair&gt; &#123; private Text first; private Text second; public TextPair() &#123; set(new Text(), new Text()); &#125; public TextPair(String first, String second) &#123; set(new Text(first), new Text(second)); &#125; public TextPair(Text first, Text second) &#123; set(first, second); &#125; public void set(Text first, Text second) &#123; this.first = first; this.second = second; &#125; public Text getFirst() &#123; return first; &#125; public Text getSecond() &#123; return second; &#125; @Override public void write(DataOutput out) throws IOException &#123; first.write(out); second.write(out); &#125; @Override public void readFields(DataInput in) throws IOException &#123; first.readFields(in); second.readFields(in); &#125; @Override public int hashCode() &#123; return first.hashCode() * 163 + second.hashCode(); &#125; @Override public boolean equals(Object o) &#123; if (o instanceof TextPair) &#123; TextPair tp = (TextPair) o; return first.equals(tp.first) &amp;&amp; second.equals(tp.second); &#125; return false; &#125; @Override public String toString() &#123; return first + \"\\t\" + second; &#125; @Override public int compareTo(TextPair tp) &#123; int cmp = first.compareTo(tp.first); if (cmp != 0) &#123; return cmp; &#125; return second.compareTo(tp.second); &#125; // ^^ TextPair // vv TextPairComparator public static class Comparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public Comparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); int cmp = TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); if (cmp != 0) &#123; return cmp; &#125; return TEXT_COMPARATOR.compare(b1, s1 + firstL1, l1 - firstL1, b2, s2 + firstL2, l2 - firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; &#125; static &#123; WritableComparator.define(TextPair.class, new Comparator()); &#125; // ^^ TextPairComparator // vv TextPairFirstComparator public static class FirstComparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public FirstComparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); return TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; if (a instanceof TextPair &amp;&amp; b instanceof TextPair) &#123; return ((TextPair) a).first.compareTo(((TextPair) b).first); &#125; return super.compare(a, b); &#125; &#125; // ^^ TextPairFirstComparator // vv TextPair&#125;// ^^ TextPair","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.cc/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems(2)","date":"2017-05-15T08:43:49.000Z","path":"2017/05/15/Machine-Learning-Recommender-Systems-2/","text":"1. ååŒè¿‡æ»¤ï¼Œç»™ç”¨æˆ·æ¨èç‰©å“1.1 åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤ç®—æ³•UserCF ä¸»è¦é€šè¿‡åˆ†æç”¨æˆ·çš„è¡Œä¸ºè®°å½•ï¼Œè®¡ç®—ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦ è¯¥ç®—æ³•è®¤ä¸ºç‰©å“Aå’Œç‰©å“Bå…·æœ‰å¾ˆå¤§çš„ç›¸ä¼¼åº¦æ˜¯å› ä¸ºå–œæ¬¢ç‰©å“Açš„ç”¨æˆ·å¤§éƒ¨åˆ†ä¹Ÿå–œæ¬¢ç‰©å“B 1.1.1 æ­¥éª¤ æ‰¾åˆ°å’Œç›®æ ‡ç”¨æˆ·ç›¸ä¼¼çš„ç”¨æˆ·é›†åˆ æ‰¾åˆ°è¿™ä¸ªé›†åˆä¸­çš„ç”¨æˆ·å–œæ¬¢ï¼Œä¸”å’Œç›®æ ‡ç”¨æˆ·æ²¡æœ‰å¬è¯´è¿‡çš„çš„ç‰©å“æ¨èç»™ç›®æ ‡ç”¨æˆ· 1.1.2 è®¡ç®—ç›¸ä¼¼åº¦ å¾—åˆ°ç”¨æˆ·ä¹‹é—´çš„å…´è¶£ç›¸ä¼¼åº¦ä¹‹åï¼Œå¯»æ‰¾æœ€ç›¸è¿‘çš„Kä¸ªç”¨æˆ· 1.2åŸºäºç‰©å“çš„ååŒè¿‡æ»¤ç®—æ³•1.2.1 è®¡ç®—æ­¥éª¤ è®¡ç®—ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦ æ ¹æ®ç‰©å“çš„ç›¸ä¼¼åº¦å’Œç”¨æˆ·çš„å†å²è¡Œä¸ºç»™ç”¨æˆ·ç”Ÿæˆæ¨èåˆ—è¡¨ 1.2.2 è®¡ç®—ç›¸ä¼¼åº¦ 1.3 éšè¯­ä¹‰æ¨¡å‹ UserCF æ‰¾åˆ°å’Œä»–ä»¬çœ‹äº†åŒæ ·ä¹¦çš„å…¶ä»–ç”¨æˆ·ï¼Œå³æ˜¯å…´è¶£ç›¸ä¼¼ç”¨æˆ·ã€‚ç„¶åç»™ç”¨æˆ·æ¨èè¿™äº›ç”¨æˆ·ç›¸ä¼¼ç”¨æˆ·æ‰€å–œæ¬¢çš„å…¶ä»–ä¹¦ç± ItemCF åœ¨å·²ç»çœ‹è¿‡çš„ä¹¦ä¸­ï¼Œå¯»æ‰¾ç›¸ä¼¼çš„ä¹¦ï¼Œå³å’Œè¿™äº›çœ‹è¿‡çš„ä¹¦ï¼ŒåŒæ—¶å‡ºç°åœ¨å…¶ä»–ç”¨æˆ·çš„çœ‹è¿‡çš„ä¹¦ä¸­ï¼Œç„¶åæ¨èè¿™äº›å…¶ä»–çš„ä¹¦ å…¶ä»–æ–¹æ³• å¯ä»¥å¯¹ç‰©å“çš„å…´è¶£è¿›è¡Œåˆ†ç±»ï¼Œå¯¹äºæŸä¸ªç”¨æˆ·ï¼Œé¦–å…ˆå¾—åˆ°ä»–çš„å…´è¶£åˆ†ç±»ï¼Œç„¶åä»è¿™äº›åˆ†ç±»ä¸­æŒ‘å‡ºä»–å¯èƒ½å–œæ¬¢çš„ç‰©å“ å¦‚ä½•ç»™ç‰©å“è¿›è¡Œåˆ†ç±» éšå«è¯­ä¹‰åˆ†ææŠ€æœ¯(latent variable analysis) å¦‚ä½•ç¡®å®šç”¨æˆ·çš„å…´è¶£ï¼Œå³å¯¹å“ªäº›ç±»çš„ç‰©å“æ„Ÿå…´è¶£ï¼Œä»¥åŠæ„Ÿå…´è¶£çš„ç¨‹åº¦ å¯¹äºä¸€ä¸ªç»™å®šçš„ç±»ï¼Œé€‰æ‹©å“ªäº›å±äºè¿™ä¸ªç±»çš„ç‰©å“æ¨èç»™ç”¨æˆ·ï¼Ÿä»¥åŠå¦‚ä½•ç¡®å®šè¿™äº›ç‰©å“åœ¨è¿™ä¸ªä¸­çš„æƒé‡ã€‚å³å¦‚ä½•åœ¨è¿™ä¸ªç±»ä¸­ï¼ŒæŒ‘é€‰å‡ºåˆé€‚çš„ç‰©å“æ¨èç»™ç”¨æˆ· 1.3.1 LFM (latent factor model)1.4 ç”¨æˆ·æ ‡ç­¾æ•°æ® User Generated Content2. éœ€è¦è§£å†³çš„é—®é¢˜ - è¯„åˆ†é¢„æµ‹ï¼ˆç”¨æˆ·Aå¯¹ç”µå½±xçš„è¯„åˆ†é¢„æµ‹ï¼‰ 2.1 å®éªŒæ–¹æ³•2.1.1 åˆ’åˆ†è®­ç»ƒé›† ä¸æ—¶é—´æ— å…³ï¼Œå¯ä»¥å‡åŒ€åˆ†å¸ƒéšæœºæ¢åˆ†æ•°æ®é›†ã€‚å³å¯¹æ¯ä¸€ä¸ªç”¨æˆ·ï¼Œéšæœºé€‰å–ä¸€äº›è¯„åˆ†è®°å½•ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä¸‹çš„ä½œä¸ºæµ‹è¯•é›† ä¸æ—¶é—´ç›¸å…³ï¼Œé‚£ä¹ˆéœ€è¦å°†ç”¨æˆ·çš„æ—§è¡Œä¸ºä½œä¸ºè®­ç»ƒé›†ï¼Œè®²ç”¨æˆ·çš„æ–°è¡Œä¸ºä½œä¸ºæµ‹è¯•é›† ä¾‹å­ Netflixçš„è¯„åˆ†é¢„æµ‹ç³»ç»Ÿä¸­ï¼Œå°†æ¯ä¸ªç”¨æˆ·çš„è¯„åˆ†è®°å½•æŒ‰ç…§ä»æ—©åˆ°æ™šè¿›è¡Œæ’åºï¼Œç„¶åå°†ç”¨æˆ·æœ€åçš„10%çš„è¯„åˆ†è®°å½•ä½œä¸ºæµ‹è¯•é›†ï¼Œ90%çš„è¯„åˆ†è®°å½•ä½œä¸ºè®­ç»ƒé›†ã€‚ 2.1.2 è¯„åˆ†æ ‡å‡† RMSE = \\frac {\\sqrt {\\sum_{(u, i) \\in T} (r_{ui} - \\hat r_{ui})^2 }} {|Test|}2.1.3 è¯„åˆ†é¢„æµ‹ç®—æ³• å¹³å‡å€¼ æœ€ç®€å•çš„æ–¹æ³•ï¼šåˆ©ç”¨å¹³å‡å€¼é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ† å…¨å±€å¹³å‡å€¼ è®¡ç®—åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸­ï¼Œæ‰€æœ‰è¯„åˆ†è®°å½•çš„è¯„åˆ†å¹³å‡å€¼ ç”¨æˆ·è¯„åˆ†å¹³å‡å€¼ è®¡ç®—ç”¨æˆ·uåœ¨è®­ç»ƒé›†ä¸­æ‰€ç»™å‡ºçš„è¯„åˆ†çš„å¹³å‡å€¼ ç‰©å“è¯„åˆ†å¹³å‡å€¼è®¡ç®—è¯¥ç‰©å“wåœ¨è®­ç»ƒé›†ä¸­è¢«è¯„ä»·äº†çš„è¯„åˆ†çš„å¹³å‡å€¼ ç”¨æˆ·å¯¹ç‰©å“åˆ†ç±»çš„å¹³å‡å€¼ å‡è®¾è¿™é‡Œæœ‰ä¸¤ä¸ªåˆ†ç±»ï¼Œä¸€ä¸ªæ˜¯ç”¨æˆ·åˆ†ç±»å‡½æ•°Uï¼Œä¸€ä¸ªæ˜¯ç‰©å“åˆ†ç±»Wï¼ŒU(u)å®šä¹‰äº†ç”¨æˆ·uæ‰€å±çš„åˆ†ç±»ï¼ŒW(w)å®šä¹‰äº†ç‰©å“wæ‰€å±çš„åˆ†ç±»ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è®­ç»ƒé›†ä¸­åŒç±»ç”¨æˆ·å¯¹åŒç±»ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ† ä¹‹å‰æ˜¯ä¸‰ç§å¹³å‡å€¼å…¶å®æ˜¯ç”¨æˆ·å¯¹ç‰©å“åˆ†ç±»çš„å¹³å‡å€¼çš„ä¸€ç§ç‰¹ä¾‹ U(u) = 0ï¼ŒW(w) = 0ï¼Œé‚£ä¹ˆå°±æ˜¯å…¨å±€å¹³å‡å€¼ U(u) = uï¼ŒW(w) = 0ï¼Œé‚£ä¹ˆå°±æ˜¯ç”¨æˆ·è¯„åˆ†å¹³å‡å€¼ U(u) = 0ï¼ŒW(w) = wï¼Œé‚£ä¹ˆå°±æ˜¯ç‰©å“è¯„åˆ†å¹³å‡å€¼ åœ¨ä»¥ä¸Šçš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è€ƒè™‘åˆ°ç”¨æˆ·çš„æ´»è·ƒåº¦å’Œç‰©å“çš„æµè¡Œç¨‹åº¦ã€‚å®é™…ä¸Šå¯ä»¥å°†è¿™ä¸¤ç‚¹è€ƒè™‘è¿›å»ï¼Œå¯¹æ´»è·ƒç”¨æˆ·å’Œæµè¡Œçš„ç‰©å“ç»™å®šä¸€ç‚¹penalty 2.1.4 åŸºäºé¢†åŸŸçš„æ–¹æ³•ï¼ˆåŸºäºç”¨æˆ·çš„é¢†åŸŸå’ŒåŸºäºç‰©å“çš„é¢†åŸŸç®—æ³•ï¼‰ åŸºäºç”¨æˆ·çš„é¢†åŸŸç®—æ³• \\hat r_{ui} = \\overline r_u + \\frac {\\sum_{v \\in S(u, K) \\bigcap N(i)} w_{uv}*(r_{vi} - \\overline r_v)} {\\sum_{v \\in S(u, K) \\bigcap N(i)} |w_{uv}|}E(r_u) æ˜¯ç”¨æˆ·uå¯¹ä»–ç‚¹è¯„è¿‡çš„æ‰€æœ‰ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼ S(u, K) æ˜¯å’Œç”¨æˆ·uå…´è¶£æœ€ç›¸ä¼¼çš„Kä¸ªç”¨æˆ·çš„é›†åˆ N(i) æ˜¯å¯¹ç‰©å“iç‚¹è¯„è¿‡åˆ†æ•°çš„ç”¨æˆ·é›†åˆ r_vi æ˜¯ç”¨æˆ·vå¯¹ç‰©å“içš„è¯„åˆ† E(r_v) æ˜¯ç”¨æˆ·vå¯¹ä»–è¯„åˆ†è¿‡çš„æ‰€æœ‰ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼ w_uv æ˜¯ç”¨æˆ·uå’Œvä¹‹é—´çš„ç›¸ä¼¼åº¦ â€‹ åŸºäºç‰©å“çš„é¢†åŸŸç®—æ³• \\hat r_{ui} = \\overline r_i + \\frac {\\sum_{j \\in S(i, K) \\bigcap N(u)} w_{ij}*(r_{uj} - \\overline r_i)} {\\sum_{j \\in S(i, K) \\bigcap N(u)} |w_{ij}|}E(r_i) æ˜¯ç‰©å“içš„å¹³å‡åˆ†ï¼Œæ˜¯æ‰€æœ‰ç”¨æˆ·å¯¹ç‰©å“iç‚¹è¯„è¿‡çš„åˆ†æ•°çš„å¹³å‡å€¼ S(i, K) æ˜¯å’Œç‰©å“iæœ€ç›¸ä¼¼çš„Kä¸ªç‰©å“çš„é›†åˆ N(u) æ˜¯ç”¨æˆ·uç‚¹è¯„å¤šåˆ†æ•°çš„ç‰©å“é›†åˆ r_uj æ˜¯ç”¨æˆ·uå¯¹ç‰©å“jçš„è¯„åˆ† w_ij æ˜¯ç‰©å“iå’Œjä¹‹é—´çš„ç›¸ä¼¼åº¦ 2.1.5 è®¡ç®—ç›¸ä¼¼åº¦ ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆcosine similarity) w_{ij} = \\frac {\\sum_{u \\in U} r_{ui} * r_{uj}} {\\sqrt{\\sum_{u \\in U}r_{ui}^2 * \\sum_{u \\in U}r_{uj}^2}} çš®å°”é€Šç³»æ•°ï¼ˆpearson correlationï¼‰ w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_i) * (r_{uj} - \\overline r_j)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_i)^2 * \\sum_{u \\in U}(r_{uj} - \\overline r_j)^2}} ä¿®æ­£ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆadjust cosine similarityï¼‰ï¼ˆåœ¨MovieLensæ•°æ®é›†ä¸Šæ•ˆæœæœ€å¥½ï¼‰ w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_u) * (r_{uj} - \\overline r_u)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_u)^2 * \\sum_{u \\in U}(r_{uj} - \\overline r_u)^2}} 2.1.6 éšè¯­ä¹‰æ¨¡å‹çš„çŸ©é˜µåˆ†è§£æ¨¡å‹ ï¼ˆLatent Factor Model)éšå«ç±»åˆ«æ¨¡å‹ã€éšè¯­ä¹‰æ¨¡å‹ç­‰ï¼Œåœ¨æœ¬è´¨ä¸Šéƒ½æ˜¯ä¸ºäº†æ‰¾å‡ºæŸä¸€ä¸œè¥¿çš„æ½œåœ¨çš„ä¸»é¢˜æˆ–è€…åˆ†ç±»ã€‚åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå¯ä»¥åŸºäºç”¨æˆ·çš„è¡Œä¸ºåˆ©ç”¨éšè¯­ä¹‰æ¨¡å‹ï¼Œå¯¹itemè¿›è¡Œè‡ªåŠ¨èšç±»ï¼Œè¿™æ ·å¯ä»¥é¿å…äº†äººä¸ºåˆ†ç±»çš„åå·®ã€‚ ä¸¾ä¾‹è¯´æ˜ï¼š â€‹ ç”¨æˆ·Aå–œæ¬¢çœ‹æ•°å­¦ï¼Œå†å²å’Œè®¡ç®—æœºçš„ä¹¦ç± â€‹ ç”¨æˆ·Bå–œæ¬¢çœ‹æœºå™¨å­¦ä¹ ï¼Œç¼–ç¨‹è¯­è¨€å’Œç¦»æ•£æ•°å­¦æ–¹é¢çš„ä¹¦ç± â€‹ ç”¨æˆ·Cå–œæ¬¢çœ‹å¤§å¸ˆçš„ä½œå“ï¼Œæ¯”å¦‚ä¸“é—¨çœ‹Knuthæˆ–è€…Jiawei Hançš„ä¹¦ç± é‚£ä¹ˆç³»ç»Ÿåœ¨å¯¹ç”¨æˆ·çš„å–œå¥½è¿›è¡Œæ¨èçš„æ—¶å€™ï¼Œéœ€è¦æ‰¾å‡ºåŒå±äºç”¨æˆ·å…´è¶£åœˆå­çš„ä¹¦ç±ã€‚å¯¹äºä¹‹å‰æåˆ°è¿‡çš„ä¸‰ä¸ªç”¨æˆ·æ¥è¯´ â€‹ ç”¨æˆ·Açš„åœˆå­ï¼šæ•°å­¦ã€è®¡ç®—æœºã€å†å² â€‹ ç”¨æˆ·Bçš„åœˆå­ï¼šè¿™ä¸‰æœ¬ä¹¦å¯ä»¥åŒæ—¶åˆ†åˆ°è®¡ç®—æœºçš„åœˆå­ï¼Œä½†æ˜¯ç¦»æ•£æ•°å­¦å´åˆå¯ä»¥åˆ†åˆ°æ•°å­¦åœˆå­å» â€‹ ç”¨æˆ·Cçš„åœˆå­ï¼šæ ¹æ®ä½œè€…ä¸åŒæ¥åˆ’åˆ†åœˆå­ï¼Œé‚£ä¹ˆè¿™ä¸ªåœˆå­å°±å’Œä¹‹å‰ç”¨æˆ·Aã€Bçš„è§’åº¦æ˜¯ä¸åŒçš„ã€‚ å‡è®¾è®©äººå·¥æ¥å®Œæˆä¹‹å‰ä¹¦ç±çš„åˆ†ç±»ï¼Œé‚£ä¹ˆç»å¸¸ä¼šç¢°åˆ°åˆ’åˆ†çš„ç²’åº¦ä¸åŒï¼Œè§’åº¦ä¸åŒç­‰æƒ…å†µã€‚ åŒæ—¶ï¼Œéœ€è¦æ³¨æ„ä¸€ä¸‹ä¸¤ç‚¹ï¼š ç”¨æˆ·Aå¯¹è¿™ä¸‰ä¸ªç±»åˆ«ä¹¦ç±æ„Ÿå…´è¶£ï¼Œä¸ä»£è¡¨ä¸å¯¹å…¶ä»–ç±»åˆ«çš„æ•°æ®æ„Ÿå…´è¶£ åŒä¸€æœ¬ä¹¦å¯ä»¥å±äºå¤šä¸ªç±»åˆ«ï¼Œæ‰€ä»¥æ¯æœ¬ä¹¦åœ¨æ¯ä¸ªç±»åˆ«é‡Œé¢éƒ½æœ‰ä¸€ä¸ªæƒé‡ï¼Œæƒé‡å€¼è¶Šå¤§ï¼Œè¯´æ˜å±äºè¿™ä¸ªç±»åˆ«çš„å¯èƒ½æ€§è¶Šé«˜ é‚£ä¹ˆï¼ŒLFMæ˜¯å¦‚ä½•è§£å†³ä¸Šé¢çš„å‡ ä¸ªé—®é¢˜çš„å‘¢ï¼Ÿ å›ç­”ä¸Šé¢çš„é—®é¢˜å‰ï¼Œæˆ‘ä»¬å¯ä»¥æ€è€ƒä¸€ä¸‹æˆ‘ä»¬éœ€è¦åšçš„å“ªäº›å·¥ä½œã€‚ å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰çš„Userçœ‹åšåˆ—ï¼ŒæŠŠæ‰€æœ‰çš„Itemçœ‹åšè¡Œï¼Œæ„å»ºä¸€ä¸ªmxnçš„äºŒç»´çŸ©é˜µï¼Œå¦‚ä¸‹å›¾ æœ€å·¦è¾¹çš„RçŸ©é˜µæ˜¯ä¸€ä¸ªuser-itemçŸ©é˜µï¼ŒçŸ©é˜µå€¼Rijè¡¨ç¤ºçš„æ˜¯ç”¨æˆ·iå¯¹item jçš„å…´è¶£åº¦ï¼Œæˆ–è€…æ˜¯è¯„åˆ†ã€‚é‚£ä¹ˆæˆ‘ä»¬çš„è¯„åˆ†é¢„æµ‹ï¼Œå°±å¯ä»¥è½¬æ¢æˆå¯¹è¿™ä¸ªçŸ©é˜µä¸­çš„æŸäº›å€¼ï¼ˆç¼ºå¤±å€¼ï¼‰çš„é¢„æµ‹ï¼ŒåŒæ—¶éœ€è¦ä¿è¯æˆ‘ä»¬çš„é¢„æµ‹å€¼å¯¹äºè¿™ä¸ªçŸ©é˜µçš„æ‰°åŠ¨çš„æœ€å°çš„ã€‚ï¼ˆå³è¡¥å…¨ä¹‹åçŸ©é˜µçš„ç‰¹å¾å€¼å’Œè¡¥å…¨ä¹‹å‰çš„ç‰¹å¾å€¼ç›¸å·®ä¸å¤§ï¼Œå…·ä½“è§SVDåˆ†è§£ï¼‰ è€Œå³è¾¹çš„ä¸¤ä¸ªPå’ŒQçŸ©é˜µå°±æ˜¯LFMæ‰€åšçš„ï¼ŒLFMç®—æ³•ä»æ•°æ®é›†æ±‡æ€»æŠ½å‡ºè‹¥å¹²ä¸ªclassï¼Œè®¡ç®—å‡ºæ‰€æœ‰userå¯¹è¿™äº›classçš„æ„Ÿå…´è¶£é•¿åº¦ï¼Œå³PçŸ©é˜µã€‚åŒæ—¶è®¡ç®—å‡ºæ‰€æœ‰itemåœ¨è¿™äº›classä¸­çš„æƒé‡å€¼ï¼Œå³QçŸ©é˜µã€‚PçŸ©é˜µä½œä¸ºuserå’Œitemä¹‹é—´è¿æ¥çš„æ¡¥æ¢ï¼Œæ‰€ä»¥Rå¯ä»¥è¡¨ç¤ºä¸ºPçŸ©é˜µå’ŒQçŸ©é˜µç›¸ä¹˜ã€‚ R_{UI} = P_U Q_I = \\sum_{k=1}^K = P_{U,k}Q_{k,I}ä»¥ä¸‹æ˜¯LFMçš„ä¼˜ç‚¹ï¼š ä¸éœ€è¦å…³å¿ƒçŸ©é˜µPæ˜¯æ€ä¹ˆæ„å»ºçš„ï¼Œå³ä¸éœ€è¦å¦‚ä½•ç»™ç‰©å“è¿›è¡Œèšç±»ã€åˆ’åˆ†ç­‰ï¼ˆè§’åº¦ï¼Œç²’åº¦ç­‰ï¼‰ QçŸ©é˜µä¸­ï¼Œå¯¹äºä¸€ä¸ªitemå¹¶ä¸æ˜¯æ˜ç¡®ç»™åˆ’åˆ†åˆ°æŸä¸€ä¸ªåˆ†ç±»ï¼Œè€Œæ˜¯è®¡ç®—è¿™ä¸ªitemå±äºè¿™äº›æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡ï¼Œå€¼è¶Šå¤§å¯èƒ½æ€§è¶Šé«˜ åŒç†ï¼ŒPçŸ©é˜µä¸­ï¼Œå¯¹äºä¸€ä¸ªuserå¹¶æ²¡æœ‰é™å®šåœ¨æŸäº›classä¸­ï¼Œè€Œæ˜¯è®¡ç®—è¿™ä¸ªuserå¯¹äºè¿™äº›classesçš„æ„Ÿå…´è¶£ç¨‹åº¦ è™½ç„¶æˆ‘ä»¬çŸ¥é“äº†LFMä¸ºæˆ‘ä»¬åšäº†å“ªäº›å·¥ä½œï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯ä¸çŸ¥é“è¯¥å¦‚ä½•æ±‚è§£å‡ºçŸ©é˜µPå’ŒçŸ©é˜µQä¸­çš„å‚æ•°å€¼ï¼Œä¸€èˆ¬çš„åšæ³•æ˜¯æœ€ä¼˜æŸå¤±å‡½æ•°æ¥æ±‚å‚æ•°ã€‚ ä¼ ç»Ÿçš„SVDåˆ†è§£ ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼Œç»™å®šä¸€ä¸ªuser-itemçš„çŸ©é˜µRã€‚ é¦–å…ˆéœ€è¦å¯¹è¯„åˆ†çŸ©é˜µRä¸­çš„ç¼ºå¤±å€¼è¿›è¡Œç®€å•çš„è¡¥å…¨ï¼Œæ¯”å¦‚ç”¨å…¨å±€å¹³å‡å€¼ï¼Œæˆ–è€…ç”¨æˆ·/ç‰©å“çš„å¹³å‡å€¼è¡¥å…¨ï¼Œå¾—åˆ°è¡¥å…¨åçš„çŸ©é˜µRâ€™ å¾—åˆ°è¡¥å…¨åçš„çŸ©é˜µRâ€™ï¼Œæ¥ç€å¯ä»¥åˆ©ç”¨SVDåˆ†è§£ï¼Œå°†Râ€™åˆ†è§£æˆå¦‚ä¸‹å½¢å¼ R' = U^TSV \\\\ R \\in R^{m * n} \\\\ U \\in R^{k * m} \\\\ V \\in R^{k * n} \\\\ S \\in R^{k * k}Uå’ŒVæ˜¯ä¸¤ä¸ªæ­£äº¤çŸ©é˜µï¼ŒSæ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šçš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯çŸ©é˜µçš„å¥‡å¼‚å€¼ã€‚ ä¸ºäº†å¯¹Râ€™è¿›è¡Œé™ç»´ï¼Œå¯ä»¥å–æœ€å¤§çš„fä¸ªå¥‡å¼‚å€¼ç»„æˆå¯¹ç„¦çŸ©é˜µSfï¼Œå¹¶ä¸”æ‰¾åˆ°è¿™ä¸ªfä¸ªå¥‡å¼‚å€¼æ±‡ä¸­æ¯ä¸ªå€¼åœ¨Uã€VçŸ©é˜µä¸­å¯¹åº”çš„è¡Œå’Œåˆ—ï¼Œå¾—åˆ°Ufã€Vfï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªé™ç»´åçš„è¯„åˆ†çŸ©é˜µï¼š R_f'=U_f^TS_fV_fè¯¥æ–¹æ³•çš„ä¸€äº›ç¼ºç‚¹ï¼š åœ¨ç°å®ä¸­ï¼ŒRçŸ©é˜µåŸºæœ¬ä¸Šä¼šæ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œå³95%çš„æ•°æ®æ˜¯ç¼ºå¤±çš„ï¼ŒåŒæ—¶è¯¥çŸ©é˜µéå¸¸çš„å¤§ã€‚ä¸€ç»è¡¥å…¨ï¼Œè¯¥çŸ©é˜µå°±æ˜¯ä¸€ä¸ªç¨ å¯†çŸ©é˜µï¼Œå‚¨å­˜å¼€é”€éå¸¸çš„å¤§ è®¡ç®—å¤æ‚åº¦éå¸¸çš„é«˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¡¥å…¨ä¹‹åçš„ç¨ å¯†çŸ©é˜µ â€‹ Funk-SVDåˆ†è§£ï¼Œå³ Latent Factor Modelï¼ˆLFMï¼‰ http://sifter.org/~simon/journal/20061211.html ä»çŸ©é˜µçš„è§’åº¦ï¼Œå°†è¯„åˆ†çŸ©é˜µRåˆ†è§£æˆä¸¤ä¸ªä½çº¬åº¦ç›¸ä¹˜ï¼š \\hat R = P^TQ \\\\ R \\in R^{m*n} \\\\ P \\in R^{f*m} \\\\ Q \\in R^{f*n}Pã€Qæ˜¯ä¸¤ä¸ªé™ç»´åçš„çŸ©é˜µï¼Œé‚£ä¹ˆå¯¹äºç”¨æˆ·uå¯¹äºç‰©å“içš„è¯„åˆ†çš„é¢„æµ‹å€¼å¯ä»¥^R(u, i) = ^r_uiï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—ï¼š \\hat r_{ui} =b_{ui} + \\sum_fp_{uf}q_{if} \\\\ p_{uf} = P(u, f) \\\\ p_{if} = Q(i, f)Simon Funk-SVDçš„æ€æƒ³æ˜¯ç›´æ¥é€šè¿‡è®­ç»ƒé›†ä¸­çš„è§‚å¯Ÿå€¼ï¼Œåˆ©ç”¨æœ€å°åŒ–RMSEå­¦ä¹ Pã€QçŸ©é˜µã€‚ æŸå¤±å‡½æ•°çš„è®¡ç®—ï¼š C(p, q) = \\sum_{(u, i) \\in Train} (r_{ui} - \\hat r_{ui})^2 = \\sum_{(u, i) \\in Train}(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})^2 + \\lambda(||p_u||^2 + ||q_i||^2) \\\\ \\hat r_{ui} = \\mu + b_u + b_i + p_u^Tq_iè¦æœ€å°åŒ–ä¸Šé¢çš„æŸå¤±å‡½æ•°ï¼Œå¯ä»¥åˆ©ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ã€‚ä»¥ä¸‹æ˜¯ç®€å•çš„æ¨å¯¼è¯¥å…¬å¼ ä¸Šé¢çš„cost functionä¸­æ¬§ä¸¤ä¸ªå‚æ•°på’Œqï¼Œé¦–å…ˆ å¯¹ä»–ä»¬åˆ†åˆ«æ±‚åå¯¼ï¼Œæ±‚å‡ºæœ€å¿«ä¸‹é™çš„æ–¹å‘ \\frac {âˆ‚C} {âˆ‚p_{uf}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})*q_{ik} + 2\\lambda p_{uk} \\\\ \\frac {âˆ‚C} {âˆ‚q_{if}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})*p_{uk} + 2\\lambda q_{ik} ç„¶åæ ¹æ®éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œéœ€è¦å°†å‚æ•°æ²¿æœ€å¿«çš„ä¸‹é™æ–¹å‘å‰è¿›ï¼Œå³å¯å¾—åˆ°å¦‚ä¸‹çš„é€’æ¨å…¬å¼ï¼š p_{uf} = p_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) * q_{ik} - \\lambda p_{uk}) \\\\ q_{if} = q_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) * p_{uk} - \\lambda q_{ik}) æ‰€ä»¥ï¼Œæ‰§è¡ŒLFMéœ€è¦ï¼š æ ¹æ®æ•°æ®é›†åˆå§‹åŒ–På’ŒQçŸ©é˜µï¼ˆå¦‚ä½•åˆå§‹åŒ–ï¼‰ ç¡®å®šå››ä¸ªå‚æ•°ï¼šåˆ†ç±»ä¹¦Fï¼Œè¿­ä»£æ¬¡æ•°Nï¼Œå­¦ä¹ é€Ÿç‡Î±ï¼ˆÎ± = 0.9ï¼‰ å’Œæ­£åˆ™åŒ–å‚æ•°*Î» â€‹ ä¼ªä»£ç  1234567891011121314151617181920def LFM(user_items, F, N, alpha, lambda): #åˆå§‹åŒ–P,QçŸ©é˜µ [P, Q] = InitModel(user_items, F) #å¼€å§‹è¿­ä»£ For step in range(0, N): #ä»æ•°æ®é›†ä¸­ä¾æ¬¡å–å‡ºuserä»¥åŠè¯¥userå–œæ¬¢çš„itermsé›† for user, items in user_item.iterms(): #éšæœºæŠ½æ ·ï¼Œä¸ºuseræŠ½å–ä¸itemsæ•°é‡ç›¸å½“çš„è´Ÿæ ·æœ¬ï¼Œå¹¶å°†æ­£è´Ÿæ ·æœ¬åˆå¹¶ï¼Œç”¨äºä¼˜åŒ–è®¡ç®— samples = RandSelectNegativeSamples(items) #ä¾æ¬¡è·å–itemå’Œuserå¯¹è¯¥itemçš„å…´è¶£åº¦ for item, rui in samples.items(): #æ ¹æ®å½“å‰å‚æ•°è®¡ç®—è¯¯å·® eui = eui - Predict(user, item) #ä¼˜åŒ–å‚æ•° for f in range(0, F): P[user][f] += alpha * (eui * Q[f][item] - lambda * P[user][f]) Q[f][item] += alpha * (eui * P[user][f] - lambda * Q[f][item]) #æ¯æ¬¡è¿­ä»£å®Œåï¼Œéƒ½è¦é™ä½å­¦ä¹ é€Ÿç‡ã€‚ä¸€å¼€å§‹çš„æ—¶å€™ç”±äºç¦»æœ€ä¼˜å€¼ç›¸å·®ç”šè¿œï¼Œå› æ­¤å¿«é€Ÿä¸‹é™ï¼› #å½“ä¼˜åŒ–åˆ°ä¸€å®šç¨‹åº¦åï¼Œå°±éœ€è¦æ”¾æ…¢å­¦ä¹ é€Ÿç‡ï¼Œæ…¢æ…¢çš„æ¥è¿‘æœ€ä¼˜å€¼ã€‚ alpha *= 0.9 Baseline Estimats å¯¹æ¯”åŸºçº¿è€ƒè™‘åˆ°é‡å£éš¾è°ƒï¼Œæœ‰äº›userä¼šç»™å‡ºæ¯”è¾ƒé«˜çš„åˆ†æ•°ï¼Œæœ‰äº›è¦å»ä¸¥æ ¼çš„usersä¼šç»™å‡ºæ¯”è¾ƒä½çš„åˆ†æ•°ï¼Œè€Œæœ‰äº›è´¨é‡å¥½çš„å•†å“ä¼šå¾—åˆ°æ¯”è¾ƒé«˜çš„åˆ†æ•°ï¼Œè´¨é‡å·®çš„åˆ†æ•°è¾ƒä½ã€‚æ‰€ä»¥ä¸ºäº†è°ƒæ•´è¿™äº›ï¼Œå¼•å…¥äº†baseline estimateã€‚æ¯”å¦‚ä¸ºäº†ä¼°è®¡æŸä¸ªç”¨æˆ·uä¼šç»™ç”µå½±iæ‰“çš„è¯„åˆ†ï¼š b_{ui} = u + b_u + b_i $\\mu$ æ˜¯è¯¥ç‰©å“çš„æ•´ä½“å¹³å‡å€¼ $b_u$ æ˜¯ç”¨æˆ·æ‰“åˆ†ç›¸å¯¹æ•´ä½“ç”¨æˆ·æ‰“åˆ†å¹³å‡å€¼çš„åå·® $b_i$ æ˜¯è¯¥ç‰©å“ç›¸å¯¹æ•´ä½“å¹³å‡å€¼çš„åå·® ä¸¾ä¸ªä¾‹å­ï¼šé¢„æµ‹è±†ç“£ç”¨æˆ·å°æ˜ç»™ç”µå½±æ³°å¦å°¼å…‹å·çš„è¯„åˆ† æ³°å¦å°¼å…‹å·åœ¨è±†ç“£ä¸Šçš„å¹³å‡åˆ†æ•°æ˜¯3.7åˆ†(u) æ³°å¦å°¼å…‹å·çš„å¹³å‡åˆ†æ•°åˆæ¯”æ‰€æœ‰ç”µå½±åœ¨è±†ç“£ä¸Šçš„å¹³å‡åˆ†æ•°é«˜0.5åˆ†($b_i$) ä½†æ˜¯å°æ˜æ˜¯ä¸ªç”µå½±çˆ±å¥½è€…ï¼Œæ¯”å¹³å‡ç”¨æˆ·æ‰“åˆ†åä½0.3åˆ†($b_u$) æ‰€ä»¥ä¸è€ƒè™‘regularizedçš„è¯ï¼Œé¢„æµ‹å°æ˜ç»™æ³°å¦å°¼å…‹å·çš„è¯„åˆ†åº”è¯¥æ˜¯ 3.7 - 0.3 + 0.5 = 3.9 æ‰€ä»¥è¿™é‡Œï¼Œå¾—åˆ° $b_u$ å’Œ $b_i$ çš„å€¼å¾ˆé‡è¦ è¿™é‡Œå¯¹$b_u$å’Œ$b_i$åŠ å…¥äº†penalityï¼Œä¸ºäº†é˜²æ­¢overfittingã€‚ è¿™é‡Œ$r_ui$æ˜¯æˆ‘ä»¬è®­ç»ƒæ•°æ®ä¸­çš„å·²çŸ¥ratingï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ $\\mu$æ˜¯æ•´ä½“å¹³å‡å€¼ï¼Œä¹Ÿå¯ä»¥æ ¹æ®è®­ç»ƒæ•°æ®è®¡ç®—å‡ºæ¥ $\\lambda_2$å’Œ$\\lambda_3$æ˜¯æˆ‘ä»¬æ‰‹åŠ¨è®¾ç½®çš„å‚æ•°ï¼ŒMovieLensæ•°æ®ä¸Šï¼Œ20æ¯”è¾ƒåˆé€‚ R(u)å’ŒR(i)ä¸ºç”¨æˆ·uratingè¿‡çš„ç‰©å“çš„é›†åˆï¼Œå’Œç‰©å“iè¢«ratingè¿‡ç”¨æˆ·çš„é›†åˆ æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™äº›æ•°æ®ï¼Œè®¡ç®—å‡º$b_u$å’Œ$b_i$b_i$ï¼ˆæ³¨æ„ç»´åº¦ï¼‰ b_i = \\frac {\\sum_{u \\in R(u)}(r_{ui} - \\mu)} {\\lambda_2 + |R(i)|} \\\\ b_u = \\frac {\\sum_{i \\in R(u)}(r_{ui} - \\mu - b_i)} {\\lambda_3 + |R(u)|}é™¤äº†ä¸Šé¢çš„æ–¹æ³•ï¼Œè¿˜æœ‰ä¸€ç§æ›´ä¸ºç®€ä¾¿çš„æ–¹æ³•è®¡ç®—$b_u$å’Œ$b_i$ï¼Œå°±æ˜¯ç›´æ¥ä½¿ç”¨userï¼Œitemçš„ratingçš„å¹³å‡å€¼ä¼°è®¡ b_u = \\frac {\\sum R(u)} {len(R(u))} \\\\ b_i = \\frac {\\sum R(i)} {len(R(i))}Neighborhood Models item-oriented algorithm: a rating is estimated using known rating made by the same user on similarity items. user-oriented algorithm: estimate unknown ratings based on recorded ratings of like minded users. Similarity measure between items Pearson correlation è®¡ç®—ç‰©å“iå’Œjçš„ç›¸ä¼¼åº¦ s_{ij} = \\frac {n_{ij}} {n_{ij} + \\lambda_2}Ï_{ij} n_ij è¡¨ç¤ºéƒ½å¯¹ç‰©å“iå’Œjè¯„åˆ†è¿‡çš„ç”¨æˆ·çš„æ•°é‡ Ïijæ˜¯çš®å°”é€Šç³»æ•°ï¼Œé€šå¸¸å–ï¼Ÿï¼Ÿï¼Ÿ Î»2 é€šå¸¸å–100 é¢„æµ‹è¯„åˆ† \\hat r_{ui} = b_{ui} + \\frac{\\sum_{j \\in S^k(i; u)} s_{ij}(r_{ui} - b_{ui})} {\\sum_{j \\in S^k(i; u)}s_{ij}} åœ¨ç”¨æˆ·uæ‰€æœ‰è¯„åˆ†è¿‡çš„ç‰©å“ä¸­ï¼Œæ‰¾åˆ°ç›¸ä¼¼åº¦å’Œiæœ€é«˜çš„kä¸ªç‰©å“ï¼ˆk-neighborsï¼‰ï¼Œç”¨$S^k(i; u)$è¡¨ç¤º ä½†æ˜¯è¿™ç§ç®—æ³•è¿˜æ˜¯æœ‰ä¸€äº›å±€é™æ€§ï¼Œæ¯”å¦‚å¯¹äºä¸¤ä¸ªå®Œå…¨æ²¡æœ‰å…³ç³»çš„ç‰©å“ä¹‹é—´çš„é¢„æµ‹ã€‚æˆ–è€…æ˜¯å¯¹äºæŸäº›ç‰©å“ï¼Œæœ€ä¸ºç›¸ä¼¼çš„kä¸ªç‰©å“ç¼ºå¤±ï¼Œæ‰€ä»¥å¯ä»¥ä¿®æ­£ä»¥ä¸Šçš„å…¬å¼ï¼ˆä¸æ˜¯å¾ˆæ˜ç™½è¿™é‡Œï¼‰ \\hat r_{ui} = b_{ui} + \\sum_{j \\in S^k(i; u)} \\theta^u_{ij}(r_{ui} - b_{ui}) \\\\ \\{\\theta^u_{ij} | j \\in S^k(i; u)\\}","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆäºŒï¼‰","date":"2017-05-04T14:13:45.000Z","path":"2017/05/04/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆäºŒï¼‰/","text":"1. Mapper 2. Reducer 3. MapReduceæ•°æ®éƒ½æ˜¯ä»¥ key-values pairs çš„å½¢å¼åœ¨Mapperå’ŒReduceä¹‹é—´ä¼ é€’çš„ Mapperè¾“å‡ºçš„ key-value pairs åº”è¯¥å’Œ Reducerè¾“å…¥çš„ key-value pairs ç±»å‹æ˜¯ä¸€æ ·çš„ åœ¨Reducerä¸­ï¼Œæ˜¯ key-valueLists pairs çš„å½¢å¼ 4. Deriverä¹Ÿå°±æ˜¯åˆå§‹åŒ–é…ç½®MRç„¶åè°ƒç”¨æ‰§è¡Œï¼Œä¸€èˆ¬å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ï¼š 123456789101112131415161718192021222324252627282930313233// è€APIpublic void run(String inputPath, String outputPath) throws Exception &#123; JobConf conf = new JobConf(WordCount.class); conf.setJobName(\"wordcount\"); // the keys are words (strings) conf.setOutputKeyClass(Text.class); // the values are counts (ints) conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(MapClass.class); conf.setReducerClass(Reduce.class); FileInputFormat.addInputPath(conf, new Path(inputPath)); FileOutputFormat.setOutputPath(conf, new Path(outputPath)); JobClient.runJob(conf);&#125;// æ–°APIpublic void run(String IN, String OUT) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1);&#125; 5. Data Flowåœ¨hadoopä¸­æ‰€æœ‰çš„Mapperå’ŒReduceréƒ½æ˜¯ç‹¬ç«‹å·¥ä½œçš„ï¼Œè¿™ä¹Ÿæ˜¯hadoopåˆ†å¸ƒå¼èƒ½å¤Ÿç¨³å®šè¿è¡Œçš„åŸå› ä¹‹ä¸€(æœ‰åˆ©äºå®¹é”™å¤„ç†)ã€‚åœ¨MapReduceæ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œåªæœ‰ä¸€æ¬¡æ•°æ®ç›¸äº’äº¤äº’ï¼Œå°±æ˜¯Mapperåˆ°Reducerè¿™ä¸ªè¿‡ç¨‹ã€‚ä»Mapperè¾“å‡ºçš„æ‰€æœ‰çš„intermediate dataä¼šè¢«ç»Ÿä¸€shuffle(å¿…é¡»è¦ç­‰æ‰€æœ‰MRæ‰§è¡Œå®Œæ¯•å—ï¼Ÿ)ï¼Œç„¶ååŒä¸€ä¸ªkeyçš„key-value pairs ä¼šè¢«åˆ†é…åˆ°åŒä¸€ä¸ªreducerä¸­å»ã€‚ 6. A Closer Lookç¬¬äº”éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„æ˜¯MRçš„å®è§‚æµç¨‹ï¼Œå…·ä½“çš„æµç¨‹å…·ä½“å¯ä»¥åˆ†ä¸º Map -&gt; Combiner -&gt; Partitioner -&gt; Sort -&gt; Shuffle -&gt; Sort -&gt; Reduce(è¿™é‡Œå¯¹shuffleçš„å®šä¹‰æœ‰ç‚¹ä¸åŒï¼Œä¸ªäººè®¤ä¸ºä»mapçš„è¾“å‡ºåˆ°reduceçš„è¾“å…¥è¿™æ®µè¿‡ç¨‹å¯ä»¥ç§°ä¹‹ä¸ºshuffleã€‚åŒæ—¶æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œcombineræ˜¯åœ¨mapperæœ€ç»ˆè¾“å‡ºå‰å¤šæ¬¡è°ƒç”¨çš„ï¼Œä»¥åŠåœ¨reduceré‡Œä¹Ÿæœ‰è°ƒç”¨) ä¸‹å›¾ä¸­ï¼Œinput filesåœ¨è¿›å…¥åˆ°Mapperä¹‹å‰æ—¶ï¼Œä¼šå¯¹è¿™äº›æ–‡ä»¶splitï¼Œå› ä¸ºä¸€ä¸ªmapperä¸€èˆ¬æ˜¯64MBæˆ–è€…128MBï¼Œå½“å¤§äºçš„æ—¶å€™éœ€è¦å¯¹è¾“å…¥æ–‡ä»¶å¤„ç†ï¼Œç„¶åä¼ ç»™RecordReadersï¼Œä»¥key-value pairsçš„å½¢å¼ä¼ ç»™mapperã€‚ å¯¹äºæ‰€æœ‰Mapperçš„outputï¼Œå…ˆä¼šå¯¹å…¶è¿›è¡Œparitionæ“ä½œï¼Œä¹Ÿå°±æ˜¯å†³å®šå»å“ªä¸€ä¸ªReducerã€‚å½“ç¡®å®šå¥½å“ªäº›Reducerï¼Œè¿™äº›key-value paris å°±ä¼šä¼ å…¥åˆ°è¯¥Reducerç›¸åº”çš„åˆ†åŒºï¼Œç„¶åå¯¹é½è¿›è¡Œæ’åºã€‚æœ€åå°†sortå¥½çš„key-valueLists ä¼ å…¥åˆ°Reducerè¿›è¡Œå¤„ç†ã€‚ 7. ShuffleHadoop The Definitive Guide P197 Mapper æ ¹æ®å®˜æ–¹å›¾ï¼ŒMapperçš„outputå‡ºæ¥çš„key-values pairsä¼šå…ˆè¿›å…¥åˆ°buffer memory(é»˜è®¤100MBå¤§å°)ï¼Œä½†bufferåˆ°80%å®¹é‡çš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹ä¼šspillåˆ°diskå»ï¼Œå¦‚æœæ­¤æ—¶bufferè¿˜æœªæ…¢çš„æƒ…å†µä¸‹ï¼Œmapperç»§ç»­è¾“å‡ºåˆ°bufferï¼Œå¦‚æœæ»¡äº†çš„è¯mapperä¼šè¢«blockï¼Œç›´åˆ°å¯ä»¥å†™å…¥ã€‚ åœ¨bufferä¸­çš„å†…å®¹spillåˆ°diskä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªpartitionerçš„æ­¥éª¤ã€‚å¯¹è¿™äº›å³å°†å†™å…¥åˆ°diskçš„å†…å®¹åˆ†ç»„ï¼ŒåŒä¸€ä¸ªkeyå’ŒåŒä¸€ä¸ªreducerçš„ker-value pairsä¼šåœ¨ä¸€èµ·ã€‚ç„¶åè¿™äº›key-value pairsæ’åº(sort)ã€‚å¦‚æœæ­¤æ—¶æˆ‘ä»¬å®šä¹‰äº†combiner functionï¼Œåœ¨è¾“å‡ºå‰ï¼Œè¿™äº›pairsä¼šå…ˆcombineï¼Œç„¶åè¾“å‡ºã€‚ä¹Ÿå°±æ˜¯è¯´combiner functionæ˜¯åœ¨partitionerä¹‹å? (Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer) æ¯å½“bufferè¾¾åˆ°é‚£ä¸ªthresholdçš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹å†™å…¥åˆ°diskä¸­(æ³¨æ„æ˜¯æ¯ä¸ªclusterè‡ªå·±çš„local diskï¼Œè€Œä¸æ˜¯HDFS)ï¼Œæ­¤æ—¶ä¼šæ–°å»ºä¸€ä¸ªä¸´æ—¶çš„spillæ–‡ä»¶ï¼Œæ¯æ¬¡å†™å…¥éƒ½ä¼šæ–°å»ºä¸€ä¸ªï¼Œç„¶åè¿™ä¸ªmapç»“æŸå‰è¿™äº›spill filesä¼šè¢«mergeåˆ°ä¸€ä¸ªpartitionedå’Œsortedçš„æ–‡ä»¶é‡Œå»ã€‚é™¤äº†ä¹‹å‰partitionçš„è¾“å‡ºåè°ƒç”¨äº†ä¸€æ¬¡combinerä¸€æ¬¡ä¹‹å¤–ï¼Œä½†åˆå¹¶è¿™äº›ä¸ªspill files( &gt;=3 )çš„æ—¶å€™ï¼Œä¼šç»§ç»­è°ƒç”¨combinerå»åˆå¹¶åŒä¸€ä¸ªreduceré‡Œçš„åŒä¸€ä¸ªkeyçš„valueï¼Œæ‰€ä»¥combineråœ¨output fileè¢«ç”Ÿæˆä¹‹å‰ï¼Œä¼šè¢«è°ƒç”¨è®¸å¤šæ¬¡ï¼Œä»¥å‡å°‘ä¹‹åioçš„æ¬¡æ•°ã€‚ä½†å½“spill filesåªæ˜¯1ä¸ªæˆ–åˆ™ä¸¤ä¸ªçš„æ—¶å€™ï¼Œå¹¶ä¸ä¼šè°ƒç”¨combinerã€‚æ³¨æ„æœ‰äº›æƒ…å†µä¸‹combinerå¹¶ä¸é€‚åˆä½¿ç”¨ï¼Œæ¯”å¦‚æ±‚å¹³å‡å€¼ã€‚ è€Œä¸åŒmapperç”Ÿæˆçš„spillæ–‡ä»¶æœ€ç»ˆä¼šè¢«mergeæˆ {key:[v1, v2, v3â€¦], â€¦}è¿™ç§å½¢å¼ã€‚(è¿™é‡Œæœ‰ç–‘é—®ï¼Œåšå®¢å›¾å’Œå®˜æ–¹å›¾æœ‰ç‚¹ä¸ä¸€æ ·ã€‚æŒ‰ç…§å®˜æ–¹å›¾çš„ç†è§£ï¼Œä¸€ä¸ªmapperå¯¹åº”çš„æ˜¯ä¸€ä¸ªspill fileï¼Œæ‰€ä»¥æœ€ç»ˆæ˜¯å¤šä¸ªspill filesï¼Ÿè¿˜æ˜¯è¿™äº›spill filesåœ¨ä¼ ç»™Reducerä¹‹å‰ä¼šè¢«mergeæˆä¸Šè¿°çš„listå½¢å¼ï¼Ÿä¸€ä¸ªclusterä¸€ä¸ªæœ€ç»ˆçš„output file)â€‹ Reducer ä¹‹å‰mapperç«¯çš„æ‰€æœ‰å·¥ä½œå·²ç»å®Œæˆäº†ï¼Œæ‰€æœ‰çš„mapperçš„outputéƒ½å·²ç»è¢«å†™å…¥åˆ°äº†ä¸€ä¸ªoutputæ–‡ä»¶é‡Œé¢å»äº†ã€‚é‚£ä¹ˆReducerå°±æ˜¯è¦æŠŠè¿™ä¸ªæ–‡ä»¶é‡Œé¢çš„key-valueLists pairs åˆ†ç»™ä¸åŒçš„reducersï¼Œè€Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¹‹ä¸ºFetchï¼Œå°±æ˜¯å°†ç›¸åº”çš„key-valueList pairs æ‹‰å–åˆ°ç›¸åº”çš„reducersé‡Œé¢å»ã€‚ åœ¨Reduceré˜¶æ®µï¼Œæ¯ä¸ªreducerä¼šè°ƒç”¨çº¿ç¨‹ä»å¤šä¸ªä¸åŒçš„clusterçš„output fileé‡Œé¢fetchæ•°æ®ï¼Œç„¶åå¯¹è¿™äº›æ•°æ®mergeã€‚è¿™é‡Œçš„è¿‡ç¨‹å’Œä¹‹å‰çš„mapperæœ‰ç‚¹åƒã€‚reducerä¹Ÿæœ‰ä¸€ä¸ªbuffer memory(é€šè¿‡JVMçš„heap sizeæ¥è®¾ç½®)ï¼Œfetchçš„æ•°æ®ä¼šè¢«ä¼ åˆ°é‡Œé¢(å¦‚æœæ”¾å¾—ä¸‹)ï¼Œå½“bufferè¾¾åˆ°thresholdçš„å€¼çš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹ä¼šè¢«mergeç„¶åspillåˆ°diské‡Œé¢å»(å®é™…ä¸Šæœ‰å¤šç§å½¢å¼å­˜æ”¾è¿™ä¸ªæ–‡ä»¶ï¼Œè¿™é‡Œä¸è®¨è®º)ï¼Œå¦‚æœä¹‹å‰æˆ‘ä»¬å·²ç»å®šä¹‰äº†combinerï¼Œè¿™é‡Œcombinerä¹Ÿä¼šè¢«è°ƒç”¨ã€‚ç›´åˆ°æ‰€æœ‰çš„mapperçš„output fileéƒ½è¢«fetchåˆ°ä¸€ä¸ªæ–‡ä»¶é‡Œå»ï¼Œreducerä¼šåœ¨è¾“å…¥å‰sorté‡Œé¢çš„å†…å®¹(å®é™…ä¸Šmergeçš„æ—¶å€™å·²ç»sortäº†)ï¼Œç„¶åè¿™ä¸ªå·²ç»æ’å¥½åºçš„fileå°±ä¼šè¢«ä¼ åˆ°reduceré‡Œé¢æ‰§è¡Œï¼Œæœ€ç»ˆè¾“å‡ºåˆ°HDFSä¸Šã€‚","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.cc/tags/MapReduce/"}]},{"title":"Sparkç¬”è®°","date":"2017-04-10T06:55:40.000Z","path":"2017/04/10/Sparkç¬”è®°/","text":"1. Resilient Distributed Dataset (RDD)RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators. RDDæ˜¯ä¸€ä¸ªå®¹é”™çš„ã€å¹¶è¡Œçš„æ•°æ®ç»“æ„ï¼Œå¯ä»¥è®©ç”¨æˆ·æ˜¾ç¤ºåœ°å°†æ•°æ®å­˜å‚¨åˆ°ç£ç›˜å’Œå†…å­˜ä¸­ï¼Œå¹¶èƒ½æ§åˆ¶æ•°æ®çš„åˆ†åŒºã€‚ Resilient RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Formally, an RDD is a read-only, partitioned collection of records. Rdds can be created throught deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel. There two ways to create RDDs. Parallelizing parallelizing an existing collection in your driver program Referencing reference a dataset in an external storage system, such as a shared file system, HDFS, HBase, or ant data source offering a Hadoop Input Format Sparkæä¾›äº†RDDä¸Šçš„ä¸¤ç±»æ“ä½œï¼štransformation å’Œ action Transformation: return a new RDD å½“å¯¹RDDè¿›è¡Œtransformationæ“ä½œçš„æ—¶å€™ï¼Œè¿™äº›æ“ä½œä¸ä¼šç«‹åˆ»å°±æ‰§è¡Œï¼Œè€Œæ˜¯å°†è¿™äº›æ“ä½œè®°å½•ä¸‹æ¥ã€‚å¦‚æœæœ‰å¤šä¸ªtransformationæ“ä½œæ—¶ï¼Œæ¯ä¸€æ¬¡å˜æ¢éƒ½æ˜¯ä¸€ä¸ªæ¥ç€ä¸€ä¸ªï¼Œæ­¤æ—¶å°±å½¢æˆäº†ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ã€‚è¿™ä¸ªå›¾å°±æ˜¯æ•°æ®å®¹é”™çš„å…³é”®æ‰€åœ¨ã€‚å¦‚æœå‡ºç°æ•°æ®ä¸¢å¤±çš„æ—¶å€™ï¼Œåªéœ€è¦æŸ¥æ‰¾è¿™ä¸ªå›¾å°±èƒ½æ ¹æ®ä¸¢å¤±çš„RDDè¿›è¡Œæ•°æ®æ¢å¤ Action: evaluates and returns a new value å½“å¯¹RDDåšactionæ“ä½œçš„æ—¶å€™ï¼Œè¿™ç±»actionä¸€èˆ¬ä½œç”¨æ˜¯è¿”å›ä¸€ä¸ªå€¼æˆ–è€…æ•°ç»„ç­‰ï¼Œæˆ–è€…æ˜¯å°†æ•°æ®æŒä¹…åŒ–åˆ°ç£ç›˜å½“ä¸­ã€‚æ­¤æ—¶æ‰§è¡Œactionï¼Œä¼šçœŸæ­£çš„æäº¤jobï¼Œæ‰§è¡Œä¹‹å‰çš„transformationè®°å½•çš„DAGã€‚æ‰§è¡ŒDAGç­–ç•¥ä¸­ï¼Œä¼šæœ‰å¤šä¸ªstageï¼Œæ¯ä¸€ä¸ªstageé’Ÿæœ‰å¤šä¸ªtaskï¼Œè¿™äº›taskå°±ä¼šè¢«åˆ†é…åˆ°å„ä¸ªnodesè¿›è¡Œæ‰§è¡Œ The difference between flatMap and Map flatMap Example map: å¯¹RDDæ¯ä¸ªå…ƒç´ è½¬æ¢ï¼Œå°†å‡½æ•°ç”¨äºRDDä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œå°†è¿”å›å€¼æ„æˆæ–°çš„RDDã€‚flatMap: å¯¹RDDæ¯ä¸ªå…ƒç´ è½¬æ¢, ç„¶åå†æ‰å¹³åŒ–ï¼ˆå³å°†æ‰€æœ‰å¯¹è±¡åˆå¹¶ä¸ºä¸€ä¸ªå¯¹è±¡ï¼‰ã€‚å°†å‡½æ•°åº”ç”¨äºrddä¹‹ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå°†è¿”å›çš„è¿­ä»£å™¨çš„æ‰€æœ‰å†…å®¹æ„æˆæ–°çš„rdd Example1ï¼š 123456// data æœ‰ä¸¤è¡Œæ•°æ®ï¼Œç¬¬ä¸€è¡Œ a b cï¼Œç¬¬äºŒè¡Œ 1 2 3scala&gt; data.map(line1 =&gt; line1.split(\",\")).collect()res11: Array[Array[String]] = Array(Array(a, b, c),Array(1, 2, 3))scala&gt; data.flatMap(line1 =&gt; line1.split(\",\")).collect()res13: Array[String] = Array(a, b, c, 1, 2, 3) Example2ï¼š 1234567891011scala&gt; val rdd = sc.parallelize(List(\"coffee panda\",\"happy panda\",\"happiest panda party\"))scala&gt; rdd.map(x=&gt;x).collectres0: Array[String] = Array(coffee panda, happy panda, happiest panda party) scala&gt; rdd.map(x=&gt;x.split(\" \")).collectres1: Array[Array[String]] = Array(Array(coffee, panda), Array(happy, panda), Array(happiest, panda, party))// ç›¸æ¯”ä¹‹å‰ï¼ŒflatMapå°†å‡ ä¸ªarrayåˆå¹¶æˆäº†ä¸€ä¸ªarrayscala&gt; rdd.flatMap(x=&gt;x.split(\" \")).collectres2: Array[String] = Array(coffee, panda, happy, panda, happiest, panda, party) 2. References ç†è§£Sparkçš„æ ¸å¿ƒRDD Spark RDD API Examples","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.cc/tags/Spark/"}]},{"title":"å­—ç¬¦ä¸²æœç´¢ç®—æ³• - BM","date":"2017-04-02T04:28:44.000Z","path":"2017/04/02/å­—ç¬¦ä¸²æœç´¢ç®—æ³•-BM/","text":"å½“æˆ‘ä»¬è¦åœ¨æŸä¸€ä¸ªæ–‡æœ¬ä¸­è¦åŒ¹é…æŸä¸€ä¸ªå­—ç¬¦ä¸²çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ä¸€ä¸ªä¸€ä¸ªåŒ¹é…ï¼Œä¹Ÿå°±æ˜¯ä»å¤´å¼€å§‹åŒ¹é…æ–‡æœ¬å’Œå­—ç¬¦ä¸²ï¼Œä½†å‘ç°ä¸åŒçš„æ—¶å€™ï¼Œå°±æŠŠæ•´ä¸ªå­—ç¬¦ä¸²å³ç§»ä¸€ä½ï¼Œç„¶åä»å¤´é‡æ–°å’Œæ–‡æœ¬æ¯”è¾ƒã€‚å½“å…¨éƒ¨ç›¸åŒçš„æ—¶å€™ï¼Œåˆ™å³ç§»å­—ç¬¦ä¸²çš„é•¿åº¦ã€‚åœ¨ä¹‹å‰çš„ KMP ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬æ˜¯æ‰¾åˆ°å°½å¯èƒ½å³ç§»çš„æœ€å¤§ä½æ•°ï¼Œè€Œä¸æ˜¯ä¸€ä½ä¸€ä½çš„ç§»åŠ¨ï¼Œè¿™æ ·ç›¸æ¯”åŸå…ˆæ•ˆç‡å·²ç»æé«˜äº†å¾ˆå¤šäº†ã€‚ä½†åœ¨å¤§éƒ¨åˆ†çš„ç¼–è¾‘å™¨ä¸­ï¼Œâ€œæŸ¥æ‰¾â€åŠŸèƒ½ä½¿ç”¨çš„ä¸æ˜¯ KMP ï¼Œ è€Œæ˜¯ BM ç®—æ³•ã€‚ 1. Boyer-Moore Algorithmå’Œ KMP ä¸åŒçš„æ˜¯ï¼ŒBM æ˜¯ä» Pattern P å€’ç€åŒ¹é…ä¸Šæ¥çš„ é¦–å…ˆå®šä¹‰ä¸¤ä¸ªé¢„å¤„ç†çš„æ–¹æ³• åå­—ç¬¦ (Bad Character Heuristic) å½“æ–‡æœ¬ T ä¸­çš„æŸä¸ªå­—ç¬¦è·Ÿ Pattern P çš„æŸä¸ªå­—ç¬¦ä¸åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬ç§°æ–‡æœ¬ T ä¸­çš„è¿™ä¸ªä¸åŒ¹é…çš„å­—ç¬¦ä¸ºåå­—ç¬¦ã€‚ å¥½åç¼€ (Good Suffix Heuristic) å½“æ–‡æœ¬ T ä¸­çš„æŸä¸ªå­—ç¬¦è·Ÿ Pattern P çš„æŸä¸ªå­—ç¬¦ä¸åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬ç§°æ–‡æœ¬ T ä¸­çš„å·²ç»åŒ¹é…çš„å­—ç¬¦ä¸²ä¸ºå¥½åç¼€ã€‚(å› ä¸ºç®—æ³•ä»å°¾å·´åˆ°å¤´éƒ¨æ¯”è¾ƒ) å¥½åç¼€çš„ä½ç½®ä»¥æœ€åä¸€ä¸ªå­—ç¬¦ä¸ºå‡†ã€‚å‡å®šâ€ABCDEFâ€çš„â€EFâ€æ˜¯å¥½åç¼€ï¼Œåˆ™å®ƒçš„ä½ç½®ä»¥â€Fâ€ä¸ºå‡†ï¼Œå³5ï¼ˆä»0å¼€å§‹è®¡ç®—ï¼‰ã€‚ å¦‚æœâ€å¥½åç¼€â€åœ¨æœç´¢è¯ä¸­åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®ä¸º -1ã€‚æ¯”å¦‚ï¼Œâ€EFâ€åœ¨â€ABCDEFâ€ä¹‹ä¸­åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®ä¸º-1ï¼ˆå³æœªå‡ºç°ï¼‰ã€‚ å¦‚æœâ€å¥½åç¼€â€æœ‰å¤šä¸ªï¼Œåˆ™é™¤äº†æœ€é•¿çš„é‚£ä¸ªâ€å¥½åç¼€â€ï¼Œå…¶ä»–â€å¥½åç¼€â€çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®å¿…é¡»åœ¨å¤´éƒ¨ã€‚æ¯”å¦‚ï¼Œå‡å®šâ€BABCDABâ€çš„â€å¥½åç¼€â€æ˜¯â€DABâ€ã€â€ABâ€ã€â€Bâ€ï¼Œè¯·é—®è¿™æ—¶â€å¥½åç¼€â€çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®æ˜¯ä»€ä¹ˆï¼Ÿå›ç­”æ˜¯ï¼Œæ­¤æ—¶é‡‡ç”¨çš„å¥½åç¼€æ˜¯â€Bâ€ï¼Œå®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®æ˜¯å¤´éƒ¨ï¼Œå³ç¬¬0ä½ã€‚è¿™ä¸ªè§„åˆ™ä¹Ÿå¯ä»¥è¿™æ ·è¡¨è¾¾ï¼šå¦‚æœæœ€é•¿çš„é‚£ä¸ªâ€å¥½åç¼€â€åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å¯ä»¥æŠŠæœç´¢è¯æ”¹å†™æˆå¦‚ä¸‹å½¢å¼è¿›è¡Œä½ç½®è®¡ç®—â€(DA)BABCDABâ€ï¼Œå³è™šæ‹ŸåŠ å…¥æœ€å‰é¢çš„â€DAâ€ã€‚ æŒ‰ç…§æ­£å¸¸æ¯”è¾ƒæ–¹æ³•ï¼Œå½“å‘ç°ä¸€ä¸ªåå­—ç¬¦çš„æ—¶å€™ï¼Œä¼šå‡ºç°ä¸€ä¸‹ä¸‰ç§æƒ…å†µ åå­—ç¬¦å®Œå…¨ä¸å‡ºç°åœ¨ Pattern P ä¸­ï¼Œé‚£ä¹ˆå°±å¯ä»¥å®Œå…¨è·³è¿‡è¿™ä¸ªå­—ç¬¦ï¼Œå› ä¸ºæˆ‘ä»¬æ ¹æœ¬ä¸å¯èƒ½åŒ¹é…åˆ°è¿™ä¸ªå­—ç¬¦ã€‚ åå­—ç¬¦å‡ºç°åœ¨è¿˜æœªæ¯”è¾ƒçš„å‰ç¼€å½“ä¸­ (åªå‡ºç°åœ¨å‰ç¼€ï¼Œå‡ºç°åœ¨å‰ç¼€å’Œåç¼€) æ­¤æ—¶å°† Pattern P å³ç§»å¯¹é½è¿™ä¸¤ä¸ªå­—ç¬¦ï¼Œç„¶åä»å°¾éƒ¨ç»§ç»­æ¯”è¾ƒ åå­—ç¬¦åªå‡ºç°åœ¨ä¹‹å‰çš„å¥½åç¼€å½“ä¸­ï¼ˆä¹Ÿå°±æ˜¯å·²ç»æ¯”è¾ƒè¿‡äº†ï¼‰ å¯¹è¿™ç§æƒ…å†µä¸åšå¤„ç† æ ¹æ®ä¸Šé¢å‡ ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥æ€»ç»“å‡ºä»¥ä¸‹åå­—ç¬¦è§„åˆ™ï¼š åç§»ä½æ•° = åå­—ç¬¦çš„ä½ç½® - æœç´¢è¯ä¸­çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½® è¿™é‡Œæœ‰ä¸¤ç§ç‰¹æ®Šæƒ…å†µ å¦‚æœåå­—ç¬¦ä¸å­˜åœ¨äº Pattern P ä¸­ï¼Œåˆ™æœ€åä¸€æ¬¡å‡ºç°çš„ä½ç½®ä¸º -1ã€‚ å¦‚æœåå­—ç¬¦åœ¨ Pattern P ä¸­çš„ä½ç½®ä½äºå¤±é…ä½ç½®çš„å³ä¾§ï¼Œåˆ™æ­¤å¯å‘æ³•ä¸æä¾›ä»»ä½•å»ºè®®ã€‚ é™¤äº†åå­—ç¬¦ï¼Œæˆ‘ä»¬åŒæ ·å¯ä»¥åˆ©ç”¨å¥½åç¼€æ¥æé«˜æŸ¥æ‰¾æ•ˆç‡ æ¨¡å¼åç§»ä½æ•° = å¥½åç¼€åœ¨æ¨¡å¼ä¸­çš„å½“å‰ä½ç½® - å¥½åç¼€åœ¨æ¨¡å¼ä¸­æœ€å³å‡ºç°ä¸”å‰ç¼€å­—ç¬¦ä¸åŒçš„ä½ç½® ä¸¾ä¸ªæ —å­è®¡ç®—ï¼š 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 åå­—ç¬¦çš„ç§»åŠ¨ æ­¤æ—¶åå­—ç¬¦çš„ä½ç½®æ˜¯4ï¼Œåå­—ç¬¦ I ä¸åœ¨å­—ç¬¦ä¸²å½“ä¸­ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸ºï¼š 4 - (-1) = 5 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 å¥½åç¼€çš„ç§»åŠ¨ æ­¤æ—¶å¥½åç¼€æ˜¯ MPLEï¼Œå…¶ä¸­ PLE å‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³ä½ç½®ä¸º (01) 2ã€‚ æ­¤æ—¶å¥½åç¼€çš„å­—ç¬¦æ˜¯8(ä»¥æœ€åä¸€ä¸ªä¸ºå‡†)ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸ºï¼š8 - 2 = 6 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å¥½åç¼€çš„æŸ¥æ‰¾æ•ˆç‡æ›´é«˜ã€‚ å¦‚ä½•è®¡ç®—å¥½åç¼€æˆ‘ä»¬ä¹‹å‰å·²ç»æè¿‡äº†ï¼Œç»§ç»­ä»¥ä¸Šé¢æœ€åä¸€ä¸ªä¾‹å­è®²è§£ã€‚ MPLE : æœªå‡ºç°ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› PLE : æœªå‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› LE : æœªå‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› E : å‡ºç°åœ¨å¤´éƒ¨ï¼Œè¡¥å……è™šæ‹Ÿå­—ç¬¦ â€˜MPLâ€™Eï¼Œå‰ç¼€å­—ç¬¦ä¸ºç©ºï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º 0ï¼› æ­¤æ—¶ï¼Œæ‰€æœ‰çš„â€å¥½åç¼€â€ï¼ˆMPLEã€PLEã€LEã€Eï¼‰ä¹‹ä¸­ï¼Œåªæœ‰â€Eâ€åœ¨â€EXAMPLEâ€è¿˜å‡ºç°åœ¨å¤´éƒ¨ï¼Œæ‰€ä»¥åç§» 6 - 0 = 6ä½ã€‚ å¦‚æœä¹‹å‰æˆ‘ä»¬åªåˆ©ç”¨åå­—ç¬¦çš„è¯ï¼Œç§»åŠ¨çš„ä½æ•°åªæœ‰ 2 - (-1) = 3ä½ æ‰€ä»¥æˆ‘ä»¬æœ€ç»ˆçš„ç§»åŠ¨ä½ç½®ï¼Œæ˜¯åœ¨è¿™ä¸¤è€…ä¹‹é—´å–æœ€å¤§å€¼æ¥ç§»åŠ¨ã€‚ æ‰€ä»¥åœ¨æŸ¥æ‰¾ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é¢„è®¡ç®—ç”Ÿæˆåå­—ç¬¦è§„åˆ™è¡¨ å’Œ å¥½åç¼€è§„åˆ™è¡¨ï¼Œåˆ°æ—¶å€™åªéœ€è¦æŸ¥è¡¨å°±å¯ä»¥äº†ã€‚ç»§ç»­ä¸Šé¢çš„ä¾‹å­ã€‚ æ ¹æ®ä¸Šé¢è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬å°† Pattern P å³ç§»6ä½ã€‚ç„¶åé‡æ–°ä»å°¾éƒ¨å¼€å§‹æ¯”è¾ƒ æ–‡æœ¬ä¸­Pä¸åŒ¹é…ï¼Œæ­¤æ—¶åªèƒ½ä½¿ç”¨åå­—ç¬¦è§„åˆ™ï¼ŒPä¸Šä¸€æ¬¡å‡ºç°çš„ä½ç½®ä¸º4ï¼Œåˆ™ç§»åŠ¨6-4=2ä½ ç§»åŠ¨2ä½åï¼Œå‘ç°æ–‡æœ¬ä¸­çš„E ä¸ Pattern Pä¸­çš„E åŒ¹é…ï¼Œåˆ™ç»§ç»­å€’åºæ¯”è¾ƒï¼Œç›´åˆ°å‘ç°å…¨éƒ¨åŒ¹é…ï¼Œåˆ™åŒ¹é…åˆ°çš„ç¬¬ä¸€ä¸ªå®Œæ•´çš„æ¨¡å¼ P è¢«å‘ç°ã€‚ ç»§ç»­ä¸‹å»åˆ™æ˜¯ä¾æ®å¥½åç¼€è§„åˆ™è®¡ç®—å¥½åç¼€ â€œEâ€ çš„åç§»ä½ç½®ä¸º 6 - 0 = 6 ä½ï¼Œç„¶åç»§ç»­å€’åºæ¯”è¾ƒæ—¶å‘ç°å·²è¶…å‡ºæ–‡æœ¬ T çš„èŒƒå›´ï¼Œæœç´¢ç»“æŸã€‚ 2. References å­—ç¬¦ä¸²åŒ¹é…çš„Boyer-Mooreç®—æ³• Boyer-Moore å­—ç¬¦ä¸²åŒ¹é…ç®—æ³•","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.cc/tags/Algorithm/"}]},{"title":"PostgreSQLå¤ä¹ ç¬”è®°","date":"2017-03-23T23:38:55.000Z","path":"2017/03/24/PostgreSQLå¤ä¹ ç¬”è®°/","text":"1 å¸¸è§é—®é¢˜1.1 åŒ¹é…ä¸€è‡´çš„ç¼–ç è§„åˆ™123SET character_set_client=&apos;utf8&apos;; -- gbkSET character_set_connection=&apos;utf8&apos;; -- gbkSET character_set_results=&apos;utf8&apos;; -- gbk 1234-- åˆ›å»ºæ•°æ®åº“æ—¶å€™è®¾ç½®CREATE DATABASE `test`CHARACTER SET &apos;utf8&apos;COLLATE &apos;utf8_general_ci&apos;; 1.2 å¦‚ä½•ç†è§£ç´¢å¼•ç´¢å¼•å­—é¢ä¸Šç†è§£å°±æ˜¯å¯¹æ•°æ®æ‰€å»ºç«‹ç›®å½•ï¼Œå®ƒå¯ä»¥åŠ å¿«æˆ‘ä»¬çš„æŸ¥è¯¢é€Ÿåº¦ï¼Œä½†æ˜¯åŒæ—¶ä¹Ÿé™ä½äº†å¢åˆ æ”¹çš„é€Ÿåº¦ã€‚ åˆ›å»ºåŸåˆ™ ä¸è¦è¿‡åº¦ä½¿ç”¨ç´¢å¼• æœ€å¥½åœ¨æŸ¥è¯¢é¢‘ç¹çš„åˆ—ä¸Šä½¿ç”¨ç´¢å¼• å¦‚æœæ„å»ºç´¢å¼•ï¼Œè¿™ä¸€åˆ—å°½é‡æ˜¯ç¦»æ•£å€¼ï¼Œè€Œä¸è¦è¿‡äºè¿ç»­çš„åŒºé—´ ç´¢å¼•çš„ç±»å‹ æ™®é€šçš„ç´¢å¼• index å”¯ä¸€çš„ç´¢å¼• unique index ä¸€å¼ è¡¨ä¸Šï¼Œåªèƒ½æœ‰ä¸€ä¸ªä¸»é”®ï¼Œä½†æ˜¯å¯ä»¥æœ‰ä¸€ä¸ªæˆ–æ˜¯å¤šä¸ªå”¯ä¸€ç´¢å¼• ä¸»é”®ç´¢å¼• primary key ä¸èƒ½é‡å¤ 12-- æŸ¥çœ‹ä¸€å¼ è¡¨ä¸Šçš„æ‰€æœ‰ç´¢å¼•show index from TABLE_NAMES; 1.3 æ¨¡ç³ŠæŸ¥è¯¢ % åŒ¹é…ä»»æ„å­—ç¬¦ _ åŒ¹é…å•ä¸ªå­—ç¬¦ ä¸¾ä¸ªæ —å­ 1234567891011SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC%&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC_&apos;; 1.4 ç†è§£ COUNT è§3.1 èšé›†å‡½æ•° 1.5 ç†è§£ UNION å’Œ UNION ALL UNION ç”¨äºåˆå¹¶ä¸¤ä¸ªæˆ–æ˜¯å¤šä¸ªSELECTè¯­å¥çš„ç»“æœé›† æ³¨æ„ï¼š SELECTè¯­å¥å¿…é¡»æ‹¥æœ‰ç›¸åŒçš„æ•°é‡çš„åˆ— åˆ—çš„éœ€è¦æ‹¥æœ‰ç›¸ä¼¼çš„æ•°æ®ç±»å‹ æ¯æ¡SELECTè¯­å¥ä¸­çš„åˆ—çš„é¡ºåºå¿…é¡»æ˜¯ä¸€è‡´çš„ ç»“æœä¸å…è®¸æœ‰é‡å¤ï¼Œå¦åˆ™ä½¿ç”¨ UNION ALL 123456SELECT mid, sex, ageFROM TABLE_1UNION -- UNIAON ALL -- å…è®¸æœ‰é‡å¤çš„å€¼å‡ºç°åœ¨ç»“æœé›†ä¸­SELECT mid, sex, ageFROM TABLE_2;-- ORDER BY mid; -- å¯ä»¥å¯¹å…¶ç»“æœè¿›è¡Œæ’åºï¼Œæ³¨æ„çš„æ˜¯æ’åºåªæ˜¯é’ˆå¯¹åˆå¹¶åçš„ç»“æœé›†æ’åº 1.6 ç†è§£ JOINï¼ˆå·¦é“¾æ¥ï¼Œå†…é“¾æ¥å’Œå¤–é“¾æ¥ï¼‰ ä¸åŒçš„å‡ ç§JOINç±»å‹ï¼Œä»¥åŠä¹‹é—´çš„å·®å¼‚ JOINï¼šå¦‚æœè¡¨ä¸­è‡³å°‘æœ‰ä¸€ä¸ªåŒ¹é…ï¼Œåˆ™è¿”å›è¡Œ LEFT JOINï¼šå³ä½¿å³è¡¨ä¸­æ²¡æœ‰åŒ¹é…ï¼Œä¹Ÿä»å·¦è¾¹è¿”å›æ‰€æœ‰çš„è¡Œ RIGHT JOINï¼šå³ä½¿å·¦è¡¨ä¸­æ²¡æœ‰åŒ¹é…ï¼Œä¹Ÿä»å³è¡¨ä¸­è¿”å›æ‰€æœ‰çš„è¡Œ FULL JOINï¼šåªè¦å…¶ä¸­ä¸€ä¸ªè¡¨å­˜åœ¨åŒ¹é…ï¼Œå°±è¿”å›è¡Œ INNER JOIN å¹³å¸¸æˆ‘ä»¬éœ€è¦é“¾æ¥ä¸¤ä¸ªè¡¨çš„æ—¶å€™ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹æ³• 123SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨JOINæ¥å®ç°ä¸Šé¢çš„è¯­å¥ 12345SELECT Person.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName LEFT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName æ³¨æ„åˆ°ä¸Šé¢çš„ï¼Œå·¦è¾¹æ‰€æœ‰çš„è¡Œéƒ½è¿”å›äº†ï¼Œå³ä½¿æ²¡æœ‰å‡ºç°åœ¨å³è¡¨å½“ä¸­ï¼Œæ²¡æœ‰çš„å€¼ä¸ºNULL RIGHT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName æ³¨æ„ï¼Œå³ä½¿å·¦è¾¹æ²¡æœ‰å…¨éƒ¨åŒ¹é…åˆ°å³è¾¹ï¼Œä¾ç„¶åœ¨æœ€åçš„OrderNoä¸­ï¼Œè¿”å›äº†å³è¡¨æ‰€æœ‰çš„è¡Œæ•°ï¼Œæ²¡æœ‰çš„å€¼ä¸ºNULL 1.7 ç†è§£ HAVING è§3.2 åœ¨SQLä¸­å¢åŠ HAVINGå­å¥çš„åŸå› æ˜¯ç”±äºWHEREä¸­æ— æ³•ä½¿ç”¨èšåˆå‡½æ•° 2 æ•°æ®åº“çš„åŸºæœ¬æ“ä½œ2.1 è¡¨çš„ä½¿ç”¨ ä½¿ç”¨åˆ—çº¦æŸå»ºè¡¨ 12345678CREATE [TEMPORARY] TABLE è¡¨å ( -- TEMPORARY å»ºç«‹ä¸€å¼ ä¸´æ—¶çš„è¡¨col_not_null INT NOT NULL, -- åˆ—å ç±»å‹ &#123;çº¦æŸ1 çº¦æŸ2 ...&#125;col_unique INT UNIQUE,col_prikey INT PRIMARY KEY, -- NOT NULL + UNIQUE ä¸»é”®col_default INT DEFAULT 42,col_check INT CHECK(col_check &lt; 42)col_ref INT REFERENCES -- çº¦æŸè¿™ä¸ªå€¼å¿…é¡»æ˜¯å¦ä¸€ç‹¬ç«‹çš„è¡¨çš„æŸä¸ªåˆ—ä¸­çš„æŸä¸ªå€¼); ä½¿ç”¨è¡¨çº§çº¦æŸå»ºè¡¨ 1234567CREATE TABLE è¡¨å ( myKey_1 INT, myKey_2 INT, myString varchar(15), CONSTRAINT cs1 CHECK (myString &lt;&gt; &apos;&apos;), -- ä¸èƒ½ä½ç©ºå­—ç¬¦ä¸² CONSTRAINT CS2 PRIMARY KEY (myKey_1, myKey_2)); -- ä¿®æ”¹è¡¨ç»“æ„ æ·»åŠ æ–°åˆ— 1ALTER TABLE è¡¨å ADD COLUMN åˆ—å ç±»å‹; é‡å‘½åæ–°æ·»åŠ çš„åˆ— 1ALTER TABLE è¡¨å RENAME COLUMN åˆ—å TO æ–°åˆ—å; æ”¹å˜ä¸€äº›çº¦æŸå’Œå…¶ä»–è§„åˆ™ 12ALTER TABLE è¡¨å DROP CONSTRAINT cs1; -- DROPçº¦æŸALTER TABLE è¡¨å ADD CONSTRAINT cs3 UNIQUE(åˆ—å); --æ·»åŠ æ–°çš„çº¦æŸ ä¿®æ”¹åˆ—çš„ç±»å‹ 1ALTER TABLE è¡¨å ALTER åˆ—å TYPE æ–°ç±»å‹; é‡å‘½åè¡¨å 1ALTER TABLE è¡¨å RENAME TO æ–°è¡¨å; ä½¿ç”¨ä¸´æ—¶è¡¨ ä¸´æ—¶è¡¨çš„åŠŸèƒ½åŸºæœ¬å’Œè¡¨æ˜¯å·®ä¸å¤šçš„ï¼ŒåŒºåˆ«åœ¨äºå½“ä½ çš„ä¼šè¯ç»“æŸæ—¶ï¼Œä½ ä¸æ•°æ®åº“è¿æ¥æ–­å¼€åï¼Œä¸´æ—¶è¡¨ä¼šè‡ªåŠ¨è¢«åˆ é™¤ã€‚ é”®çš„çº¦æŸ ä½œä¸ºä¸€ä¸ªåˆ—çš„çº¦æŸçš„å¤–é”®ï¼ˆåˆ—çº¦æŸï¼‰ 1234567CREATE TABLE è¡¨å ( ... ... customer_id INTEGER NOT NUll REFERENCES customer(customer_id), -- å…³è”åˆ°customerè¡¨ ... ...);-- REFERENCES å¤–è¡¨å(å¤–è¡¨åä¸­çš„åˆ—) è¡¨çº§çº¦æŸ 123456CREATE TABLE è¡¨å( ... ... customer_id INTEGER NOT NULL, ... ... CONSTRAINT è¡¨å_åˆ—å_fk FOREIGN KEY(åˆ—å) REFERENCES å¤–è¡¨å(å¤–é¢ä¸­çš„åˆ—)) æ³¨æ„ï¼šæ¯”è¾ƒæ¨èçš„æ˜¯ä½¿ç”¨è¡¨çº§çº¦æŸï¼Œè€Œä¸æ˜¯æ··å’Œç§ç”¨è¡¨çº§å’Œåˆ—çº§çº¦æŸ â€‹ çº¦æŸåè¡¨å_åˆ—å_fkå…è®¸å¤–é¢æ›´å®¹æ˜“å®šä½é”™è¯¯èµ„æº 2.2 è§†å›¾ å»ºç«‹è§†å›¾ 123CREATE VIEW è§†å›¾çš„åå­— AS selectç³»åˆ—è¯­å¥;-- ä¾‹å­CREATE VIEW item_price AS SELECT item_id, description, sell_price FROM item; å½“è§†å›¾å»ºç«‹å¥½çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨è¡¨ä¸€æ ·æ¥æŸ¥è¯¢è¿™ä¸ªè§†å›¾ï¼Œå¯ä»¥ä½¿ç”¨SELECTæˆ–WHEREè¯­å¥ç­‰ã€‚ æ¯æ¬¡åœ¨è§†å›¾ä¸­æ‰§è¡ŒSELECTæ—¶ï¼Œæ•°æ®éƒ½ä¼šè¢«é‡å»ºï¼Œæ‰€ä»¥æ•°æ®æ€»æ˜¯æœ€æ–°çš„ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåœ¨è§†å›¾è¢«å»ºç«‹çš„æ—¶å€™å†»ç»“çš„æ‹·è´ã€‚ä¹Ÿå°±æ˜¯å½“ä¸ä¹‹ç›¸å…³çš„è¡¨çš„æ•°æ®å‘ç”Ÿè¯¥è¡¨çš„æ—¶å€™ï¼ŒVIEWé‡Œé¢çš„æ•°æ®ä¹Ÿéšä¹‹æ”¹å˜ï¼Œè€Œä¸æ˜¯å‚¨å­˜äº†å»ºç«‹VIEWçš„æ—¶å€™çš„æ‹·è´å¯¹è±¡ã€‚æˆ–è€…ä¹Ÿå¯ä»¥ç†è§£ç±»ä¼¼æŒ‡é’ˆæŒ‡å‘åŸå…ˆçš„è¡¨ï¼Œå½“åŸå…ˆçš„è¡¨å‘ç”Ÿå˜åŒ–ï¼Œè¿™è¾¹çš„æ•°æ®è‡ªç„¶è€Œç„¶çš„å°±èƒ½å¤Ÿè¯»å–å‡ºæ¥ã€‚ å½“ç„¶ï¼ŒSELECTè¯­å¥æ˜¯å¯ä»¥åœ¨å¤šä¸ªä¸åŒçš„è¡¨ä¸­æå–æ•°æ®çš„ã€‚ åˆ é™¤å’Œæ›¿æ¢VIEW 12DROP VIEW åå­—; -- ä¸å½±å“æˆ‘ä»¬å·²æœ‰çš„æ•°æ®CREATE OR REPLACE VIEW åå­— AS æ–°çš„selectç³»åˆ—è¯­å¥; ä¸€äº›ä¸VIEWå¸¸ç”¨çš„æŒ‡ä»¤ 12\\dv -- æŸ¥çœ‹å½“å‰æ•°æ®åº“ä¸­çš„æ‰€æœ‰çš„VIEW\\d VIEWçš„åå­— -- æŸ¥çœ‹å…·ä½“çš„æŸä¸€ä¸ªVIEWçš„ç»“æ„ 2.3 INSERTè¯­å¥ åŸºæœ¬æ’å…¥è¯­å¥ 123INSERT INTO è¡¨å VALUES (æ¯åˆ—çš„å€¼çš„åˆ—è¡¨);INSERT INTO customer VALUES(18, &apos;Mr&apos;, &apos;Jeff&apos;, &apos;Baggott&apos;, &apos;Midtown Street A\\\\33&apos;, &apos;Milltown&apos;, &apos;MT9 8NQ&apos;, &apos;746 3956&apos;); PSï¼šè¿™ç§æ“ä½œå¾ˆå±é™©ï¼ŒSQLæ³¨å…¥æ”»å‡» æ¨èçš„å®‰å…¨æ–¹æ³• 12345INSERT INTO è¡¨å(åˆ—åçš„åˆ—è¡¨) VALUES(è·Ÿä¹‹å‰çš„åˆ—çš„åˆ—è¡¨å¯¹åº”åˆ—çš„æ•°å€¼);INSERT INTO customer(customer_id, title, fname, lname, addressline, ...)VALUES(19, &apos;Mrs&apos;, &apos;Sarah&apos;, &apos;Harvey&apos;, &apos;84 Willow Way&apos;, ...); PSï¼šé¿å…åœ¨æ’å…¥æ•°æ®çš„æ—¶å€™ä¸ºserialç±»å‹çš„æ•°æ®æä¾›æ•°å€¼ï¼Œå› ä¸ºè¿™ä¸ªæ˜¯ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ çš„ è®¿é—®åºåˆ—ç”Ÿæˆå™¨ åºåˆ—ç”Ÿæˆå™¨æ€» æ˜¯è¢«å‘½åä¸º&lt;è¡¨å&gt;_&lt;åˆ—å&gt;_seq 123currval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;);nextval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;);setval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;, æ–°çš„å€¼); æ’å…¥ç©ºå€¼ 123INSERT INTO customer VALUES(16, &apos;Mr&apos;, NULL, &apos;Smith&apos;,&apos;23 Harlestone&apos;, &apos;Milltown&apos;, &apos;MT7 7HI&apos;, &apos;746 3725&apos;); ä½¿ç”¨ \\copy å‘½ä»¤ æ­¥éª¤ å…ˆç”Ÿæˆå¦‚ä¸‹æ ¼å¼çš„æ•°æ® â€‹ å†ç”Ÿæˆå¦‚ä¸‹æ ¼å¼çš„æ•°æ®ï¼Œä¿å­˜æˆ.sqlæ‹“å±•åçš„æ–‡æœ¬æ–‡ä»¶ â€‹ ä½¿ç”¨\\copyå‘½ä»¤å¯¼å…¥æ•°æ® â€‹ PSï¼šSQL é‡Œå¤´çš„ COPY å‘½ä»¤æœ‰ä¸€ä¸ªä¼˜ç‚¹ï¼šå®ƒæ˜æ˜¾æ¯”\\copy å‘½ä»¤å¿«ï¼Œå› ä¸ºå®ƒç›´æ¥é€šè¿‡æœåŠ¡å™¨è¿›ç¨‹æ‰§è¡Œã€‚\\copy å‘½ä»¤æ˜¯åœ¨å®¢æˆ·è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œæœ‰å¯èƒ½éœ€è¦é€šè¿‡ç½‘ç»œä¼ è¾“æ‰€æœ‰æ•°æ®ã€‚è€Œä¸” COPY åœ¨å‘ç”Ÿé”™è¯¯çš„æ—¶å€™ä¼šæ›´å¯é ã€‚é™¤éä½ æœ‰å¤§é‡çš„æ•°æ®ï¼Œå¦åˆ™åŒºåˆ«ä¸ä¼šå¤ªæ˜æ˜¾ã€‚ 2.5 ä»æ•°æ®åº“ä¸­åˆ é™¤æ•°æ® DELETEè¯­å¥ è¯­æ³•ç±»ä¼¼äºUPDATEè¯­å¥ 1DELETE FROM è¡¨å WHERE æ¡ä»¶; TRUNCATEè¯­å¥ï¼ˆä¸æ¨èï¼Œå› ä¸ºä¸å®‰å…¨ï¼‰ TRUNCATEè¯­å¥æ˜¯æŠŠè¡¨ä¸­æ‰€æœ‰çš„æ•°æ®éƒ½åˆ é™¤ï¼Œä½†æ˜¯ä¿ç•™è¿™å¼ è¡¨çš„ç»“æ„ï¼Œä¹Ÿå°±æ˜¯è¯´æœ€åå‰©ä¸‹äº†ä¸€å¼ ç©ºè¡¨ï¼Œæ‰€æœ‰çš„è¡Œéƒ½è¢«åˆ é™¤äº†ã€‚ 1TRUNCATE TABLE è¡¨å; DROPè¯­å¥ DROPè¯­å¥å°±æ˜¯åˆ é™¤äº†æ•´å¼ è¡¨çš„å†…å®¹ï¼ŒåŒ…æ‹¬è¡¨çš„ç»“æ„ã€‚DROPå®Œä¹‹åï¼Œè¿™å¼ è¡¨å°±æ˜¯ä¸å­˜åœ¨çš„äº† 1DROP TABLE è¡¨å; 2.6 ä¿®æ”¹æ•°æ®åº“ä¸­çš„æ•°æ® UPDATEè¯­å¥ 1UPDATE è¡¨å SET åˆ—å = å€¼ WHERE æ¡ä»¶; 1UPDATE customer SET town = &apos;Leicester&apos;, zipcode = &apos;LE4 2WQ&apos; WHERE ä¸€äº›æ¡ä»¶; å¦‚æœæ²¡æœ‰WHEREå­å¥çš„è¯ï¼Œä¼šå¯¼è‡´è¡¨ä¸­çš„å¾ˆå¤šç”šè‡³æ˜¯æ‰€æœ‰çš„è¡Œéƒ½è¢«åŒæ—¶æ›´æ–°äº† é€šè¿‡å¦ä¸€ä¸ªè¡¨æ›´æ–° 1UPDATE è¡¨å FROM è¡¨å WHERE æ¡ä»¶; 3 é«˜çº§æ•°æ®é€‰æ‹©3.1 èšé›†å‡½æ•° Group By and count(*) é”™è¯¯ä½¿ç”¨ 1SELECT count(*), town FROM customer; æ­£ç¡®ä½¿ç”¨ 1SELECT count(*), town FROM customer GROUP BY town; ç»“æœæ˜¯è·å¾—ä¸€ä¸ªåŸé•‡çš„åˆ—è¡¨ä»¥åŠæ¯ä¸ªåŸé•‡çš„å®¢æˆ·æ•°é‡ï¼ˆcount(*)) åŒæ—¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨ä¸¤ä¸ªcolumns nameåœ¨GROUP BYä¸­ï¼Œç„¶åç”¨ORDER BYæŒ‡å®šæ’åˆ—é¡ºåºã€‚æ²¡æœ‰GROUP BYçš„è¯æŒ‰ç…§GROUP BYä¸­çš„townï¼Œlnameæ’åº Having Havingæ˜¯ä¸€ç§ç”¨äºèšé›†å‡½æ•°çš„WHEREä»å¥ï¼Œæˆ‘ä»¬ä½¿ç”¨HAVINGæ¥çº¦æŸè¿”å›çš„ç»“æœä¸ºé’ˆå¯¹ç‰¹å®šçš„èšé›†çš„æ¡ä»¶ä¸ºçœŸçš„è¡Œï¼Œæ¯”å¦‚count(*) &gt; 1 PSï¼šèšé›†å‡½æ•°æ— æ³•åœ¨WHEREä»å¥ä¸­ä½¿ç”¨ï¼Œåªèƒ½ç”¨åœ¨HAVINGä»å¥ä¸­ ä¸¾ä¸ªæ —å­ï¼š é€‰å‡ºæœ‰è¶…è¿‡ä¸€ä¸ªå®¢æˆ·çš„åŸé•‡ï¼Œåœ¨é‡Œä½¿ç”¨ä¸€ä¸ªHAVINGä»å¥æ¥çº¦æŸå¤§ä¸€çš„è¡Œ SELECTä¸­ä»å¥çš„ä¼˜å…ˆåº¦ 1234567SELECTFROM WHEREGROUP BYHAVINGORDER BY -- DESC ä¸‹é™LIMIT -- ç”¨äºé™åˆ¶rowsæ˜¯è¡Œæ•° mysql count(Column_Name) count(*) ç»Ÿè®¡æ‰€æœ‰çš„è¡Œ count(column_name) ç»Ÿè®¡æ‰€è¿™ä¸ªåˆ—ä¸­å€¼ä¸æ˜¯NULLçš„è¡Œ count(Distinct column) åªç»Ÿè®¡è¿™ä¸ªåˆ—ä¸­å”¯ä¸€çš„æƒ…å†µï¼Œä¸é‡å¤ç»Ÿè®¡ min min å‡½æ•°ä½¿ç”¨ä¸€ä¸ªåˆ—ååšå‚æ•°ä¸”è¿”å›è¿™ä¸ªåˆ—ä¸­æœ€å°çš„å€¼ã€‚å¯¹äº numeric ç±»å‹çš„åˆ—ï¼Œç»“æœåº”è¯¥å’Œé¢„æœŸä¸€ æ ·ã€‚å¯¹äºæ—¶æ€ç±»å‹ï¼Œä¾‹å¦‚ date çš„å€¼ï¼Œå®ƒè¿”å›æœ€å°çš„æ—¥æœŸï¼Œæ—¥æœŸæ—¢å¯ä»¥æ˜¯è¿‡å»ä¹Ÿå¯ä»¥æ˜¯æœªæ¥ã€‚å¯¹äºå˜é•¿çš„å­—ç¬¦ä¸²ï¼ˆvarchar ç±» å‹ï¼‰ï¼Œç»“æœå¯èƒ½å’Œé¢„æœŸæœ‰ç‚¹ä¸åŒï¼šå®ƒåœ¨å­—ç¬¦ä¸²å³è¾¹æ·»åŠ ç©ºç™½åå†è¿›è¡Œæ¯”è¾ƒã€‚ min å‡½æ•°å¿½ç•¥ NULL å€¼ã€‚å¿½ç•¥ NULL å€¼ æ˜¯æ‰€æœ‰çš„èšé›†å‡½æ•°çš„ä¸€ä¸ªç‰¹ç‚¹ï¼Œé™¤äº† count(*)ï¼ˆå½“ç„¶ï¼Œæ˜¯å¦ä¸€ä¸ªç”µè¯å·ç æ˜¯æœ€å°å€¼åˆæ˜¯å¦ä¸€ä¸ªé—®é¢˜äº† PSï¼šå°å¿ƒåœ¨ varchar ç±»å‹çš„åˆ—ä¸­ä½¿ç”¨ min æˆ–è€… maxï¼Œå› ä¸ºç»“æœå¯èƒ½ä¸æ˜¯ä½ é¢„æœŸçš„ã€‚ max sum Sum å‡½æ•°ä½¿ç”¨ä¸€ä¸ªåˆ—åä½œä¸ºå‚æ•°å¹¶æä¾›åˆ—çš„å†…å®¹çš„åˆè®¡ã€‚å’Œ min å’Œ max ä¸€æ ·ï¼ŒNULL å€¼è¢«å¿½ç•¥ã€‚ å’Œ count ä¸€æ ·ï¼Œsum å‡½æ•°æ”¯æŒ DISTINCT å˜ä½“ã€‚ä½ å¯ä»¥è®©å®ƒåªç»Ÿè®¡ä¸é‡å¤å€¼çš„å’Œï¼Œæ‰€ä»¥å¤šæ¡å€¼ç›¸åŒçš„è¡Œåªä¼šè¢«åŠ ä¸€ æ¬¡ avg æˆ‘ä»¬è¦çœ‹çš„æœ€åä¸€ä¸ªèšé›†å‡½æ•°æ˜¯ avgï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªåˆ—ååšå‚æ•°å¹¶è¿”å›è¿™ä¸ªåˆ—æ•°å€¼çš„å¹³å‡å€¼ã€‚å’Œ sum ä¸€æ ·ï¼Œå®ƒå¿½ç•¥ NULL å€¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹ â€‹ 3.2 å­æŸ¥è¯¢ é—®é¢˜ä¸€ æ‰¾åˆ°ä»·æ ¼æ¯”å¹³å‡ä»·æ ¼é«˜çš„å•†å“é¡¹ç›® æ–¹æ³•ä¸€ï¼ˆåœŸæ–¹æ³•ï¼‰ æ–¹æ³•äºŒï¼ˆç”¨åµŒå¥—WHEREä»å¥ï¼‰ é—®é¢˜äºŒ æ‰¾åˆ°é‚£äº›æˆæœ¬é«˜äºå¹³å‡æˆæœ¬ä½†å”®ä»·ä½äºå¹³å”®ä»·çš„äº§å“ æ–¹æ³•ä¸€ï¼ˆåœŸæ–¹æ³•ï¼‰ æ–¹æ³•äºŒï¼ˆç”¨åµŒå¥—WHEREä»å¥ï¼‰ é—®é¢˜ä¸‰ - è¿”å›å¤šè¡Œè®°å½•çš„å­æŸ¥è¯¢ ä¹‹å‰çš„ä¸¤ä¸ªé—®é¢˜ä¸­ï¼ŒWHEREä¸­çš„å­æŸ¥è¯¢ä¸­çš„SELECTå­—å¥è¿”å›çš„æœ€ååªæœ‰ä¸€ä¸ªå€¼â€”â€”å› ä¸ºç”¨äº†count()èšé›†å‡½æ•°ã€‚å¦‚æœWHEREä¸­çš„SELECTå­å¥è¿”å›å¤šä¸ªç»“æœå€¼å‘¢ï¼Ÿ ç­”æ¡ˆæ˜¯ç”¨ WHERE column_name IN (RESULTS) å½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨NOT IN æ¥æ’å‡ºé€‰é¡¹ 3.3 ç›¸å…³å­æŸ¥è¯¢â€‹ åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œè¿™é‡Œçš„ä¸¤ä¸ªSELECTå®é™…ä¸Šæ˜¯ä¸ç›¸å…³çš„ï¼Œä¹Ÿå°±æ˜¯åœ¨å†…éƒ¨çš„SELECTçš„ç»“æœåŸºç¡€ä¸Šï¼Œå¤–éƒ¨SELECTå†åšç»§ç»­æŸ¥è¯¢ â€‹ ä½†æ˜¯ç›¸å…³å­æŸ¥è¯¢åˆ™æ˜¯å†…å¤–çš„SELECTä¸­ï¼Œè¡¨ä¸è¡¨ä¹‹é—´æ˜¯æœ‰å…³ç³»çš„ æ ¼å¼ PSï¼šå»ºè®®åœ¨ç›¸å…³å­æŸ¥è¯¢ä¸­ä½¿ç”¨è¡¨çš„åˆ«å 3.4 UNIONé“¾æ¥ æ ¼å¼ 12345SELECT town FROM tcustUNIONSELECT town FROM customer;SELECT town, zipcode FROM tcust UNION SELECT town, zipcode FROM customer; PSï¼šUNION è¿æ¥çš„ä½¿ç”¨æœ‰ä¸€äº›é™åˆ¶ã€‚ä½ è¦è¿æ¥çš„ä¸¤ä¸ªä»ä¸¤ä¸ªè¡¨ä¸­æŸ¥æ‰¾åˆ—è¡¨çš„åˆ—å¿…é¡»æœ‰ç›¸åŒåˆ—æ•°ï¼Œè€Œä¸”é€‰æ‹©çš„æ¯ä¸ªåˆ—å¿…é¡»éƒ½æœ‰ç›¸å…¼å®¹çš„ç±»å‹ã€‚ è¿™ä¸ªæŸ¥è¯¢ï¼Œè™½ç„¶éå¸¸æ— æ„ä¹‰ï¼Œä½†æ˜¯æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸º PostgreSQL å¯ä»¥è¿æ¥è¿™ä¸¤ä¸ªåˆ—ï¼Œå³ä½¿ title æ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„åˆ—è€Œ town æ˜¯ä¸€ä¸ªå˜é•¿çš„åˆ—ï¼Œå› ä¸ºä»–ä»¬éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ã€‚ä¾‹å¦‚å¦‚æœæˆ‘ä»¬å°è¯•è¿æ¥ customer_id å’Œ townï¼ŒPostgreSQL ä¼šå‘Šè¯‰æˆ‘ä»¬ æ— æ³•åšåˆ°ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªåˆ—çš„ç±»å‹ä¸åŒã€‚ 3.5 è‡ªè¿æ¥ 3.6 å¤–é“¾æ¥4. è¡¨çš„ç®¡ç†â€‹ 5. äº‹åŠ¡å’Œé”6. References","tags":[{"name":"SQL","slug":"SQL","permalink":"http://chenson.cc/tags/SQL/"}]},{"title":"Machine Learning - Linear Regression and Logistic Regression","date":"2017-03-17T01:06:30.000Z","path":"2017/03/17/Machine-Learning-Linear-Regression-and-Logistic-Regression/","text":"1. ä¸‰è¦ç´ å½“ä¸€å¼€å§‹æ¥è§¦Andrewåœ¨Courseraä¸Šçš„MLå…¬å¼€è¯¾çš„æ—¶å€™ï¼Œå¯¹çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’è¿™ä¸¤ç§æ¨¡å‹æœ‰ä¸ªå¤§ä½“çš„è®¤è¯†ã€‚ä½†æ˜¯åœ¨ä¸Šå®Œcs229çš„å‰ä¸‰èŠ‚è¯¾ï¼Œåˆæ­¥äº†è§£äº†è¿™ä¸¤ç§æ¨¡å‹èƒŒåçš„æ•°å­¦æ¨¡å‹ï¼ŒLinear Regressionå’ŒLogistic RegressionèƒŒåçš„æ¦‚ç‡åˆ†å¸ƒï¼Œäº†è§£åˆ°äº†è¿™ä¸¤ç§æ¦‚ç‡åˆ†å¸ƒå…¶å®åªæ˜¯exponential familyä¸­çš„ç‰¹ä¾‹ã€‚ä½†åŒæ—¶ä¹Ÿå¼€å§‹å¯¹ä¸€äº›æ¦‚å¿µæ€§çš„ä¸œè¥¿æ„Ÿè§‰å¾ˆæ¨¡ç³Šï¼Œæ‰€ä»¥è§‰å¾—æœ‰å¿…è¦å¥½å¥½æ•´ç†ä¸€ä¸‹è¿™éƒ¨åˆ†çš„å†…å®¹ã€‚ 1. 1 Hypothesisé¦–å…ˆå¯¹äºæ ·æœ¬æ•°æ®ï¼Œè¾“å…¥xå’Œè¾“å‡º$\\hat y$ä¹‹é—´æ˜¯é€šè¿‡Target functionåœ¨è½¬æ¢çš„ï¼Œä¹Ÿå°±æ˜¯ Target function f(x) = $\\hat y$ã€‚ä½†æ˜¯æˆ‘ä»¬å¹¶ä¸çŸ¥é“è¿™ä¸ªf(x)éƒ½æ˜¯æ€æ ·çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å‡è®¾äº†è¿™ä¹ˆä¸€ä¸ªHypothesis functionå»æ¨¡æ‹Ÿè¿™ä¸ªTarget functionï¼Œä½¿å¾—æˆ‘ä»¬ç”¨åŒæ ·çš„è¾“å…¥xä¼šçš„ä¸€ä¸ªé¢„æµ‹å€¼$\\hat y$ï¼Œä½¿å¾—è¿™ä¸ªä¸$\\hat y$æ–­é€¼è¿‘çœŸå®å€¼yã€‚ 1.1.1 Linear Regression H(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx1.1.2 Logistic Regression H(x) = g(\\theta^Tx) = \\frac 1 {1 + e^{-\\theta^Tx}} \\tag{1}Logitä¸Šé¢å…¬å¼(1)ç­‰ä»·äºï¼ˆå³log oddsï¼Œlogitï¼‰ \\ln \\frac y {1 - y} = \\theta^T x = \\ln \\frac {p(y=1| x; \\theta)} {p(y=0| x; \\theta)} \\tag{2} å…¬å¼(1)çš„æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ \\begin{align} y =& \\frac 1 {1 + e^{-\\theta^Tx}} \\\\ =& \\frac {1+e^{-\\theta^Tx} - e^{-\\theta^Tx}} {1+e^{-\\theta^Tx}} \\\\ =& 1 - \\frac {e^{-\\theta^Tx}} {1+e^{-\\theta^Tx}} \\end{align}æ­¤æ—¶æœ‰ 1-y = \\frac {e^{-\\theta^Tx}} {1 + e^{-\\theta^Tx}}æ­¤æ—¶ç”¨$\\frac {y} {1-y}$ï¼Œä¸”å°†ä¸Šè¿°çš„å¼å­ä»£å…¥åˆ°é‡Œé¢ï¼Œåˆ™æœ‰ \\begin{align} \\frac {y} {1-y} =& \\frac {\\frac 1 {1 + e^{-\\theta^Tx}}} {\\frac {e^{-\\theta^Tx}} {1 + e^{-\\theta^Tx}}} \\\\ =& \\frac 1 {e^{-\\theta^Tx}} \\\\ =& e^{\\theta^Tx} \\end{align}ä¸¤è¾¹åŒæ—¶å–ä»¥eä¸ºåº•çš„å¯¹æ•°ï¼Œæ­¤æ—¶æœ‰ \\begin{align} \\ln {\\frac y {1-y}} =& \\ln {e^{\\theta^Tx}} \\\\ =& \\theta^Tx \\end{align}ä¼¼ç„¶å‡½æ•°æ ¹æ®ä¸Šé¢å…¬å¼ï¼Œæœ‰å¦‚ä¸‹å‡è®¾ \\begin{align} p(y^{(i)}=1|x^{(i)};\\theta) =& h_{\\theta}(x^{(i)}) \\\\ p(y^{(i)}=0|x^{(i)};\\theta) =& 1- h_{\\theta}(x^{(i)}) \\\\ p(y^{(i)}|x^{(i)};\\theta) =& h_\\theta(x^{(i)})^{y^{(i)}} \\cdot (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\end{align}å½“$y_i$ = 0 æ—¶å€™æœ‰ \\begin{align} p(y^{(i)}=0|x^{(i)};\\theta) =& h_\\theta(x^{(i)})^{0} \\cdot (1-h_\\theta(x^{(i)}))^{(1-0)} \\\\ =&1-h_\\theta(x^{(i)}) \\end{align}å½“$y_i$ = 1 æ—¶å€™æœ‰ \\begin{align}p(y^{(i)}=1|x^{(i)};\\theta) =& h_\\theta(x^{(i)})^{1} \\cdot (1-h_\\theta(x^{(i)}))^{(1-1)} \\\\=& h_\\theta(x^{(i)})\\end{align}æ­¤æ—¶å¯ä»¥å†™å‡ºå¯¹äºæ‰€æœ‰æ ·æœ¬çš„ä¼¼ç„¶å‡½æ•° \\begin{align} \\mathcal L(\\theta) =& \\prod_{i=1}^m p(y^{(i)}|x^{(i)};\\theta) \\\\ =& \\prod_{i=1}^m h_\\theta(x^{(i)})^{y^{(i)}} \\cdot (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\end{align}ä¸¤è¾¹åŒæ—¶å–å¯¹æ•°ï¼Œæ­¤æ—¶æœ‰ \\begin{align} L(\\theta) =& \\log(\\mathcal L(\\theta)) \\\\ =& \\log(\\prod_{i=1}^m h_\\theta(x^{(i)})^{y^{(i)}} \\cdot (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}) \\\\ =& \\sum_{i=1}^m \\log(h_\\theta(x^{(i)})^{y^{(i)}} + (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}) \\\\ =& \\sum_{i=1}^m y^{(i)}\\log(h_\\theta(x^{(i)}) + (1-y^{(i)})(1-h_\\theta(x^{(i)}))) \\end{align}1. 2 Cost functionCost functionå‘¢ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥å«åšError functionï¼Œå°±æ˜¯ç”¨æˆ‘ä»¬ä¸Šé¢å‡è®¾çš„Hypothe functionæ‰€é¢„æµ‹å‡ºæ¥çš„å€¼$\\hat y$å’ŒçœŸå®å€¼yä¹‹é—´çš„è¯¯å·®ã€‚è€Œæˆ‘ä»¬éœ€è¦åšçš„æ˜¯æ ¹æ®å‡è®¾å‡ºçš„Hypothesis functionï¼Œå–ä¸€ä¸ªåˆé€‚çš„æƒé‡å€¼ï¼Œå³thetaçš„å€¼ï¼Œä½¿å…¶å–çš„ä¸€ä¸ªè¾ƒä½çš„costï¼Œä¹Ÿå°±æ˜¯è¿™é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®æœ€å°ã€‚ Ordinary Least Squares (Square Loss Function) å¸¸ç”¨çš„æ–¹æ³•æ˜¯æœ€å°äºŒä¹˜æ³• J(\\theta) = \\frac 1 2 \\sum_{i=i}^m (h_\\theta(x^{(i)}) - y^{(i)})^2â€‹ å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»æ¦‚ç‡çš„è§’åº¦æ¥ç†è§£è¿™ä¸ªé—®é¢˜ y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}â€‹ è¿™é‡Œçš„$\\epsilon$æ˜¯æˆ‘ä»¬é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œè¿™ä¸ªé—®é¢˜æˆ‘ä»¬ä¼šç•™åˆ°åé¢é‡ç‚¹è®²è§£ã€‚ 0-1 Loss Function l(h_\\theta(x^{(i)}), y^{(i)}) = -y^{(i)}\\log(h_\\theta(x^{(i)})) - (1-y^{(i)})\\log(1 - h_\\theta(x^{(i)}))å½“$y_i$ = 0 æ—¶å€™æœ‰ \\begin {align} l(h_\\theta(x^{(i)}), y^{(i)}=0) =& -0\\log(h_\\theta(x^{(i)})) - (1-0)\\log(1 - h_\\theta(x^{(i)}))\\\\ =&\\log(1-h_\\theta(x^{(i)})) \\end {align}å½“$y_i$ = 1 æ—¶å€™æœ‰ \\begin {align} l(h_\\theta(x^{(i)}), y^{(i)}=0) =& -1\\log(h_\\theta(x^{(i)})) - (1-1)\\log(1 - h_\\theta(x^{(i)}))\\\\ =&-\\log(h_\\theta(x^{(i)})) \\end {align}åˆ™å¯¹äºå…¨éƒ¨æ ·æœ¬çš„æŸå¤±å‡½æ•°æœ‰ J(\\theta) = -\\frac 1 m\\sum_{i=1}^m [y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]ä»”ç»†çœ‹çœ‹è¿™å…¬å¼ï¼Œæ˜¯ä¸æ˜¯å’Œä¹‹å‰çš„æœ€å¤§ä¼¼ç„¶å‡½æ•°åŸºæœ¬ä¸€è‡´ï¼Ÿæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ‰æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°å’Œæœ€å°åŒ–æŸå¤±å‡½æ•°æ˜¯ç­‰ä¸‹çš„ã€‚ Absolute Loss Function Log Loss Function 1.3 Otimization Algorithmè‡³äºæ€ä¹ˆä½¿å¾—ä¸Šé¢çš„cost functionæœ€å°å‘¢ï¼Œå› ä¸ºå¯¹äºæŸäº›æ•°æ®ï¼Œå…¶featuresæœ‰æˆåƒä¸Šç™¾ä¸ªï¼Œæˆ‘ä»¬å¾ˆéš¾å»æ‰¾åˆ°è¿™ä¸ªæœ€å°çš„æå€¼ç‚¹ï¼Œä½¿å¾—cost functionæœ€ä¸‹ï¼Œæ‰€ä»¥è¿™ä¸ªAlgorithmå°±æ˜¯ä¼˜åŒ–å‡½æ•°ï¼Œç”¨æ¥æ‰¾cost functionçš„æœ€å°å€¼çš„ã€‚å¸¸ç”¨çš„æ–¹æ³•æœ‰å¦‚ä¸‹ Gradient Descent åœ¨æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯ LMS update rules(Least Mean Squares) \\theta_j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)} = \\alpha e^{(i)} x^{(i)} å½“æˆ‘ä»¬çš„é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹å‰çš„è¯¯å·®Ïµå¾ˆå°æ—¶ï¼Œæˆ‘ä»¬å°±åªéœ€è¦å¯¹Î¸åšå‡ºå¾ˆå°çš„è°ƒæ•´ï¼Œåä¹‹ï¼Œè¯´æ˜å½“å‰çš„Î¸ä¸å¯¹ï¼Œéœ€è¦è°ƒæ•´çš„å¹…åº¦æ¯”è¾ƒå¤§ã€‚ç›´åˆ°æœ€åæ”¶æ•›ä¸ºæ­¢ã€‚ ä¸Šé¢Repeatä¸­çš„æ­¥éª¤å®é™…æ˜¯ç­‰åŒäºcost functionå¯¹Î¸æ±‚å¯¼çš„è¿‡ç¨‹ï¼Œæ‰€ä»¥ä¸ºäº†ä¿è¯æ”¶æ•›çš„æ•ˆæœï¼Œcost functionåº”è¯¥æ˜¯è¦ convex fuctionï¼Œå°±ä¸ä¼šå¯¼è‡´åœç•™åœ¨äº†local optimç‚¹ã€‚ æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼š å¯è§†åŒ–åå¤§æ¦‚çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š æ ¹æ®å¯¹äºå“ªäº›Î¸æ±‚å¯¼ï¼ŒGradient Descentè¿˜å¯ä»¥ç»§ç»­åˆ†æˆä¸åŒçš„å‡ ç§æ–¹æ³• Batch Gradient Descent Stochastic Gradient Descent Mini Batch Gradient Descent SGD with mini-batch é—®é¢˜ï¼šä¸ºä»€ä¹ˆä¸‹é™æ˜¯ - ï¼ŒÎ¸å¤§Jä¸€å®šå¤§å—ï¼Ÿ Normal Equation (linear regression) æ¨å¯¼è¿‡ç¨‹æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦çš„æ•°å­¦çŸ¥è¯†æ¯”è¾ƒå¤šï¼Œè¿™é‡Œåªç»™å‡ºç»“è®ºã€‚æƒ³è¦çœ‹å…·ä½“æ¨å¯¼è¿‡ç¨‹çš„è¿˜è¯´çœ‹cs229çš„ç¬¬äºŒèŠ‚è¯¾å§ ï¼šï¼‰(cs229-notes-1, p11) \\theta = (X^TX)^{-1}X^T\\vec y Newton BGFS Simulated Annealing 2. Generalized Linear Model (GLM) å¹¿ä¹‰çº¿æ€§æ¨¡å‹ä¹‹å‰æˆ‘ä»¬åœ¨cost functionä¸­æåˆ°è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¦‚ç‡çš„è§’åº¦æ¥ç†è§£è¯¯å·®è¿™ä¸ªé—®é¢˜ã€‚å¯¹äºLinear Regressionå’ŒLogistic Regressionï¼Œæˆ‘ä»¬éƒ½å¯ä»¥å‡è®¾ï¼š y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}è¿™é‡Œè¯¯å·®Ïµå‡è®¾ä¸ºIID (independently and identically distributed) 2.1 Linear Regressionåœ¨Linear Regressionä¸­ï¼Œyæ˜¯è¿ç»­çš„å€¼ï¼Œæ‰€ä»¥è¯¯å·®Ïµä¹Ÿæ˜¯ä¸€ä¸ªè¿ç»­çš„å€¼ã€‚å‡è®¾è¯¯å·®Ïµæ˜¯ç¬¦åˆGaussian Distribution (Normal Distribution)ï¼Œæ‰€ä»¥æœ‰ Gaussian Distribution y | x; \\theta âˆ¼ N (Î¼, Ïƒ^2) Probability of error â€‹ Ïƒå®é™…ä¸Šæ˜¯ä¸å½±å“æ¦‚ç‡çš„åˆ†å¸ƒçš„ï¼Œæ‰€ä»¥å‡è®¾Ïƒ = 1ï¼Œæ‰€ä»¥è¿™é‡Œå¯ä»¥å¿½ç•¥äº†ã€‚å› æ­¤ä¹Ÿå°±æ˜¯ç­‰åŒäºå¦‚ä¸‹ Likelihood L(\\theta) = L(\\theta; X, \\vec y) = p(\\vec y | X; \\theta) â€‹ ä»¥ä¸Šæ˜¯åœ¨ç»™å®šè¾“å…¥xå’Œæƒé‡Î¸ä¸‹ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å€¼æ˜¯çœŸå®å€¼yçš„æ¦‚ç‡ï¼Œæ‰€ä»¥è¿™ä¸ªæ¦‚ç‡å‘¢ï¼Œå½“ç„¶æ˜¯è¶Šé«˜è¶Šå¥½å•¦ã€‚æˆ‘ä»¬å°±æ˜¯è¦æƒ³åŠæ³•å» maximum likelihoodã€‚ â€‹ å¯¹è¿™ä¸ªæ¦‚ç‡å–ä¸ªlogï¼ˆä¸å½±å“ç»“æœï¼‰ï¼Œæœ‰ â€‹ å¯ä»¥çœ‹åˆ°æœ€ç»ˆçš„å¼å­é‡Œé¢ï¼Œæˆ‘ä»¬å°±æ˜¯è¦æ±‚cost functionçš„æœ€å°å€¼ã€‚ 2.2 Logistic Regressionåœ¨Logistic Regressionä¸­ï¼Œ yæ˜¯ç¦»æ•£çš„å€¼ {0, 1}ï¼Œæ‰€ä»¥è¯¯å·®Ïµä¹Ÿæ˜¯ä¸€ä¸ªç¦»æ•£çš„å€¼ {0, 1}ã€‚å‡è®¾è¯¯å·®Ïµæ˜¯ç¬¦åˆBernoulli Distributionï¼Œæ‰€ä»¥æœ‰ Bernoulli distribution Probability of error æŠŠè¯¯å·®Ïµä»£å…¥åˆ°ä¸Šé¢çš„bernoulli functionï¼Œå¯å¾— Likelihood â€‹ åŒæ ·ï¼Œå¯¹ä¸Šé¢å»logä¹‹åæœ‰ â€‹ åŒæ ·ï¼Œæˆ‘ä»¬å°½é‡è¦maximize the likelihoodï¼Œå°±ç›¸å½“äºè¦æœ€å°åŒ–åé¢çš„é‚£éƒ¨åˆ†ï¼ˆcost functionï¼‰ã€‚è¿™é‡Œå¯ä»¥ç”¨Gradient Ascentç®—æ³•æ¥æ±‚æœ€å¤§å€¼ï¼Œä½†æ˜¯å’ŒGradient Descentä¸ä¸€æ ·ï¼ˆä¸ºä»€ä¹ˆï¼‰ \\theta := \\theta + \\alphaâˆ‡_\\theta l(\\theta)â€‹ æ³¨æ„è¿™é‡Œæ˜¯åŠ å·ï¼Œåœ¨æ±‚cost functionçš„æœ€å°å€¼æ—¶ï¼Œç”¨çš„æ˜¯å‡å·ã€‚ â€‹ å¯¹å…¶æ±‚å¯¼å¯å¾— â€‹ æ‰€ä»¥æœ‰ â€‹ Digression Perception ï¼Ÿ ï¼Ÿ ï¼Ÿ 2.3 GLMä¼¼ä¹åˆ°ç°åœ¨ï¼Œè®²äº†åŠå¤©éƒ½ä¹Ÿæ²¡è®²ä»€ä¹ˆæ˜¯å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œå®é™…ä¸Šå‘¢ï¼Œä¸Šé¢æˆ‘ä»¬å·²ç»ä»è¯¯å·®æ¦‚ç‡çš„è§’åº¦ä¸Šæ¥åˆ†æäº†çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’ä¸¤ç§ç‰¹ä¾‹ï¼Œå› ä¸ºä»–ä»¬è¯¯å·®æœä»çš„æ¦‚ç‡åˆ†å¸ƒéƒ½æ˜¯å±äºExponential Familyä¸­çš„ä¸€ç§ã€‚ The Exponential Family Î· è¢«ç§°ä½œnatural parameterï¼Œå®ƒæ˜¯æŒ‡æ•°åˆ†å¸ƒæ—å”¯ä¸€çš„å‚æ•°T(y) è¢«ç§°ä½œsufficient statisticï¼Œå¾ˆå¤šæƒ…å†µä¸‹T(y)=y a(Î·) è¢«ç§°ä½œ log partition functionTå‡½æ•°ã€aå‡½æ•°ã€bå‡½æ•°å…±åŒç¡®å®šä¸€ç§åˆ†å¸ƒ é‚£ä¹ˆè¿™ä¸ªæ¨¡å‹å’Œä¸Šé¢æˆ‘ä»¬æåˆ°è¿‡çš„Gaussian Distribution å’ŒBernoulli Distributionæœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿå…¶å®ä¸Šé¢çš„è¿™å‡ ä¸ªå‚æ•°å–ä¸åŒçš„å€¼çš„æ—¶å€™ï¼Œå³å¯å¾—åˆ°ä¸åŒçš„åˆ†å¸ƒæ¨¡å‹ Gaussian Distribution Bernolli Distribution 2.4 å¦‚ä½•æ„å»ºä¸€ä¸ªGLMæ¨¡å‹åœ¨ä¸Šé¢æˆ‘ä»¬åªæ˜¯çœ‹åˆ°äº†ä¸€ä¸ªé€šç”¨çš„GLMæ¦‚ç‡æ¨¡å‹ å®é™…ä¸Šå¯¹äºæ„å»ºè¿™ä¹ˆä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œéœ€è¦ä½œå‡ºä¸‰ä¸ªå‡è®¾ä½œä¸ºå‰ææ¡ä»¶ï¼š p(y | x; Î¸) âˆ¼ ExponentialFamily(Î·). å¯¹äºç»™å®šçš„è¾“å…¥xï¼ŒÎ¸å’Œè¾“å‡ºyéœ€è¦æœä»æŸä¸€ç§æŒ‡æ•°åˆ†å¸ƒï¼Œè¿™ä¸ªæŒ‡æ•°åˆ†å¸ƒç”±Î· å†³å®šçš„ å¯¹äºç»™å®šçš„è¾“å…¥xï¼Œé¢„æµ‹T(y)çš„å€¼ï¼Œä¸”ç»å¸¸T(y) = yã€‚è€Œæˆ‘ä»¬æ˜¯é¢„æµ‹æ˜¯H(x) éœ€è¦æ»¡è¶³ H(x) = E[y|x] å¯¹äºè‡ªç„¶å‚æ•°Î·å’Œè¾“å…¥xä¹‹é—´ï¼Œéœ€è¦å­˜åœ¨ç›¸å…³æ€§å…³ç³»çš„ï¼Œå³ï¼šÎ· = Î¸T x","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"}]},{"title":"å­—ç¬¦ä¸²æœç´¢ç®—æ³• - KMP","date":"2017-03-15T06:48:47.000Z","path":"2017/03/15/å­—ç¬¦ä¸²æœç´¢ç®—æ³•-KMP/","text":"1. ç†è§£ä¸€ï¼šéƒ¨åˆ†åŒ¹é…è¡¨+å·²åŒ¹é…æ•°å­—1.1 Partial Match Table ä¸Šæ¥å…ˆä¸Šä¸ªç»“è®ºï¼Œè¿™ä¸ªå…ˆæš‚æ—¶ä¸ç®¡æ€ä¹ˆç”Ÿæˆï¼Œç”¨äºKMPè¡¨çš„ç§»åŠ¨ã€‚ ç§»åŠ¨ä½æ•° = å·²åŒ¹é…çš„å­—ç¬¦æ•° - å¯¹åº”çš„éƒ¨åˆ†åŒ¹é…å€¼ åŒ¹é…åˆ°äº†ç¬¬å…­ä¸ªå­—ç¬¦Bï¼ŒBåœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯2ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯6 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 6 - 2 = 4ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨4ä½ã€‚ åŒ¹é…åˆ°äº†ç¬¬ä¸‰ä¸ªå­—ç¬¦Cï¼ŒCçš„å‰ä¸€ä¸ªå­—ç¬¦Båœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯0ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯2 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 2 - 0 = 2ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨2ä½ å› ä¸ºç¬¬ä¸€ä¸ªå­—ç¬¦ä¸åŒ¹é…ï¼Œå°±å°†æ•´ä¸ªå­—ç¬¦ä¸²å‘åç§»ä¸€ä½ åŒ¹é…åˆ°äº†ç¬¬å…­ä¸ªå­—ç¬¦Bï¼ŒBåœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯2ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯6 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 6 - 2 = 4ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨4ä½ é€ä¸ªæ¯”è¾ƒï¼Œç›´åˆ°å®Œå…¨åŒ¹é… å¦‚æœè¿˜éœ€è¦ç»§ç»­æœç´¢çš„è¯ï¼ŒDåœ¨ä¸Šè¡¨ä¸­çš„å€¼ä¸º0ï¼ŒåŒ¹é…åˆ°çš„ä¸ªæ•°ä¸º7ï¼Œç§»åŠ¨çš„ä½æ•°= 7 - 0 = 7ï¼Œå°†æ•´ä¸ªå­—ç¬¦ä¸²å¾€åç§»åŠ¨7ä½ã€‚æ¥ç€å°±æ˜¯é‡å¤ä¹‹å‰çš„æ¯”è¾ƒæ­¥éª¤äº†ã€‚ 1.2 è®¡ç®— Partial Match Table â€‹ è¿™é‡Œéœ€è¦ç†è§£ä¸¤ä¸ªæ¦‚å¿µï¼šå‰ç¼€å’Œåç¼€ â€‹ â€œå‰ç¼€â€ æŒ‡é™¤äº†æœ€åä¸€ä¸ªå­—ç¬¦ä»¥å¤–ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²çš„å…¨éƒ¨å¤´éƒ¨ç»„åˆï¼› â€‹ â€œåç¼€â€ æŒ‡é™¤äº†ç¬¬ä¸€ä¸ªå­—ç¬¦ä»¥å¤–ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²çš„å…¨éƒ¨å°¾éƒ¨ç»„åˆã€‚ â€‹ è€Œæˆ‘ä»¬éœ€è¦çš„Partial Match Tableå°±æ˜¯å‰ç¼€å’Œåç¼€çš„æœ€é•¿å…±æœ‰å…ƒç´ çš„é•¿åº¦ â€‹ ç»§ç»­ä»¥ä¸Šé¢çš„ä¾‹å­è®²è§£ â€œAâ€çš„å‰ç¼€å’Œåç¼€éƒ½ä¸ºç©ºé›†ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABâ€çš„å‰ç¼€ä¸º[A]ï¼Œåç¼€ä¸º[B]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABCâ€çš„å‰ç¼€ä¸º[A, AB]ï¼Œåç¼€ä¸º[BC, C]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦0ï¼›â€œABCDâ€çš„å‰ç¼€ä¸º[A, AB, ABC]ï¼Œåç¼€ä¸º[BCD, CD, D]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABCDAâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD]ï¼Œåç¼€ä¸º[BCDA, CDA, DA, A]ï¼Œå…±æœ‰å…ƒç´ ä¸ºâ€Aâ€ï¼Œé•¿åº¦ä¸º1ï¼›â€œABCDABâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD, ABCDA]ï¼Œåç¼€ä¸º[BCDAB, CDAB, DAB, AB, B]ï¼Œå…±æœ‰å…ƒç´ ä¸ºâ€ABâ€ï¼Œé•¿åº¦ä¸º2ï¼›â€œABCDABDâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD, ABCDA, ABCDAB]ï¼Œåç¼€ä¸º[BCDABD, CDABD, DABD, ABD, BD, D]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ã€‚ äº†è§£äº†KMPçš„åŸç†ä¹‹åï¼Œæ¥çœ‹ä¸€ä¸‹ä»£ç è¯¥æ€ä¹ˆå†™ã€‚ ä¸¾ä¸ªæ —å­ï¼š Text = a b a c a a b a c c a b a c a b a a b b Pattern = a b a c a b æ ¹æ®å‰é¢çš„Partial Match Table, æˆ‘ä»¬å¯ä»¥ç®—å‡ºPatternçš„è¿™ä¸ªè¡¨ P a b a c a b steps 0 0 1 0 1 2 æ­¤æ—¶æˆ‘ä»¬ç”¨ä¸¤ä¸ªæŒ‡é’ˆ i å’Œ j æ¥è¡¨ç¤º Text å’Œ Pattern ä¸­çš„å­—ç¬¦ã€‚ å½“ T[ i : i + j ] == P[ 1 : j ] çš„æ—¶å€™ï¼Œå°±æ˜¯ Text ä¸­åŒ…å«äº†æˆ‘ä»¬éœ€è¦æŸ¥æ‰¾çš„ Pattern å…ˆè®© i å’Œ j éƒ½ä» 1 å¼€å§‹ï¼ˆpythonä»£ç ä¸­ä»0å¼€å§‹ï¼‰ å½“T[ i ] = P[ j ]çš„æ—¶å€™ï¼Œæ­¤æ—¶æŒ‡é’ˆåœ¨ Text å’Œ Pattern ä¸Šéƒ½å¾€å‰å„èµ°ä¸€æ­¥ï¼Œå³j+1ï¼Œi+1 å½“ i = 6ï¼Œj = 6 çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºT[ i ] != P[ j ]ï¼Œæ­¤æ—¶ j å°±ä¸èƒ½å†ç»§ç»­å¾€å‰èµ°äº†ï¼Œéœ€è¦é€€å›å»å‡ æ­¥ã€‚ é‚£ä¹ˆåˆ°åº•æ˜¯å‡ æ­¥å‘¢ï¼Œç»è¿‡ä¸Šé¢æŸ¥è¡¨ï¼Œæ­¤æ—¶åŒ¹é…åˆ°5ï¼Œé‡å¤çš„å­—ç¬¦ä¸²ä¸ªæ•°ä¸º1ï¼Œæ„æ€æ˜¯å¯¹äºè¿™ä¸ªå­—ç¬¦ä¸² abacaï¼Œabaca å’Œ abaca ä¸­æœ‰ä¸€ä¸ªé‡å¤äº†ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦å†æ¯”è¾ƒè¿™ä¸ªï¼Œè·³è¿‡è¿™ä¸ªå­—ç¬¦ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸º 6 - 1 = 5ï¼Œå°†å­—ç¬¦ä¸² Pattern å‘å‰æŒª5ä½ï¼Œæ–°çš„ j å°±ç­‰äº1äº†ï¼Œç„¶åé‡å¤ä¹‹å‰çš„æ­¥éª¤ã€‚ 1.3 Pythonä»£ç 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091DEBUG = TrueDEBUG = False# ç”Ÿæˆ partial match tabledef PMT(sent): length = len(sent) result = [0] for i in range(2, length + 1): sub_sent = sent[0 : i] sub_len = len(sub_sent) surfix = [] prefix = [] for j in range(sub_len): prefix.append(sub_sent[0 : j]) surfix.append(sub_sent[j + 1: sub_len ]) prefix.remove('') surfix.remove('') all_fix = list(set(prefix + surfix)) max_len = 0 com_str = '' for fix in all_fix: if fix in prefix and fix in surfix: if len(fix) &gt; max_len: max_len = len(fix) com_str = fix result.append(max_len) return result# KMPç®—æ³•def KMP(text, pattern): if (len(text) &lt; len(pattern)): return 'Not Match' pmt = PMT(pattern) if DEBUG: print('Current Pattern is = ', pattern) print('Partial Match Table = ', pmt) i = 0 j = 0 while(i &lt; len(text)): if (text[i] == pattern[j]): if (j == (len(pattern) - 1)): return 'Match, the position in text is: &#123;&#125; - &#123;&#125; = &#123;&#125;'.format(i - j, i, text[i-j: i]) i += 1 j += 1 continue else: if j &gt;= 1: j = pmt[j - 1] else: i += 1 return 'Not Match'# æµ‹è¯•å‡½æ•°def test(text, pattern): result = KMP(text, pattern) print('Resutl = ', result) print('Pattern =', pattern) if DEBUG: for i in range(len(text)): print('&#123;:3s&#125;'.format(str(i)), end='-') print('') for i in range(len(text)): print('&#123;:3s&#125;'.format(text[i]), end='-') print('') print('\\n')# æµ‹è¯•éƒ¨åˆ†text = 'BBC ABCDAB ABCDABCDABDE'pattern = 'ABCDABD'test(text, pattern)text = 'a b a c a a b a c c a b a c a b a a b b'pattern = 'a b a c a b'test(text, pattern)text = 'BBC ABCDAB ABCDABCDABE'pattern = 'ABCDABD'test(text, pattern)text = 'BBC AB'pattern = 'ABCDABD'test(text, pattern)text = 'ABCDABD'pattern = 'ABCDABD'test(text, pattern) 1.4 æµ‹è¯•ç»“æœ12345678910111213141516171819202122232425Text = BBC ABCDAB ABCDABCDABDEPattern = ABCDABDResutl = Match, the position in text is: 15 - 21 = ABCDABText = a b a c a a b a c c a b a c a b a a b bPattern = a b a c a bResutl = Match, the position in text is: 20 - 30 = a b a c a Text = BBC ABCDAB ABCDABCDABEPattern = ABCDABDResutl = Not MatchText = BBC ABPattern = ABCDABDResutl = Not MatchText = ABCDABDPattern = ABCDABDResutl = Match, the position in text is: 0 - 6 = ABCDAB[Finished in 0.1s] 2. ç†è§£äºŒï¼šéƒ¨åˆ†åŒ¹é…è¡¨+idx2.1 Partial Match Tableé¦–å…ˆç”Ÿæˆå’Œä¸Šé¢ä¸€æ ·çš„å‰ç¼€åç¼€è¡¨(0-base å’Œ 1-base) 2.2 Example 0-base æ‰¾åˆ°åŸæ–‡å’Œæœç´¢è¯ä¸åŒçš„é‚£ä¸ªå­—ç¬¦é‡Œé¢çš„åŒ¹é…å€¼ï¼Œç„¶åæŠŠæœç´¢å­—ç¬¦ä¸²å³ç§»åˆ°idx=åŒ¹é…å€¼çš„ä½ç½®ï¼Œè¿‡ç¨‹å¦‚ä¸‹å›¾ 1-base ä¸0-baseä¸åŒçš„æ˜¯ï¼Œæ‰¾çš„ä¸æ˜¯æœ€åä¸€ä¸ªä¸åŒçš„å­—ç¬¦ è€Œæ˜¯æœ€åä¸€ä¸ªç›¸åŒçš„å­—ç¬¦é‡Œé¢çš„åŒ¹é…å€¼ï¼Œç„¶åæŠŠæœç´¢å­—ç¬¦ä¸²å³ç§»åˆ°idx=åŒ¹é…å€¼çš„ä½ç½®ï¼Œè¿‡ç¨‹å¦‚ä¸‹å›¾ 3. References é˜®ä¸€å³° - å­—ç¬¦ä¸²åŒ¹é…çš„KMPç®—æ³•","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.cc/tags/Algorithm/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸€ï¼‰","date":"2017-03-06T13:44:33.000Z","path":"2017/03/06/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸€ï¼‰/","text":"1. Hadoop1.1 åˆè¯†Hadoopéå¸¸å¥½çš„Tutorial åœ¨å­¦ä¹ hadoopä¹‹å‰ï¼Œæˆ‘è§‰å¾—æœ‰å¿…è¦äº†è§£ä¸€ä¸‹hadoopçš„åŸºæœ¬æ„æˆä»¥åŠä¸€äº›æœ¯è¯­ã€‚ Block HDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks. By making a block large enough, the time to transfer the data from the diskcan be made to be significantly larger than the time to seek to the start of the block.Thus the time to transfer a large file made of multiple blocks operates at the disk transferrate.A quick calculation shows that if the seek time is around 10 ms, and the transfer rateis 100 MB/s, then to make the seek time 1% of the transfer time, we need to make theblock size around 100 MB. The default is actually 64 MB, although many HDFS installationsuse 128 MB blocks. This figure will continue to be revised upward as transferspeeds grow with new generations of disk drives.This argument shouldnâ€™t be taken too far, however. Map tasks in MapReduce normallyoperate on one block at a time, so if you have too few tasks (fewer than nodes in thecluster), your jobs will run slower than they could otherwise. Node ç®€å•çš„è¯´å°±æ˜¯ä¸€å°ä¸»æœºï¼Œä¸€å°ç”µè„‘ã€‚åœ¨hadoopä¸­ï¼Œæœ‰NameNode, DataNode, Secondary NameNode, JobTracker Node, TaskTracker Node, CheckpointNode å’Œ BackupNodeã€‚å¯¹ä¸€ä¸ªclusterï¼ŒNameNodeåªèƒ½æœ‰ä¸€ä¸ªï¼ŒDataNodeå¯ä»¥æœ‰å¤šä¸ª Rack ä¸­æ–‡æœºæŸœ/æœºæ¶ï¼Œå°±æ˜¯ç”¨æ¥å­˜æ”¾nodeçš„storageï¼Œé€šå¸¸ä¸€ä¸ªrackæœ‰å‡ åä¸ªnodesç»„æˆï¼Œè¿™äº›nodeså­˜æ”¾åœ¨åŒä¸€ä¸ªæœºæŸœï¼Œè¿æ¥ä¸€ä¸ªäº¤æ¢æœº A Node is simply a computer. This is typically non-enterprise, commodity hardware for nodes that contain data. Storage of Nodes is called as rack. A rack is a collection of 30 or 40 nodes that are physically stored close together and are all connected to the same network switch. Network bandwidth between any two nodes in rack is greater than bandwidth between two nodes on different racks.A Hadoop Cluster is a collection of racks. 1.2 Major Components Distributed Filesystem â€” hadoopä¸­æ˜¯HDFS MapReduce 1.5 Install configuration123456# å¾…æ•´ç†hadoop fshadoop dfs# fs refers to any file system, it oucld be local or HDFS# but dfs refers to only HDFS file system 2. MapReduce2.1 åˆè¯†MapReduce æ•´ä¸ªè¿‡ç¨‹å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼ŒInputï¼Œ MapReduce and Output åœ¨inputå’Œoutputé˜¶æ®µï¼Œæ•°æ®æ˜¯å­˜åœ¨HDFSæ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œå…¶ç³»ç»Ÿçš„block sizeå¤§å°é»˜è®¤æ˜¯64/128MBã€‚ åœ¨MapReduceä¸­ï¼Œåˆå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼ŒMap and Reduceï¼Œæ•°æ®ä»map functionåˆ°reduce functionæ˜¯å­˜åœ¨local diskä¸­ï¼Œ(soreing in HDFS with replication would be overkill)ï¼Œç„¶åé€šè¿‡networkä¼ è¾“æ•°æ®. åœ¨æ¯ä¸ªé˜¶æ®µä¸­ï¼Œinputå’Œoutputçš„æ•°æ®éƒ½æ˜¯ä»¥ (key, values) æ ¼å¼è¿›è¡Œå¤„ç†çš„ï¼Œç„¶åé€šè¿‡ map function å’Œ reduce function è¿›è¡Œå¤„ç†ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œinput dataçš„keyæ˜¯ä»æ•°æ®æ–‡ä»¶å¼€å§‹å¤„çš„è¡Œæ•°çš„åç§»é‡ï¼Œä½†æ˜¯map functionè¾“å‡ºçš„keyæ˜¯å¹´ä»½æ•°æ®ï¼Œä»¥åŠreduce functionè¾“å‡ºçš„keyæ˜¯ä¹Ÿä¸åŒçš„ã€‚æ‰€ä»¥è¿™ä¸‰ä¸ªkey-value pairsæ˜¯ä¸åŒçš„ã€‚ åŸå§‹æ•°æ® Key-Values ä»¥ä¸Šä¸ºåŸå§‹æ•°æ®ä¸­inputè¿›æ¥åçš„key-valuesçš„æ•°æ®ã€‚ç„¶åmap functioné˜¶æ®µï¼Œæå–å‡ºä¸Šé¢æ–‡ä»¶ä¸­çš„ 1950 å’Œ 0001 ä¹‹ç±»çš„æ•°æ®ï¼Œç»„æˆæ–°çš„key-valuesä½œä¸ºè¾“å‡ºç»™ä¸‹ä¸€é˜¶æ®µã€‚ Key-Values in Map Function åœ¨å°†Map Functionçš„è¾“å‡ºä¼ ç»™Reduce Functionä¹‹å‰ï¼Œå®é™…ä¸ŠMapReduce Frameworkè¿˜æ˜¯æœ‰å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªå¤„ç†æ­¥éª¤ã€‚ä»æœ€ä¸Šçš„å›¾ä¸€ä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥çœ‹åˆ°Mapå’ŒReduceä¹‹é—´æœ‰ä¸€ä¸ª Shuffle çš„è¿‡ç¨‹ã€‚å› ä¸ºä¹‹å‰æˆ‘ä»¬æåˆ°äº†ï¼ŒMapçš„è¿‡ç¨‹ä¸­ï¼Œåªæ˜¯å®ç°äº†ä¸€ä¸ªkey-valueåŒ¹é…çš„è¿‡ç¨‹ï¼Œæ‰€æœ‰å‡ºæ¥çš„æ•°æ®ä¹Ÿæ˜¯æ— åºçš„ï¼Œè€Œ Shuffle å°±æ˜¯å¯¹è¿™ä¸ªè¾“å‡º sort &amp; group çš„è¿‡ç¨‹ï¼Œç„¶åå°†è¾“å‡ºä¼ ç»™ Reduce Function è¿›è¡Œå¤„ç† å½“æ•°æ®ä»Reduce Functionä¸­å¤„ç†å®Œåå‡ºæ¥çš„å¤§æ¦‚å¦‚ä¸‹ï¼Œæ³¨æ„è¿™ä¸ªreduceåªæ˜¯é€‰æ‹©æœ€å¤§å€¼ï¼Œå…¶ä»–reduce functionå¯èƒ½åšçš„æ˜¯ç»Ÿè®¡æˆ–è€…å®ç°å…¶ä»–åŠŸèƒ½ã€‚ ç°åœ¨å†çœ‹å¦å¤–ä¸€ä¸ªç»å…¸çš„WordCountçš„ä¾‹å­ åœ¨Hadoopç³»ç»Ÿä¸­ï¼Œå¤„ç†ä¸€ä¸ªwordcountçš„ä»»åŠ¡å¯ä»¥å¤§è‡´åˆ†æˆå››ä¸ªä¸»è¦é˜¶æ®µï¼Œinputï¼Œmapï¼Œreduceï¼Œoutputã€‚å…¶ä¸­ Map å’Œ Reduce å¯ä»¥ç»§ç»­ç»†åˆ†ï¼Œå³åˆ†æˆå¤šä¸ª map tasks å’Œ reduce tasksã€‚ è¿™äº›tasksç„¶åè¢« YARN ç»™åˆ†é…é›†ç¾¤ä¸­å¤šå°ä¸åŒçš„æœºå™¨å¤„ç†ã€‚è¿™å…¶ä¸­çš„ç»†èŠ‚ç­‰åˆ°å¾€åå†è®¨è®ºã€‚ ä¸Šé¢æåˆ°çš„åˆ†æˆå¤šä¸ªtasksæ—¶ï¼Œåº”è¯¥æ˜¯input dataåˆ‡ç‰‡åˆ†ç»™å¤šä¸ªmapsï¼ˆè€Œä¸æ˜¯ä¸€ä¸ªå¤§çš„mapåˆ†æˆå¤šä¸ªå°çš„tasksï¼‰ï¼Œ æ¯ä¸ªMapReduceåˆ†åˆ°ä¸€ä¸ªfixed-sized çš„æ•°æ®ï¼Œé€šå¸¸æ˜¯64/128MBï¼Œè¿™ä¸ªè¿‡ç¨‹å«åš input splitsã€‚ç„¶åæ¯ä¸ªsplitåˆ†é…ä¸€ä¸ªmap taskï¼ŒåŒæ—¶è¿è¡Œåœ¨ä¸åŒçš„æœºå™¨ä¸Šå¤„ç†ã€‚è¿™æ ·åˆ’åˆ†çš„å¥½å¤„æ˜¯æœ‰åˆ©äºload-balancingï¼Œå¯¹äºæ€§èƒ½è¾ƒå¥½çš„æœºå™¨å¯ä»¥å¤„ç†æ›´è¿‡æ˜¯splitsã€‚ 2.2 Data Flow ä¸Šå›¾å¯ä»¥çœ‹å‡ºhadoopçš„æ•´ä¸ªæ•°æ®æµå‘ï¼Œå…¶ä¸­è™šçº¿ä»£è¡¨æ˜¯åœ¨ä¸€ä¸ªnodeï¼Œå®çº¿ä»£è¡¨çš„æ˜¯ä¸åŒnodeä¹‹é—´ã€‚åœ¨åŒä¸€ä¸ªnodeä¹‹é—´ï¼Œæ•°æ®çš„è¯»å–å­˜å‚¨å°±æœ‰é€Ÿåº¦ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸åŒnodeä¹‹é—´ï¼Œä¹Ÿå°±æ˜¯ä¸åŒä¸»æœºä¹‹é—´ï¼Œå°±å¿…é¡»é€šè¿‡networkè¿›è¡Œä¼ è¾“ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚ Partition å½“åªæœ‰ä¸€ä¸ªreduceçš„æ—¶å€™ï¼Œmap functionçš„outputå½“ç„¶å°±ç›´æ¥ä¼ ç»™è¿™ä¸ªreduceäº†ã€‚ä½†æ˜¯å½“æœ‰å¤šä¸ªreduceçš„æ—¶å€™ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿæ­¤æ—¶mapä¼šå°†å…¶è¾“å‡ºè¿›è¡Œpartition(åˆ†åŒº)ï¼Œæ¯ä¸€ä¸ªreduceçš„ä»»åŠ¡éƒ½ä¼šåˆ›å»ºä¸€ä¸ªåˆ†åŒºï¼Œä¸”æ¯ä¸€ä¸ªreduce taskéƒ½ä¼šæœ‰ä¸€ä¸ªpartition (There can be many keys (and their associated values)in each partition, but the records for any given key are all in a single partition)ï¼Œä¹Ÿå°±æ˜¯è¯´åŒä¸€ä¸ªkeyä¼šåœ¨åŒä¸€ä¸ªpartitionä¸­ã€‚ Shuffle and Sort åœ¨mapå’Œreduceä¹‹é—´çš„data flowæ˜¯Shuffleï¼Œä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªreduceå¯ä»¥æ¥å—æ¥è‡ªå¤šä¸ªä¸åŒçš„mapçš„outputï¼Œå…¶ä¸­åŒ…å«äº†sortï¼Œpartitionç­‰è¿‡ç¨‹ã€‚ Combiner Functions ä¹‹å‰æˆ‘ä»¬è®¨è®ºè¿‡ï¼Œdata flowåœ¨mapå’Œreduceä¹‹é—´æ˜¯é€šè¿‡networkè¿›è¡Œä¼ è¾“çš„ï¼Œä½†æˆ‘ä»¬çŸ¥é“map functionçš„outputæ˜¯ä¸€ä¸ªä¸ªkey-valueçš„é”®å€¼å¯¹çš„ï¼Œè¿™äº›key-value parisä¸­ï¼Œæœ‰äº›æ˜¯å¯ä»¥é€šè¿‡combiner functionè¿›è¡Œcombineçš„ï¼Œè¿™æ ·åšçš„ç›®çš„æ˜¯å‡å°mapå’Œreduceä¹‹é—´ä¼ è¾“çš„æ•°æ®å¤§å°ï¼ŒåŠ å¿«ä¼ è¾“æ•°æ®ã€‚ Combiner Functionåœ¨è®¸å¤šæƒ…å†µå’Œ Reduce Functionæ˜¯å¾ˆåƒçš„ï¼Œå› ä¸ºåšçš„å·¥ä½œå’Œreduceæ˜¯æ¯”è¾ƒç±»ä¼¼çš„ï¼Œåªæ˜¯å¤„ç†çš„æ˜¯å±€éƒ¨mapçš„output(å› æ­¤Combineræ˜¯è¿è¡Œåœ¨map outputç«¯)ï¼Œå‡å°‘data flowçš„sizeã€‚ä½†æ˜¯å¯¹äºæ˜¯å¦è°ƒç”¨combiner functionï¼Œè¿™ä¸ªæ˜¯ä¸ç¡®å®šçš„ã€‚å› ä¸ºæœ‰äº›æƒ…å†µä¸‹çš„outputæ˜¯ä¸é€‚åˆè¿›è¡Œcombineï¼Œæœ‰äº›åˆ™åˆæ˜¯è¦å¤šæ¬¡è°ƒç”¨è¿›è¡Œåˆå¹¶ã€‚å› ä¸ºè¿™ä¸ªï¼ŒCombineræ˜¯å¯é€‰çš„ï¼Œå³å¯ä»¥è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥ä¸è°ƒç”¨ï¼Œå½“ä¸è°ƒç”¨çš„æ—¶å€™ï¼Œå°±å¿…éœ€ä¸èƒ½å½±å“ç¨‹åºçš„æ­£å¸¸è¿è¡Œã€‚æ‰€ä»¥Combinerçš„inputå’Œoutputæ˜¯ä¸€æ ·çš„ï¼Œå’ŒMapperçš„outputã€Reducerçš„inputä¸€æ ·ã€‚ å¯¹äºæœ‰äº›ç‰¹æ®Šæƒ…å†µï¼Œç”šè‡³è¿reduce functionéƒ½ä¸éœ€è¦ã€‚ ä¸¾ä¸ªæ —å­ï¼š é€‚ç”¨æƒ…å†µï¼ˆCommutative &amp; Associativeï¼‰ â€‹ Reduce Function Combiner Function â€‹ Commutative: max(a, b) = max(b, a) â€‹ Associative: max(max(a, b), c) = max(a, max(b, c)) ä¸é€‚ç”¨æƒ…å†µï¼š ä¼ªä»£ç  In-Combiner Function Advantage ç›¸æ¯”Combinerï¼ŒIn-Combinerçš„æ•ˆç‡æ›´é«˜ã€‚ å¯ä»¥å‡å°‘ä¸€äº›Mapperå’ŒReducerä¹‹é—´çš„key-value pairsï¼Œå¯ä»¥å‡å°‘å¤„ç†è¿™éƒ¨åˆ†çš„å¼€é”€ã€‚å› ä¸ºCombineråªæ˜¯å‡å°‘äº†ä¸€äº›Mapperå’ŒReducerä¹‹é—´çš„intermediate dataï¼Œä½†æ˜¯å¹¶æ²¡å‡å°‘ä»Mapperçš„outputå‡ºæ¥çš„key-value pairsçš„æ•°é‡ã€‚ä½†æ˜¯In-Combineræ˜¯æ˜¯Mapper çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯è¯´key-value pairsåœ¨Mapper è¾“å‡ºå‰å°±å·²ç»å‡å°‘äº†ã€‚ å‡å°‘äº†key-value pairså¯ä»¥å‡å°‘ç³»ç»Ÿçš„object serialization and deserialization çš„å¼€é”€ï¼Œå³åƒåœ¾å›æ”¶æœºåˆ¶ Disadvantage å†…å­˜ä½¿ç”¨ï¼Œå› ä¸ºè¦ä¿å­˜ä¸€ä¸ªarrayåœ¨å†…å­˜ä¸­ï¼Œå½“æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™æœ‰å¯èƒ½ä¼šçˆ†äº†ã€‚è§£å†³æ–¹æ¡ˆ æœ‰ä¸¤ä¸ªï¼Œç¬¬ä¸€æ˜¯é™åˆ¶arrayçš„ä¸ªæ•°ï¼Œç¬¬äºŒæ˜¯é™åˆ¶å†…å­˜çš„ä½¿ç”¨ã€‚å½“è¿™ä¿©åˆ°è¾¾æŸä¸€ä¸ªé˜ˆå€¼çš„æ—¶å€™ï¼Œå°±å‘é€ç»™Reducerã€‚ ç¬¬äºŒæ˜¯è®²ä¸€ä¸ªMapçš„è¿‡ç¨‹åˆ†æˆå‡ ä¸ªéƒ¨åˆ†ï¼Œå¯¼è‡´debugä¸­å¯èƒ½å‡ºç°oedering-dependent bugsï¼Œè°ƒè¯•å¯èƒ½æ¯”è¾ƒå›°éš¾ã€‚ â€‹ â€‹ â€‹","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.cc/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Recommender-Systems/","text":"1. What is Recommender Systemså¯¹äºæ¨èç³»ç»Ÿçš„å®šä¹‰ï¼Œæˆ‘ä»¬å…ˆä¸¾å‡ ä¸ªä¾‹å­æ¥ç†è§£ä¸€ä¸‹ã€‚ ç”µå½±ç½‘ç«™ç»™ç”¨æˆ·æ¨èç”µå½±ï¼Œå¯ä»¥æ ¹æ®è¯¥ç”¨æˆ·ä»¥å¾€çš„è¯„åˆ†ï¼Œæ¯”å¦‚ç»™æµªæ¼«çˆ±æƒ…ç”µå½±è¯„åˆ†é«˜ï¼Œç»™åŠ¨ä½œç‰‡è¯„åˆ†è¾ƒä½ï¼Œé‚£ä¹ˆç³»ç»Ÿå¯ä»¥æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œç»™ç”¨æˆ·æ¨èåå‘æµªæ¼«çˆ±æƒ…çš„ç”µå½± å¦‚æœæ˜¯æ–°ç”¨æˆ·å‘¢ï¼Ÿæˆ‘ä»¬æ²¡æœ‰è¯¥ç”¨æˆ·çš„è¯„åˆ†ä¿¡æ¯ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ ¹æ®æ•´ä¸ªç³»ç»Ÿä¸­ï¼ŒæŸäº›ç”µå½±è¯„åˆ†è¾ƒé«˜è¿›è¡Œæ¨è é‚£ä¹ˆå¦‚æœæ˜¯æ–°ç½‘ç«™ï¼Œæ–°ç”¨æˆ·å‘¢ï¼Ÿ ä»¥ä¸Šä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ¨èç³»ç»Ÿåˆ†æˆä¸¤ç±»ã€‚ Content-based systems Content-basedï¼Œå°±æ˜¯åŸºäºå·²æœ‰çš„ä¿¡æ¯è¿›è¡Œæ¨èã€‚å…·ä½“å“ªäº›ä¿¡æ¯å‘¢ï¼Ÿåœ¨ä¸Šé¢çš„ç”µå½±æ¨èç³»ç»Ÿä¸­ï¼Œæœ‰ä¸¤ç±»ä¿¡æ¯éœ€è¦åˆ†æã€‚ ç¬¬ä¸€ï¼Œæ˜¯Userçš„è¯„åˆ†ä¿¡æ¯ï¼Œæ¯”å¦‚ç»™çˆ±æƒ…ç‰‡è¯„åˆ†é«˜ï¼Œç»™åŠ¨ä½œç‰‡è¯„åˆ†ä½ã€‚ ç¬¬äºŒï¼Œæ˜¯Movieçš„ç‰¹å¾ä¿¡æ¯ï¼Œæ¯”å¦‚è¿™éƒ¨ç”µå½±åå‘çˆ±æƒ…ç‰‡å¤šä¸€äº›ï¼Œä½†ä¹Ÿæœ‰ä¸€éƒ¨åˆ†æç¬‘ã€‚æ‰€ä»¥åœ¨Aï¼ˆçˆ±æƒ…ç‰‡ï¼‰å’ŒBï¼ˆæç¬‘ç‰‡ï¼‰ä¸­ï¼Œ Açš„æƒé‡æ›´é«˜ï¼ŒBçš„è¾ƒä½ åŸºäºä»¥ä¸Šä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥ç»™ç”¨æˆ·æ¨èä»–æ‰€å–œæ¬¢çš„ç”µå½±ã€‚ Collaborative filterring systems ååŒè¿‡æ»¤å™¨ï¼Œåˆ™æ˜¯åŸºäºç”¨æˆ·/ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦è¿›è¡Œæ¨èçš„ã€‚å³ç”¨æˆ·Aå’Œç”¨æˆ·Béƒ½å–œæ¬¢çˆ±æƒ…ã€æµªæ¼«ç”µå½±ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠç”¨æˆ·Aè¯„åˆ†è¿‡çš„çˆ±æƒ…æµªæ¼«ç”µå½±ï¼Œæ¨èç»™ç”¨æˆ·Bã€‚ 2. Content-based systems2.1 Problem Analysisä»¥ç”µå½±æ¨èç³»ç»Ÿä¸ºä¾‹ï¼Œå‡è®¾æˆ‘ä»¬å·²ç»å¯¹ç³»ç»Ÿä¸­çš„ç”µå½±ç‰¹å¾æœ‰äº†è¾ƒä¸ºå®Œå–„ï¼Œå³æˆ‘ä»¬çŸ¥é“æŸéƒ¨ç”µå½±å±äºçˆ±æƒ…ç‰‡å¤šå°‘åˆ†ï¼Œå±äºåŠ¨ä½œç‰‡å¤šå°‘åˆ†ã€‚ é‚£ä¹ˆæˆ‘ä»¬ç°åœ¨ä»¥Aliceä¸ºä¾‹ï¼Œå¥¹å¯¹ä¸¤éƒ¨çˆ±æƒ…ç‰‡è¯„åˆ†æ¯”è¾ƒé«˜ï¼Œå¯¹äºä¸¤éƒ¨åŠ¨ä½œç‰‡è¯„åˆ†ä¸º0ã€‚é‚£ä¹ˆç³»ç»Ÿå°±å¯ä»¥ç»™Aliceæ¨èåå‘çˆ±æƒ…æµªæ¼«çš„ï¼Œä¸”ä¸æ€ä¹ˆå±äºåŠ¨ä½œç‰‡çš„ç”µå½±ã€‚ Movies Alice - Î¸(1) Bob - Î¸(2) Carol - Î¸(3) Dave - Î¸(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love ï¼Ÿ 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 2.2 Optimization Objectiveå®é™…ä¸Šæˆ‘ä»¬å·²ç»å‡è®¾ä¹‹å‰å¯¹æ‰€æœ‰ç”µå½±çš„ç‰¹å¾è¿›è¡Œäº†ç»Ÿè®¡ï¼Œæ‰€ä»¥æ­¤æ—¶æœ‰ç”µå½±ç‰¹å¾å‘é‡Xï¼Œä»¥åŠç”¨æˆ·å¯¹äºç”µå½±çš„è¯„åˆ†Yå‘é‡ã€‚æ ¹æ®æ­¤æ—¶å·²æœ‰çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡ºthetaçš„å€¼ã€‚æ‰€ä»¥èƒ½å¤Ÿå¯¹äºé‚£ä¹ˆæ²¡æœ‰è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œæ ¹æ®thetaå’Œxæ±‚å‡ºåˆ†æ•°yã€‚ å› ä¸ºä¸€å¼€å§‹theatçš„å€¼æ˜¯éšæœºçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨Linear Regressionçš„æ–¹æ³•ï¼Œä¸æ–­å‡å°‘cost functionçš„å€¼æ±‚å‡ºthetaã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºè¿™é‡Œæ˜¯å¤šä¸ªç”¨æˆ·ï¼Œæ¯ä¸€ä¸ªç”¨æˆ·æˆ‘ä»¬æ±‚å‡ºä¸€ä¸ªthetaå€¼ã€‚æœ€åå¯¹äºå¤šä¸ªç”¨æˆ·ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡ºå¤šä¸ªthetaå€¼ã€‚ Actually, we can assume that we have known all features about the all movies, that is x1, x2, â€¦, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 2.2 Gradient descent update 2.3 Gradient descent in Logistic Regression Question: Why we donâ€™t need $\\frac 1 m$? Anwser: As there are only one user However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to learn theta and features at the same time 3. Collaborative filtering3.1 Proble motivation Movies Alice - Î¸(1) Bob - Î¸(2) Carol - Î¸(3) Dave - Î¸(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love ? 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 3.2 How to do åœ¨ä¹‹å‰éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬äº†è§£åˆ°äº†content-basedï¼Œæ˜¯å·²çŸ¥ x å’Œ yï¼Œæ±‚ thetaã€‚ Assume: \\theta^{(1)} = \\begin{bmatrix} 0 \\\\ 5 \\\\ 0 \\end{bmatrix}, \\space\\space \\theta^{(2)} = \\begin{bmatrix} 0 \\\\ 5 \\\\ 0 \\end{bmatrix}, \\space\\space \\theta^{(3)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 5 \\end{bmatrix}, \\space\\space \\theta^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 5 \\end{bmatrix}, \\space\\space x^{(1)} = \\begin{bmatrix} 1 \\\\ 1.0 \\\\ 0.0 \\end{bmatrix}For Movie 1, we can calculate the result of Movie1 rating by all users. \\theta^{(1)} \\* x^{(1)} \\approx 5 \\\\\\ \\theta^{(2)} \\* x^{(1)} \\approx 5 \\\\\\ \\theta^{(3)} \\* x^{(1)} \\approx 0 \\\\\\ \\theta^{(4)} \\* x^{(1)} \\approx 0 ä½†æ˜¯å¯¹äºæœ‰äº›æƒ…å†µï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“xçš„ç‰¹å¾å€¼ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ é€†å‘æ€è€ƒï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ theat å’Œ yï¼Œæ¥æ±‚ x çš„å€¼ã€‚ é‚£ä¹ˆå¯¹äº thetaå’Œxçš„å€¼éƒ½ä¸çŸ¥é“çš„æƒ…å†µä¸‹å‘¢ï¼Ÿ å¯¹æ¯”ç‰¹å¾ Linear Regression Collaborative filtering ç‰¹æ€§å‘é‡X å·²çŸ¥æ•°æ® å¾…æ±‚è§£æ•°æ® æƒé‡ Î¸ å¾…æ±‚è§£æ•°æ® å¾…æ±‚è§£æ•°æ® yå€¼ å·²çŸ¥æ•°æ® å·²çŸ¥æ•°æ® 3.3 Optimization Algorithm For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. Î¸ -> x -> Î¸ -> x -> Î¸ -> x -> Î¸ -> x -> ...Actually, these two steps are Linear Regression, we shoud do that simultaneously to update theta and x. 3.4 Collaborative filtering Optimization Algorithm å®é™…ä¸Šï¼Œä¸Šé¢æ˜¯ä¸¤ä¸ª LRçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢ä¸¤æ­¥åˆå¹¶åˆ°ä¸€èµ·ï¼Œè¿™ä¸ªå°±æ˜¯collaborative filterringï¼Œ æ­¤æ—¶çš„optimizatino object å°±ä» J(theta) å’Œ J(X) å˜ä¸ºäº† J(theta, X)ã€‚ å…·ä½“æ­¥éª¤å¦‚ä¸‹ 3.5 Vectorization: Low rank matrix factorizationé¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæŠŠè¯„åˆ†Yç”¨å‘é‡è¡¨ç¤ºå‡ºæ¥ï¼ŒåŒæ—¶è¡¨ç¤ºä¸ºThetaå’ŒXä¸¤ä¸ªçŸ©é˜µçš„ä¹˜ç§¯ Y= \\begin{bmatrix} 5 & 5 & 0 & 0 \\\\ 5 & ? & ?& 0 \\\\ ? & 4 & 0 & ? \\\\ 0 & 0 & 5 & 4 \\\\ 0 & 0 & 5 & 0\\end{bmatrix} = \\begin{bmatrix} (\\theta^{(1)})^T(x^{(1)}) & (\\theta^{(2)})^T(x^{(1)}) & ... & (\\theta^{(n_u)})^T(x^{(1)}) \\\\ (\\theta^{(1)})^T(x^{(2)}) & (\\theta^{(2)})^T(x^{(2)}) & ... & (\\theta^{(n_u)})^T(x^{(2)}) \\\\ ... & ... & ... & ... \\\\ (\\theta^{(1)})^T(x^{(n_m)}) & (\\theta^{(2)})^T(x^{(n_m)}) & ... & (\\theta^{(n_u)})^T(x^{(n_m)}) \\end{bmatrix} = X * \\Theta', R \\in (n_m Ã— n_u) X = \\begin{bmatrix} ---(x^{(1)})^T--- \\\\ ---(x^{(2)})^T--- \\\\ ... \\\\ ---(x^{(n_m)})^T--- \\end{bmatrix}, x^{(n_m)} = \\begin{bmatrix} x^{(n_m)}_1 \\\\ x^{(n_m)}_2 \\\\ ... \\\\ x^{(n_m)}_n \\end{bmatrix}, R \\in (n_m Ã— n) \\Theta = \\begin{bmatrix} ---(\\theta^{(1)})^T--- \\\\ ---(\\theta^{(2)})^T--- \\\\ ... \\\\ ---(\\theta^{(n_u)})^T--- \\end{bmatrix}, \\theta^{(n_u)} = \\begin{bmatrix} \\theta^{(n_u)}_1 \\\\ \\theta^{(n_u)}_2 \\\\ ... \\\\ \\theta^{(n_u)}_n \\end{bmatrix}, R \\in (n_u Ã— n)3.6 Mean Normalizationå¯¹äºé‚£äº›æ–°æ³¨å†Œç”¨æˆ·ï¼Œç³»ç»Ÿä¸­æ²¡æœ‰è®°å½•ä»–ä»¬çš„åå¥½ï¼Œåˆ™é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚ å…ˆè®¡ç®—å‡ºæ¯éƒ¨ç”µå½±è¯„åˆ†çš„å¹³å‡å€¼muï¼Œç„¶åæŠŠæ‰€æœ‰çš„è¯„åˆ†éƒ½å‡å»å¹³å‡å€¼ï¼ˆæ­¤åå¤„ç†è¿‡çš„è¯„åˆ†å¹³å‡å€¼ä¸º0ï¼‰ã€‚è™½ç„¶è¿™æ ·åšå¯¹æœ‰è¯„åˆ†è®°å½•ç”¨æˆ·æ˜¯å¤šä½™çš„ï¼Œä½†å´å¯ä»¥å§æ²¡æœ‰è¯„åˆ†è®°å½•çš„ç”¨æˆ·ç»™ç»Ÿä¸€è¿›æ¥ï¼Œé¿å…å…¨æ˜¯0çš„æƒ…å†µã€‚ 4. Implement Algorithm4.1 Cost Function without Regularization Tipsï¼šè¿™é‡Œéœ€è¦è®¡ç®—çš„åªæ˜¯é’ˆå¯¹é‚£äº›å·²ç»è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œå¯¹äºç”¨æˆ·æ²¡æœ‰è¯„åˆ†è¿‡çš„ä¸éœ€è¦è®¡ç®—ã€‚ 4.2 Collaborative filtering gradient \\frac {\\partial J} {\\partial x_k^{(1)}} , \\frac {\\partial J} {\\partial x_k^{(2)}} , ..., \\frac {\\partial J} {\\partial x_k^{(n_m)}} \\space\\space for \\space each \\space movie\\\\ \\frac {\\partial J} {\\partial \\theta_k^{(1)}} , \\frac {\\partial J} {\\partial \\theta_k^{(2)}} , ..., \\frac {\\partial J} {\\partial \\theta_k^{(n_u)}} \\space\\space for \\space each \\space userTipsï¼š å¯¹äºä½¿ç”¨vectorizationæ–¹æ³•ï¼Œæœ€ç»ˆåªæœ‰ä¸¤ä¸ªfor-loopï¼Œä¸€ä¸ªè®¡ç®—$X_{grad}$ï¼Œä¸€ä¸ªè®¡ç®—$Theta_{grad}$ å¦‚ä½•å¯¹Xå’ŒThetaæ±‚åå¯¼æ•°ï¼Ÿ (Theta_{grad}(i, :))^T = \\begin{bmatrix} \\frac {\\partial J} {\\partial \\theta^{(i)}_1} \\\\ \\frac {\\partial J} {\\partial \\theta^{(i)}_2} \\\\ ... \\\\ \\frac {\\partial J} {\\partial \\theta^{(i)}_n} \\end{bmatrix} åŒæ ·ï¼Œæˆ‘ä»¬åªéœ€è€ƒè™‘ç”¨æˆ·å·²ç»è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œç”¨å…¶ä½œä¸ºè®­ç»ƒæ ·æœ¬ å› ä¸ºVectorizationéå¸¸å®¹æ˜“æä¹±å„ä¸ªmatrixï¼Œæ‰€ä»¥å»ºè®®å…ˆæ•´ç†ä¸€ä¸‹å„ä¸ªmatrixçš„sizeï¼Œè®¡ç®—æ—¶å¯ä»¥æ ¹æ®matrixçš„sizeè¿›è¡Œè®¡ç®—ã€‚ 4.3 Implementationæ³¨æ„è¿™é‡Œå¹¶æ²¡æœ‰ç»™å‡ºå®Œæ•´çš„ä»£ç  (Octave/Matlab)ï¼Œéƒ½åªæ˜¯ä¸»è¦çš„éƒ¨åˆ†ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950% Theta : nu x n% X : nm x n% Y : nm x nu% R : nm x nupred = X * Theta'; %nm x nu 'diff = pred - Y;% Cost Function with regularizationJ = 0.5 * sum(sum((diff.^2) .* R));J = J + (lambda * 0.5) * sum(sum(Theta.^2)); % regularized term of theta.J = J + (lambda * 0.5) * sum(sum(X.^2)); % regularized term of x.% calculate Xfor i = 1 : num_movies, % the row vector of all users that have rated movie i idx = find(R(i, :) == 1); % (1 * r) % the list of users who have rated on movie i Theta_temp = Theta(idx, :); % (r * n) Y_temp = Y(i, idx); % (1 * r) X_temp = X(i, :); % (1 * n) % ((1 * n) * (n * r) -(1 * r)) * (r * n) = (1 * n) X_grad(i, :) = (X_temp * Theta_temp' - Y_temp) * Theta_temp; %' % regularization X_grad(i, :) = X_grad(i, :) + lambda * X_temp;end% calculate Thetafor i = 1 : num_users, % the row vector of all movies that user i has rated idx = find(R(:, i) == 1)'; % (1 * r) ' Theta_temp = Theta(i, :); % (1 * n) Y_temp = Y(idx, i); % (r * 1) X_temp = X(idx, :); % (r * n) % ((r * n) * (n * 1) - (r * 1)) * (r * n) = (1 * n) Theta_grad(i, :) = (X_temp * Theta_temp' - Y_temp)' * X_temp; % regularization Theta_grad(i, :) = Theta_grad(i, :) + lambda * Theta_temp;endgrad = [X_grad(:); Theta_grad(:)]; â€‹â€‹â€‹","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"}]},{"title":"Machine Learning - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-SVM/","text":"1. Optimisation Objective h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\\\ z = -\\theta^Tx Why we need do that? 2. Hypothesis Function2.1 Logistic Regression \\frac 1 m \\sum\\_{i=1}^m [ y^{(i)} (-log(h\\_\\theta(x^{(i)})) + (1 - y^{(i)}) (-log(1 - h\\_\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum\\_{j=1}^n \\theta\\_j^22.2 Support Vector Machine C \\sum\\_{i=1}^m [y^{(i)} cost\\_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost\\_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum\\_{j=1}^n \\theta\\_j^2, \\space \\space \\space \\space \\space C = \\frac 1 \\lambdaAnalysisï¼š ä¸ºäº†ä½¿å¾—cost functionå–å¾—æœ€å°å€¼ï¼Œæˆ‘ä»¬ä»¤C*W + Péƒ¨åˆ†ä¸­ï¼ŒC*Wä¸ºé›¶ã€‚å³ï¼š å½“ y = 1æ—¶ï¼Œ cost1 = 0ï¼Œæ‰€ä»¥ z &gt;= 1 å½“ y = 0æ—¶ï¼Œ cost0 = 0ï¼Œæ‰€ä»¥ z &lt;= -1 Noteï¼š1. cost0 and cost1 å¯¹åº”çš„æ˜¯ä¸Šå›¾ä¸­å·¦å³ä¸¤è¾¹çš„cost functionï¼Œå› ä¸ºy=0å’Œy=1çš„ç›®æ ‡å‡½æ•°ã€‚ å¸¸æ•°Cå–ä¸€ä¸ªå¾ˆå¤§çš„å€¼æ—¶æ¯”è¾ƒå¥½ã€‚å› ä¸ºC*W + Pï¼Œ æ‰€ä»¥Cå¤§åˆ™Wä¼šå˜å°ï¼Œå³ç›¸å¯¹penalityå°±ä¼šå˜å¤§ï¼ŒWä¼šå˜å° ä¸ºä»€ä¹ˆè¦é‡æ–°é€‰å®šä¸€ä¸ªcost function ï¼Ÿï¼ˆé€»è¾‘å›å½’çš„ä¸´ç•Œç‚¹ä¸º0ï¼Œä½†æ˜¯SVMçš„ä¸´ç•Œç‚¹æ˜¯1ï¼Œæ‰€ä»¥SVMæ›´åŠ ç²¾ç¡®ã€‚ ï¼‰ å¯¹åº”çš„çº¿æ€§é€»è¾‘å›å½’ï¼Ÿå³æ¬¡æ•°ä¸å¤§äº1çš„ï¼Ÿ Decision Boundary ä¸æ˜¯ä¸€æ¡ç›´çº¿çš„æƒ…å†µ 3. Large Margin Classifier12ç»“è®ºï¼šå¸¸æ•°Cå–ä¸€ä¸ªæ¯”è¾ƒå¤§çš„å€¼æ¯”è¾ƒå®¹æ˜“è·å¾—Large Margin ClassifierCå¤§ï¼Œåˆ™æ¯”è¾ƒå®¹æ˜“è·å¾— ä»¥ä¸Šä¸ºä¸¤ç±»åˆ†å¸ƒæ¯”è¾ƒå‡åŒ€çš„æ—¶å€™ï¼ŒDecision Boundaryä¸ºå›¾ä¸­é»‘è‰²çš„çº¿ï¼Œæ‰€æœ‰ç‚¹ç¦»é»‘è‰²çš„è·ç¦»éƒ½ç›¸å¯¹æ¯”è¾ƒå¤§æ¯”è¾ƒå‡åŒ€ï¼Œä½†æ˜¯å½“å­˜åœ¨å¹²æ‰°ç‚¹çš„æ—¶å€™å¦‚ä¸‹å›¾ï¼ŒDecision Boundaryä¼šç”±é»‘è‰²å˜ä¸ºç²‰çº¢è‰²ã€‚æ‰€ä»¥Cçš„å–å€¼ä¸èƒ½å¤ªå¤§ï¼Œä¹Ÿä¸èƒ½å¤ªå°ã€‚éœ€è¦æ±‚å‡ºæœ€ä¼˜è§£ 4. Mathmatics Behind Large Margin Classification4.1 Vector Inner Product Noteï¼š å¦‚ä½•æ±‚æŠ•å½±pçš„å€¼ï¼Ÿ å½“è§’åº¦ &lt; 90Â°ï¼Œpä¸ºæ­£æ•°ã€‚å½“è§’åº¦ &gt; 90Â°æ—¶ï¼Œpä¸ºè´Ÿæ•°ã€‚ å‘é‡å†…ç§¯ u^Tv = ||u|| Â· ||v|| Â· cosÎ¸ = ||u|| Â· p\\_{v,u} = ||v|| Â· p\\_{u,v} = u\\_1v\\_1+u\\_2v\\_2 4.2 SVM Cost Function C \\sum\\_{i=1}^m [y^{(i)} cost\\_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost\\_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum\\_{j=1}^n \\theta\\_j^2, \\space \\space \\space \\space \\space C = \\frac 1 \\lambdaå½“Cå–ä¸€ä¸ªä¸€ä¸ªå¾ˆå¤§çš„å€¼æ—¶ï¼Œcost functionåªå‰©ä¸‹åé¢Pçš„éƒ¨åˆ†ã€‚ å‡è®¾Î¸0 = 0 \\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + ... + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + ... + \\theta^2_n) = \\frac 1 2 ||\\theta||^2 æ‰€ä»¥ï¼š \\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\ p^{(i)}||\\theta|| >= 1, if \\space\\space y^{(i)} = 1 \\\\ p^{(i)}||\\theta|| = 1, if \\space\\space y^{(i)} = 1 \\\\ p^{(i)}||\\theta|| = 0 \\\\ \\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + ... >= 0å³å°†fnå®šä¹‰ä¸ºxçš„å¹‚æ¬¡é¡¹ç»„åˆï¼Œå¦‚ä¸‹ï¼š f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, ... ä½†æ˜¯åœ¨SVMä¸­ï¼Œæˆ‘ä»¬è¦é‡æ–°å®šä¹‰fnï¼Œå¼•å…¥Kernelçš„æ¦‚å¿µï¼Œå³ç”¨ kernel functionæ¥è¡¨ç¤ºfnã€‚ Note: l æ˜¯landmarkï¼Œä¸”å¦‚æœtraining setsé‡Œé¢çš„æ•°é‡ä¸ºnçš„è¯ï¼Œåˆ™landmarkçš„æ•°é‡ä¹Ÿä¸ºnã€‚ å‡è®¾training setsæ•°é‡ä¸ºnï¼Œåˆ™å¯¹äºä¸€ä¸ªæ–°çš„exampleæ¥è¯´ï¼Œå¯è®¡ç®—å‡ºnä¸ªæ–°çš„ç‰¹å¾f1â€¦fnã€‚ç„¶åç”¨æ–°çš„ç‰¹å¾ï¼Œå¯¹è¯¥exampleè¿›è¡Œåˆ¤æ–­ï¼ˆä½ç»´è½¬ä¸ºé«˜ç»´çš„è¿‡ç¨‹ï¼‰ kernel functionä¸ºguassian functionã€‚å½“xä¸landmark lè¶Šæ¥è¿‘æ—¶ï¼Œä¸¤ç‚¹çš„è·ç¦»è¶Šå°ï¼Œå€¼æ¥è¿‘1 5.2 SVM with Kernels å¯¹æ¯”ä¹‹å‰çš„cost functionï¼Œå¯ä»¥å‘ç°è¿™é‡ŒÎ¸å’Œf(x)è·Ÿä¹‹å‰çš„ä¸åŒã€‚ åœ¨logistic regression ä¸­ï¼ŒÎ¸çš„ç»´åº¦ä¸º(n+1) x 1, åŒ…å«Î¸0ï¼Œ ä¸”nä¸ºå•ä¸ªexampleçš„ç‰¹å¾ä¸ªæ•° åœ¨SVM with kernelä¸­ï¼Œf(x)çš„ä¸ªæ•°ä¸ºmï¼Œå…¶ä¸­mæ˜¯training setsä¸­çš„ä¸ªæ•°ï¼Œæ‰€ä»¥Î¸çš„ç»´åº¦åº”è¯¥æ˜¯(m+1)x1 Steps ç»™å®šä¸€ç»„training setsï¼Œæ ¹æ®æ¯ä¸ªexampleï¼Œé€‰å–mä¸ªlandmarkç‚¹ è®¡ç®—æ¯ä¸€ä¸ªexampleä¸æ‰€æœ‰landmarkçš„ç›¸è¯†åº¦ï¼Œç›¸åŒä¸º1ï¼Œéå¸¸ä¸åŒæ¥è¿‘ä¸º0ã€‚è®¡ç®—ç›¸è¯†åº¦çš„kernel functionä¸ºGaussian Function æœ€ç»ˆï¼Œå¯¹äºæ¯ä¸€ä¸ªexampleé‡Œé¢éƒ½å¯ä»¥è®¡ç®—å‡ºmä¸ªæ–°çš„featureï¼Œæ‰€ä»¥å¯¹äºè¿™ä¸ªtraining setsè€Œè¨€ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªm*mçš„çŸ©é˜µï¼Ÿ å°†å¾—åˆ°çš„m*mçš„çŸ©é˜µï¼Œä»£å…¥åˆ°Hypothesisä¸­ï¼Œè®¡ç®—å‡ºÎ¸çš„å€¼ã€‚ 5.4 SVM parameters C = 1/Î» Large C Small Î» Large Î¸ Lower Bias High Variance Over Fitting Small C Large Î» Small Î¸ Higher Bias Low Variance Under Fitting Ïƒ Large Ïƒ more smoothly Higher Bias Lower Variance Under Fitting Small Ïƒ less smoothly Lower Bias Higher Variance Over Fitting","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.cc/tags/SVM/"}]},{"title":"Linuxå¸¸ç”¨æŒ‡ä»¤","date":"2017-02-19T01:16:24.000Z","path":"2017/02/19/Linuxå¸¸ç”¨æŒ‡ä»¤/","text":"æœ‰æ—¶å€™è®°ä¸œè¥¿è€æ˜¯è®°ä¸ä½ï¼Œè™½ç„¶æ˜¯ç»å¸¸ä½¿ç”¨ï¼Œæ‰€ä»¥å¯¹å¸¸ç”¨çš„linuxæŒ‡ä»¤åšä¸ªç®€å•è®°å½•ï¼Œæ–¹ä¾¿ä»¥åä½¿ç”¨ï¼ˆä¸æ–­æ›´æ–°â€¦ â€¦ï¼‰ã€‚ 1. scpæ–‡ä»¶ä¼ è¾“ (security copy)12# eg. scp chenson@127.0.0.1:~/Home/test . ~/Home/local_test_pathscp PATH_OF_SRC PATH_OF_DST 2. æµ‹å†…å­˜çš„ä½¿ç”¨ç‡12345678valgrind --tool=massif --pages-as-heap=yesms_print massif æ–‡ä»¶åvalgrind --tool=massif --pages-as-heap=yes ./bwtsearch testfiles/shopping.bwt l.idx \"system\"ms_print massif.out.3116 1find &lt;PATH TO FOLDER&gt; -ls | tr -s &apos; &apos;|cut -d&apos; &apos; -f 7| paste -sd+ | bc 3. å¼ºåˆ¶å†™å…¥123456789# vii commond model, Try the below command:w !sudo tee %Explanation:w â€“ write!sudo â€“ call shell sudo commandtee â€“ the output of write (:w) command is redirected using tee% â€“ current file name 4. æŸ¥çœ‹äºŒè¿›åˆ¶æ–‡ä»¶vimä¸‹ 1:%!xxd linuxä¸‹ 1xxd -b file_name 5. æŸ¥çœ‹å…¬ç½‘IP1curl http://members.3322.org/dyndns/getip 6. æŸ¥çœ‹é‚£ä¸ªè¿›ç¨‹å ç”¨ï¼š1lsof -i:9000 # MAC 7. æŸ¥çœ‹ç›‘å¬ç«¯å£ï¼š12netstat -ntlp # linuxlsof -i:port_num # MAC 8. æµ‹æ–‡ä»¶å¤¹å¤§å°123456find &lt;PATH TO FOLDER&gt; -ls | tr -s ' '|cut -d' ' -f 7| paste -sd+ |bcdu -ah --max-depth=1ï¼Œå…¶ä¸­aè¡¨ç¤ºæ˜¾ç¤ºç›®å½•ä¸‹æ‰€æœ‰çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ï¼ˆä¸å«å­ç›®å½•ï¼‰ï¼Œhè¡¨ç¤ºä»¥äººç±»èƒ½çœ‹æ‡‚çš„æ–¹å¼ï¼Œmax-depthè¡¨ç¤ºç›®å½•çš„æ·±åº¦ã€‚du -sh : æŸ¥çœ‹å½“å‰ç›®å½•æ€»å…±å çš„å®¹é‡ã€‚è€Œä¸å•ç‹¬åˆ—å‡ºå„å­é¡¹å ç”¨çš„å®¹é‡ 9. æ–‡ä»¶ç³»ç»Ÿç©ºé—´ä½¿ç”¨æƒ…å†µ1234567891011# æŸ¥çœ‹ç³»ç»Ÿä¸­æ–‡ä»¶çš„ä½¿ç”¨æƒ…å†µdf -h# æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹å„ä¸ªæ–‡ä»¶åŠç›®å½•å ç”¨ç©ºé—´å¤§å°du -sh *# æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹æ–‡ä»¶å¤¹çš„å¤§å°du -h folder_name# æŸ¥çœ‹è¯¥æ–‡ä»¶å¤¹çš„å¤§å°df -hs folder_name å¯ä»¥çœ‹åˆ°ä¸‹é¢æœ‰ä¸€å—æ•°æ®ç›˜ï¼ŒæŒ‚è½½åœ¨/data1ä¸‹ å¦‚æœç³»ç»Ÿæœ‰å•ç‹¬çš„æ•°æ®ç›˜ï¼Œä¸”æ•°æ®ç›˜æ²¡æœ‰åˆ†åŒºå’ŒæŒ‚è½½ï¼Œä½¿ç”¨df -hå‘½ä»¤æ˜¯çœ‹ä¸åˆ°çš„ï¼Œå¯ä»¥ä½¿ç”¨fdisk -læŸ¥çœ‹ï¼Œå¯ä»¥çœ‹åˆ°æœ‰å“ªäº›ç¡¬ç›˜ ä¸‹å›¾å¯çœ‹åˆ°æ•°æ®ç›˜ä¸ºDisk /dev/vdb å¯ä»¥çœ‹åˆ°ä¸Šå›¾æœ‰53.7Gçš„æœªåˆ†åŒºæ•°æ®ç›˜ï¼Œç„¶åæœ‰1771.7Gçš„å·²åˆ†åŒºçš„æ•°æ®ç›˜ã€‚ References åˆ†åŒºæ•™ç¨‹ UbuntuæŸ¥çœ‹ç£ç›˜ä½¿ç”¨æƒ…å†µ 10. åŠ å¯†æ–‡ä»¶å¤§å°12# oJ5A93Fhzip -r -P password utils.zip utils 11. æ‰“åŒ…+å‹ç¼© å‹ç¼© 12345678910111213tar czvf my.tar.gz file1 file2 ....fileNtar -cvf jpg.tar *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆtar.jpg tar -czf jpg.tar.gz *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨gzipå‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªgzipå‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.gz tar -cjf jpg.tar.bz2 *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨bzip2å‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªbzip2å‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.bz2tar -cZf jpg.tar.Z *.jpg //å°†ç›®å½•é‡Œæ‰€æœ‰jpgæ–‡ä»¶æ‰“åŒ…æˆjpg.taråï¼Œå¹¶ä¸”å°†å…¶ç”¨compresså‹ç¼©ï¼Œç”Ÿæˆä¸€ä¸ªumcompresså‹ç¼©è¿‡çš„åŒ…ï¼Œå‘½åä¸ºjpg.tar.Zrar a jpg.rar *.jpg //raræ ¼å¼çš„å‹ç¼©ï¼Œéœ€è¦å…ˆä¸‹è½½rar for linuxzip jpg.zip *.jpg //zipæ ¼å¼çš„å‹ç¼©ï¼Œéœ€è¦å…ˆä¸‹è½½zip for linux è§£å‹ 1234567891011tar -xvf file.tar //è§£å‹ taråŒ…tar -xzvf file.tar.gz //è§£å‹tar.gztar -xjvf file.tar.bz2 //è§£å‹ tar.bz2tar -xZvf file.tar.Z //è§£å‹tar.Zunrar e file.rar //è§£å‹rarunzip file.zip //è§£å‹zip 12. æŸ¥æ‰¾æ–‡ä»¶12345678910111213141516171819202122$find ~ -name \"*.txt\" -print #åœ¨$HOMEä¸­æŸ¥.txtæ–‡ä»¶å¹¶æ˜¾ç¤º$find . -name \"*.txt\" -print$find . -name \"[A-Z]*\" -print #æŸ¥ä»¥å¤§å†™å­—æ¯å¼€å¤´çš„æ–‡ä»¶$find /etc -name \"host*\" -print #æŸ¥ä»¥hostå¼€å¤´çš„æ–‡ä»¶$find . -name \"[a-z][a-z][0â€“9][0â€“9].txt\" -print #æŸ¥ä»¥ä¸¤ä¸ªå°å†™å­—æ¯å’Œä¸¤ä¸ªæ•°å­—å¼€å¤´çš„txtæ–‡ä»¶$find . -perm 755 -print$find . -perm -007 -exec ls -l &#123;&#125; \\; #æŸ¥æ‰€æœ‰ç”¨æˆ·éƒ½å¯è¯»å†™æ‰§è¡Œçš„æ–‡ä»¶åŒ-perm 777$find . -type d -print$find . ! -type d -print $find . -type l -print$find . -size +1000000c -print #æŸ¥é•¿åº¦å¤§äº1Mbçš„æ–‡ä»¶$find . -size 100c -print # æŸ¥é•¿åº¦ä¸º100cçš„æ–‡ä»¶$find . -size +10 -print #æŸ¥é•¿åº¦è¶…è¿‡æœŸä½œåºŸ10å—çš„æ–‡ä»¶ï¼ˆ1å—=512å­—èŠ‚ï¼‰$cd /$find etc home apps -depth -print | cpio -ivcdC65536 -o /dev/rmt0$find /etc -name \"passwd*\" -exec grep \"cnscn\" &#123;&#125; \\; #çœ‹æ˜¯å¦å­˜åœ¨cnscnç”¨æˆ·$find . -name \"yao*\" | xargs file$find . -name \"yao*\" | xargs echo \"\" &gt; /tmp/core.log$find . -name \"yao*\" | xargs chmod o-w 13. é€’å½’åˆ é™¤æ‰€æœ‰æ–‡ä»¶å¤¹ä¸‹çš„ç‰¹å®šç±»å‹æ–‡ä»¶1find . -name '*.ttteset' -type f -print -exec rm -rf &#123;&#125; \\; 14. åå°è¿è¡ŒæŒ‡å®šç¨‹åºï¼ˆä¸æŒ‚æ–­åœ°è¿è¡Œå‘½ä»¤ï¼‰ nohub 123456789101112131415161718192021nohup python main.py 80 &gt; ~/log/WeChatAccountLog.file 2&gt;&amp;1 &amp;è¯¥å‘½ä»¤çš„ä¸€èˆ¬å½¢å¼ä¸ºï¼šnohup command &amp;å¦‚æœä½¿ç”¨nohupå‘½ä»¤æäº¤ä½œä¸šï¼Œé‚£ä¹ˆåœ¨ç¼ºçœæƒ…å†µä¸‹è¯¥ä½œä¸šçš„æ‰€æœ‰è¾“å‡ºéƒ½è¢«é‡å®šå‘åˆ°ä¸€ä¸ªåä¸ºnohup.outçš„æ–‡ä»¶ä¸­ï¼Œé™¤éå¦å¤–æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼šcommand&gt;out.fileæ˜¯å°†commandçš„è¾“å‡ºé‡å®šå‘åˆ°out.fileæ–‡ä»¶ï¼Œå³è¾“å‡ºå†…å®¹ä¸æ‰“å°åˆ°å±å¹•ä¸Šï¼Œè€Œæ˜¯è¾“å‡ºåˆ°out.fileæ–‡ä»¶ä¸­ã€‚2&gt;&amp;1 æ˜¯å°†æ ‡å‡†å‡ºé”™é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡ºï¼Œè¿™é‡Œçš„æ ‡å‡†è¾“å‡ºå·²ç»é‡å®šå‘åˆ°äº†out.fileæ–‡ä»¶ï¼Œå³å°†æ ‡å‡†å‡ºé”™ä¹Ÿè¾“å‡ºåˆ°out.fileæ–‡ä»¶ä¸­ã€‚æœ€åä¸€ä¸ª&amp;ï¼Œ æ˜¯è®©è¯¥å‘½ä»¤åœ¨åå°æ‰§è¡Œã€‚è¯•æƒ³2&gt;1ä»£è¡¨ä»€ä¹ˆï¼Œ2ä¸&gt;ç»“åˆä»£è¡¨é”™è¯¯é‡å®šå‘ï¼Œè€Œ1åˆ™ä»£è¡¨é”™è¯¯é‡å®šå‘åˆ°ä¸€ä¸ªæ–‡ä»¶1ï¼Œè€Œä¸ä»£è¡¨æ ‡å‡†è¾“å‡ºï¼›æ¢æˆ2&gt;&amp;1ï¼Œ&amp;ä¸1ç»“åˆå°±ä»£è¡¨æ ‡å‡†è¾“å‡ºäº†ï¼Œå°±å˜æˆé”™è¯¯é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡º.# å¯ä»¥å°†ä¸€ä¸ªæ­£åœ¨å‰å°æ‰§è¡Œçš„å‘½ä»¤æ”¾åˆ°åå°ï¼Œå¹¶ä¸”å¤„äºæš‚åœçŠ¶æ€ã€‚ctrl + z # ç»ˆæ­¢å‰å°å‘½ä»¤ã€‚Ctrl+c # æŸ¥çœ‹å½“å‰æœ‰å¤šå°‘åœ¨åå°è¿è¡Œçš„å‘½ä»¤jobs ä½¿ç”¨ fg %n å…³é—­ screen 1 15. åå°ä¸Šä¼ /ä¸‹è½½æ•°æ®1å¦å¤–æœ‰ä¸¤ä¸ªå¸¸ç”¨çš„ftpå·¥å…·ncftpgetå’Œncftpputï¼Œå¯ä»¥å®ç°åå°çš„ftpä¸Šä¼ å’Œä¸‹è½½ï¼Œè¿™æ ·å°±å¯ä»¥åˆ©ç”¨è¿™äº›å‘½ä»¤åœ¨åå°ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶äº†ã€‚ 16. Docker åŠ é€Ÿå™¨åœ°å€12https://cr.console.aliyun.com/cn-qingdao/mirrorshttps://q90v31vp.mirror.aliyuncs.com 17. æŸ¥æ‰¾æŒ‡å®šå†…å®¹1find . | xargs grep &quot;keyword&quot; | egrep &quot;keyword&quot; # å½“å‰æ–‡ä»¶ç›®å½•ä¸‹ 18. æŸ¥çœ‹ç³»ç»Ÿglibcæ”¯æŒçš„ç‰ˆæœ¬123strings /lib64/libc.so.6 |grep GLIBC_rpm -qa |grep glibc vsftpd, kaiqi ftp fuwu choongqi dashuju taojian ambari-agent restart","tags":[{"name":"Linux","slug":"Linux","permalink":"http://chenson.cc/tags/Linux/"}]},{"title":"Pythonæ•°æ®åˆ†æç¬”è®°ï¼ˆä¸€ï¼‰","date":"2016-12-15T11:37:30.000Z","path":"2016/12/15/Pythonæ•°æ®åˆ†æç¬”è®°ï¼ˆä¸€ï¼‰/","text":"1. å¸¸è§é—®é¢˜ Pandas.dataframeé‡Œé¢ .values, .iloc, .ix, .loc çš„åŒºåˆ« Different Choices for Indexing loc: only work on index / labeliloc: work on position, from 0, 1, 2, â€¦ â€¦ix: You can get data from dataframe without it being in the indexat: get scalar values. Itâ€™s a very fast lociat: Get scalar values. Itâ€™s a very fast iloc 12345678910111213141516171819202122232425262728&gt;&gt;&gt; df = pd.DataFrame(&#123;'A':['a', 'b', 'c'], 'B':[54, 67, 89]&#125;, index=[100, 200, 300])&gt;&gt;&gt; df A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df.iloc[0] # ç”¨ä½ç½®æ¥ç´¢å¼•A aB 54&gt;&gt;&gt; df.loc[100] # ç”¨åˆå§‹åŒ–æ—¶è®¾ç½®çš„indexæ¥ç´¢å¼•ï¼Œä¹Ÿå°±æ˜¯è‡ªå·±ç»™rowè®¾ç½®çš„labelA aB 54&gt;&gt;&gt; df.loc[100:300] A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df['A'] # ç´¢å¼• columns100 a200 b300 cName: A, dtype: object&gt;&gt;&gt; df.A # ç´¢å¼• columns100 a200 b300 cName: A, dtype: object Pandas å’Œ Numpyä¹‹é—´çš„è½¬æ¢ np.ndarray è½¬åŒ–ä¸º pd.dataframe 1pd.DataFrame(example) pd.dataframe è½¬åŒ–ä¸º np.ndarray 1example.values[:, :] è¯»å†™æ•ˆç‡çš„å¯¹æ¯” npyè¯»å†™æ•ˆç‡æœ€é«˜ï¼Œä½†æœ€è´¹ç¡¬ç›˜ç©ºé—´ï¼Œæ¯”å¦‚np.load(), np.save() csvå…¶æ¬¡ï¼Œæ¯”å¦‚pd.Dataframe.to_csv()ï¼Œpd.load_csv() txtè¯»å†™ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å¾ˆå¿«ï¼Œä½†æ˜¯éœ€è¦é¢‘ç¹çš„splitï¼Œå¯¹æ ¼å¼è§„èŒƒçš„æ•°æ®æ¯”è¾ƒéº»çƒ¦ è‡³äºç®€å•çš„excelå’Œwordï¼Œå¯ä»¥ç”¨xlrd,xlwtæ¥æ“ä½œ 2. Numpy Nç»´æ•°ç»„å¯¹è±¡ï¼Œå¯ä»¥åˆ©ç”¨è¿™ç§æ•°ç»„å¯¹è±¡å¯¹æ•´å—æ•°æ®è¿›è¡Œä¸€äº›ç§‘å­¦è¿ç®—ï¼Œå°±æ˜¯æŠŠarrayå½“åšä¸€ç§å¯¹è±¡é‡Œæ“ä½œã€‚è¿™å’ŒPythonä¸­çš„arrayæ˜¯ä¸åŒçš„ã€‚ ä¸¾ä¸ªæ —å­ï¼š åœ¨Pythonä¸­ 123&gt;&gt;&gt; data = [1, 2, 3]&gt;&gt;&gt; data * 10[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3] Numpyçš„ndarray 12345&gt;&gt;&gt; data2 = np.array([1, 2, 3])&gt;&gt;&gt; data2array([1, 2, 3])&gt;&gt;&gt; data2 * 10array([10, 20, 30]) è½´(axes) å’Œ ç§©(rank) è½´è¡¨ç¤ºçš„æ˜¯ä¸€ç§ç»´åº¦ï¼Œå¦‚ä¸€ç»´çš„æ•°æ®ï¼ŒäºŒç»´çš„æ•°æ®ï¼Œä¸‰ç»´çš„æ•°æ®ç­‰ 12345678910111213141516171819202122&gt;&gt;&gt; data3 = np.array([1, 2, 3]) # æ³¨æ„è¿™é‡Œçš„**æ–¹æ‹¬å·**&gt;&gt;&gt; data3.ndim # æŸ¥çœ‹ç»´åº¦1&gt;&gt;&gt; data3 = np.array([[1,2,3], [1,2,3], [1,2,3]])&gt;&gt;&gt; data3.ndim2&gt;&gt;&gt; data3 = np.array([[[1], [1], [1]], [[1], [1], [1]],[[1], [1], [1]]])&gt;&gt;&gt; data3.ndim3&gt;&gt;&gt; data3.shape(3, 3, 1) # ç»´åº¦ä»æœ€å¤–å±‚åˆ°é‡Œå±‚&gt;&gt;&gt; data3.size9 # 3 * 3 * 1 = 9#ä¸€ä¸ªç”¨æ¥æè¿°æ•°ç»„ä¸­å…ƒç´ ç±»å‹çš„å¯¹è±¡ï¼Œå¯ä»¥é€šè¿‡åˆ›é€ æˆ–æŒ‡å®šdtypeä½¿ç”¨æ ‡å‡†Pythonç±»å‹ã€‚å¦å¤–NumPyæä¾›å®ƒè‡ªå·±çš„æ•°æ®ç±»å‹ã€‚&gt;&gt;&gt; data3.dtype# æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚å¤§å°ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå…ƒç´ ç±»å‹ä¸ºfloat64çš„æ•°ç»„itemsizå±æ€§å€¼ä¸º8(=64/8),åˆå¦‚ï¼Œä¸€ä¸ªå…ƒç´ ç±»å‹ä¸ºcomplex32çš„æ•°ç»„itemå±æ€§ä¸º4(=32/8).&gt;&gt;&gt; data3.itermsize# åŒ…å«å®é™…æ•°ç»„å…ƒç´ çš„ç¼“å†²åŒºï¼Œé€šå¸¸æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨è¿™ä¸ªå±æ€§ï¼Œå› ä¸ºæˆ‘ä»¬æ€»æ˜¯é€šè¿‡ç´¢å¼•æ¥ä½¿ç”¨æ•°ç»„ä¸­çš„å…ƒç´ ã€‚&gt;&gt;&gt; data3.data å¸¸ç”¨çš„æ•°ç»„åˆ›å»ºå‡½æ•° æ‰“å°æ•°ç»„ 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(6) # 1d array&gt;&gt;&gt; print(a)[0 1 2 3 4 5]&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(12).reshape(4,3) # 2d array&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]&gt;&gt;&gt;&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4) # 3d array&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] å¦‚æœä¸€ä¸ªæ•°ç»„ç”¨æ¥æ‰“å°å¤ªå¤§äº†ï¼ŒNumPyè‡ªåŠ¨çœç•¥ä¸­é—´éƒ¨åˆ†è€Œåªæ‰“å°è§’è½ 1234567891011&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt;&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] ç¦ç”¨è¿™ç§reshapeæ¥æ‰“å°æ•´ä¸ªæ•°ç»„ï¼Œéœ€è¦å¯¹printoptionå‚æ•°è¿›è¡Œè®¾ç½® 1&gt;&gt;&gt; np.set_printoptions(threshold='nan') åŸºæœ¬çš„æ•°æ®è¿ç®— 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; b = np.array(list(range(5, 10)))&gt;&gt;&gt; a + barray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; a - barray([-5, -5, -5, -5, -5])&gt;&gt;&gt; a * barray([ 0, 6, 14, 24, 36])&gt;&gt;&gt; a += b&gt;&gt;&gt; aarray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; c = np.ones(5)&gt;&gt;&gt; carray([ 1., 1., 1., 1., 1.])&gt;&gt;&gt; c + aarray([ 6., 8., 10., 12., 14.])&gt;&gt;&gt; c = np.ones(5, dtype=int)&gt;&gt;&gt; c + aarray([ 6, 8, 10, 12, 14]) æ•°ç»„çš„è¿ç®— è¿™å°±ç±»ä¼¼åœ¨Matlab/Octaveä¸­ï¼Œå¯¹matrix/arrayä¸­çš„æ•°æ®æ‰§è¡Œæ‰¹é‡è¿ç®—ï¼Œå³Vectorizationï¼Œå‰ææ˜¯matrix/arrayçš„å¤§å°å¿…é¡»æ»¡è¶³å¯¹åº”çš„è¦æ±‚ã€‚ ç”¨æ•°ç»„è¡¨è¾¾å¼å¯ä»¥ä»£æ›¿å¾ªç¯æ“ä½œï¼ŒçŸ¢é‡åŒ–çš„è¿ç®—æ˜¯Numpyçš„ä¼˜åŠ¿ã€‚ æ•°ç»„è½¬ç½®å’Œè½´å¯¹æ¢ 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.T # æ•°ç»„è½¬ç½®ï¼Œè½´å¯¹æ¢array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]])&gt;&gt;&gt; np.dot(a.T, a) # å†…ç§¯array([[125, 140, 155, 170, 185], [140, 158, 176, 194, 212], [155, 176, 197, 218, 239], [170, 194, 218, 242, 266], [185, 212, 239, 266, 293]])&gt;&gt;&gt; b = np.arange(16).reshape((2, 2, 4))&gt;&gt;&gt; barray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt;&gt;&gt; b.transpose((1, 0 , 2)) # å¯¹é«˜ç»´æ•°ç»„ï¼Œtransposeéœ€è¦å¾—è¦ä¸€ä¸ªç”±è½´ç¼–å·ç»„æˆçš„å…ƒç»„æ‰èƒ½å¯¹è¿™äº›è½´è¿›è¡Œè½¬ç½®array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) ç´¢å¼•å’Œåˆ‡ç‰‡ 1234567891011121314151617181920212223242526272829303132333435# ä¸€ç»´æ•°æ®&gt;&gt;&gt; a = np.arange(5)array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:]array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[1 : 4]array([1, 2, 3])&gt;&gt;&gt; a[1]1# äºŒç»´æ•°æ®&gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6]])array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:, 1:3]array([[2, 3], [5, 6]])&gt;&gt;&gt; b[1][2]6&gt;&gt;&gt; b[1, 2]6# ä¸‰ç»´æ•°ç»„&gt;&gt;&gt; c = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; c[1]array([[ 7, 8, 9], [10, 11, 12]]) é€šç”¨å‡½æ•° P111 ç»™ array æ·»åŠ  columns å’Œ rows 123456789101112# æ–¹æ³•ä¸€np.c_[array1, array2] # æ·»åŠ  columnsnp.r_[array1, array2] # æ·»åŠ  row# æ–¹æ³•äºŒ è¢«æ’å…¥çš„è¡Œnp.insert(a, 2, values=b, axis=1) # æ·»åŠ  columns# æ–¹æ³•ä¸‰a = np.concatenate((a, -np.ones((a.shape[0], 1))), axis=1) # æ·»åŠ  columns# æ–¹æ³•å››np.column_stack((a,b)) 3. Pandasåœ¨Pandasä¸­ï¼ŒSerieså’ŒDataFrameæ˜¯ä¸¤ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„ Series ç±»ä¼¼ä¸€ç»´æ•°ç»„ï¼Œç”±ä¸€ç»„æ•°æ®ï¼ˆå„ç§Numptæ•°æ®ç±»å‹ ( list, dictç­‰ )ï¼‰å’Œä¸€ç»„å¯¹äºçš„æ•°æ®æ ‡ç­¾ç»„æˆ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&gt;&gt;&gt; obj = pd.Series(list(range(3, 8)))&gt;&gt;&gt; obj0 31 42 53 64 7dtype: int64&gt;&gt;&gt; obj.valuesarray([3, 4, 5, 6, 7])&gt;&gt;&gt; obj.index # è¿™é‡Œå°±æ˜¯ index ç´¢å¼•ï¼Œæ²¡æœ‰è®¾ç½®çš„æ˜¯æ—¶å€™ç³»ç»Ÿä¼šè‡ªåŠ¨è®¾ç½®ä¸ºinteger indexRangeIndex(start=0, stop=5, step=1)&gt;&gt;&gt; obj[1] # å¯ä»¥ç”¨è¿™ç´¢å¼•ï¼Œè·Ÿlistç”¨æ³•ç±»ä¼¼ï¼Œä½†æ˜¯listä¸­åªèƒ½æ˜¯æ•°å­—ï¼Œä½†pandasä¸­å¯ä»¥è‡ªå®šä¹‰indexçš„ç´¢å¼•4# è‡ªå®šä¹‰ç´¢å¼•ï¼Œæ™®é€šçš„listå’ŒNumpyçš„æ•°ç»„å°±ä¸è¡Œ&gt;&gt;&gt; obj2 = pd.Series([1,3,4], index=['a', 'b', 'c'])&gt;&gt;&gt; obj2['a']1# ç´¢å¼•å¯ä»¥ç›´æ¥ç”¨æ¥æ•°ç»„è¿ç®—ï¼Œè¿™äº›åœ¨æ•°æ®æ¸…æ´—çš„æ—¶å€™æ¯”è¾ƒå¸¸ç”¨&gt;&gt;&gt; index = obj2 &gt; 2&gt;&gt;&gt; indexa Falseb Truec Truedtype: bool &gt;&gt;&gt; obj2[index]b 3c 4dtype: int64# å…¶ä»–æ“ä½œ&gt;&gt;&gt; np.log(obj2)a 0.000000b 1.098612c 1.386294dtype: float64 # å½“åšå­—å…¸æ¥index&gt;&gt;&gt; 2 in obj2False &gt;&gt;&gt; 3 in obj2False&gt;&gt;&gt; 'a' in obj2 # åªèƒ½æ˜¯index (key)ï¼Œä¸èƒ½æ˜¯valuesTrue# ç”¨å­—å…¸æ¥åˆå§‹åŒ– Series&gt;&gt;&gt; obj3 = &#123;'one': 1, 'two': 2, 'three' : 3&#125;&gt;&gt;&gt; obj3&#123;'one': 1, 'three': 3, 'two': 2&#125;&gt;&gt;&gt; index = ['One', 'Two', 'Three']&gt;&gt;&gt; value = [1, 2, 3]&gt;&gt;&gt; obj4 = pd.Series(value, index)&gt;&gt;&gt; objOne 1Two 2Three 3dtype: int64 # æ‰‹åŠ¨ä¿®æ”¹ç´¢å¼•ï¼Œä¸”ç´¢å¼•çš„å€¼æ˜¯ä¸èƒ½é‡å¤çš„&gt;&gt;&gt; obj4.index = ['one', 'two', 'Three']&gt;&gt;&gt; obj4one 1two 2Three 3dtype: int64 # é‡å»ºç´¢å¼•ï¼Œå¦‚æœç´¢å¼•ä¸å­˜åœ¨çš„å€¼ï¼Œåˆ™å¼•å…¥NaN&gt;&gt;&gt; obj4.reindex(['one', 'two', 'Threeee']) # fill_value=0one 1.0two 2.0Threeee NaNdtype: float64# å¯é€‰ ffill/pad å‘å‰å¡«å……æˆ–è€…bfill/backfill å‘åå¡«å……&gt;&gt;&gt; obj4.ffill()one 1.0two 2.0Threeee 2.0dtype: float64# å½“ç„¶ä¹Ÿå¯ä»¥drop columnsçš„å†…å®¹&gt;&gt;&gt; obj4.drop('Threeee')one 1.0two 2.0dtype: float64 # ä½†æ˜¯ä¸èƒ½è¿™ä¹ˆä¿®æ”¹æ•°å€¼&gt;&gt;&gt; obj4.values = [4, 5, 6]---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2702 else:-&gt; 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):AttributeError: can't set attributeDuring handling of the above exception, another exception occurred:AttributeError Traceback (most recent call last)&lt;ipython-input-41-b4392e8960b0&gt; in &lt;module&gt;()----&gt; 1 obj4.values = [4, 5, 6]/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):-&gt; 2705 object.__setattr__(self, name, value) 2706 2707 # ----------------------------------------------------------------------AttributeError: can't set attribute â€‹ DataFrame DataFrameæ˜¯ä¸€ä¸ªè¡¨æ ¼å‹çš„æ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†ä¸€ç»„æœ‰åºçš„åˆ—ï¼Œæ¯åˆ—å¯ä»¥æ˜¯ä¸åŒçš„å€¼ç±»å‹ã€‚æ‰€æœ‰å¯ä»¥çœ‹åšè¿™æ˜¯ä¸€ä¸ªäºŒç»´çš„æ•°ç»„ï¼Œæœ‰è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼• åˆ›å»ºDataFrameå’ŒåŸºç¡€æ“ä½œ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; dates = pd.date_range('20170305', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6, 4), index=dates, &gt;&gt;&gt; columns=list('ABCD'))&gt;&gt;&gt; df A B C D2017-03-05 -1.616361 0.027641 0.328074 1.0386272017-03-06 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 -0.350594 0.231137 -0.004755 0.6014602017-03-08 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 -0.227631 -0.186816 -0.370522 0.343896# å½“ç„¶ä¹Ÿå¯ä»¥æ‰‹åŠ¨ä¼ è¿›æ¥åˆ›å»ºdf&gt;&gt;&gt; df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)&gt;&gt;&gt; df2Ones A B C D2017-03-05 1 -1.616361 0.027641 0.328074 1.0386272017-03-06 1 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 1 -0.350594 0.231137 -0.004755 0.6014602017-03-08 1 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 1 -0.227631 -0.186816 -0.370522 0.343896# çœ‹DataFrameçš„attributes&gt;&gt;&gt; df.&lt;TAB&gt;df.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D æŸ¥çœ‹DataFrameé‡Œé¢çš„æ•°æ® 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt;&gt; df.head() # é»˜è®¤5è¡Œ A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401&gt;&gt;&gt; df.tail(3) # æ‰‹åŠ¨è®¾ç½®æ‰“å°çš„è¡Œæ•° A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988&gt;&gt;&gt; df.index # index æ˜¯ row çš„ ç´¢å¼•DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df.columnsIndex([u'A', u'B', u'C', u'D'], dtype='object')&gt;&gt;&gt; df.valuesarray([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804&gt;&gt;&gt; df.sort_index(axis=1, ascending=False) # æ˜¯å¦æŒ‰ç…§ columnsçš„å€¼ä¸‹é™æ¥æ’åº D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690&gt;&gt;&gt; df.sort_values(by='B') # æŒ‰ç…§columns B å‡åºæ¥ A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 ç´¢å¼•columnså’Œrows 4. References pythonï¼Œnumpyï¼Œpandasæ•°æ®å¤„ç†ä¹‹å°æŠ€å·§","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.cc/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.cc/tags/Pandas/"}]}]