[{"title":"聚类算法：K-Means及扩展算法K-Modes、K-Prototype初探","date":"2018-11-27T05:23:30.000Z","path":"2018/11/27/聚类算法：K-Means及扩展算法K-Modes、K-Prototype初探/","text":"由于最近正在参与的自动化建模平台需要用到这一算法，但sklearn里面的聚类算法只支持数值型的，无法用到类别型的特征上，所以就研究了K-Modes和K-Prototypes这两个算法。具体算法的思想和源码如下。 1. k-Means Algorightm K-Means K-Means++ K-Means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321class KMeans(BaseEstimator, ClusterMixin, TransformerMixin): \"\"\"K-Means clustering Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : &#123;'k-means++', 'random' or an ndarray&#125; Method for initialization, defaults to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. n_init : int, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, default: 300 Maximum number of iterations of the k-means algorithm for a single run. tol : float, default: 1e-4 Relative tolerance with regards to inertia to declare convergence precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances verbose : int, default 0 Verbosity mode. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. Attributes ---------- cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : Labels of each point inertia_ : float Sum of squared distances of samples to their closest cluster center. Examples -------- &gt;&gt;&gt; from sklearn.cluster import KMeans &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], ... [4, 2], [4, 4], [4, 0]]) &gt;&gt;&gt; kmeans = KMeans(n_clusters=2, random_state=0).fit(X) &gt;&gt;&gt; kmeans.labels_ array([0, 0, 0, 1, 1, 1], dtype=int32) &gt;&gt;&gt; kmeans.predict([[0, 0], [4, 4]]) array([0, 1], dtype=int32) &gt;&gt;&gt; kmeans.cluster_centers_ array([[1., 2.], [4., 2.]]) See also -------- MiniBatchKMeans Alternative online implementation that does incremental updates of the centers positions using mini-batches. For large scale learning (say n_samples &gt; 10k) MiniBatchKMeans is probably much faster than the default batch implementation. Notes ------ The k-means problem is solved using Lloyd's algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, 'How slow is the k-means method?' SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That's why it can be useful to restart it several times. \"\"\" def __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=1e-4, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto'): self.n_clusters = n_clusters self.init = init self.max_iter = max_iter self.tol = tol self.precompute_distances = precompute_distances self.n_init = n_init self.verbose = verbose self.random_state = random_state self.copy_x = copy_x self.n_jobs = n_jobs self.algorithm = algorithm def _check_test_data(self, X): X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES) n_samples, n_features = X.shape expected_n_features = self.cluster_centers_.shape[1] if not n_features == expected_n_features: raise ValueError(\"Incorrect number of features. \" \"Got %d features, expected %d\" % ( n_features, expected_n_features)) return X def fit(self, X, y=None, sample_weight=None): \"\"\"Compute k-means clustering. Parameters ---------- X : array-like or sparse matrix, shape=(n_samples, n_features) Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) \"\"\" random_state = check_random_state(self.random_state) self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \\ k_means( X, n_clusters=self.n_clusters, sample_weight=sample_weight, init=self.init, n_init=self.n_init, max_iter=self.max_iter, verbose=self.verbose, precompute_distances=self.precompute_distances, tol=self.tol, random_state=random_state, copy_x=self.copy_x, n_jobs=self.n_jobs, algorithm=self.algorithm, return_n_iter=True) return self def fit_predict(self, X, y=None, sample_weight=None): \"\"\"Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" return self.fit(X, sample_weight=sample_weight).labels_ def fit_transform(self, X, y=None, sample_weight=None): \"\"\"Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" # Currently, this just skips a copy of the data if it is not in # np.array or CSR format already. # XXX This skips _check_test_data, which may change the dtype; # we should refactor the input validation. return self.fit(X, sample_weight=sample_weight)._transform(X) def transform(self, X): \"\"\"Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by `transform` will typically be dense. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) return self._transform(X) def _transform(self, X): \"\"\"guts of transform method; no input validation\"\"\" return euclidean_distances(X, self.cluster_centers_) def predict(self, X, sample_weight=None): \"\"\"Predict the closest cluster each sample in X belongs to. In the vector quantization literature, `cluster_centers_` is called the code book and each value returned by `predict` is the index of the closest code in the code book. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to predict. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return _labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[0] def score(self, X, y=None, sample_weight=None): \"\"\"Opposite of the value of X on the K-means objective. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- score : float Opposite of the value of X on the K-means objective. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return -_labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[1] k_means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239def k_means(X, n_clusters, sample_weight=None, init='k-means++', precompute_distances='auto', n_init=10, max_iter=300, verbose=False, tol=1e-4, random_state=None, copy_x=True, n_jobs=1, algorithm=\"auto\", return_n_iter=False): \"\"\"K-means clustering algorithm. Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- X : array-like or sparse matrix, shape (n_samples, n_features) The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. verbose : boolean, optional Verbosity mode. tol : float, optional The relative increment in the results before declaring convergence. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. return_n_iter : bool, optional Whether or not to return the number of iterations. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). best_n_iter : int Number of iterations corresponding to the best results. Returned only if `return_n_iter` is set to True. \"\"\" if n_init &lt;= 0: raise ValueError(\"Invalid number of initializations.\" \" n_init=%d must be bigger than zero.\" % n_init) random_state = check_random_state(random_state) if max_iter &lt;= 0: raise ValueError('Number of iterations should be a positive number,' ' got %d instead' % max_iter) # avoid forcing order when copy_x=False order = \"C\" if copy_x else None X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32], order=order, copy=copy_x) # verify that the number of samples given is larger than k if _num_samples(X) &lt; n_clusters: raise ValueError(\"n_samples=%d should be &gt;= n_clusters=%d\" % ( _num_samples(X), n_clusters)) tol = _tolerance(X, tol) # If the distances are precomputed every job will create a matrix of shape # (n_clusters, n_samples). To stop KMeans from eating up memory we only # activate this if the created matrix is guaranteed to be under 100MB. 12 # million entries consume a little under 100MB if they are of type double. if precompute_distances == 'auto': n_samples = X.shape[0] precompute_distances = (n_clusters * n_samples) &lt; 12e6 elif isinstance(precompute_distances, bool): pass else: raise ValueError(\"precompute_distances should be 'auto' or True/False\" \", but a value of %r was passed\" % precompute_distances) # Validate init array if hasattr(init, '__array__'): init = check_array(init, dtype=X.dtype.type, copy=True) _validate_center_shape(X, n_clusters, init) if n_init != 1: warnings.warn( 'Explicit initial center position passed: ' 'performing only one init in k-means instead of n_init=%d' % n_init, RuntimeWarning, stacklevel=2) n_init = 1 # subtract of mean of x for more accurate distance computations if not sp.issparse(X): X_mean = X.mean(axis=0) # The copy was already done above X -= X_mean if hasattr(init, '__array__'): init -= X_mean # precompute squared norms of data points x_squared_norms = row_norms(X, squared=True) best_labels, best_inertia, best_centers = None, None, None if n_clusters == 1: # elkan doesn't make sense for a single cluster, full will produce # the right result. algorithm = \"full\" if algorithm == \"auto\": algorithm = \"full\" if sp.issparse(X) else 'elkan' if algorithm == \"full\": kmeans_single = _kmeans_single_lloyd elif algorithm == \"elkan\": kmeans_single = _kmeans_single_elkan else: raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\" \" %s\" % str(algorithm)) if n_jobs == 1: # For a single thread, less memory is needed if we just store one set # of the best results (as opposed to one set per run per thread). for it in range(n_init): # run a k-means once # 可选：kmeans_single_lloyd or kmeans_single_elkan labels, inertia, centers, n_iter_ = kmeans_single( X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, precompute_distances=precompute_distances, tol=tol, x_squared_norms=x_squared_norms, random_state=random_state) # determine if these results are the best so far if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia best_n_iter = n_iter_ else: # parallelisation of k-means runs seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(kmeans_single)(X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, tol=tol, precompute_distances=precompute_distances, x_squared_norms=x_squared_norms, # Change seed to ensure variety random_state=seed) for seed in seeds) # Get results with the lowest inertia labels, inertia, centers, n_iters = zip(*results) best = np.argmin(inertia) best_labels = labels[best] best_inertia = inertia[best] best_centers = centers[best] best_n_iter = n_iters[best] if not sp.issparse(X): if not copy_x: X += X_mean best_centers += X_mean distinct_clusters = len(set(best_labels)) if distinct_clusters &lt; n_clusters: warnings.warn(\"Number of distinct clusters (&#123;&#125;) found smaller than \" \"n_clusters (&#123;&#125;). Possibly due to duplicate points \" \"in X.\".format(distinct_clusters, n_clusters), ConvergenceWarning, stacklevel=2) if return_n_iter: return best_centers, best_labels, best_inertia, best_n_iter else: return best_centers, best_labels, best_inertia _kmeans_single_lloyd 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127def _kmeans_single_lloyd(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): \"\"\"A single run of k-means, assumes preparation completed prior. Parameters ---------- X : array-like of floats, shape (n_samples, n_features) The observations to cluster. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,) The weights for each observation in X. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (k, p) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. tol : float, optional The relative increment in the results before declaring convergence. verbose : boolean, optional Verbosity mode x_squared_norms : array Precomputed x_squared_norms. precompute_distances : boolean, default: True Precompute distances (faster but takes more memory). random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). n_iter : int Number of iterations run. \"\"\" random_state = check_random_state(random_state) sample_weight = _check_sample_weight(X, sample_weight) best_labels, best_inertia, best_centers = None, None, None # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) if verbose: print(\"Initialization complete\") # Allocate memory to store the distances for each sample to its # closer center for reallocation in case of ties distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype) # iterations for i in range(max_iter): centers_old = centers.copy() # labels assignment is also called the E-step of EM labels, inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, centers, precompute_distances=precompute_distances, distances=distances) # computation of the means is also called the M-step of EM if sp.issparse(X): centers = _k_means._centers_sparse(X, sample_weight, labels, n_clusters, distances) else: centers = _k_means._centers_dense(X, sample_weight, labels, n_clusters, distances) if verbose: print(\"Iteration %2d, inertia %.3f\" % (i, inertia)) if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia center_shift_total = squared_norm(centers_old - centers) if center_shift_total &lt;= tol: if verbose: print(\"Converged at iteration %d: \" \"center shift %e within tolerance %e\" % (i, center_shift_total, tol)) break if center_shift_total &gt; 0: # rerun E-step in case of non-convergence so that predicted labels # match cluster centers best_labels, best_inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, best_centers, precompute_distances=precompute_distances, distances=distances) return best_labels, best_inertia, best_centers, i + 1 _kmeans_single_elkan 123456789101112131415161718192021222324252627def _kmeans_single_elkan(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): if sp.issparse(X): raise TypeError(\"algorithm='elkan' not supported for sparse input X\") random_state = check_random_state(random_state) if x_squared_norms is None: x_squared_norms = row_norms(X, squared=True) # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) centers = np.ascontiguousarray(centers) if verbose: print('Initialization complete') checked_sample_weight = _check_sample_weight(X, sample_weight) centers, labels, n_iter = k_means_elkan(X, checked_sample_weight, n_clusters, centers, tol=tol, max_iter=max_iter, verbose=verbose) if sample_weight is None: inertia = np.sum((X - centers[labels]) ** 2, dtype=np.float64) else: sq_distances = np.sum((X - centers[labels]) ** 2, axis=1, dtype=np.float64) * checked_sample_weight inertia = np.sum(sq_distances, dtype=np.float64) return labels, inertia, centers, n_iter 2. K-Modes Algorithm step1：随机确定k个聚类中心$C_1$, $C_2$ … $C_k$，$C_i$是长度为M的向量，$C_i$ = [$C_{1i}$, $C_{2i}$, … , $C_{mi}$] step2：对于样本$x_j$ (j=1,2,…,N)，分别比较其与k个中心之间的距离 这里的**距离为不同属性值的个数**，假如$x_1$=[1, 2, 1, 3], $C_1$=[1, 2, 3, 4]，那么x1与C1之间的距离为2 step3：将$x_j$划分到距离最小的簇，在全部的样本都被划分完毕之后，重新确定簇中心，向量$C_i$中的每一个分量都更新为簇i中的众数 step4：重复步骤二和三，直到总距离（各个簇中样本与各自簇中心距离之和）不再降低，返回最后的聚类结果 KModes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152class KModes(BaseEstimator, ClusterMixin): \"\"\"k-modes clustering algorithm for categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. cat_dissim : func, default: matching_dissim Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : &#123;'Huang', 'Cao', 'random' or an ndarray&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centroids. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. verbose : int, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, cat_dissim=matching_dissim, init='Cao', n_init=1, verbose=0, random_state=None, n_jobs=1): self.n_clusters = n_clusters # 簇的个数 self.max_iter = max_iter # self.cat_dissim = cat_dissim # 类别间距离的计算方法 self.init = init # 初始中心点的选取方法 &#123;'Huang', 'Cao', 'random' # or an ndarray&#125; self.n_init = n_init # 运行次数取最佳值 self.verbose = verbose # self.random_state = random_state # self.n_jobs = n_jobs # if ((isinstance(self.init, str) and self.init == 'Cao') or hasattr(self.init, '__array__')) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method and algorithm are deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, **kwargs): \"\"\"Compute k-modes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] \"\"\" random_state = check_random_state(self.random_state) self._enc_cluster_centroids, self._enc_map, self.labels_,\\ self.cost_, self.n_iter_ = k_modes(X, self.n_clusters, self.max_iter, self.cat_dissim, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def fit_predict(self, X, y=None, **kwargs): \"\"\"Compute cluster centroids and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). \"\"\" return self.fit(X, **kwargs).predict(X, **kwargs) def predict(self, X, **kwargs): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" if self.verbose and self.cat_dissim == ng_dissim: print(\"Ng's dissimilarity measure was used to train this model, \" \"but now that it is predicting the model will fall back to \" \"using simple matching dissimilarity.\") X = check_array(X, dtype=None) X, _ = encode_features(X, enc_map=self._enc_map) return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return decode_centroids(self._enc_cluster_centroids, self._enc_map) else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_modes 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def k_modes(X, n_clusters, max_iter, dissim, init, n_init, verbose, random_state, n_jobs): \"\"\"k-modes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-modes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values X = check_array(X, dtype=None) # Convert the categorical values in X to integers for speed. # Based on the unique values in X, we can make a mapping to achieve this. X, enc_map = encode_features(X) n_points, n_attrs = X.shape assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = unique results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_modes_single)(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best] k_modes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, random_state): \"\"\" X : n_clusters : n_points : n_points, n_attrs = X.shape n_attrs : max_iter : Maximum number of iterations of the k-modes algorithm for a single run. dissim : func, default: matching_dissim, &#123;matching_dissim, ng_dissim&#125; Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : inti_no : verbose : random_state : \"\"\" random_state = check_random_state(random_state) # _____ INIT _____ # 初始化，选取中心点 if verbose: print(\"Init: initializing centroids\") # 1. method huang if isinstance(init, str) and init.lower() == 'huang': centroids = init_huang(X, n_clusters, dissim, random_state) # 2. method cao elif isinstance(init, str) and init.lower() == 'cao': centroids = init_cao(X, n_clusters, dissim) # 3. 随机选取 elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = X[seeds] # 4. 使用提供好的array elif hasattr(init, '__array__'): # Make sure init is a 2D array. if len(init.shape) == 1: init = np.atleast_2d(init).T assert init.shape[0] == n_clusters, \\ \"Wrong number of initial centroids in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[0], n_clusters) assert init.shape[1] == n_attrs, \\ \"Wrong number of attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[1], n_attrs) centroids = np.asarray(init, dtype=np.uint16) else: raise NotImplementedError if verbose: print(\"Init: initializing clusters\") # rows : n_clusters, cols : n_points membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # cl_attr_freq is a list of lists with dictionaries that contain the # frequencies of values per cluster and attribute. # rows : n_clusters, cols : n_attrs cl_attr_freq = [[defaultdict(int) for _ in range(n_attrs)] for _ in range(n_clusters)] for ipoint, curpoint in enumerate(X): # Initial assignment to clusters # 返回距离最近的中心点 clust = np.argmin(dissim(centroids, curpoint, X=X, membship=membship)) membship[clust, ipoint] = 1 # Count attribute values per cluster. for iattr, curattr in enumerate(curpoint): cl_attr_freq[clust][iattr][curattr] += 1 # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(n_attrs): if sum(membship[ik]) == 0: # Empty centroid, choose randomly centroids[ik, iattr] = random_state.choice(X[:, iattr]) else: centroids[ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_modes_iter(X, centroids, cl_attr_freq, membship, dissim, random_state) # All points seen in this iteration labels, ncost = _labels_cost(X, centroids, dissim, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, cost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, cost)) return centroids, labels, cost, itr 3. K-prototype Algorithm KPrototypes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160class KPrototypes(kmodes.KModes): \"\"\"k-protoypes clustering algorithm for mixed numerical/categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. num_dissim : func, default: euclidian_dissim Dissimilarity function used by the algorithm for numerical variables. Defaults to the Euclidian dissimilarity function. cat_dissim : func, default: matching_dissim Dissimilarity function used by the kmodes algorithm for categorical variables. Defaults to the matching dissimilarity function. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. init : &#123;'Huang', 'Cao', 'random' or a list of ndarrays&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If a list of ndarrays is passed, it should be of length 2, with shapes (n_clusters, n_features) for numerical and categorical data respectively. These are the initial centroids. gamma : float, default: None Weighing factor that determines relative importance of numerical vs. categorical attributes (see discussion in Huang [1997]). By default, automatically calculated from data. verbose : integer, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. gamma : float The (potentially calculated) weighing factor. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, num_dissim=euclidean_dissim, cat_dissim=matching_dissim, init='Huang', n_init=10, gamma=None, verbose=0, random_state=None, n_jobs=1): super(KPrototypes, self).__init__(n_clusters, max_iter, cat_dissim, init, verbose=verbose, random_state=random_state, n_jobs=n_jobs) self.num_dissim = num_dissim self.gamma = gamma self.n_init = n_init if isinstance(self.init, list) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method is deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, categorical=None): \"\"\"Compute k-prototypes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] categorical : Index of columns that contain categorical data \"\"\" random_state = check_random_state(self.random_state) # If self.gamma is None, gamma will be automatically determined from # the data. The function below returns its value. self._enc_cluster_centroids, self._enc_map, self.labels_, self.cost_,\\ self.n_iter_, self.gamma = k_prototypes(X, categorical, self.n_clusters, self.max_iter, self.num_dissim, self.cat_dissim, self.gamma, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def predict(self, X, categorical=None): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. categorical : Index of columns that contain categorical data Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) Xcat, _ = encode_features(Xcat, enc_map=self._enc_map) return _labels_cost(Xnum, Xcat, self._enc_cluster_centroids, self.num_dissim, self.cat_dissim, self.gamma)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return [ self._enc_cluster_centroids[0], decode_centroids(self._enc_cluster_centroids[1], self._enc_map) ] else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_prototypes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677def k_prototypes(X, categorical, n_clusters, max_iter, num_dissim, cat_dissim, gamma, init, n_init, verbose, random_state, n_jobs): \"\"\"k-prototypes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-prototypes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values if categorical is None or not categorical: raise NotImplementedError( \"No categorical data selected, effectively doing k-means. \" \"Present a list of categorical columns, or use scikit-learn's \" \"KMeans instead.\" ) if isinstance(categorical, int): categorical = [categorical] assert len(categorical) != X.shape[1], \\ \"All columns are categorical, use k-modes instead of k-prototypes.\" assert max(categorical) &lt; X.shape[1], \\ \"Categorical index larger than number of columns.\" ncatattrs = len(categorical) nnumattrs = X.shape[1] - ncatattrs n_points = X.shape[0] assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) # Convert the categorical values in Xcat to integers for speed. # Based on the unique values in Xcat, we can make a mapping to achieve this. Xcat, enc_map = encode_features(Xcat) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = list(_split_num_cat(unique, categorical)) init[1], _ = encode_features(init[1], enc_map) # Estimate a good value for gamma, which determines the weighing of # categorical values in clusters (see Huang [1997]). if gamma is None: gamma = 0.5 * Xnum.std() results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_prototypes_single)(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) # Note: return gamma in case it was automatically determined. return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best], gamma k_prototypes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132def k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, random_state): # For numerical part of initialization, we don't have a guarantee # that there is not an empty cluster, so we need to retry until # there is none. random_state = check_random_state(random_state) init_tries = 0 # 初始化几个中心点 # 可能存在失败的情况 while True: init_tries += 1 # _____ INIT _____ if verbose: print(\"Init: initializing centroids\") if isinstance(init, str) and init.lower() == 'huang': centroids = kmodes.init_huang(Xcat, n_clusters, cat_dissim, random_state) elif isinstance(init, str) and init.lower() == 'cao': centroids = kmodes.init_cao(Xcat, n_clusters, cat_dissim) elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = Xcat[seeds] elif isinstance(init, list): # Make sure inits are 2D arrays. init = [np.atleast_2d(cur_init).T if len(cur_init.shape) == 1 else cur_init for cur_init in init] assert init[0].shape[0] == n_clusters, \\ \"Wrong number of initial numerical centroids in init \" \\ \"(&#123;&#125;, should be &#123;&#125;).\".format(init[0].shape[0], n_clusters) assert init[0].shape[1] == nnumattrs, \\ \"Wrong number of numerical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[0].shape[1], nnumattrs) assert init[1].shape[0] == n_clusters, \\ \"Wrong number of initial categorical centroids in init (&#123;&#125;, \" \\ \"should be &#123;&#125;).\".format(init[1].shape[0], n_clusters) assert init[1].shape[1] == ncatattrs, \\ \"Wrong number of categorical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[1].shape[1], ncatattrs) centroids = [np.asarray(init[0], dtype=np.float64), np.asarray(init[1], dtype=np.uint16)] else: raise NotImplementedError(\"Initialization method not supported.\") if not isinstance(init, list): # Numerical is initialized by drawing from normal distribution, # categorical following the k-modes methods. meanx = np.mean(Xnum, axis=0) stdx = np.std(Xnum, axis=0) centroids = [ meanx + random_state.randn(n_clusters, nnumattrs) * stdx, centroids ] if verbose: print(\"Init: initializing clusters\") # 计算对应组内的成员 membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # Keep track of the sum of attribute values per cluster so that we # can do k-means on the numerical attributes. cl_attr_sum = np.zeros((n_clusters, nnumattrs), dtype=np.float64) # Same for the membership sum per cluster cl_memb_sum = np.zeros(n_clusters, dtype=int) # cl_attr_freq is a list of lists with dictionaries that contain # the frequencies of values per cluster and attribute. cl_attr_freq = [[defaultdict(int) for _ in range(ncatattrs)] for _ in range(n_clusters)] for ipoint in range(n_points): # Initial assignment to clusters # 计算初始的归属类，分别由类别的和数值型数据组成 # gamma是类别特征的权重 clust = np.argmin( num_dissim(centroids[0], Xnum[ipoint]) + gamma * cat_dissim(centroids[1], Xcat[ipoint], X=Xcat, membship=membship) ) membship[clust, ipoint] = 1 cl_memb_sum[clust] += 1 # Count attribute values per cluster. for iattr, curattr in enumerate(Xnum[ipoint]): cl_attr_sum[clust, iattr] += curattr for iattr, curattr in enumerate(Xcat[ipoint]): cl_attr_freq[clust][iattr][curattr] += 1 # If no empty clusters, then consider initialization finalized. # 如果每一组的成员个数都大于0的话，则算是初始化成功，否则需要重新初始化 if membship.sum(axis=1).min() &gt; 0: break # TODO: 如果不修改随机种子或者其他的数据，每次跑的结果都是一样的 if init_tries == MAX_INIT_TRIES: # Could not get rid of empty clusters. Randomly # initialize instead. init = 'random' elif init_tries == RAISE_INIT_TRIES: raise ValueError( \"Clustering algorithm could not initialize. \" \"Consider assigning the initial clusters manually.\" ) # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(nnumattrs): centroids[0][ik, iattr] = cl_attr_sum[ik, iattr] / cl_memb_sum[ik] for iattr in range(ncatattrs): centroids[1][ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_prototypes_iter(Xnum, Xcat, centroids, cl_attr_sum, cl_memb_sum, cl_attr_freq, membship, num_dissim, cat_dissim, gamma, random_state) # All points seen in this iteration labels, ncost = _labels_cost(Xnum, Xcat, centroids, num_dissim, cat_dissim, gamma, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run: &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, ncost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, ncost)) return centroids, labels, cost, itr","tags":[{"name":"Cluster","slug":"Cluster","permalink":"http://chenson.com/tags/Cluster/"}]},{"title":"关于Python的Mixin模式初探","date":"2018-11-19T23:25:13.000Z","path":"2018/11/20/关于Python的Mixin模式初探/","text":"1. 经典类 / 新式类 经典类 (Python2.2之前的版本) 经典类是一种没有继承的类，实例类型都是type类型(???)。如果经典类被作为父类，子类在调用父类的构造函数时就会出错。这时候MRO的方法为DFS（深度优先搜索（子节点顺序：从左到右）） 1234import inspect # inspect.getmro（A）可以查看经典类的MRO顺序class A: def __init__(self): print('This is a classic class.') http://python.jobbole.com/85685/ 新式类 为了使类和内置类型更加统一(???)，引入了新式类。新式类的每个类都继承于一个基类，可以是自定义类或者其它类，默认承于object。子类可以调用父类的构造函数。 这时有两种MRO的方法 如果是经典类MRO为DFS（深度优先搜索（子节点顺序：从左到右））。 如果是新式类MRO为BFS（广度优先搜索（子节点顺序：从左到右））。 123class A(object): def __init__(self): print('This is a new-style class.') 1234567891011121314151617181920class D(object): pass class E(object): pass class F(object): pass class C(D, F): pass class B(E, D): pass class A(B, C): pass if __name__ == '__main__': print A.__mro__ 2. 多重继承super https://blog.csdn.net/qwertyupoiuytr/article/details/56439134 3. Mix-inMixin表示的是Mix-in，表示这个类是作为功能添加到子类中，而不是作为父类，它的作用同Java中的接口。 普通类多重继承下，只能有一个普通父类和若干个Mixin类（保持主干单一） Mixin类不能继承普通类（避免钻石继承，重复调用） Mixin 类应该单一职责（参考 Java 的 interface 设计，Mixin 和此极其相似，只不过附带实现而已） 使用Mixin类实现多重继承要非常小心 首先必须是表示一种功能，而不是某个物品 必须责任单一，如果有多个功能，那么就写多个Mixin类 然后不能依赖于子类实现 即使子类没有继承这个Mixin类，也可以照样工作，就是缺失某个功能","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"}]},{"title":"在线学习算法FTRL初探","date":"2018-11-18T07:36:28.000Z","path":"2018/11/18/在线学习算法FTRL初探/","text":"","tags":[]},{"title":"Wide_and_Deep模型初探","date":"2018-11-18T06:01:03.000Z","path":"2018/11/18/Wide-and-Deep模型初探/","text":"1. BackgroundWide and Deep是Google在16年左右提出的模型，主要应用在了Google Play的应用推荐系统中，同时国内的一些大厂也在各自的业务中有使用到这一模型，比如美团。而该算法的核心主要在于结合了线性模型的记忆能力Memorization 和深度神经网络的泛化能力Generaization，且在训练过程中，同时优化这两个模型，即Jointly Training。 2. Overview 如上图所示，该模型的主要结构是中间Wide&amp;Deep Models那个部分，其主要由左边的Wide Models 和右边的Deep Models组成。 3. Wide Part最左端的就是一个普通的广义线性模型，不做过多的解释。 该部分的模型，输入的特征可以是连续特征，也可以是稀疏的离散特征。但该模型可以对离散特征之间做特征交叉，即Cross-Product，为了可以从历史数据中发现特征之间的相关信息，然后对交叉过后的特征做One-Hot处理。 4. Deep Part最右部分是一个DNN模型，一个前馈的深度神经网络。 该部分的模型，输入的特征同样可以是连续特征，也可以是稀疏的离散特征。但和上面部分的离散特征处理方式不同，这里对离散特征做了Embedding，即将高维稀疏的离散特征转化为低纬的稠密特征。 Embedding的初始权重是随机赋予的，然后在训练过程中，不断参与优化。 所以训练的过程大致是需要做Embedding的向量先转化成Dense Vector，然后和其他的连续型或者数值型的特征合并一起，喂给接下去的几层DNN。 其中隐含层的计算方法为$$a^{l + 1} = f(W^{(l)}a^{(l)} + b^{(l)})$$这里 l是隐含层数， f是激活函数，通常使用ReLUs作为激活函数 5. Wide&amp;Deep Part如上图中间部分所示，两个模型最后是合并到一起训练的，并将两个模型的结果的加权和作为最终的预测结果（最终喂给了同一个LR模型去训练），具体如下公式所示。$$P(Y=1 | x) = \\sigma(w^T_{wide}[x, \\phi(x)] + w^T_{deep}a^{(l_f)} + b)$$这里$\\sigma(·)$是sigmoid激活函数。 在论文中作者也特别的提到，这里的Joinly Training 和Ensemble模型是不一样的。对于Ensemble模型，各个子模型是单独训练的，各自的预测结果不影响其他的子模型，只是最后大家的预测结果通过某一种方式，比如投票，加权等，最终得出一个预测结果。 Jointly Training的预测结果，最终会通过Back-Propagating the Gradients，传播到Wide Part和Deep Part。这里两部分模型采用了不同的训练方法： Wide Models采用了FTRL + L1正则 Deep Models采用了AdaGrad 6. Conclusion在最终的测试部分，我自己准备了一份数据用于测试该模型。 Wide Part Deep Part Wide &amp; Deep Part 从上面的结果可以看出，从Wide Models到Deep Models，训练效果提升还是比较大的，但从Deep Models到Wide&amp;Deep Models训练结果非但没有提升，还有了一丝丝的下降，且这个我也已经测试了很多测，主要修改了Embedding中和Wide Models中的一些参数，后者始终没有超过前者的效果。但在网上找的一些测试数据中，Wide&amp;Deep Models还是有一丝丝的提升。原因我分析了一些可能由如下造成 数据集本身可能不太适合这个算法 数据处理方式，因为有部分数据有空值，我基本只是简单的填充了一下 Wide Models训练数据 训练参数 7. References 深度学习在美团点评推荐平台排序中的运用 Wide &amp; Deep Learning for Recommender Systems 简单易学的深度学习算法——Wide &amp; Deep Learning","tags":[{"name":"Wide&Deep","slug":"Wide-Deep","permalink":"http://chenson.com/tags/Wide-Deep/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://chenson.com/tags/Pytorch/"}]},{"title":"图数据库-反欺诈网络设计","date":"2018-08-18T06:45:01.000Z","path":"2018/08/18/图数据库-反欺诈网络设计/","text":"","tags":[]},{"title":"图数据库-Neo4j-常用算法","date":"2018-08-18T06:03:23.000Z","path":"2018/08/18/图数据库-Neo4j-常用算法/","text":"本次主要学习图数据库中常用到的一些算法，以及如何在Neo4j中调用，所以这一篇偏实战，每个算法的原理就简单的提一下。 1. 图数据库中常用的算法 PathFinding &amp; Search 一般用来发现Nodes之间的最短路径，常用算法有如下几种 Google Search Results Dijkstra - 边不能为负值 Folyd - 边可以为负值，有向图、无向图 Bellman-Ford SPFA Centrality 一般用来计算这个图中节点的中心性，用来发现比较重要的那些Nodes。这些中心性可以有很多种，比如 Degree Centrality - 度中心性 Weighted Degree Centrality - 加权度中心性 Betweenness Centrality - 介数中心性 Closeness Centrality - 紧度中心性 Community Detection 基于社区发现算法和图分析Neo4j解读《权力的游戏》 用于发现这个图中局部联系比较紧密的Nodes，类似我们学过的聚类算法。 Strongly Connected Components Weakly Connected Components (Union Find) Label Propagation Lovain Modularity Triangle Count and Average Clustering Coefficient 2. 路径搜索算法 Shortest Path 1234567MATCH (start:Loc&#123;name:&quot;A&quot;&#125;), (end:Loc&#123;name:&quot;F&quot;&#125;)CALL algo.shortestPath.stream(start, end, &quot;cost&quot;)YIELD nodeId, costMATCH (other:Loc) WHERE id(other) = nodeIdRETURN other.name AS name, cost Single Source Shortest Path 123456MATCH (n:Loc &#123;name:&quot;A&quot;&#125;)CALL algo.shortestPath.deltaStepping.stream(n, &quot;cost&quot;, 3.0YIELD nodeId, distanceMATCH (destination) WHERE id(destination) = nodeIdRETURN destination.name AS destination, distance All Pairs Shortest Path 1234567891011CALL algo.allShortestPaths.stream(&quot;cost&quot;,&#123;nodeQuery:&quot;Loc&quot;,defaultValue:1.0&#125;)YIELD sourceNodeId, targetNodeId, distanceWITH sourceNodeId, targetNodeId, distanceWHERE algo.isFinite(distance) = trueMATCH (source:Loc) WHERE id(source) = sourceNodeIdMATCH (target:Loc) WHERE id(target) = targetNodeIdWITH source, target, distance WHERE source &lt;&gt; targetRETURN source.name AS source, target.name AS target, distanceORDER BY distance DESCLIMIT 10 Minimum Weight Spanning Tree 12345MATCH (n:Place &#123;id:&quot;D&quot;&#125;)CALL algo.spanningTree.minimum(&quot;Place&quot;, &quot;LINK&quot;, &quot;cost&quot;, id(n), &#123;write:true, writeProperty:&quot;MINST&quot;&#125;)YIELD loadMillis, computeMillis, writeMillis, effectiveNodeCountRETURN loadMillis, computeMillis, writeMillis, effectiveNodeCount; CASE 123456789101112131415MERGE (a:Loc &#123;name:&quot;A&quot;&#125;)MERGE (b:Loc &#123;name:&quot;B&quot;&#125;)MERGE (c:Loc &#123;name:&quot;C&quot;&#125;)MERGE (d:Loc &#123;name:&quot;D&quot;&#125;)MERGE (e:Loc &#123;name:&quot;E&quot;&#125;)MERGE (f:Loc &#123;name:&quot;F&quot;&#125;)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(b)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(c)MERGE (a)-[:ROAD &#123;cost:100&#125;]-&gt;(d)MERGE (b)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:80&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:30&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:80&#125;]-&gt;(f)MERGE (e)-[:ROAD &#123;cost:40&#125;]-&gt;(f); 3. 中心性算法 PageRank 123456CALL algo.pageRank.stream(&quot;Page&quot;, &quot;LINKS&quot;,&#123;iterations:20&#125;)YIELD nodeId, scoreMATCH (node) WHERE id(node) = nodeIdRETURN node.name AS page,scoreORDER BY score DESC Degree Centrality Betweenness Centrality 12345CALL algo.betweenness.stream(&quot;User&quot;, &quot;MANAGES&quot;, &#123;direction:&quot;out&quot;&#125;)YIELD nodeId, centralityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user,centralityORDER BY centrality DESC; Closeness Centrality 123456CALL algo.closeness.stream(&quot;Node&quot;, &quot;LINK&quot;)YIELD nodeId, centralityMATCH (n:Node) WHERE id(n) = nodeIdRETURN n.id AS node, centralityORDER BY centrality DESCLIMIT 20; CASE 12345678910111213141516171819202122MERGE (home:Page &#123;name:&quot;Home&quot;&#125;)MERGE (about:Page &#123;name:&quot;About&quot;&#125;)MERGE (product:Page &#123;name:&quot;Product&quot;&#125;)MERGE (links:Page &#123;name:&quot;Links&quot;&#125;)MERGE (a:Page &#123;name:&quot;Site A&quot;&#125;)MERGE (b:Page &#123;name:&quot;Site B&quot;&#125;)MERGE (c:Page &#123;name:&quot;Site C&quot;&#125;)MERGE (d:Page &#123;name:&quot;Site D&quot;&#125;)MERGE (home)-[:LINKS]-&gt;(about)MERGE (about)-[:LINKS]-&gt;(home)MERGE (product)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(product)MERGE (links)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(links)MERGE (links)-[:LINKS]-&gt;(a)MERGE (a)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(b)MERGE (b)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(c)MERGE (c)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(d)MERGE (d)-[:LINKS]-&gt;(home) 4. 社区发现算法 Strongly Connected Components 1234CALL algo.scc.stream(&quot;User&quot;,&quot;FOLLOWS&quot;)YIELD nodeId, partitionMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS name, partition Weakly Connected Components (Union Find) 1234CALL algo.unionFind.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId,setIdMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS user, setId Label Propagation 12CALL algo.labelPropagation.stream(&quot;User&quot;, &quot;FOLLOWS&quot;, &#123;direction: &quot;OUTGOING&quot;, iterations: 10&#125;) Lovain Modularity 12345CALL algo.louvain.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId, communityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user, communityORDER BY community; Triangle Count and Average Clustering Coefficient 123456CALL algo.triangle.stream(&quot;Person&quot;,&quot;KNOWS&quot;)YIELD nodeA,nodeB,nodeCMATCH (a:Person) WHERE id(a) = nodeAMATCH (b:Person) WHERE id(b) = nodeBMATCH (c:Person) WHERE id(c) = nodeCRETURN a.id AS nodeA, b.id AS nodeB, c.id AS node 5. References Neo4j in deep 官方文档：Comprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"图数据库","slug":"图数据库","permalink":"http://chenson.com/tags/图数据库/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.com/tags/Neo4j/"}]},{"title":"图数据库-Neo4j-初探","date":"2018-08-17T05:15:46.000Z","path":"2018/08/17/图数据库-Neo4j-初探/","text":"本次初探主要学习如何安装Neo4j，以及Cypher的基本语法。 1. 安装Neo4j Desktop版本 neo4j-desktop Server版本（Community版) 比较建议安装这个版本，因为Desktop版本的老是闪退，且要激活之类的。 下载Neo4j数据库 neo4j-server-community 下载常用算法的插件 graph-algorithms neo4j-graph-algorithms apoc-procedures neo4j-apoc-procedures 将下载下来的算法插件放入到$NEO4J_HOME/plugins文件夹下 Service版修改配置文件$NEO4J_HOME/conf/neo4j.conf 1234567891011# 解决登入的时候报没有授权的错误dbms.security.auth_enabled=false# 添加下载的算法插件dbms.security.procedures.unrestricted=apoc.*,algo.*apoc.import.file.enabled=true#增加页缓存到至少4G，推荐20G:dbms.memory.pagecache.size=4g#JVM堆保存留内存从1G起，最大4G:dbms.memory.heap.initial_size=1gdbms.memory.heap.max_size=4g 启动/停止 (把server所在的路径添加到系统的PATH) 1234567# 建议将neo4j所在的路径条件到系统$PATH当中，# export NEO4J_HOME=&quot;path-to-neo4j&quot;$NEO4J_HOME/bin/neo4j start$NEO4J_HOME/bin/neo4j console$NEO4J_HOME/bin/neo4j stop$NEO4J_HOME/bin/neo4j start -u neo4j -p neo4j$NEO4J_HOME/bin/cypher-shell 1CALL dbms.procedures() // 查看neo4j可用的进程，包括刚刚安装的插件 2. Cypher基本语法 Nodes基本语法 在Cypher里面通过一对小括号代表一个节点 () 代表匹配任意一个节点 (node1) 代表匹配任意一个节点，并给它起了一个别名 (:Lable) 代表查询一个类型的数据 (person:Lable) 代表查询一个类型的数据，并给它起了一个别名 (person:Lable {name:”小王”}) 查询某个类型下，节点属性满足某个值的数据 (person:Lable {name:”小王”,age:23}) 节点的属性可以同时存在多个，是一个AND的关系 Relationship基本语法 系用一对-组成，关系分有方向的进和出，如果是无方向就是进和出都查询 –&gt; 指向一个节点 -[role]-&gt; 给关系加个别名 -[:acted_in]-&gt; 访问某一类关系 -[role:acted_in]-&gt; 访问某一类关系，并加了别名 -[role:acted_in {roles:[“neo”,”Hadoop“]}]-&gt; 创建/删除节点 1234567891011121314151617181920212223// 插入一个Artist类别的节点，而且这个节点有一个属性为Name，值为Lady GagaCREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;&#125;)// 创建并返回CREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;, Gemder:&quot;Femal&quot;&#125;) return a// 一次性创建多个CREATE (a:Album &#123; Name: &quot;Killers&quot;&#125;), (b:Album &#123; Name: &quot;Fear of the Dark&quot;&#125;) RETURN a, bCREATE (a:Album &#123; Name: &quot;Piece of Mind&quot;&#125;) CREATE (b:Album &#123; Name: &quot;Somewhere in Time&quot;&#125;) RETURN a, b// 删除节点，如果这个节点和其他节点有连接的话，不能单单删除这个节点MATCH (a:Album &#123;Name: &quot;Killers&quot;&#125;) DELETE a// 一次性删除多个节点MATCH (a:Artist &#123;Name: &quot;Iron Maiden&quot;&#125;), (b:Album &#123;Name: &quot;Powerslave&quot;&#125;) DELETE a, b // 删除所有节点MATCH (n) DELETE n 创建/删除关系 123456789101112131415161718192021222324252627282930// 对Lady Gaga和专辑PieceOfMind之间创建一个released的关系MATCH (a:Artist), (b:Album)WHERE a.Name = &quot;Lady Gaga&quot; AND b.Name = &quot;Piece of Mind&quot;CREATE (a)-[r:RELEASED]-&gt;(b)RETURN rMATCH (a:Artist), (b:Album), (p:Person)WHERE a.Name = &quot;Strapping Young Lad&quot; AND b.Name = &quot;Heavy as a Really Heavy Thing&quot; AND p.Name = &quot;Devin Townsend&quot; CREATE (p)-[pr:PRODUCED]-&gt;(b), (p)-[pf:PERFORMED_ON]-&gt;(b), (p)-[pl:PLAYS_IN]-&gt;(a)RETURN a, b, p // 删除指定的关系MATCH (:Artist)-[r:RELEASED]-(:Album) DELETE r MATCH (:Artist &#123;Name: &quot;Strapping Young Lad&quot;&#125;)-[r:RELEASED]-(:Album &#123;Name: &quot;Heavy as a Really Heavy Thing&quot;&#125;) DELETE r // 删除所有的关系MATCH ()-[r:RELEASED]-() DELETE r // 清除所有节点和关系 MATCH (n)OPTIONAL MATCH(n)-[r]-()DELETE n,r // 删除整个数据库MATCH (n) DETACH DELETE n 创建/删除约束 同SQL一样，Neo4j数据库支持对Node或relationship的属性的UNIQUE约束 123CREATE CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUEDROP CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUE 创建/删除索引 123456CREATE INDEX ON :Album(Name) // View the schema:schemaDROP INDEX ON :Album(Name) 更新一个节点/边 12MATCH (n:Person &#123; name: &quot;Andres&quot; &#125;)SET n.name = &quot;Taylor&quot;; 筛选过滤 123456789// WHEREMATCH (p1: Person)-[r:friend]-&gt;(p2: Person) WHERE p1.name=~&quot;K.+&quot; or p2.age=24 or &quot;neo&quot; in r.rels RETURN p1, r, p2 // NOT MATCH (p:Person)-[:ACTED_IN]-&gt;(m)WHERE NOT (p)-[:DIRECTED]-&gt;()RETURN p, m 结果集返回 12345MATCH (p:Person)RETURN p, p.name AS name, upper(p.name), coalesce(p.nickname,&quot;n/a&quot;) AS nickname, &#123; name: p.name, label:head(labels(p))&#125; AS person MATCH (n) RETURN DISTINCT n.name; 聚合函数 Cypher支持count, sum, avg, min, max 聚合的时候null会被跳过 count 语法 支持 count( distinct role ) 123456MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations// 收集聚合结果MATCH (m:Movie)&lt;-[:ACTED_IN]-(a:Person)RETURN m.title AS movie, collect(a.name) AS cast, count(*) AS actors 排序和分页 123MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a, count(*) AS appearancesORDER BY appearances DESC SKIP 3 LIMIT 10; Union 联合 12345MATCH (actor:Person)-[r:ACTED_IN]-&gt;(movie:Movie)RETURN actor.name AS name, type(r) AS acted_in, movie.title AS titleUNION （ALL）MATCH (director:Person)-[r:DIRECTED]-&gt;(movie:Movie)RETURN director.name AS name, type(r) AS acted_in, movie.title AS title With语句 with语句给Cypher提供了强大的pipeline能力，可以一个或者query的输出，或者下一个query的输入 和return语句非常类似，唯一不同的是，with的每一个结果，必须使用别名标识。 使用with我们可以在查询结果里面在继续嵌套查询。 1234MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)WITH p, count(*) AS appearances, COLLECT(m.Title) AS moviesWHERE appearances &gt; 1RETURN p.name, appearances, movies 有点类似SQL中的having，这里是with + where两个一起来实现的。 查询最短路径 12MATCH (ms:Person &#123; name: &quot;Node A&quot; &#125;),(cs:Person &#123; name:&quot;Node B&quot; &#125;), p = shortestPath((ms)-[r:Follow]-(cs)) RETURN p; 加载数据 Cypher Neo4j Couldn’t load the external resource neo4j初探 加载存在本地server上的数据，会在路径前面自动加个前缀 /path-to-neo4j/neo4j-community-3.4.5/import，即Server对应所在的路径下的import 12345678910111213141516// 加载addressLOAD CSV WITH HEADERS FROM &quot;file:///data/addresses.csv&quot; AS csvLineCREATE (p:Person &#123;id: toInt(csvLine.id), email: csvLine.address &#125;)// 加载emailLOAD CSV WITH HEADERS FROM &quot;file:///data/emails.csv&quot; AS csvLineCREATE (e:Email &#123;id: toInt(csvLine.id), time: csvLine.time, content: csvLine.content &#125;) // 创建收发关系USING PERIODIC COMMIT 500 // 分段加载LOAD CSV WITH HEADERS FROM &quot;file:///data/relations.csv&quot; AS csvLineMATCH (p1:Person &#123;id: toInt(csvLine.fromId)&#125;),(e:Email &#123; id: toInt(csvLine.emailId)&#125;),(p2:Person&#123; id: toInt(csvLine.toId)&#125;)CREATE UNIQUE (p1)-[:FROM]-&gt;(e)CREATE(e)-[:TO]-&gt;(p2) 如果需要导入其他地方的，可以使用 123456789LOAD CSV FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine[0], Name: csvLine[1]&#125;)// 使用csv中的header LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine.Id, Name: csvLine.Track, Length: csvLine.Length&#125;) // 自定义csv文件中的分隔符LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLine FIELDTERMINATOR &quot;;&quot; 使用 neo4j-import 导入数据 使用neo4j-import导入数据 使用条件 需要先关闭neo4j 无法再原有的数据库添加，只能重新生成一个数据库 导入文件格式为csv 参数 –into：数据库名称 –bad-tolerance：能容忍的错误数据条数（即超过指定条数程序直接挂掉），默认1000 –multiline-fields：是否允许多行插入（即有些换行的数据也可读取） –nodes：插入节点 –relationships：插入关系 更多参数可允许命令bin/neo4j-import 1bin/neo4j-import --multiline-fields=true --bad-tolerance=1000000 --into graph.db --id-type string --nodes:person node.csv --relationships:related relation_header.csv,relation.csv 运行完后，将生成的graph.db放入data/databases，覆盖原有数据库，启动运行即可 3. References Neo4j的简单搭建与使用 Neo4j Tutorial Neo4j的查询语法笔记 官方文档：Comprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"图数据库","slug":"图数据库","permalink":"http://chenson.com/tags/图数据库/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.com/tags/Neo4j/"}]},{"title":"Mac系统下matplotlib显示中文","date":"2018-04-10T07:08:20.000Z","path":"2018/04/10/Mac系统下matplotlib显示中文/","text":"1. Mac系统下让matplotlib显示中文现在的时间是2018.04.10，更新一下Mac系统下的解决方法 我的环境：anaconda3 + Python 3.6.3 添加字体 添加 SimHei 字体（simhei.ttf文件）到 ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf 中； 下载地址：黑体字体simhei.ttf 我用的是 anaconda3 下的 python 环境，这个地址对应你正在使用的 python 安装地址 ​ 修改matplotlib配置文件 12cd ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-datavi matplotlibrc # 编辑配置文件 找到 font.sans-serif 添加 SimHei 到字体列表 （如图，大约在211行） 同时修改 axes.unicode_minus，将 True 改为 False，作用就是解决负号’-‘显示为方块的问题（大约在330行） 删除缓存文件 Mac 系统下删除 ~/.matplotlib/ 下的所有缓存文件 12rm -rf ~/.matplotlib/*.cacherm -rf ~/.matplotlib/fontList.json Linux、CentOS 删除 ~/.cache/matplotlib目录下的两个缓存文件（同上） 注意 rm -rf 命令，确认路径没错在用 ​ 画图测试 未修改配置文件，需要添加如下代码： 1234567891011121314#coding:utf-8 import matplotlib #指定默认字体 matplotlib.rcParams['font.sans-serif'] = ['SimHei'] matplotlib.rcParams['font.family']='sans-serif' #解决负号'-'显示为方块的问题 matplotlib.rcParams['axes.unicode_minus'] = False from matplotlib.font_manager import _rebuild_rebuild()plt.plot([-1,2,-5,3]) plt.title(u'中文',fontproperties=myfont) plt.show() ​ 如果已经修改了 matplotlib 配置文件，则不需要上述代码，直接画图即可。 ​ 2. References 彻底解决matplotlib中文乱码问题 matplotlib图例中文乱码?","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"http://chenson.com/tags/Jupyter/"}]},{"title":"深度学习笔记-CNN","date":"2018-03-20T05:45:37.000Z","path":"2018/03/20/深度学习笔记-CNN/","text":"非常简单的深度学习笔记，总结一下CNN的一些概念，以免老是忘记了。 1. 卷积神经网络 CNN - (Convolutional Neural Network) 基本结构 输入层 —&gt;卷积层 —&gt; 激活函数 —&gt;池化层 —&gt; 全连接层 神经网络 - Neural Network 大家都了解，就不解释了。 卷积 - Convolutional 裂墙推荐知乎上的这篇文章通俗理解『卷积』——从傅里叶变换到滤波器，从信号处理这个本质的角度来理解卷积，非常棒👍 卷积的意思就是，神经网络是对图片上的一小块区域进行处理，这张做法加强了图片信息的连续性，使得神经网络可以看到图片上的图形，而非是一个个离散的点。 激活函数 常用的几个激活函数是 Relu，Sigmoid，Tanh和Softplus 感受野 - Receptive Field 中文名真的是… … 太太太难理解了，直接看英文名Receptive Field比较直观一些。 基本定义就是 卷积神经网络的每一层输出的特征图（Feature ap）上的像素点在原图像上映射的区域大小。 第一层卷积 输入为10x10的图片，经过3x3的kernel，输出为8x8的大小（这里只考虑单层） 且在output1中的每一个像素点，都受到原始图像对应的3x3区域内的影响，第一场的感受野为3，用字母表示RF1=3 第二层卷积 在第一层卷积的输出output1下，经过第二层3x3的kernel卷积，输出的大小为6x6 如果从output2往回推的话，output2上的一个像素点，受到output1上三个像素点的影响，而这三个像素点又总共受到输入层五个像素点的影响，所以第二层的感受野RF2=5 第三层卷积 此时kernel3依旧为3x3，根据上面推，RF3=7 池化 - Pooling 池化大致就是一个筛选过滤的过程，能过将左边的layer中有用的信息筛选出来，给右边的使用分析。因为在卷积的过程中为了保留更多的信息，不压缩长宽比。原始图片很大的话，卷积出来的结果也是很大，且比较稀疏的。所以这时候如果用池化层压缩的话，可以尽可能的保留信息和压缩数据的大小。 2. PyTorch代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class CNN(nn.Module): def __init__(self, in_dim, n_class): super(CNN, self).__init__() # 表示将一个有序的模块写在一起 # 相当于神经网络的层按顺序放一起方便结构显示 self.conv = nn.Sequential( # 卷积层，有五个参数： # in_channels: 表示的是输入卷积层的图片厚度 # out_channels: 比偶奥数的是要输出的厚度 # kernel_size: 表示的是卷积核的大小，一个数字的话表示长宽相等的卷积核 # stride: 表示卷积核滑动的步长 # padding: 表示在图片走位填充0的多少，padding=0表示不填充，padding=1四周都填充1维 nn.Conv2d(in_dim, 6, 3, stride=1, padding=1), # 激活函数，里面有一个参数inplace # False，表示新创建一个对象对其修改 # True，表示直接对这个对象进行修改 nn.ReLU(True), # 最大池化层，也有平均池化层等，里面的参数有: # kernel_size: 表示池化的窗口的大小，和卷积里面的kernel_size是一样的 # stride: 也和卷积层里面一样，需要自己设置滑动步长 # padding 和卷积层一样，默认是0 nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5, stride=1, padding=0), nn.ReLU(True), nn.MaxPool2d(2, 2) ) # self.fc = nn.Sequential( nn.Linear(400, 120), nn.Linear(120, 84), nn.Linear(84, n_class) ) def forward(self, x): out = self.conv(x) # batch_size out = out.view(out.size(0), -1) out = self.fc(out) return out # 定义loss和optimizercriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 # 开始训练 for epoch in range(num_epoches): print('epoch &#123;&#125;'.format(epoch + 1)) print('*' * 10) running_loss = 0.0 running_acc = 0.0 for i, data in enumerate(train_loader, 1): img, label = data if use_gpu: img = img.cuda() label = label.cuda() img = Variable(img) label = Variable(label) # 向前传播 out = model(img) loss = criterion(out, label) running_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() accuracy = (pred == label).float().mean() running_acc += num_correct.data[0] # 向后传播 optimizer.zero_grad() loss.backward() optimizer.step() # ========================= Log ====================== step = epoch * len(train_loader) + i # (1) Log the scalar values info = &#123;'loss' : loss.data[0], 'accuracy' : accuracy.data[0]&#125; for tag, value in info.items(): logger.scalar_summary(tag, value, step) # (2) Log values and gradients of the parameters (histogram) for tag, value in model.named_parameters(): tag = tag.replace('.', '/') logger.histo_summary(tag, to_np(value), step) logger.histo_summary(tag + '/grad', to_np(value.grad), step) # (3) Log the images info = &#123;'images': to_np(img.view(-1, 28, 28)[:10])&#125; for tag, images in info.items(): logger.image_summary(tag, images, step) if i % 300 == 0: print('[&#123;&#125;/&#123;&#125;] Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, num_epoches, running_loss / (batch_size * i), running_acc / (batch_size * i))) print('Finish &#123;&#125; epoch, Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, running_loss / (len(train_dataset)), running_acc / (len( train_dataset)))) model.eval() eval_loss = 0 eval_acc = 0 for data in test_loader: img, label = data if use_gpu: img = Variable(img, volatile=True).cuda() label = Variable(label, volatile=True).cuda() else: img = Variable(img, volatile=True) label = Variable(label, volatile=True) out = model(img) loss = criterion(out, label) eval_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() eval_acc += num_correct.data[0] print('Test Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format(eval_loss / (len( test_dataset)), eval_acc / (len(test_dataset)))) print()# 保存模型torch.save(model.state_dict(), './model/cnn.pth') 3. References 莫烦 - 什么是卷积神经网络 CNN (Convolutional Neural Network) 什么是感受野 通俗理解『卷积』——从傅里叶变换到滤波器","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.com/tags/Deep-Learning/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://chenson.com/tags/PyTorch/"}]},{"title":"Docker常用指令","date":"2018-03-13T05:12:33.000Z","path":"2018/03/13/Docker常用指令/","text":"1. docker镜像构建12cd analyze-serverdocker build -t analyze-server . 2. 创建12345docker create analyze-server # 创建容器，但处于停止状态docker run analyze-server # 创建并启动容器# 启动容器，并挂载文件docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server 3. 查看12docker ps # 查看当前运行的容器docker ps -a # 查看所有的容器，包括停止的 4. 启动1docker start [NAME]/[CONTAINER ID] # 通过docker start来启动之前已经停止的docker_run镜像 1docker exec -it analyze /bin/bash # 在运行的容器中执行后面的指令 交互型容器：运行在前台，容器中使用exit命令或者调用docker stop、docker kill命令，容器停止。 docker -it --name analyze -i -t -i 打开容器的标准输入 -t 告诉docker为容器创建一个命令行终端 --name analyze 指定容器名称，可以不填(随机) analyze-server 告诉我们使用什么镜像来启动容器 /bin/bash 告诉docker要在容器里面执行此命令 后台型容器：运行在后台，创建后与终端无关，只有调用docker stop、docker kill命令才能使容器停止。 1docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server -d 使容器在后台运行。 -c 可以调整容器的CPU优先级。默认情况下，所有的容器拥有相同的CPU优先级和CPU调度周期，但你可以通过Docker来通知内核给予某个或某几个容器更多的CPU计算周期。比如，我们使用-c或者–cpu-shares =0启动了C0、C1、C2三个容器，使用-c/–cpu-shares=512启动了C3容器。这时，C0、C1、C2可以100%的使用CPU资源（1024），但C3只能使用50%的CPU资源（512）。如果这个主机的操作系统是时序调度类型的，每个CPU时间片是100微秒，那么C0、C1、C2将完全使用掉这100微秒，而C3只能使用50微秒。 -c 后的命令是循环，从而保持容器的运行 5. 停止12docker stop [NAME]/[CONTAINER ID] # 退出容器docker kill [NAME]/[CONTAINER ID] # 强制停止一个容器 6. 删除12345docker rm [NAME]/[CONTAINER ID] # 不能够删除一个正在运行的容器，会报错。需要先停止容器docker rm &apos;docker ps -a -q&apos; # 一次性删除所有容器 # -a标志列出所有容器， # -q标志只列出容器的ID，然后传递给rm命令，依次删除容器 7. References Docker 简易教程 Docker 命令大全","tags":[{"name":"docker","slug":"docker","permalink":"http://chenson.com/tags/docker/"}]},{"title":"Hadoop2.7.4完全分布式集群搭建和测试","date":"2017-10-10T05:24:58.000Z","path":"2017/10/10/Hadoop2-7-4完全分布式集群搭建和测试/","text":"1. 环境配置1.1 环境说明 1.2 修改机器名称和hosts等 vi /etc/sysconfig/network 1234HOSTNAME=hadoop-masterHOSTNAME=hadoop-salve1HOSTNAME=hadoop-salve2HOSTNAME=hadoop-salve3 执行 reboot 后生效 sudo vi /etc/hostname 1234# 相应修改三台机器hadoop-masterhadoop-slave1hadoop-slave2 sudo vi /etc/hosts 1234567127.0.0.1 localhost localhost.localdomain VM-0-6-ubuntu# Hadoop Cluster# 【注意】：用内网IP，若用公网IP，则无法启动master上的9000监听端口172.17.6 hadoop-master172.17.11 hadoop-salve1 172.17.7 hadoop-salve2 1.5. SSH无密码验证配置 安装 ssh 123sudo apt-get install openssh-serverps -e | grep &quot;ssh&quot;ssh localhost 生成密钥 pair 12345678910# 查看权限ls -aldrwxr-x--x 2 root root 4096 Dec 23 2015 .ssh# 给用户权限# sudo chown ubuntu .sshchmod 700 .ssh # 生成密钥ssh-keygen -t rsa 在 master 上导入 authorized_keys 123456789# 重要sudo chmod 700 .ssh sudo chmod 640 .ssh/authorized_keyssudo chown $USER .sshsudo chown $USER .ssh/authorized_keyscat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# 同时给其他的 salver 在各个机器验证一下 2. 所需软件2.1 JDK软件tutorial 1 安装 JDK 123456sudo add-apt-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installersudo apt-get install openjdk-8-jdksudo apt-get install openjdk-8-jre 配置环境变量 vi /etc/profile 123456789101112131415# JAVA# 1. AWS EC2 Ubuntu 16export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64# 2. 腾讯云 Ubuntu 14export JAVA_HOME=/usr/lib/jvm/java-8-oracle# 3. MAC OSexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Homeexport JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 执行命令使之生效 source /etc/profile 2.2 Hadoop 软件 下载软件 123wget http://apache.uberglobalmirror.com/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gztar xvf hadoop-2.7.4.tar.gz 配置 vi ~/.bashrc 设置 Hadoop 的环境变量 1234export HADOOP_HOMEexport PATHexport HADOOP_CONF_DIRexport YARN_CONF_DIR 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# ~/.bashrc: executed by bash(1) for non-login shells.# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)# for examples# If not running interactively, don&apos;t do anythingcase $- in *i*) ;; *) return;;esac# don&apos;t put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don&apos;t overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern &quot;**&quot; used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)[ -x /usr/bin/lesspipe ] &amp;&amp; eval &quot;$(SHELL=/bin/sh lesspipe)&quot;# set variable identifying the chroot you work in (used in the prompt below)if [ -z &quot;$&#123;debian_chroot:-&#125;&quot; ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we &quot;want&quot; color)case &quot;$TERM&quot; in xterm-color|*-256color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the prompt#force_color_prompt=yesif [ -n &quot;$force_color_prompt&quot; ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it&apos;s compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ &quot;$color_prompt&quot; = yes ]; then PS1=&apos;$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ &apos;else PS1=&apos;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h:\\w\\$ &apos;fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase &quot;$TERM&quot; inxterm*|rxvt*) PS1=&quot;\\[\\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h: \\w\\a\\]$PS1&quot; ;;*) ;;esac# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval &quot;$(dircolors -b ~/.dircolors)&quot; || eval &quot;$(dircolors -b)&quot; alias ls=&apos;ls --color=auto&apos; #alias dir=&apos;dir --color=auto&apos; #alias vdir=&apos;vdir --color=auto&apos; alias grep=&apos;grep --color=auto&apos; alias fgrep=&apos;fgrep --color=auto&apos; alias egrep=&apos;egrep --color=auto&apos;fi# colored GCC warnings and errors#export GCC_COLORS=&apos;error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01&apos;# some more ls aliasesalias ll=&apos;ls -alF&apos;alias la=&apos;ls -A&apos;alias l=&apos;ls -CF&apos;# Add an &quot;alert&quot; alias for long running commands. Use like so:# sleep 10; alertalias alert=&apos;notify-send --urgency=low -i &quot;$([ $? = 0 ] &amp;&amp; echo terminal || echo error)&quot; &quot;$(history|tail -n1|sed -e &apos;\\&apos;&apos;s/^\\s*[0-9]\\+\\s*//;s/[;&amp;|]\\s*alert$//&apos;\\&apos;&apos;)&quot;&apos;# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don&apos;t need to enable# this, if it&apos;s already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi# HADOOPexport HADOOP_HOME=/home/ubuntu/workdir/hadoop-2.7.4export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=$CLASSPATH:$HADOOP_HOME/bin# JAVAexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib# LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/home/ubuntu/workdir/hadoop-2.7.4/lib/native 1export CLASSPATH=$CLASSPATH:/home/ubuntu/workdir/hadoop-2.7.4/etc/hadoop:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/contrib/capacity-scheduler/*.jar 在 $HADOOP_CONF_DIR 中设置 JAVA_HOME 123vi /home/hadoop-2.7.4/etc/hadoop/hadoop-env.sh 设置JAVA_HOMEvi /home/hadoop-2.7.4/etc/hadoop/mapred-env.sh 设置JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 vi slaves 这里面填写的全是slaves 12hadoop-slave1hadoop-slave2 vi $HADOOP_CONF_DIR/core-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!-- 注意这里的区别 一个适用于单机 一个适用于集群 --&gt; &lt;!-- 单机 --&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;!-- 集群 master --&gt; &lt;value&gt;hdfs://hadoop-master:9000&lt;/value&gt; &lt;description&gt;设定namenode的主机名及端口(建议不要更改端口号)&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;description&gt; 设置缓存大小 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop-2.7.4/tmp&lt;/value&gt; &lt;description&gt; 存放临时文件的目录 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/hdfs-site.xml 1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/name&lt;/value&gt; &lt;description&gt; namenode 用来持续存放命名空间和交换日志的本地文件系统路径 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/data&lt;/value&gt; &lt;description&gt; DataNode 在本地存放块文件的目录列表，用逗号分隔 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt; 设定 HDFS 存储文件的副本个数，默认为3 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; vi $HADOOP_CONF_DIR/mapred-site.xml (复制mapred-site.xml.template,再修改文件名) 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;hadoop-master:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop-master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://hadoop-master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;!-- master node 的名字 --&gt; &lt;!-- 单机和集群的区别？？ --&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop-master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop-master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;hadoop-master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop-master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345dfs.nameservices —– HDFS NN的逻辑名称，使用上面设置的myhdfsdfs.ha.namenodes.myhdfs —– 给定服务逻辑名称myhdfs的节点列表dfs.namenode.rpc-address.myhdfs.nn1 —– myhdfs中nn1节点对外服务的RPC地址dfs.namenode.http-address.myhdfs.nn1 —– myhdfs中nn1节点对外服务的http地址dfs.namenode.shared.edits.dir —– 设置一组 journalNode 的 URI 地址，active NN 将 edit log 写入这些 12345在每个节点上创建数据存储目录/home/hadoop-2.7.4/hdfs 用来存放集群数据。在主节点node上创建目录/home/hadoop-2.7.4/hdfs/name 用来存放文件系统元数据。在每个从节点上创建目录/home/hadoop-2.7.4/hdfs/data 用来存放真正的数据。所有节点上的日志目录为/home/hadoop-2.7.4/logs所有节点上的临时目录为/home/hadoop-2.7.4/tmp 1上面的配置只需要在master中配好, 然后复制到其他的slaves节点中中去 格式化 namenode 和 datanode（在 master 上执行就可以了 不需要在 slaves 上执行） 12hdfs namenode -formathdfs datanode -format 分别在 master 和 slaves 中用 jps 查看进程","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"}]},{"title":"同步异步IO","date":"2017-09-29T03:54:40.000Z","path":"2017/09/29/同步异步IO/","text":"1. 大致区别所谓的同步和异步的区别，指的是Application和kernel之间的交互方式。 如果Application不需要等待kernel的回应，那么它就是异步的（kernel会发信号通知）。 如果Application提交完IO请求后，需要等待“回执”，那么它就是同步的。 而阻塞和非阻塞，指的是Application是否等待IO操作的完成。 如果Application必须等到IO操作实际完成以后再执行接下面的操作，那么它就是阻塞的。（从发出信号开始，会一直block对应的进程知道操作完成，） 反之，如果不需要等待IO操作的完成就开始执行其他的操作，那么就是非阻塞的。（在kernel还准备数据的情况下，就立刻返回了。） 叫车之后，就一直在路口等着，车来了自己上去 ——同步，阻塞 叫车之后，一边等着一边看美女，车来了自己上去 ——同步，非阻塞 叫车之后，光顾着看美女，司机到了之后打电话给你 ——异步，非阻塞 即是：同步就是你要自己检查车来了没有；异步就是车来了司机联系你。阻塞就是等车的时候老实等着，别干别的（被阻塞）；非阻塞就是等车的时候你可以做其他事情。 2. Blocking IO 3. Nonblocking IO 4. IO multiplexing / Event driven IO使用select 和epoll，单个process可以同时处理多个网络连接的IO。基本原理就是select/epol这个function会不断的轮询所负责的所有的socekt，当某个socket有数据到达了，就通知用户进程。它的流程如下图： 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 5. Asynchronous IO 用户进程发起read操作之后，就可以立刻做其他的事情。而另一方面，从kernel的角度，当它收到一个asynchronous read之后，首先它就会立刻返回一个信号，所以不会对用户进程产生任何的block。 然后，kernel会等待数据准备晚餐，将数据拷贝到用户内存，当这一切都完成后，kernel会给用户进程发送一个signal，告诉它read操作已经完成了。","tags":[]},{"title":"多线程与多进程简单理解","date":"2017-09-22T06:28:08.000Z","path":"2017/09/22/多线程与多进程简单理解/","text":"1. 多进程直观来看，就是一个个pid，进程是程序在计算机上的一次执行活动。 创建子进程的调用是 fork() fork() 的功能就是产生子进程，调用一次，返回两次。 一次返回0，一次返回子进程的pid 子进程永远返回 0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用 getppid() 就可以拿到父进程的ID 失败返回-1 12345678910111213141516171819202122232425#include &lt;unistd.h&gt; #include &lt;sys/types.h&gt; #include &lt;stdio.h&gt; void print_exit() &#123; printf(\"the exit pid:%d/n\",getpid() ); &#125; main()&#123; pit_t, pid; atexit(print_exit); //注册该进程退出时的回调函数 pid = fork(); if (pid &lt; 0) printf(\"error in fork!\"); else if (pid == 0) // 子进程 printf(\"i am the child process, my process id is %d/n\", getpid()); else // 父进程 &#123; printf(\"i am the parent process, my process id is %d/n\", getpid()); sleep(2); wait(); &#125; &#125; 2. 多线程进程是由若干线程组成的，一个进程至少有一个线程。 线程就是把一个进程分成很多片，每一片都是可以独立的流程 linux提供的多线程的系统调用： 123456int pthread_create(pthread_t *restrict tidp, const pthread_attr_t *restrict attr, void *(*start_rtn)(void), void *restrict arg);Returns: 0 if OK, error number on failure 第一个参数为指向线程标识符的指针。第二个参数用来设置线程属性。第三个参数是线程运行函数的起始地址。最后一个参数是运行函数的参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include&lt;stdio.h&gt;#include&lt;string.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;pthread.h&gt; void* task1(void*);void* task2(void*);void usr();int p1,p2;int main()&#123; usr(); getchar(); return 1;&#125; void usr()&#123; pthread_t pid1, pid2; pthread_attr_t attr; void *p; int ret=0; pthread_attr_init(&amp;attr); // 初始化线程属性结构 pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_DETACHED); // 设置attr结构为分离 pthread_create(&amp;pid1, &amp;attr, task1, NULL); // 创建线程，返回线程号给pid1,线程属性设置为attr的属性，线程函数入口为task1，参数为NULL pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE); pthread_create(&amp;pid2, &amp;attr, task2, NULL); // 前台工作 ret=pthread_join(pid2, &amp;p); // 等待pid2返回，返回值赋给p printf(\"after pthread2:ret=%d,p=%d/n\", ret,(int)p); &#125;void* task1(void *arg1)&#123; printf(\"task1/n\"); // 艰苦而无法预料的工作，设置为分离线程，任其自生自灭 pthread_exit( (void *)1);&#125;void* task2(void *arg2)&#123; int i=0; printf(\"thread2 begin./n\"); // 继续送外卖的工作 pthread_exit((void *)2);&#125; 3. 线程 VS 进程 进程开销大，线程开销小。 线程安全：概念比较直观。一般说来，一个函数被称为线程安全的，当且仅当被多个并发线程反复调用时，它会一直产生正确的结果。 线程安全的条件：主要需要考虑的是线程之间的共享变量。属于同一进程的不同线程会共享进程内存空间中的全局区和堆，而私有的线程空间则主要包括栈和寄存器。因此，对于同一进程的不同线程来说，每个线程的局部变量都是私有的，而全局变量、局部静态变量、分配于堆的变量都是共享的。在对这些共享变量进行访问时，如果要保证线程安全，则必须通过加锁的方式 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。 多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题 多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程 4. 进程间通信4.1 单机多进程通信IPC（Inter Process Communication）包括：管道、文件、和消息传递 管道（Pipe）及有名管道（named pipe）：管道可用于具有亲缘关系进程间的通信，有名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信； 信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身；linux除了支持Unix早期信号语义函数sigal外，还支持语义符合Posix.1标准的信号函数sigaction（实际上，该函数是基于BSD的，BSD为了实现可靠信号机制，又能够统一对外接口，用sigaction函数重新实现了signal函数）； 报文（Message）队列（消息队列）：消息队列是消息的链接表，包括Posix消息队列system V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。 信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。 套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。起初是由Unix系统的BSD分支开发出来的，但现在一般可以移植到其它类Unix系统上：Linux和System V的变种都支持套接字。 4.2 分布式进程通信5. 线程通信多数的多线程都是在同一个进程下的，它们共享该进程的全局变量，我们可以通过全局变量来实现线程间通信。如果是不同的进程下的2个线程间通信，直接参考进程间通信。 6. References http://blog.csdn.net/hairetz/article/details/4281931","tags":[]},{"title":"Machine Learning - PCA","date":"2017-06-17T12:59:32.000Z","path":"2017/06/17/Machine-Learning-PCA/","text":"1. PCAPCA(Principal Components Analysis)是主成成分分析，之前也叫做Pricipal Factor Analysis。顾名思义就是分析数据里面的主要部分，是最常用的一种降维方法。那么为什么需要降维呢？ 因为在真实的训练数据中，总是会存在各种冗余的数据 比如说拿到一个汽车的样本，里面既有以“千米/每小时”度量的最大速度特征，也有“英里/小时”的最大速度特征，显然这两个特征有一个多余 拿到一个数学系的本科生期末考试成绩单，里面有三列，一列是对数学的兴趣程度，一列是复习时间，还有一列是考试成绩。我们知道要学好数学，需要有浓厚的兴趣，所以第二项与第一项强相关，第三项和第二项也是强相关。那是不是可以合并第一项和第二项呢？ 这个与第二个有点类似，假设在IR中我们建立的文档-词项矩阵中，有两个词项为“learn”和“study”，在传统的向量空间模型中，认为两者独立。然而从语义的角度来讲，两者是相似的，而且两者出现频率也类似，是不是可以合成为一个特征呢？ 因为这些冗余的数据，常常会导致我们的模型过度拟合。这个时候，就需要一种特征降维的方法来减少特征数，减少噪音和冗余，减少过度拟合的可能性。而PCA就是降维的算法之一，将原先的数据从n维映射到k维上(k&lt;n)。这里k维是全新的正交特征，这k维特征成为主元，是重新构造出出来的k维特征，而不是简单的从n维当中移去了(n-k)维，然后剩下主要的k维特征。 那么请思考一个问题：对于正交属性空间中的样本点，如何用一个超平面(直线的高维推广)，对所有的样本进行恰当的表达？那么需要找到怎么样的一个超平面来分割呢？一般需要具备如下特征： 最近重构性：样本点到这个超平面的距离都足够近 最大可分性：样本点在这个超平面上的投影尽可能的要分开，而不是重叠一起 2. 计算过程那么如何来构建呢？举个栗子： 现在有这么一组数据，x和y是两个特征 第一步：求出所有维度的平均值上面的例子中，x的平均值是1.81，y的平均值是1.91 第二步：中心化，即$\\sum x_i=0$.将所有的样本都减去这俩个平均值。比如第一个样本(2.5, 2.4) - (1.81, 1.91) = (0.69, 0.49) 第三步：求特征的协方差矩阵如果有x, y, z三个特征，分别需要求cov(x, x), cov(x, y), cov(x, z), cov(y, y), cov(y, z), cov(z, z)这几个。 当协方差大于0的时候，表示x和y若有一个增加，另一个也会增加。 当协方差下于0的时候，表示一个增加，另一个会减少 当协方差等于0的时候，表示来者之间是独立的。 协方差的绝对值越大，两者对彼此的影响也就越大 具体如下图 而我们上面的例子中只有x和y两个变量，即 第四步：求协方差的特征值和特征向量，得到 上面是两个特征值，下面是对应的特征向量。 比如特征值0.0490833989对应的特征向量为(-0.735178656, 0.677873399)T，这里的特征向量都归一化为单位单位向量 求协方差的步骤： 第五步：将特征值按照从大到小的顺序排列，选择其中个最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。 上面的例子中，特征值只有两个，我们需要选择其中最大的那个。所以这里我们选择1.28402771，对应的特征向量是 (-0.677873399, -0.735178656)T. 第六步：将样本投影到选取的特征向量上去。 假设样本数量为m，特征数量为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*m，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为$$FinalData(m\\ast k) = DataAdjust(m\\ast n) \\times EigenVectors(n \\ast k)$$所以上面的例子中$$FinalData(10\\ast 1) = DataAdjust(10\\ast 2) \\times EigenVectors(-0.677873399, -0.735178656)^T$$得到的结果是 这样就将原始的样本数据从n维特征变成了k维特征，这k维就是原始特征在k维上的投影。 下面的图描述了上面的过程： 原先所有数据是分布在x-y这个坐标系上，+表示的是样本数据。差不多是对角线上的两条线分别代表了两个正交的特征向量。由于协方差矩阵是对称的，因此其特征向量正交。然后我们将原有的样本数据投影到这个新的坐标系中去，得到转换后的数据： 从上面的图我们可以看出，在新的坐标系中，x轴基本就可以表示原有的样本数据特征。而整个过程，看起来有点像是将坐标系做了旋转。如果当k=1的时候，上面数据就只会变成一维的数据。而当原有的数据维度很高的时候，有时候为了可视化操作，我们就会对其行PCA降维。 3. PCA理论基础之前我们提到了，我们希望新的超平面具备最大可分性和最近重构性这两个特征。实际上根据这两个特征，能够分别得到主成分分析的两种等价推导。 最大可分性：最大方差理论 样本点在这个超平面上的投影尽可能尽量分开，即投影后的样本之间的方差要最大化。假设现在有五个样本分布在x-y坐标系下，如下图 投影的计算过程如下： 红色点表示某一个样本点$x^{(i)}$，蓝色点表示 $x^{(i)}$在超平面u上的投影，在这里u是直线的斜率，也是直线的方向向量，且是单位向量。蓝色的点$x^{(i)}$在u上的投影带你，离原点的距离是($x^{(i)}$, u)，即$(x^{(i)})^Tu$或者$u^Tx^{(i)}$，而蓝色点到原点的距离，就是在蓝色坐标轴上的坐标。由于这些样本点的每一维的特征均值都是0（之前归一化过），因此投影到u上的样本点的均值仍然是0. 如果我们将这这五个样本投影到某一个维度上，即从二维投影到一维上。这里为了比较不同效果，分别选了一条过原点的直线表示。如下图 从上面两幅图可以看出，左边的图上的样本间的距离比右边的要大，即左边投影后的样本点之间的方差最大，为： 最后的一个等式中，中间那部分恰好是求协方差的公式(这里用m，而不是m-1)，所以我们用$\\lambda$表示左边部分，即样本点之间的方差:$$\\lambda = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2$$用$\\sum$表示中间部分：$$\\sum = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2$$那么上面的式子可以改写为：$$\\lambda = u^T\\sum u$$之前提到了u是超平面的单位向量，即有$u^Tu = 1$，将上面左右两边的式子都乘以u，可得：$$u\\lambda = \\lambda u= uu^T\\sum u = \\sum u$$即$\\sum u = \\lambda u$，所以$\\lambda$就是$\\sum$的特征值，u是特征向量。最佳的投影直线是特征值$\\lambda$最大时对应的特征向量，其次是$\\lambda$第二大对应的特征向量，依次类推。 因此只需要对协方差矩阵进行特征值分解，得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，就可将样例$x^{(i)}$表示为下面的新样本： 其中的第j维就是$x^{(i)}$在$u_j$上的投影。通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。 最近重构性：最小平方误差理论 假设现在选的超平面是L(这个例子中是直线)，那么某一样本$x_k$到L的垂直距离为d’，那么有所有点到该直线的距离为：$$\\sum_{k=1}^n||(x_k’ - x_k)||^2$$上面这个公式称作最小平方误差(Least Squared Erroe)","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"PCA","slug":"PCA","permalink":"http://chenson.com/tags/PCA/"}]},{"title":"Machine Learning - 支持向量机","date":"2017-06-16T09:21:11.000Z","path":"2017/06/16/Machine-Learning-支持向量机/","text":"1. Margins Logistic Regression ​在logistic regression中, 如果我们要判断一个点是属于0还是1概率是θ $$p(y=1 | x; \\theta) \\\\h_\\theta(x) = g(\\theta^T x)$$ logistic regression和SVM的区别 由上图可知, C是非常靠近Boundary的, 而A是离Boundary最远的点. 所以我们非常有信心的说A是属于+, 而C比较有可能属于+. 所以对于那些距离边界比较近的点才是我们需要重点考虑的. 而这也正是logistic regression和SVM之间的区别. logistic regression考虑全局(如何考虑全局？ans：RMSE最小值)红线 SVM考虑局部(最小间隔要大于多少)绿线 2. SVM 符号说明 在Logstic中, 对于二分类问题, y ∈ {0, 1} 在SVM中, 对于二分类问题, y ∈ {-1, +1}, 且重新定义公式 $$h_{w,b} (x) = g(w^Tx + b) \\\\g(z) = 1 \\space (z\\geq 0) \\\\g(z) = -1 \\space (z &lt; 0)$$ 函数间隔 Functional Margins 第一个公式表示的是某个样本的函数间隔. 第二个公式表示的是全局的函数间隔, 也就是函数间隔为所有样本中函数间隔最小的那个函数间隔决定. 之前使用正负1来表示 y, 所以计算出来的距离都是一个非负数, 且这个数值的大小表示了对于预测结果的confidence. y 值越接近1, 表示对这个判断越肯定.$$\\hat {\\gamma}^{(i)} = y^{(i)} (w^Tx^{(i)} + b) \\\\\\hat \\gamma = \\min_{i=1,…,m} \\hat \\gamma^{(i)}$$函数间隔越大, 代表了我们对于分类的结果非常的肯定, 所以希望函数间隔越大越好, 但需要对这个间隔加上一些限制条件(后面具体讲)才行. 因为我们可以在不改变这个超平面的情况下, 只要成比例增加w和b, 就能让函数间隔任意的大. 几何距离 Geometrix Margins 下图中, 如果我们知道B点所在的超平面(separating hyperplane)的解析式, 任何其他点到该面的距离都可以用上面定义过的函数间隔来表示. $$决策边界：w^Tx + b = 0$$w是超平面的法向量, 垂直于决策边界, 也就是这个超平面. 若B是A在分割面上的投影 (AB垂直于超平面), 那么我们可以计算A到超平面的距离 γ . 假设A点是$x_i$, 那么B点为：$$\\overrightarrow {OB} = \\overrightarrow {OA} - \\overrightarrow {BA} \\ x^{(i)} - \\gamma^{(i)} · \\frac w {||w||}$$因为B点在这个超平面上, 所以我们将这个点带回超平面得到：$$w^T(x^{(i)} - \\gamma^{(i)} · \\frac w {||w||}) + b = 0$$通过上面的式子, 可以解出γ：$$w^Tx^{(i)} - \\gamma^{(i)} · \\frac {w^Tw} {||w||} + b = 0 \\\\w^Tx^{(i)} + b = \\gamma^{(i)} ||w|| \\\\\\gamma^{(i)} = (\\frac w {||w||})^Tx^{(i)} + \\frac b {||w||}$$加上前面的$y^{(i)}$, 于是我们就能得到了几何间隔：$$\\gamma^{(i)} = y^{(i)}(\\frac {w^T} {||w||}x^{(i)} + \\frac b {||w||})$$通过上面的式子, 发现当||w|| = 1时, 几何间隔就是函数间隔. 这个时候, 如果任意放大||w||, 几何间隔是不会改变的. 因为||w||也会随着被放大. 几何间隔与函数间隔的关系为：$$\\gamma^{(i)} = \\frac {\\hat \\gamma^{(i)}} {||w||}$$对于所有的训练样本, 几何间隔为：$$\\gamma = \\min_{i=1,…,m} \\gamma^{(i)}$$ 2. 间隔最大化根据上一节, 我们可以求出几何间隔γ. 如果现在需要找到一个超平面S, 使得离超平面最近的点的几何间隔越大越好, 可以用下列优化问题表示：$$\\max_{w,b} \\gamma \\\\s.t. \\space y_i(\\frac w {||w||}·x_i + \\frac b {||w||}) \\geq \\gamma, \\space i = 1, 2, …, N$$即我们希望最大化超平面(w, b)关于训练数据集的几何间隔γ, 约束条件表示超平面(w,b)关于每个训练样本点的几何间隔至少是γ. 考虑到几何间隔与函数间隔的关系式, 可以将这个问题改写成函数间隔来表示, 即：$$\\max_{w,b} \\frac {\\hat \\gamma} {||w||} \\\\s.t. \\space y_i(w·x_i + b) \\geq \\hat \\gamma, \\space i = 1, 2, …, N$$上面式子中, 函数间隔的取值并不会影响到最优化问题的解. 事实上, 假设将w和b按比例改变为λw和λb, 这时函数间隔也会被当大λ倍, 所以这个对上面的最优化问题的不等式约束是没有影响的, 对目标函数的优化也没有影响. 这样, 可以去函数间隔γ=1, 代入到上面的最优化问题, 注意到最大化 $\\frac 1 {||w||}$和最小化 $\\frac 1 2$ $||w||^2$是等价的, 也是就得到厦门的线性可分支持向量机学习的最优化问题：$$\\min_{w,b} \\frac 1 2 ||w||^2 \\\\s.t. \\space y_i(w·x_i + b) - 1 \\geq 0$$这个时候我们的问题就转化成了在线性约束下的二次规划, 可以使用二次规划的软件来解决这个优化问题,, 然后我们就可以得到我们的最优间隔分类器.实际上, 我们有更好的办法去解这个优化问题. 但在这之前, 我们需要补充一下其他的相关知识. 3. 拉格朗日对偶在约束最优化问题中, 常常利用拉格朗日对偶性(Lagrange dulity)讲原始问题转换为对偶问题, 通过解度偶问题而得到的原始问题的解. 该方法应用在许多统计学习方法中, 例如最大熵模型和支持向量机. 原始问题$$\\min_w f(w) \\\\s.t. \\space h_i(w) = 0, \\space i = 1,2,…,l$$下面是约束条件, 使用拉格朗日乘子法, 将问题转换为：$$L(w, b) = f(w) + \\sum_{i=1}^l\\beta_ih_i(w)$$这里面, βi为拉格朗日乘子(Lagrange Multipliers). 然后令偏导为0来解得w和β：$$\\frac {∂L} {∂w_i} = 0 \\\\\\frac {∂L} {∂\\beta_i} = 0$$更加广泛的约束最优化问题：$$\\min_w f(w) \\\\s.t. \\space g_i(x) ≤ 0, \\space i = 1, 2, …,k \\\\\\space \\space \\space \\space \\space \\space \\space h_i(w) = 0, \\space i = 1,2,…,l$$所以我们定义广义拉格朗日公式(Generalized Lagrangian)为：$$L(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^k \\alpha_ig_i(w) + \\sum_{i=1}^l \\beta_ih_i(w)$$其中, αi,βi为拉格朗日乘子(Lagrange Multipliers). 现在我们定义：$$\\theta_p(w) = \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} L (w, \\alpha, \\beta)$$ ​ 其中下表P代表了“primal”. 若两个约束条件当中至少有一个得不到满足的时候, 则可令αi为无穷大或βi为无穷大使得：$$\\theta_P = f(w) \\space \\space \\space (w满足原始问题的约束)\\\\\\theta_p = \\infty \\space \\space \\space (其他)\\$$​ 对于满足原始约束的w来说, θp与原始问题中的目标函数相同. 对于违反原始约束问题的w来说θp = ∞ ​ 如果考虑最小化： ​$$\\min_w\\theta_p(w) = \\min_w \\max_{\\alpha, \\beta: \\alpha_i ≥ 0 } L(w, \\alpha, \\beta)$$​ 它是与原始最优化问题相等价的, 即它们有相同的解. 对偶问题 现在看另外一个问题：$$\\theta_D(\\alpha, \\beta) = \\min_w L(w, \\alpha, \\beta)$$其中下表D代表了对偶(dual). 在原始问题中, 我们先最大化关于α和β的函数, 再最小化关于w的函数； 而对偶问题中, 我们先最小化关于w的函数, 在最大化关于α和β的函数：$$\\max_{\\alpha, \\beta: \\alpha_i ≥ 0} \\theta_D(\\alpha, \\beta) = \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} \\min_w L(w, \\alpha, \\beta)$$它们唯一的区别就在于min和max的顺序不同. 我们令:$$d^{*} = \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} \\min_w L(w, \\alpha, \\beta) \\leq \\min_w \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} L(w, \\alpha, \\beta) = p^{*}$$也就是说, 在某种情况下, 会有d = p. 这个时候我们就可把求原始问题转化成求对偶问题. 假设f和g都是凸函数(convex function), h是仿射的, 并且存在w是对所有的i, 能够使得:$$g_i(w) &lt; 0$$上述假设条件下, 一定存在w*, α*和β*使得w*是原始问题的解. α*和β*是对偶问题的解. 并且还有 p = d = L(w, α\\, β*). w*, α*和β*满足KKT条件(Karush-Kuhn-Tucker conditions)： 如果存在满足KKT条件的w*,α*,β*, 则原始条件与对偶问题一定有解. 公式(5)又称之为KKT对偶互补条件, 这个条件表明如果 α* &gt; 0, 那么就有g(w*) = 0. 即约束条件 g(w*) &lt;= 0 激活, w处于可行域的边界上. 而其他谓语可行域内部g(w*) &lt; 0的点都不起约束作用, 对应的α* = 0. 为什么要引入对偶 为什么要使用随机梯度下降 用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降(为什么？？), 只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 对偶的优势 对偶形式将权重向量w转化成实例$x_i$和$y_i$的线性组合形式。且对偶是以内积的形式出现的，可以预先使用Gram矩阵储存，用空间换时间的方法提高计算效率。原始形式每次判断误分类点时都需要进行向量点乘运算。 这里同时也为后面引入核函数做伏笔，因为感知机是神经网络和支持向量机的基础。 4. 最优间隔分类器根据上面的内容, 回顾SVM的问题$$\\min_{\\gamma,w,b} \\frac 1 2 ||w||^2 \\\\s.t. \\space y^{(i)}(w^Tx^{(i)} + b) \\geq 1 \\$$根据拉格朗日对偶问题, 修改约束条件为$$g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0$$那么就和拉格朗日公式是一样的了. 根据上面的KTT条件可知只有函数间隔是1, 线性约束式前面的系数大于0, g(w)=0, 其他的不在线上的点(g(w)&lt;0), 极值不会在他们所在的范围内取得, 因此前面的系数等于0. 考虑下图, 最大间隔分类超平面为实线： 其中一个正样本和两个负样本正好在平行于分类超平面的虚线上, 只有这三个样本对应的 αi&lt;0, 其他样本对应的 αi=0. 这三个样本就叫做支持向量机. 从这里我们可以看出, 支持向量的个数远远小于集训集的大小. 现在构造拉格朗日函数：$$L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]$$所以接下去的任务就是求解对偶问题, 根据上面的知识, 有：$$d* = \\max_{\\alpha:\\alpha_i \\geq 0} \\theta_D(\\alpha) = \\max_{\\alpha:\\alpha_i \\geq 0} \\min_{w, b} L(w, b,\\alpha)$$首先, 求L(w,b,α)关于w, b 的最小值. 令偏导数为0：$$\\frac {∂L} {∂w} = w - \\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)} = 0 \\\\\\frac {∂L} {∂b} = 0 - \\sum_{i=1}^m\\alpha_iy^{(i)}= 0$$可得：$$w = \\sum_{i=1}^m\\alpha_i y^{(i)} x^{(i)} \\\\\\frac {∂} {∂b} L(w, b, \\alpha) = \\sum_{i=1}^m\\alpha_i y^{(i)} = 0$$将求得的w带回拉格朗日函数L(w,b,α), 可得到：$$L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]$$ 计算出了min L(w,b,α), 便可以继续进行max操作, 即： 可以证明该优化问题满足KKT条件, 求得$α^{*}_i$之后(后面将如何求解), 可通过:$$w = \\sum_{i=1}^m \\alpha_i y^{(i)}x^$$求得w*, 最后通过下面式子求得b*$$b^{*} = -\\frac {\\max_{i:y^{(i)}=-1}w^{*T}x^{(i)} + \\min_{i:y^{(i)} = 1}w^{*T}x^{(i)}}{2}$$当求出所有的参数, 就可以通过$w^T$x + b 来进行分类了：$$w^T + b = (\\sum_i^m \\alpha_iy_ix_i)^T x + b= \\sum_i^m \\alpha_iy_i⟨x_i, x⟩ + b$$通过上面式子发现, 现在新来一个新数据, 只需要计算它与训练样本的内积即可. 并通过前面的KKT条件我们知道, 只有除了支持向量的那些原本, 都有$α_i$ = 0. 所以, 我们只需要将新样本与支持向量机做内积运算, 即可求出$w^T$x + b Example 假设这里有三个样本点，正样本点x1=(3,3)^T, x2=(4,3)^T, 负样本点是x3=(1,1)^T，试用感知机学习算法对偶形式求感知机模型。 ANSWER 取αi = 0，这里i=1，2，3，b=0，n=1 计算Gram矩阵$$G= \\begin {bmatrix} ||x_1||^2 &amp; x_1·x2 &amp; x_1·x3 \\ x2· x1 &amp; ||x2||^2 &amp; x_2·x3 \\\\ ||x_3· x1 &amp; x_3·x2 &amp; ||x_3||^2 \\end{bmatrix} = \\begin {bmatrix} 18 &amp; 21 &amp; 6 \\ 21 &amp; 25 &amp; 7 \\\\ 6 &amp; 7 &amp; 2 \\end{bmatrix} $$ 误分条件$$y_i(\\sum_{j=1}^N\\alpha_jy_jx_j ·x_i + b) \\leq 0$$参数更新$$\\alpha_i \\leftarrow \\alpha_i + 1 \\ b \\leftarrow b + y_i$$​ 迭代过程如下，结果列于下表 | k | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 || :–: | :–: | :–: | :–: | :–: | :–: | :–: | :–: | :—-: || | | x1 | x3 | x3 | x3 | x1 | x3 | x3 || α1 | 0 | 1 | 1 | 1 | 2 | 2 | 2 | 2 || α2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 || α3 | 0 | 0 | 1 | 2 | 2 | 3 | 4 | 5 || b | 0 | 1 | 0 | -1 | 0 | -1 | -2 | -3 | w, b分别为： $$w = 2x_1 + 0x_2 - 5x_3 = (1, 1)^T \\\\b = -3$$ 分离超平面：$$x^{(1)} + x^{(2)} - 3 = 0$$ 5. Kernels在之前的线性回归的章节中，有提到过polynomial regression。假设x是房子的面积，我们使用三个特征$x$, $x^2$, $x^3$来构造一个三次多项式。 这里面有两个概念要区分一下。x为原先房子的面积，是属性(attributes)。通过这个属性x映射出来的$x$, $x^2$, $x^3$叫做特征(features)。在这里使用ϕ来表示从属性到特征的特征映射(featuer mapping)。比如：$$ϕ(x) = \\begin{bmatrix} x \\ x^2 \\ x^3 \\end{bmatrix}$$那么在SVM中，如何使用这种特征映射呢？ 通过上面知识，我们只需要将所有出现$⟨x^{(i)}, x^{(j)}⟩$ 替换为 $⟨ϕ^{(i)}, ϕ^{(j)}⟩$ 看上去好像我们既在SVM中使用了特征映射, 又解决了数据在低维空间中线性不可分的情况. 但是, 这里有个问题. 如果我们通过特征映射得到的$ϕ(x)$是一个很高维甚至是无穷维的, 那么计算$⟨ϕ(x^{(i)}),ϕ(x^{(j)})⟩$就不是那么现实了, 计算时间会很久. 这里我们就要引出一个叫kernels的概念, 假设 Q: z在这里代表的什么？ $$K(x, z) = (x^Tz)^2 \\space \\space \\space \\space \\space \\space \\space \\space x, z \\in R^b$$ 展开K(x, z):$$K(x, z) = (\\sum_{i=1}^n x_iz_i) (\\sum_{j=1}^n x_iz_i) \\\\= \\sum_{i=1}^n \\sum_{j=1}^n x_ix_j z_i z_j \\\\= \\sum_{i,j=1}^n (x_ix_j) (z_iz_j)$$展开后我们发现，K(x, z)还可以写成$K(x, z) = ϕ(x)^T ϕ(z)$，其中：$$ϕ(x) = \\begin{bmatrix} x _1x_1 \\ x _1x_2 \\\\x _1x_3 \\ x _2x_1 \\ x _2x_2 \\ x _2x_3 \\ x _3x_1 \\ x _3x_2 \\ x _3x_3 \\ \\end{bmatrix}$$在这个例子中，映射后特征的内积和原始特征的内积的平方是等价的，也就是说我们只需要计算原始特征的内积再进行平方就可以了，并不需要先得到映射后再计算映射后特征的内积。计算原始特征内积的时间复杂度为O(n)，而计算映射特征的时间复杂度为O(n^2)。 再看另外一个kernels$$K(x, z) = (x^T z + c)^2 \\\\= \\sum_{i,j=1}^n (x_ix_j) (z_iz_j) + \\sum_{i=1}^n (\\sqrt {scx_i})( \\sqrt {scx_j} ) + c^2$$同样我们也可以改写上面的式子 所以广泛的来说，我们有$$K(x, z) = (x^T z + c)^d$$这个kernel将n维的特征映射为(d, n+d)维，即这里面对应的多项式$x_{i1}$, $x_{i2}$, …, $x_{ik}$最多到d维。尽管空间维度为O(n^d)，但计算时间仍然只是O(n)，因为我们并不需要将映射后的特征全部计算出来再计算内积。 但因为计算的是内积，有IR中的余弦相似度可孩子，如果x和z的向量夹角越小，那么核函数的值就越大。反之就越小。因此核函数值是ϕ(x)和ϕ(z)相似度。 再看另外一个很函数$$K(x, z) = exp(\\frac {||x - z||^2} {2\\sigma^2})$$在这个核函数中，如果x和z很相近，则(||x-z||~= 0)，那么核函数值为1. 如果相差很大，则(||x-z||&gt;&gt; 0), 那么很函数的值约等于0. 由于这个核函数类似于高斯分布，因此成为高斯核函数，也叫做径向基函数(Radial Basis Function 简称RBF)，它能够把原始特征映射到无穷维。 类似于高斯很函数，比较x和z的相似度，并映射到0~1之间。logistic Regression中， sigmoid函数也可以，所以还有sigmoid核函数。$$K(x, z) = tanh(\\beta · xz + b)$$下面有张图说明在低维线性不可分时，映射到高维后就可分了，使用高斯核函数。 在SVM中，对于训练样本学习出w和b参数后，对于新来的样本我们只需要计算$w^Tx + b$来判断。那么在使用了核函数之后，则需要相应的改为$w^Tϕ(x) + b$ 那么是需要先计算好ϕ(x)再进行预测呢？实际上不需要的, 之前计算过$$w^Tx + b = (\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)})^T x + b \\\\= \\sum_{i=1}^m \\alpha_i y^{(i)} ⟨x^{(i)}, x⟩ + b$$所以我们只需要将$$⟨x^{(i)}, x⟩ \\\\替换为 \\\\K(x^{(i)}, x)$$ 6. 核函数的有效性判断核函数的有效性，即判断是否存在ϕ, 使得下面式子成立$$K(x,z)=⟨ϕ(x)ϕ(z)⟩$$假设我们有核K和m个训练样本{x(1),x(2),…,x(m)}, 定义一个 (m×m) 的矩阵K$$K_{ij} = K(x^{(i)}, x^{(j)})$$如果此时K是一个有效的kerne，那么则有：$$K_{ij} = k(x^{(i)}, x^{(j)}) = ϕ(x^{(i)})ϕ(x^{(i)}) \\\\= ϕ(x^{(j)})^T ϕ(x^{(i)}) = K(x^{(j)}, x^{(i)}) = K_{ji}$$即K是对称矩阵。现在我们用ϕk(x)不澳是向量ϕ(x)的第k个元素，对任意的向量z都有： 从上面的证明我们可以得到，如果K是一个有效的kernel，那么对于在训练集上的核矩阵K一点是半正定的。事实上，这不仅仅是个必要条件，也是充分条件。有效核也叫做Mercer Kernel 7. Reference http://zhihaozhang.github.io/2014/05/11/svm3/ http://www.cnblogs.com/bourneli/p/4199990.html http://www.cnblogs.com/90zeng/p/Lagrange_duality.html","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Machine Learning - Ensemble Learning","date":"2017-06-13T04:27:45.000Z","path":"2017/06/13/Machine-Learning-Ensemble-Learning/","text":"1. 集成学习 (Ensemble Learning)在对新的数据实例进行分类的时候，集成学习通过训练好多个学习器，把这些分类器的的分类结果进行某种组合 (比如投票) 决定分类结果，以取得更好的结果，就是我们生活中那句话“三个臭皮匠顶个诸葛亮”，通过使用多个决策者共同决策一个实例的分类从而提高分类器的泛化能力。 同质集成 (Homogeneous) 需要是同种类型，比如全部是决策树或神经网络等，每个个体学习器称之为基学习器 (base learner)，相应的学习算法称为基学习算法 (base learning algorithm) 异质集成 (Heterogenous) 可以由不同的学校算法生成，这时候就不再有基学习算法，每个个体学习器称为组件学习器 (component learner) 以下我们重点讨论同质集成。 2. 分类器的选择 差异性 问题：如何选择/构建差异性的基分类器？(ans: section 3) 精度 &gt; 0.5 精度略高于50%的分类器称之为弱学习器 (weak learner) 问题：如何投票选择出最佳的预测？(ans: section 4) 如何组合 假设在二分类器中，这里有三个分类器在测试三个样本，每个分类器的正确率都是66%，那么组合出的结果大致可以分成以下几种 集成性能提升 | | test1 | test2 | test3 || :–: | :—: | :—: | :—: || h1 | right | right | wrong || h2 | wrong | right | right || h3 | right | wrong | right | 如果只是简单投票法的话，在每个test中，有两个分类器是对的，一个是错的，那么投票出来的结果是对的，最终精度可以达到100%。 集成不起作用 | | test1 | test2 | test3 || :–: | :—: | :—: | :—: || h1 | right | right | wrong || h2 | right | right | wrong || h3 | right | right | wrong | 三个分类器对三个test进行预测，恰好三个分类器对test3的情况分类都是错的，而对test1和test2的结果都预测正确，那么最终的测试结果没有影响，都是66%。 集成起负作用 | | test1 | test2 | test3 || —- | —– | —– | —– || h1 | right | wrong | wrong || h2 | wrong | right | wrong || h3 | wrong | wrong | right | 和第一种情况相反，最终的测试结果是33% 通过上面的例子，可以反映出一个问题，在同质集成中，如何构建多个误差是相互独立的基学习器。因为这些基学习器是用的同一种算法，基本也是使用同一组数据，解决的也是同一个问题。 根据个体学期的生成方式，集成学习方法大致可以分成两大类： Boosting 个体学习器之间存在强依赖关系，必须串行生成的序列化方法 Bagging 和 Random Forest 个体学习器之前不存在强依赖关系，可同时生成的并行化方法 3. 构建差异性基分类器在同一个数据集上，构建不同的，具有差异性的分类器 (好而不同)，就是通过抽样技术获取多个训练数据集，从而生成多个差异性分类器。目前主要的方法有：Bagging 和 Boosting。 3.1 Boosting提升方法是一个迭代的过程，通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重，这样错分的数据再下一轮的迭代就有更大的作用 (对错分数据进行惩罚)。 具体来说，先从初始训练集训练出一个基学习器，再根据这个基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，一直到基学习器的数目到达事先指定的T值，然后将所有的基学习器相结合。(问题：如何调整分布？) 数据的权重有两个作用，一方面我们可以使用这些权值作为抽样分布，进行对数据的抽样，另一方面分类器可以使用权值学习有利于高权重样本的分类器。把一个弱分类器提升为一个强分类器，大家可以参考AdaBoost算法 (西瓜书P173) Example 假设现在有一个线性分类器如下表 | | 预测 + | 预测 - | count || :——-: | :–: | :–: | :—: || 实际 + | 24 | 16 | 40 || 实际 - | 9 | 51 | 60 || count | 33 | 67 | 100 | 错误率计算$$\\epsilon = \\frac {(9 + 16)} {100} = 0.25$$ 错分样本的权值更新$$w_e = \\frac 1 {2*\\epsilon} = 2$$ 正确样本的权值更新$$w_r = \\frac 1 {2*(1 - \\epsilon)} = \\frac 2 3$$ 所以讲权重乘以相应的数据量，得到新的数据分布，如下 预测 + 预测 - count 实际 + 24 * 2/3 = 16 16 * 2 = 32 16+32=48 实际 - 9 * 2 = 18 51 * 2/3 = 34 18 + 34 = 52 count 16 + 18 = 34 32 + 34 = 66 100 3.2 Bagging通过对原数据集进行有放回的采样 (bootstrap sampling) 构建出大小和原数据集大小一样的新数据集D1，D2，D3…..，然后用这些新的数据集训练多个分类器H1，H2，H3….。因为是有放回的采样所以一些样本可能会出现多次，而其他样本会被忽略，理论上初始训练集中约有63.2%的样本会出现在采样集中。(西瓜书P27) 自助采样法 (Bootstrap sampling) 假设给定一个包含了m个样本的数据集D，我们需要构建一个新的数据D1，大小和D一样的。那么每次我们从D中取出一个样本入到D1中，之后把样本放回D中，重新采样，总共采样m次，那么我们可以得到一个大小为m的D1。因为我们把样本又重新放入D中，下次还有可能抽取到，导致在D1中，有些样本会出现多次，而有些样本则一次也不会出现。所以根据如下公式：$$lim_{m -&gt; \\infty} (1 - \\frac 1 m)^m -&gt; \\frac 1 e \\approx 0.368$$最终在初始样本数据集D中，有36.8%的样本是一次也没有出现的，而63.2%重复出现了，这样我们就改变了初始样本数据集的分布。所以我们可以将D1作为训练集，D\\D1，即剩下的36.8%未出现在D1中的作为测试集。这样的测试结果称之为包外估计 (out-of-bag-estimate) Bagging通过降低基分类器方差改善了泛化能力，因此Bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，Bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么Bagging方法就得不到性能的提升，甚至会减低，因为新数据集只有63%。 3.3 Random Forest随机森林是Bagging的一个拓展变体，基于Bagging框架进一步降低了了模型的方差。在这里，我们用的决策树与传统的决策树略有不同。 传统的决策树，比如C45或者CART，每当需要划分一个属性的时候，我们会将所有的属性的都进行划分一遍，然后计算其划分之后与划分之前熵的偏差，或者计算Gini指数等，然后从中选取出最佳属性进行划分。 而随机森林，对基决策树的每个结点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选取一个最优属性用于划分 (从特征的不同子集来构建树)。这里的参数k控制了随机性的引入程度： 若k=d，则基决策树和传统决策树相同。 若k=1，则是随机选择一个属性用于划分 推荐值k=log_2 d 随机森林除了基于Bagging中对于样本的扰动，还同时对数据集中的属性划分进行了扰动，这样可以增加基学习器的多样性和差异性，使得最终集成的泛化性得到提升。 3.4 增强多样性 数据样本扰动 输入属性扰动 输出表示扰动 算法参数扰动 4. 组合策略4.1 归回预测 (数值预测) 简单平均值法 就是取各个分类器结果的平均值（会不会降低这里面最好的那个分类器的精度）$$H(x) = \\frac 1 T \\sum_{i=1}^T h_i(x)$$ 加权平均法 给不同的分离器赋予不一样的权重值，然后求和$$H(x) = \\sum_{i=1}^Tw_i · h_i(x)$$ 4.2 分类预测 (类别预测) 简单投票法 每个分类器的权重大小都一样，少数服从多数 绝对多数投票法票数过半，否则拒绝预测 相对多数投票法票数最多的那个 加权投票法 给每个分类器赋予一个权重，然后根据权重来投票，得到票数高的最为输出结果 概率投票法（和简单分类有什么不同？） 有的分类器的输出是有概率信息的，比如 分类器A输出结果1的概率为75% 分类器B输出结果0的概率为80% 分类器C输出结果1的概率为52% 最终输出的结果为？？？（概率相加还是概率的平均值，相加可能性高） 4.3 学习法5. References 西瓜书第八章 - 周志华 非参考，推荐阅读 小菜鸟对周志华大神gcForest的理解 gcForest算法理解","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"Ensemble","slug":"Ensemble","permalink":"http://chenson.com/tags/Ensemble/"}]},{"title":"Hadoop权威指南笔记（三）","date":"2017-06-09T06:08:52.000Z","path":"2017/06/09/Hadoop权威指南笔记（三）/","text":"1. 为什么要使用Secondary Sort在Hadoop中，从Map到Reduce的过程中，key是不断被sort的。所以从map出来的时候写入到一个intermediate output file的时候，key是有序的。然后reduce不断的从不同的cluster里面fetch里面的key-value pairs的时候，仍然多次sort这些pairs。所以最终进入到reducer的key是有序的，但是value是无序的。如果我们需要对value也进行排序呢？Google的MR内置了函数对value也可以排序，但是Hadoop不行，我们需要自己去定制partitioner等去实现这个功能。 举个栗子： 输入文件格式如下 12345678910112015,1,242015,3,542015,1,32015,2,-432015,4,52015,3,462014,2,642015,1,42015,1,212015,2,352015,2,20 ​ 期望的输出格式如下（value是有序的） 123452014-2 642015-1 3，4，21，242015-2 -43，0，352015-3 46，562015-4 5 ​ Hadoop默认的输出格式如下（value是无序的） 123452014-2 642015-1 21，4，3，242015-2 0，35，-432015-3 56，462015-4 5 2. 解决方案 传统的解决方法 就是进入到同一个reducer的时候，这些同一个可以的values是在一个list里面的，那么我们就可以先把这个list里面的value存到内存中去，然后在内存中将这些value排序。这个方法只适用于数据量较小的时候，当数据量很大的时候，内存并不能同时存入这些values，程序就报错无法正常运行。 利用Hadoop特点的方法 既然Hadoop可以对Key进行排序，那么可以利用这点，将之前的key-value pairs 组成一个新的key，让Hadoop对这些Key进行排序，栗子如下 12345// 原先的key-value pair(2015,1) 21// 新的composite-key-value pair((2015,1),21) 21 这里的(2015,1)是我们原先就有的key，为了区分新的key，将这个原有的key (k, v1) ,称为natural key，将新的key ((k, v1), v2)称为composite key。将v2称为natural value。具体如下图 ​ 为了实现Hadoop对新的composite key进行排序，我们需要自定义partitioner和grouping来确保这些composite key中natural key相同的会被分配到同一个reducer。因为在composite key中，即使(k, v1)相同，只要v2不同，默认的partitioner就会认为这是俩个不同的key，就很有可能讲这个composite key分配到不同的reducer里去。 ​ 3.实现过程 partitioner: 将natural key相同的发送到同一个reducer里去 在同一个[]里面说明是一个reducer，value任然是无序的 123456789[((2014-2,64),64)][((2015-1,24),24), ((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,56),56), ((2015-3,46),46)][((2015-4,5),5)] ​ grouping comparator: 将natural key相同的作为一个group排序 123456789[((2014-2,64),64)][((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21), ((2015-1,24),24)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,46),46), ((2015-3,56),56)][((2015-4,5),5)] ​ 进入到Reducer的格式 (有点里不理解此时composite key里面的natural value是如何确定的) 123456789((2014-2,64), (64))((2015-1,24), (2，4，21，24))((2015-2,35), (-43,0,35))((2015-3,46), (46,56))((2015-4,5), (5)) ​ 最终的输出 12342014-2 642015-1 3,4,21,242015-2 -43,0,352015-4 5 整个流程图如下（图内数据与上面数据不符合） 4. 代码 composite key 将旧的Key（natural key）和Value组合成新的Key（composite key）的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.secondarySort;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class Entry implements WritableComparable&lt;Entry&gt; &#123; private String yearMonth; private int count; public Entry() &#123; &#125; @Override public int compareTo(Entry entry) &#123; int result = this.yearMonth.compareTo(entry.getYearMonth()); if (result == 0) &#123; result = compare(count, entry.getCount()); &#125; return result; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(yearMonth); dataOutput.writeInt(count); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.yearMonth = dataInput.readUTF(); this.count = dataInput.readInt(); &#125; public String getYearMonth() &#123; return yearMonth; &#125; public void setYearMonth(String yearMonth) &#123; this.yearMonth = yearMonth; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public static int compare(int a, int b) &#123; return a &lt; b ? -1 : (a &gt; b ? 1 : 0); &#125; @Override public String toString() &#123; return yearMonth; &#125;&#125; ​ Partitioner 1234567891011package com.secondarySort; import org.apache.hadoop.mapreduce.Partitioner; public class EntryPartitioner extends Partitioner&lt;Entry, Integer&gt; &#123; @Override public int getPartition(Entry entry, Integer integer, int numberPartitions) &#123; return Math.abs((entry.getYearMonth().hashCode() % numberPartitions)); &#125;&#125; ​ Grouping Compartor 1234567891011121314151617package com.secondarySort; import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class EntryGroupingComparator extends WritableComparator &#123; public EntryGroupingComparator() &#123; super(Entry.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; Entry a1 = (Entry) a; Entry b1 = (Entry) b; return a1.getYearMonth().compareTo(b1.getYearMonth()); &#125;&#125; ​ Mapper 1234567891011121314151617181920212223public class SecondarySortMapper extends Mapper&lt;LongWritable, Text, Entry, Text&gt; &#123; private Entry entry = new Entry(); private Text value = new Text(); @Override protected void map(LongWritable key, Text lines, Context context) throws IOException, InterruptedException &#123; String line = lines.toString(); String[] tokens = line.split(\",\"); // YYYY = tokens[0] // MM = tokens[1] // count = tokens[2] String yearMonth = tokens[0] + \"-\" + tokens[1]; int count = Integer.parseInt(tokens[2]); entry.setYearMonth(yearMonth); entry.setCount(count); value.set(tokens[2]); context.write(entry, value); &#125;&#125; ​ Reducer 123456789101112public class SecondarySortReducer extends Reducer&lt;Entry, Text, Entry, Text&gt; &#123; @Override protected void reduce(Entry key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder builder = new StringBuilder(); for (Text value : values) &#123; builder.append(value.toString()); builder.append(\",\"); &#125; context.write(key, new Text(builder.toString())); &#125;&#125; ​ Deriver 123456789101112131415Configuration conf = new Configuration();Job job = Job.getInstance(conf);job.setJarByClass(Iteblog.class);job.setJobName(\"SecondarySort\"); FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setOutputKeyClass(Entry.class);job.setOutputValueClass(Text.class); job.setMapperClass(SecondarySortMapper.class);job.setReducerClass(SecondarySortReducer.class);job.setPartitionerClass(EntryPartitioner.class);job.setGroupingComparatorClass(EntryGroupingComparator.class); 5. 常用的Secondary Sort代码 IntPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; int first, second; // public IntPair() &#123;&#125; // // public IntPair(int left, int right) &#123; // set(left, right); // &#125; public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; public String toString()&#123; return \"(\" + first + \",\" + second + \")\"; &#125; @Override public void readFields(DataInput arg0) throws IOException &#123; // TODO Auto-generated method stub first = arg0.readInt(); second = arg0.readInt(); &#125; @Override public void write(DataOutput arg0) throws IOException &#123; // TODO Auto-generated method stub arg0.writeInt(first); arg0.writeInt(second); &#125; // 关键：自定义类型的比较方法 @Override public int compareTo(IntPair arg0) &#123; // TODO Auto-generated method stub if (first != arg0.first) &#123; return first &lt; arg0.first ? -1 : 1; &#125; else if (second != arg0.second) &#123; return second &lt; arg0.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; public int hashCode() &#123; return first * 157 + second; &#125; public boolean equals(Object right) &#123; if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125;&#125;public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; &#123; // 类型要和Mapper输出的一样 @Override public int getPartition(IntPair arg0, IntWritable arg1, int arg2) &#123; // TODO Auto-generated method stub return Math.abs((arg0.getFirst() * 127) % arg2); &#125;&#125;/* * 第一种方法，实现接口RawComparator 数据类型的比较在MapReduce中式及其重要的, * Mapreduce中有一个排序阶段，key和其他的key相比较。 针对此，Hadoop 提供的一个优化是 RawComparator * * public static class GroupingComparator implements RawComparator&lt;IntPair&gt;&#123; * * @Override public int compare(IntPair arg0, IntPair arg1) &#123; // TODO * Auto-generated method stub int l = arg0.getFirst(); int r = * arg0.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; * * @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int * s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, * Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; &#125; */// 方法二public static class GroupingComparator extends WritableComparator &#123; protected GroupingComparator() &#123; super(IntPair.class, true);// 调用父类的构造函数 &#125; public int compare(WritableComparable w1, WritableComparable w2) &#123; IntPair i1 = (IntPair) w1; IntPair i2 = (IntPair) w2; int l = i1.getFirst(); int r = i2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125;&#125; ​ TextPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package TextPair;// cc TextPair A Writable implementation that stores a pair of Text objects// cc TextPairComparator A RawComparator for comparing TextPair byte representations// cc TextPairFirstComparator A custom RawComparator for comparing the first field of TextPair byte representations// vv TextPairimport java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.io.WritableUtils;public class TextPair implements WritableComparable&lt;TextPair&gt; &#123; private Text first; private Text second; public TextPair() &#123; set(new Text(), new Text()); &#125; public TextPair(String first, String second) &#123; set(new Text(first), new Text(second)); &#125; public TextPair(Text first, Text second) &#123; set(first, second); &#125; public void set(Text first, Text second) &#123; this.first = first; this.second = second; &#125; public Text getFirst() &#123; return first; &#125; public Text getSecond() &#123; return second; &#125; @Override public void write(DataOutput out) throws IOException &#123; first.write(out); second.write(out); &#125; @Override public void readFields(DataInput in) throws IOException &#123; first.readFields(in); second.readFields(in); &#125; @Override public int hashCode() &#123; return first.hashCode() * 163 + second.hashCode(); &#125; @Override public boolean equals(Object o) &#123; if (o instanceof TextPair) &#123; TextPair tp = (TextPair) o; return first.equals(tp.first) &amp;&amp; second.equals(tp.second); &#125; return false; &#125; @Override public String toString() &#123; return first + \"\\t\" + second; &#125; @Override public int compareTo(TextPair tp) &#123; int cmp = first.compareTo(tp.first); if (cmp != 0) &#123; return cmp; &#125; return second.compareTo(tp.second); &#125; // ^^ TextPair // vv TextPairComparator public static class Comparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public Comparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); int cmp = TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); if (cmp != 0) &#123; return cmp; &#125; return TEXT_COMPARATOR.compare(b1, s1 + firstL1, l1 - firstL1, b2, s2 + firstL2, l2 - firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; &#125; static &#123; WritableComparator.define(TextPair.class, new Comparator()); &#125; // ^^ TextPairComparator // vv TextPairFirstComparator public static class FirstComparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public FirstComparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); return TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; if (a instanceof TextPair &amp;&amp; b instanceof TextPair) &#123; return ((TextPair) a).first.compareTo(((TextPair) b).first); &#125; return super.compare(a, b); &#125; &#125; // ^^ TextPairFirstComparator // vv TextPair&#125;// ^^ TextPair","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems(2)","date":"2017-05-15T08:43:49.000Z","path":"2017/05/15/Machine-Learning-Recommender-Systems-2/","text":"1. 协同过滤，给用户推荐物品1.1 基于用户的协同过滤算法UserCF 主要通过分析用户的行为记录，计算物品之间的相似度 该算法认为物品A和物品B具有很大的相似度是因为喜欢物品A的用户大部分也喜欢物品B 1.1.1 步骤 找到和目标用户相似的用户集合 找到这个集合中的用户喜欢，且和目标用户没有听说过的的物品推荐给目标用户 1.1.2 计算相似度 得到用户之间的兴趣相似度之后，寻找最相近的K个用户 1.2基于物品的协同过滤算法1.2.1 计算步骤 计算物品之间的相似度 根据物品的相似度和用户的历史行为给用户生成推荐列表 1.2.2 计算相似度 1.3 隐语义模型 UserCF 找到和他们看了同样书的其他用户，即是兴趣相似用户。然后给用户推荐这些用户相似用户所喜欢的其他书籍 ItemCF 在已经看过的书中，寻找相似的书，即和这些看过的书，同时出现在其他用户的看过的书中，然后推荐这些其他的书 其他方法 可以对物品的兴趣进行分类，对于某个用户，首先得到他的兴趣分类，然后从这些分类中挑出他可能喜欢的物品 如何给物品进行分类 隐含语义分析技术(latent variable analysis) 如何确定用户的兴趣，即对哪些类的物品感兴趣，以及感兴趣的程度 对于一个给定的类，选择哪些属于这个类的物品推荐给用户？以及如何确定这些物品在这个中的权重。即如何在这个类中，挑选出合适的物品推荐给用户 1.3.1 LFM (latent factor model)1.4 用户标签数据 User Generated Content2. 需要解决的问题 - 评分预测（用户A对电影x的评分预测） 2.1 实验方法2.1.1 划分训练集 与时间无关，可以均匀分布随机换分数据集。即对每一个用户，随机选取一些评分记录作为训练集，剩下的作为测试集 与时间相关，那么需要将用户的旧行为作为训练集，讲用户的新行为作为测试集 例子 Netflix的评分预测系统中，将每个用户的评分记录按照从早到晚进行排序，然后将用户最后的10%的评分记录作为测试集，90%的评分记录作为训练集。 2.1.2 评分标准$$RMSE = \\frac {\\sqrt {\\sum_{(u, i) \\in T} (r_{ui} - \\hat r_{ui})^2 }} {|Test|}$$ 2.1.3 评分预测算法 平均值 最简单的方法：利用平均值预测用户对物品的评分 全局平均值 计算在整个训练集中，所有评分记录的评分平均值 用户评分平均值 计算用户u在训练集中所给出的评分的平均值 物品评分平均值计算该物品w在训练集中被评价了的评分的平均值 用户对物品分类的平均值 假设这里有两个分类，一个是用户分类函数U，一个是物品分类W，U(u)定义了用户u所属的分类，W(w)定义了物品w所属的分类。那么我们可以利用训练集中同类用户对同类物品评分的平均值预测用户对物品的评分 之前是三种平均值其实是用户对物品分类的平均值的一种特例 U(u) = 0，W(w) = 0，那么就是全局平均值 U(u) = u，W(w) = 0，那么就是用户评分平均值 U(u) = 0，W(w) = w，那么就是物品评分平均值 在以上的方法中，我们并没有考虑到用户的活跃度和物品的流行程度。实际上可以将这两点考虑进去，对活跃用户和流行的物品给定一点penalty 2.1.4 基于领域的方法（基于用户的领域和基于物品的领域算法） 基于用户的领域算法$$\\hat r_{ui} = \\overline r_u +\\frac {\\sum_{v \\in S(u, K) \\bigcap N(i)} w_{uv}(r_{vi} - \\overline r_v)}{\\sum_{v \\in S(u, K) \\bigcap N(i)} |w_{uv}|}$$*E(r_u) 是用户u对他点评过的所有物品评分的平均值 S(u, K) 是和用户u兴趣最相似的K个用户的集合 N(i) 是对物品i点评过分数的用户集合 r_vi 是用户v对物品i的评分 E(r_v) 是用户v对他评分过的所有物品评分的平均值 w_uv 是用户u和v之间的相似度 ​ 基于物品的领域算法$$\\hat r_{ui} = \\overline r_i +\\frac {\\sum_{j \\in S(i, K) \\bigcap N(u)} w_{ij}(r_{uj} - \\overline r_i)}{\\sum_{j \\in S(i, K) \\bigcap N(u)} |w_{ij}|}$$*E(r_i) 是物品i的平均分，是所有用户对物品i点评过的分数的平均值 S(i, K) 是和物品i最相似的K个物品的集合 N(u) 是用户u点评多分数的物品集合 r_uj 是用户u对物品j的评分 w_ij 是物品i和j之间的相似度 2.1.5 计算相似度 余弦相似度（cosine similarity)$$w_{ij} = \\frac {\\sum_{u \\in U} r_{ui} r_{uj}} {\\sqrt{\\sum_{u \\in U}r_{ui}^2 \\sum_{u \\in U}r_{uj}^2}}$$ 皮尔逊系数（pearson correlation）$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_i) (r_{uj} - \\overline r_j)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_i)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_j)^2}}$$ 修正余弦相似度（adjust cosine similarity）（在MovieLens数据集上效果最好）$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_u) (r_{uj} - \\overline r_u)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_u)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_u)^2}}$$ 2.1.6 隐语义模型的矩阵分解模型 （Latent Factor Model)隐含类别模型、隐语义模型等，在本质上都是为了找出某一东西的潜在的主题或者分类。在推荐系统中，可以基于用户的行为利用隐语义模型，对item进行自动聚类，这样可以避免了人为分类的偏差。 举例说明： ​ 用户A喜欢看数学，历史和计算机的书籍 ​ 用户B喜欢看机器学习，编程语言和离散数学方面的书籍 ​ 用户C喜欢看大师的作品，比如专门看Knuth或者Jiawei Han的书籍 那么系统在对用户的喜好进行推荐的时候，需要找出同属于用户兴趣圈子的书籍。对于之前提到过的三个用户来说 ​ 用户A的圈子：数学、计算机、历史 ​ 用户B的圈子：这三本书可以同时分到计算机的圈子，但是离散数学却又可以分到数学圈子去 ​ 用户C的圈子：根据作者不同来划分圈子，那么这个圈子就和之前用户A、B的角度是不同的。 假设让人工来完成之前书籍的分类，那么经常会碰到划分的粒度不同，角度不同等情况。 同时，需要注意一下两点： 用户A对这三个类别书籍感兴趣，不代表不对其他类别的数据感兴趣 同一本书可以属于多个类别，所以每本书在每个类别里面都有一个权重，权重值越大，说明属于这个类别的可能性越高 那么，LFM是如何解决上面的几个问题的呢？ 回答上面的问题前，我们可以思考一下我们需要做的哪些工作。 实际上，我们可以将所有的User看做列，把所有的Item看做行，构建一个mxn的二维矩阵，如下图 最左边的R矩阵是一个user-item矩阵，矩阵值Rij表示的是用户i对item j的兴趣度，或者是评分。那么我们的评分预测，就可以转换成对这个矩阵中的某些值（缺失值）的预测，同时需要保证我们的预测值对于这个矩阵的扰动的最小的。（即补全之后矩阵的特征值和补全之前的特征值相差不大，具体见SVD分解） 而右边的两个P和Q矩阵就是LFM所做的，LFM算法从数据集汇总抽出若干个class，计算出所有user对这些class的感兴趣长度，即P矩阵。同时计算出所有item在这些class中的权重值，即Q矩阵。P矩阵作为user和item之间连接的桥梁，所以R可以表示为P矩阵和Q矩阵相乘。$$R_{UI} = P_U Q_I = \\sum_{k=1}^K = P_{U,k}Q_{k,I}$$以下是LFM的优点： 不需要关心矩阵P是怎么构建的，即不需要如何给物品进行聚类、划分等（角度，粒度等） Q矩阵中，对于一个item并不是明确给划分到某一个分类，而是计算这个item属于这些所有类别的概率，值越大可能性越高 同理，P矩阵中，对于一个user并没有限定在某些class中，而是计算这个user对于这些classes的感兴趣程度 虽然我们知道了LFM为我们做了哪些工作，但是我们还是不知道该如何求解出矩阵P和矩阵Q中的参数值，一般的做法是最优损失函数来求参数。 传统的SVD分解 传统方法中，给定一个user-item的矩阵R。 首先需要对评分矩阵R中的缺失值进行简单的补全，比如用全局平均值，或者用户/物品的平均值补全，得到补全后的矩阵R’ 得到补全后的矩阵R’，接着可以利用SVD分解，将R’分解成如下形式$$R’ = U^TSV \\\\R \\in R^{m n} \\\\U \\in R^{k m} \\\\V \\in R^{k n} \\\\S \\in R^{k k}$$U和V是两个正交矩阵，S是对角矩阵，对角线上的每一个元素都是矩阵的奇异值。 为了对R’进行降维，可以取最大的f个奇异值组成对焦矩阵Sf，并且找到这个f个奇异值汇中每个值在U、V矩阵中对应的行和列，得到Uf、Vf，从而得到一个降维后的评分矩阵：$$R_f’=U_f^TS_fV_f$$该方法的一些缺点： 在现实中，R矩阵基本上会是一个稀疏矩阵，即95%的数据是缺失的，同时该矩阵非常的大。一经补全，该矩阵就是一个稠密矩阵，储存开销非常的大 计算复杂度非常的高，特别是对于补全之后的稠密矩阵 ​ Funk-SVD分解，即 Latent Factor Model（LFM） http://sifter.org/~simon/journal/20061211.html 从矩阵的角度，将评分矩阵R分解成两个低纬度相乘：$$\\hat R = P^TQ \\\\R \\in R^{mn} \\\\P \\in R^{fm} \\\\Q \\in R^{fn}$$P、Q是两个降维后的矩阵，那么对于用户u对于物品i的评分的预测值可以^R(u, i) = ^r_ui，可以通过如下公式计算：$$\\hat r_{ui} =b_{ui} + \\sum_fp_{uf}q_{if} \\\\p_{uf} = P(u, f) \\\\p_{if} = Q(i, f)$$Simon Funk-SVD的思想是直接通过训练集中的观察值，利用*最小化RMSE学习P、Q矩阵。 损失函数的计算：$$C(p, q) = \\sum_{(u, i) \\in Train} (r_{ui} - \\hat r_{ui})^2= \\sum_{(u, i) \\in Train}(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})^2 + \\lambda(||p_u||^2 + ||q_i||^2) \\\\\\hat r_{ui} = \\mu + b_u + b_i + p_u^Tq_i$$要最小化上面的损失函数，可以利用随机梯度下降法。以下是简单的推导该公式 上面的cost function中欧两个参数p和q，首先 对他们分别求偏导，求出最快下降的方向$$\\frac {∂C} {∂p_{uf}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})q_{ik} + 2\\lambda p_{uk} \\\\\\frac {∂C} {∂q_{if}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})p_{uk} + 2\\lambda q_{ik}$$ 然后根据随机梯度下降法，需要将参数沿最快的下降方向前进，即可得到如下的递推公式：$$p_{uf} = p_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) q_{ik} - \\lambda p_{uk}) \\\\q_{if} = q_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) p_{uk} - \\lambda q_{ik})$$ 所以，执行LFM需要： 根据数据集初始化P和Q矩阵（如何初始化） 确定四个参数：分类书F，迭代次数N，学习速率α（α = 0.9） 和正则化参数*λ ​ 伪代码 1234567891011121314151617181920def LFM(user_items, F, N, alpha, lambda): #初始化P,Q矩阵 [P, Q] = InitModel(user_items, F) #开始迭代 For step in range(0, N): #从数据集中依次取出user以及该user喜欢的iterms集 for user, items in user_item.iterms(): #随机抽样，为user抽取与items数量相当的负样本，并将正负样本合并，用于优化计算 samples = RandSelectNegativeSamples(items) #依次获取item和user对该item的兴趣度 for item, rui in samples.items(): #根据当前参数计算误差 eui = eui - Predict(user, item) #优化参数 for f in range(0, F): P[user][f] += alpha * (eui * Q[f][item] - lambda * P[user][f]) Q[f][item] += alpha * (eui * P[user][f] - lambda * Q[f][item]) #每次迭代完后，都要降低学习速率。一开始的时候由于离最优值相差甚远，因此快速下降； #当优化到一定程度后，就需要放慢学习速率，慢慢的接近最优值。 alpha *= 0.9 Baseline Estimats 对比基线考虑到重口难调，有些user会给出比较高的分数，有些要去严格的users会给出比较低的分数，而有些质量好的商品会得到比较高的分数，质量差的分数较低。所以为了调整这些，引入了baseline estimate。比如为了估计某个用户u会给电影i打的评分：$$b_{ui} = u + b_u + b_i$$ $\\mu$ 是该物品的整体平均值 $b_u$ 是用户打分相对整体用户打分平均值的偏差 $b_i$ 是该物品相对整体平均值的偏差 举个例子：预测豆瓣用户小明给电影泰坦尼克号的评分 泰坦尼克号在豆瓣上的平均分数是3.7分(u) 泰坦尼克号的平均分数又比所有电影在豆瓣上的平均分数高0.5分($b_i$) 但是小明是个电影爱好者，比平均用户打分偏低0.3分($b_u$) 所以不考虑regularized的话，预测小明给泰坦尼克号的评分应该是 3.7 - 0.3 + 0.5 = 3.9 所以这里，得到 $b_u$ 和 $b_i$ 的值很重要 这里对$b_u$和$b_i$加入了penality，为了防止overfitting。 这里$r_ui$是我们训练数据中的已知rating，可以直接使用 $\\mu$是整体平均值，也可以根据训练数据计算出来 $\\lambda_2$和$\\lambda_3$是我们手动设置的参数，MovieLens数据上，20比较合适 R(u)和R(i)为用户urating过的物品的集合，和物品i被rating过用户的集合 所以我们可以根据这些数据，计算出$b_u$和$b_i$b_i$（注意维度）$$b_i = \\frac {\\sum_{u \\in R(u)}(r_{ui} - \\mu)} {\\lambda_2 + |R(i)|} \\\\b_u = \\frac {\\sum_{i \\in R(u)}(r_{ui} - \\mu - b_i)} {\\lambda_3 + |R(u)|}$$除了上面的方法，还有一种更为简便的方法计算$b_u$和$b_i$，就是直接使用user，item的rating的平均值估计$$b_u = \\frac {\\sum R(u)} {len(R(u))} \\\\b_i = \\frac {\\sum R(i)} {len(R(i))}$$ Neighborhood Models item-oriented algorithm: a rating is estimated using known rating made by the same user on similarity items. user-oriented algorithm: estimate unknown ratings based on recorded ratings of like minded users. Similarity measure between items Pearson correlation 计算物品i和j的相似度$$s_{ij} = \\frac {n_{ij}} {n_{ij} + \\lambda_2}ρ_{ij}$$ n_ij 表示都对物品i和j评分过的用户的数量 ρij是皮尔逊系数，通常取？？？ λ2 通常取100 预测评分$$\\hat r_{ui} = b_{ui} +\\frac{\\sum_{j \\in S^k(i; u)} s_{ij}(r_{ui} - b_{ui})}{\\sum_{j \\in S^k(i; u)}s_{ij}}$$ 在用户u所有评分过的物品中，找到相似度和i最高的k个物品（k-neighbors），用$S^k(i; u)$表示 但是这种算法还是有一些局限性，比如对于两个完全没有关系的物品之间的预测。或者是对于某些物品，最为相似的k个物品缺失，所以可以修正以上的公式（不是很明白这里）$$\\hat r_{ui} = b_{ui} + \\sum_{j \\in S^k(i; u)} \\theta^u_{ij}(r_{ui} - b_{ui}) \\\\\\{\\theta^u_{ij} | j \\in S^k(i; u)\\}$$","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Hadoop权威指南笔记（二）","date":"2017-05-04T14:13:45.000Z","path":"2017/05/04/Hadoop权威指南笔记（二）/","text":"1. Mapper 2. Reducer 3. MapReduce数据都是以 key-values pairs 的形式在Mapper和Reduce之间传递的 Mapper输出的 key-value pairs 应该和 Reducer输入的 key-value pairs 类型是一样的 在Reducer中，是 key-valueLists pairs 的形式 4. Deriver也就是初始化配置MR然后调用执行，一般可以写成如下形式： 123456789101112131415161718192021222324252627282930313233// 老APIpublic void run(String inputPath, String outputPath) throws Exception &#123; JobConf conf = new JobConf(WordCount.class); conf.setJobName(\"wordcount\"); // the keys are words (strings) conf.setOutputKeyClass(Text.class); // the values are counts (ints) conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(MapClass.class); conf.setReducerClass(Reduce.class); FileInputFormat.addInputPath(conf, new Path(inputPath)); FileOutputFormat.setOutputPath(conf, new Path(outputPath)); JobClient.runJob(conf);&#125;// 新APIpublic void run(String IN, String OUT) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1);&#125; 5. Data Flow在hadoop中所有的Mapper和Reducer都是独立工作的，这也是hadoop分布式能够稳定运行的原因之一(有利于容错处理)。在MapReduce整个过程中，只有一次数据相互交互，就是Mapper到Reducer这个过程。从Mapper输出的所有的intermediate data会被统一shuffle(必须要等所有MR执行完毕吗？)，然后同一个key的key-value pairs 会被分配到同一个reducer中去。 6. A Closer Look第五部分中，我们看到的是MR的宏观流程，具体的流程具体可以分为 Map -&gt; Combiner -&gt; Partitioner -&gt; Sort -&gt; Shuffle -&gt; Sort -&gt; Reduce(这里对shuffle的定义有点不同，个人认为从map的输出到reduce的输入这段过程可以称之为shuffle。同时根据官方文档，combiner是在mapper最终输出前多次调用的，以及在reducer里也有调用) 下图中，input files在进入到Mapper之前时，会对这些文件split，因为一个mapper一般是64MB或者128MB，当大于的时候需要对输入文件处理，然后传给RecordReaders，以key-value pairs的形式传给mapper。 对于所有Mapper的output，先会对其进行parition操作，也就是决定去哪一个Reducer。当确定好哪些Reducer，这些key-value paris 就会传入到该Reducer相应的分区，然后对齐进行排序。最后将sort好的key-valueLists 传入到Reducer进行处理。 7. ShuffleHadoop The Definitive Guide P197 Mapper 根据官方图，Mapper的output出来的key-values pairs会先进入到buffer memory(默认100MB大小)，但buffer到80%容量的时候，buffer里面的内容会spill到disk去，如果此时buffer还未慢的情况下，mapper继续输出到buffer，如果满了的话mapper会被block，直到可以写入。 在buffer中的内容spill到disk之前，还有一个partitioner的步骤。对这些即将写入到disk的内容分组，同一个key和同一个reducer的ker-value pairs会在一起。然后这些key-value pairs排序(sort)。如果此时我们定义了combiner function，在输出前，这些pairs会先combine，然后输出。也就是说combiner function是在partitioner之后? (Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer) 每当buffer达到那个threshold的时候，buffer里面的内容写入到disk中(注意是每个cluster自己的local disk，而不是HDFS)，此时会新建一个临时的spill文件，每次写入都会新建一个，然后这个map结束前这些spill files会被merge到一个partitioned和sorted的文件里去。除了之前partition的输出后调用了一次combiner一次之外，但合并这些个spill files( &gt;=3 )的时候，会继续调用combiner去合并同一个reducer里的同一个key的value，所以combiner在output file被生成之前，会被调用许多次，以减少之后io的次数。但当spill files只是1个或则两个的时候，并不会调用combiner。注意有些情况下combiner并不适合使用，比如求平均值。 而不同mapper生成的spill文件最终会被merge成 {key:[v1, v2, v3…], …}这种形式。(这里有疑问，博客图和官方图有点不一样。按照官方图的理解，一个mapper对应的是一个spill file，所以最终是多个spill files？还是这些spill files在传给Reducer之前会被merge成上述的list形式？一个cluster一个最终的output file)​ Reducer 之前mapper端的所有工作已经完成了，所有的mapper的output都已经被写入到了一个output文件里面去了。那么Reducer就是要把这个文件里面的key-valueLists pairs 分给不同的reducers，而这个过程称之为Fetch，就是将相应的key-valueList pairs 拉取到相应的reducers里面去。 在Reducer阶段，每个reducer会调用线程从多个不同的cluster的output file里面fetch数据，然后对这些数据merge。这里的过程和之前的mapper有点像。reducer也有一个buffer memory(通过JVM的heap size来设置)，fetch的数据会被传到里面(如果放得下)，当buffer达到threshold的值的时候，buffer里面的内容会被merge然后spill到disk里面去(实际上有多种形式存放这个文件，这里不讨论)，如果之前我们已经定义了combiner，这里combiner也会被调用。直到所有的mapper的output file都被fetch到一个文件里去，reducer会在输入前sort里面的内容(实际上merge的时候已经sort了)，然后这个已经排好序的file就会被传到reducer里面执行，最终输出到HDFS上。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Spark笔记","date":"2017-04-10T06:55:40.000Z","path":"2017/04/10/Spark笔记/","text":"1. Resilient Distributed Dataset (RDD)RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators. RDD是一个容错的、并行的数据结构，可以让用户显示地将数据存储到磁盘和内存中，并能控制数据的分区。 Resilient RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Formally, an RDD is a read-only, partitioned collection of records. Rdds can be created throught deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel. There two ways to create RDDs. Parallelizing parallelizing an existing collection in your driver program Referencing reference a dataset in an external storage system, such as a shared file system, HDFS, HBase, or ant data source offering a Hadoop Input Format Spark提供了RDD上的两类操作：transformation 和 action Transformation: return a new RDD 当对RDD进行transformation操作的时候，这些操作不会立刻就执行，而是将这些操作记录下来。如果有多个transformation操作时，每一次变换都是一个接着一个，此时就形成了一个有向无环图。这个图就是数据容错的关键所在。如果出现数据丢失的时候，只需要查找这个图就能根据丢失的RDD进行数据恢复 Action: evaluates and returns a new value 当对RDD做action操作的时候，这类action一般作用是返回一个值或者数组等，或者是将数据持久化到磁盘当中。此时执行action，会真正的提交job，执行之前的transformation记录的DAG。执行DAG策略中，会有多个stage，每一个stage钟有多个task，这些task就会被分配到各个nodes进行执行 The difference between flatMap and Map flatMap Example map: 对RDD每个元素转换，将函数用于RDD中的每个元素，将返回值构成新的RDD。flatMap: 对RDD每个元素转换, 然后再扁平化（即将所有对象合并为一个对象）。将函数应用于rdd之中的每一个元素，将返回的迭代器的所有内容构成新的rdd Example1： 123456// data 有两行数据，第一行 a b c，第二行 1 2 3scala&gt; data.map(line1 =&gt; line1.split(\",\")).collect()res11: Array[Array[String]] = Array(Array(a, b, c),Array(1, 2, 3))scala&gt; data.flatMap(line1 =&gt; line1.split(\",\")).collect()res13: Array[String] = Array(a, b, c, 1, 2, 3) Example2： 1234567891011scala&gt; val rdd = sc.parallelize(List(\"coffee panda\",\"happy panda\",\"happiest panda party\"))scala&gt; rdd.map(x=&gt;x).collectres0: Array[String] = Array(coffee panda, happy panda, happiest panda party) scala&gt; rdd.map(x=&gt;x.split(\" \")).collectres1: Array[Array[String]] = Array(Array(coffee, panda), Array(happy, panda), Array(happiest, panda, party))// 相比之前，flatMap将几个array合并成了一个arrayscala&gt; rdd.flatMap(x=&gt;x.split(\" \")).collectres2: Array[String] = Array(coffee, panda, happy, panda, happiest, panda, party) 2. References 理解Spark的核心RDD Spark RDD API Examples","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.com/tags/Spark/"}]},{"title":"字符串搜索算法 - BM","date":"2017-04-02T04:28:44.000Z","path":"2017/04/02/字符串搜索算法-BM/","text":"当我们要在某一个文本中要匹配某一个字符串的时候，我们最简单的方法就是一个一个匹配，也就是从头开始匹配文本和字符串，但发现不同的时候，就把整个字符串右移一位，然后从头重新和文本比较。当全部相同的时候，则右移字符串的长度。在之前的 KMP 算法中，我们是找到尽可能右移的最大位数，而不是一位一位的移动，这样相比原先效率已经提高了很多了。但在大部分的编辑器中，“查找”功能使用的不是 KMP ， 而是 BM 算法。 1. Boyer-Moore Algorithm和 KMP 不同的是，BM 是从 Pattern P 倒着匹配上来的 首先定义两个预处理的方法 坏字符 (Bad Character Heuristic) 当文本 T 中的某个字符跟 Pattern P 的某个字符不匹配时，我们称文本 T 中的这个不匹配的字符为坏字符。 好后缀 (Good Suffix Heuristic) 当文本 T 中的某个字符跟 Pattern P 的某个字符不匹配时，我们称文本 T 中的已经匹配的字符串为好后缀。(因为算法从尾巴到头部比较) 好后缀的位置以最后一个字符为准。假定”ABCDEF”的”EF”是好后缀，则它的位置以”F”为准，即5（从0开始计算）。 如果”好后缀”在搜索词中只出现一次，则它的上一次出现位置为 -1。比如，”EF”在”ABCDEF”之中只出现一次，则它的上一次出现位置为-1（即未出现）。 如果”好后缀”有多个，则除了最长的那个”好后缀”，其他”好后缀”的上一次出现位置必须在头部。比如，假定”BABCDAB”的”好后缀”是”DAB”、”AB”、”B”，请问这时”好后缀”的上一次出现位置是什么？回答是，此时采用的好后缀是”B”，它的上一次出现位置是头部，即第0位。这个规则也可以这样表达：如果最长的那个”好后缀”只出现一次，则可以把搜索词改写成如下形式进行位置计算”(DA)BABCDAB”，即虚拟加入最前面的”DA”。 按照正常比较方法，当发现一个坏字符的时候，会出现一下三种情况 坏字符完全不出现在 Pattern P 中，那么就可以完全跳过这个字符，因为我们根本不可能匹配到这个字符。 坏字符出现在还未比较的前缀当中 (只出现在前缀，出现在前缀和后缀) 此时将 Pattern P 右移对齐这两个字符，然后从尾部继续比较 坏字符只出现在之前的好后缀当中（也就是已经比较过了） 对这种情况不做处理 根据上面几种情况，我们可以总结出以下坏字符规则： 后移位数 = 坏字符的位置 - 搜索词中的上一次出现位置 这里有两种特殊情况 如果坏字符不存在于 Pattern P 中，则最后一次出现的位置为 -1。 如果坏字符在 Pattern P 中的位置位于失配位置的右侧，则此启发法不提供任何建议。 除了坏字符，我们同样可以利用好后缀来提高查找效率 模式后移位数 = 好后缀在模式中的当前位置 - 好后缀在模式中最右出现且前缀字符不同的位置 举个栗子计算： 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 坏字符的移动 此时坏字符的位置是4，坏字符 I 不在字符串当中，移动的个数为： 4 - (-1) = 5 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 好后缀的移动 此时好后缀是 MPLE，其中 PLE 出现在头部，最右位置为 (01) 2。 此时好后缀的字符是8(以最后一个为准)，移动的个数为：8 - 2 = 6 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 相比之下，我们可以发现好后缀的查找效率更高。 如何计算好后缀我们之前已经提过了，继续以上面最后一个例子讲解。 MPLE : 未出现，最右出现的位置为 -1； PLE : 未出现在头部，最右出现的位置为 -1； LE : 未出现在头部，最右出现的位置为 -1； E : 出现在头部，补充虚拟字符 ‘MPL’E，前缀字符为空，最右出现的位置为 0； 此时，所有的”好后缀”（MPLE、PLE、LE、E）之中，只有”E”在”EXAMPLE”还出现在头部，所以后移 6 - 0 = 6位。 如果之前我们只利用坏字符的话，移动的位数只有 2 - (-1) = 3位 所以我们最终的移动位置，是在这两者之间取最大值来移动。 所以在查找之前，我们需要预计算生成坏字符规则表 和 好后缀规则表，到时候只需要查表就可以了。继续上面的例子。 根据上面计算结果，我们将 Pattern P 右移6位。然后重新从尾部开始比较 文本中P不匹配，此时只能使用坏字符规则，P上一次出现的位置为4，则移动6-4=2位 移动2位后，发现文本中的E 与 Pattern P中的E 匹配，则继续倒序比较，直到发现全部匹配，则匹配到的第一个完整的模式 P 被发现。 继续下去则是依据好后缀规则计算好后缀 “E” 的后移位置为 6 - 0 = 6 位，然后继续倒序比较时发现已超出文本 T 的范围，搜索结束。 2. References 字符串匹配的Boyer-Moore算法 Boyer-Moore 字符串匹配算法","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"PostgreSQL复习笔记","date":"2017-03-23T23:38:55.000Z","path":"2017/03/24/PostgreSQL复习笔记/","text":"1 常见问题1.1 匹配一致的编码规则123SET character_set_client=&apos;utf8&apos;; -- gbkSET character_set_connection=&apos;utf8&apos;; -- gbkSET character_set_results=&apos;utf8&apos;; -- gbk 1234-- 创建数据库时候设置CREATE DATABASE `test`CHARACTER SET &apos;utf8&apos;COLLATE &apos;utf8_general_ci&apos;; 1.2 如何理解索引索引字面上理解就是对数据所建立目录，它可以加快我们的查询速度，但是同时也降低了增删改的速度。 创建原则 不要过度使用索引 最好在查询频繁的列上使用索引 如果构建索引，这一列尽量是离散值，而不要过于连续的区间 索引的类型 普通的索引 index 唯一的索引 unique index 一张表上，只能有一个主键，但是可以有一个或是多个唯一索引 主键索引 primary key 不能重复 12-- 查看一张表上的所有索引show index from TABLE_NAMES; 1.3 模糊查询 % 匹配任意字符 _ 匹配单个字符 举个栗子 1234567891011SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC%&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC_&apos;; 1.4 理解 COUNT 见3.1 聚集函数 1.5 理解 UNION 和 UNION ALL UNION 用于合并两个或是多个SELECT语句的结果集 注意： SELECT语句必须拥有相同的数量的列 列的需要拥有相似的数据类型 每条SELECT语句中的列的顺序必须是一致的 结果不允许有重复，否则使用 UNION ALL 123456SELECT mid, sex, ageFROM TABLE_1UNION -- UNIAON ALL -- 允许有重复的值出现在结果集中SELECT mid, sex, ageFROM TABLE_2;-- ORDER BY mid; -- 可以对其结果进行排序，注意的是排序只是针对合并后的结果集排序 1.6 理解 JOIN（左链接，内链接和外链接） 不同的几种JOIN类型，以及之间的差异 JOIN：如果表中至少有一个匹配，则返回行 LEFT JOIN：即使右表中没有匹配，也从左边返回所有的行 RIGHT JOIN：即使左表中没有匹配，也从右表中返回所有的行 FULL JOIN：只要其中一个表存在匹配，就返回行 INNER JOIN 平常我们需要链接两个表的时候，可以用以下方法 123SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P 同时，我们也可以使用JOIN来实现上面的语句 12345SELECT Person.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName LEFT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意到上面的，左边所有的行都返回了，即使没有出现在右表当中，没有的值为NULL RIGHT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意，即使左边没有全部匹配到右边，依然在最后的OrderNo中，返回了右表所有的行数，没有的值为NULL 1.7 理解 HAVING 见3.2 在SQL中增加HAVING子句的原因是由于WHERE中无法使用聚合函数 2 数据库的基本操作2.1 表的使用 使用列约束建表 12345678CREATE [TEMPORARY] TABLE 表名 ( -- TEMPORARY 建立一张临时的表col_not_null INT NOT NULL, -- 列名 类型 &#123;约束1 约束2 ...&#125;col_unique INT UNIQUE,col_prikey INT PRIMARY KEY, -- NOT NULL + UNIQUE 主键col_default INT DEFAULT 42,col_check INT CHECK(col_check &lt; 42)col_ref INT REFERENCES -- 约束这个值必须是另一独立的表的某个列中的某个值); 使用表级约束建表 1234567CREATE TABLE 表名 ( myKey_1 INT, myKey_2 INT, myString varchar(15), CONSTRAINT cs1 CHECK (myString &lt;&gt; &apos;&apos;), -- 不能位空字符串 CONSTRAINT CS2 PRIMARY KEY (myKey_1, myKey_2)); -- 修改表结构 添加新列 1ALTER TABLE 表名 ADD COLUMN 列名 类型; 重命名新添加的列 1ALTER TABLE 表名 RENAME COLUMN 列名 TO 新列名; 改变一些约束和其他规则 12ALTER TABLE 表名 DROP CONSTRAINT cs1; -- DROP约束ALTER TABLE 表名 ADD CONSTRAINT cs3 UNIQUE(列名); --添加新的约束 修改列的类型 1ALTER TABLE 表名 ALTER 列名 TYPE 新类型; 重命名表名 1ALTER TABLE 表名 RENAME TO 新表名; 使用临时表 临时表的功能基本和表是差不多的，区别在于当你的会话结束时，你与数据库连接断开后，临时表会自动被删除。 键的约束 作为一个列的约束的外键（列约束） 1234567CREATE TABLE 表名 ( ... ... customer_id INTEGER NOT NUll REFERENCES customer(customer_id), -- 关联到customer表 ... ...);-- REFERENCES 外表名(外表名中的列) 表级约束 123456CREATE TABLE 表名( ... ... customer_id INTEGER NOT NULL, ... ... CONSTRAINT 表名_列名_fk FOREIGN KEY(列名) REFERENCES 外表名(外面中的列)) 注意：比较推荐的是使用表级约束，而不是混和私用表级和列级约束 ​ 约束名表名_列名_fk允许外面更容易定位错误资源 2.2 视图 建立视图 123CREATE VIEW 视图的名字 AS select系列语句;-- 例子CREATE VIEW item_price AS SELECT item_id, description, sell_price FROM item; 当视图建立好的时候，我们可以像使用表一样来查询这个视图，可以使用SELECT或WHERE语句等。 每次在视图中执行SELECT时，数据都会被重建，所以数据总是最新的，而不是一个在视图被建立的时候冻结的拷贝。也就是当与之相关的表的数据发生该表的时候，VIEW里面的数据也随之改变，而不是储存了建立VIEW的时候的拷贝对象。或者也可以理解类似指针指向原先的表，当原先的表发生变化，这边的数据自然而然的就能够读取出来。 当然，SELECT语句是可以在多个不同的表中提取数据的。 删除和替换VIEW 12DROP VIEW 名字; -- 不影响我们已有的数据CREATE OR REPLACE VIEW 名字 AS 新的select系列语句; 一些与VIEW常用的指令 12\\dv -- 查看当前数据库中的所有的VIEW\\d VIEW的名字 -- 查看具体的某一个VIEW的结构 2.3 INSERT语句 基本插入语句 123INSERT INTO 表名 VALUES (每列的值的列表);INSERT INTO customer VALUES(18, &apos;Mr&apos;, &apos;Jeff&apos;, &apos;Baggott&apos;, &apos;Midtown Street A\\\\33&apos;, &apos;Milltown&apos;, &apos;MT9 8NQ&apos;, &apos;746 3956&apos;); PS：这种操作很危险，SQL注入攻击 推荐的安全方法 12345INSERT INTO 表名(列名的列表) VALUES(跟之前的列的列表对应列的数值);INSERT INTO customer(customer_id, title, fname, lname, addressline, ...)VALUES(19, &apos;Mrs&apos;, &apos;Sarah&apos;, &apos;Harvey&apos;, &apos;84 Willow Way&apos;, ...); PS：避免在插入数据的时候为serial类型的数据提供数值，因为这个是系统自动添加的 访问序列生成器 序列生成器总 是被命名为&lt;表名&gt;_&lt;列名&gt;_seq 123currval(&apos;序列生成器名&apos;);nextval(&apos;序列生成器名&apos;);setval(&apos;序列生成器名&apos;, 新的值); 插入空值 123INSERT INTO customer VALUES(16, &apos;Mr&apos;, NULL, &apos;Smith&apos;,&apos;23 Harlestone&apos;, &apos;Milltown&apos;, &apos;MT7 7HI&apos;, &apos;746 3725&apos;); 使用 \\copy 命令 步骤 先生成如下格式的数据 ​ 再生成如下格式的数据，保存成.sql拓展名的文本文件 ​ 使用\\copy命令导入数据 ​ PS：SQL 里头的 COPY 命令有一个优点：它明显比\\copy 命令快，因为它直接通过服务器进程执行。\\copy 命令是在客户进程中执行，有可能需要通过网络传输所有数据。而且 COPY 在发生错误的时候会更可靠。除非你有大量的数据，否则区别不会太明显。 2.5 从数据库中删除数据 DELETE语句 语法类似于UPDATE语句 1DELETE FROM 表名 WHERE 条件; TRUNCATE语句（不推荐，因为不安全） TRUNCATE语句是把表中所有的数据都删除，但是保留这张表的结构，也就是说最后剩下了一张空表，所有的行都被删除了。 1TRUNCATE TABLE 表名; DROP语句 DROP语句就是删除了整张表的内容，包括表的结构。DROP完之后，这张表就是不存在的了 1DROP TABLE 表名; 2.6 修改数据库中的数据 UPDATE语句 1UPDATE 表名 SET 列名 = 值 WHERE 条件; 1UPDATE customer SET town = &apos;Leicester&apos;, zipcode = &apos;LE4 2WQ&apos; WHERE 一些条件; 如果没有WHERE子句的话，会导致表中的很多甚至是所有的行都被同时更新了 通过另一个表更新 1UPDATE 表名 FROM 表名 WHERE 条件; 3 高级数据选择3.1 聚集函数 Group By and count(*) 错误使用 1SELECT count(*), town FROM customer; 正确使用 1SELECT count(*), town FROM customer GROUP BY town; 结果是获得一个城镇的列表以及每个城镇的客户数量（count(*)) 同时我们也可以用两个columns name在GROUP BY中，然后用ORDER BY指定排列顺序。没有GROUP BY的话按照GROUP BY中的town，lname排序 Having Having是一种用于聚集函数的WHERE从句，我们使用HAVING来约束返回的结果为针对特定的聚集的条件为真的行，比如count(*) &gt; 1 PS：聚集函数无法在WHERE从句中使用，只能用在HAVING从句中 举个栗子： 选出有超过一个客户的城镇，在里使用一个HAVING从句来约束大一的行 SELECT中从句的优先度 1234567SELECTFROM WHEREGROUP BYHAVINGORDER BY -- DESC 下降LIMIT -- 用于限制rows是行数 mysql count(Column_Name) count(*) 统计所有的行 count(column_name) 统计所这个列中值不是NULL的行 count(Distinct column) 只统计这个列中唯一的情况，不重复统计 min min 函数使用一个列名做参数且返回这个列中最小的值。对于 numeric 类型的列，结果应该和预期一 样。对于时态类型，例如 date 的值，它返回最小的日期，日期既可以是过去也可以是未来。对于变长的字符串（varchar 类 型），结果可能和预期有点不同：它在字符串右边添加空白后再进行比较。 min 函数忽略 NULL 值。忽略 NULL 值 是所有的聚集函数的一个特点，除了 count(*)（当然，是否一个电话号码是最小值又是另一个问题了 PS：小心在 varchar 类型的列中使用 min 或者 max，因为结果可能不是你预期的。 max sum Sum 函数使用一个列名作为参数并提供列的内容的合计。和 min 和 max 一样，NULL 值被忽略。 和 count 一样，sum 函数支持 DISTINCT 变体。你可以让它只统计不重复值的和，所以多条值相同的行只会被加一 次 avg 我们要看的最后一个聚集函数是 avg，它使用一个列名做参数并返回这个列数值的平均值。和 sum 一样，它忽略 NULL 值。这里是一个示例 ​ 3.2 子查询 问题一 找到价格比平均价格高的商品项目 方法一（土方法） 方法二（用嵌套WHERE从句） 问题二 找到那些成本高于平均成本但售价低于平售价的产品 方法一（土方法） 方法二（用嵌套WHERE从句） 问题三 - 返回多行记录的子查询 之前的两个问题中，WHERE中的子查询中的SELECT字句返回的最后只有一个值——因为用了count()聚集函数。如果WHERE中的SELECT子句返回多个结果值呢？ 答案是用 WHERE column_name IN (RESULTS) 当然也可以使用NOT IN 来排出选项 3.3 相关子查询​ 在之前的例子中，这里的两个SELECT实际上是不相关的，也就是在内部的SELECT的结果基础上，外部SELECT再做继续查询 ​ 但是相关子查询则是内外的SELECT中，表与表之间是有关系的 格式 PS：建议在相关子查询中使用表的别名 3.4 UNION链接 格式 12345SELECT town FROM tcustUNIONSELECT town FROM customer;SELECT town, zipcode FROM tcust UNION SELECT town, zipcode FROM customer; PS：UNION 连接的使用有一些限制。你要连接的两个从两个表中查找列表的列必须有相同列数，而且选择的每个列必须都有相兼容的类型。 这个查询，虽然非常无意义，但是是有效的，因为 PostgreSQL 可以连接这两个列，即使 title 是一个固定长度的列而 town 是一个变长的列，因为他们都是字符串类型。例如如果我们尝试连接 customer_id 和 town，PostgreSQL 会告诉我们 无法做到，因为这两个列的类型不同。 3.5 自连接 3.6 外链接4. 表的管理​ 5. 事务和锁6. References","tags":[{"name":"SQL","slug":"SQL","permalink":"http://chenson.com/tags/SQL/"}]},{"title":"Machine Learning - Linear Regression and Logistic Regression","date":"2017-03-17T01:06:30.000Z","path":"2017/03/17/Machine-Learning-Linear-Regression-and-Logistic-Regression/","text":"1. 三要素当一开始接触Andrew在Coursera上的ML公开课的时候，对线性回归和逻辑回归这两种模型有个大体的认识。但是在上完cs229的前三节课，初步了解了这两种模型背后的数学模型，Linear Regression和Logistic Regression背后的概率分布，了解到了这两种概率分布其实只是exponential family中的特例。但同时也开始对一些概念性的东西感觉很模糊，所以觉得有必要好好整理一下这部分的内容。 1. 1 Hypothesis首先对于样本数据，输入x和输出y之间是通过Target function在转换的，也就是 Target function f(x) = y。但是我们并不知道这个f(x)都是怎样的，所以我们假设了这么一个Hypothesis function去模拟这个Target function，使得我们用同样的输入x会的一个预测值y’，使得这个y’不断逼近真实值y。 Linear Regression$$H(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx$$ Logistic Regression$$H(x) = g(\\theta^Tx) = \\frac 1 {1 + e^{-\\theta^Tx}}$$等价于（即log odds，logit）$$ln \\frac y {1 - y} = \\theta^T x = ln \\frac {p(y=1| x; \\theta)} {p(y=0| x; \\theta)}$$ 1. 2 Cost functionCost function呢，实际上也可以叫做Error function，就是用我们上面假设的Hypothe function所预测出来的值y’和真实值y之间的误差。而我们需要做的是根据假设出的Hypothesis function，取一个合适的权重值，即theta的值，使其取的一个较低的cost，也就是这预测值与真实值之间的误差最小。 Ordinary Least Squares (Square Loss Function) 常用的方法是最小二乘法 $$J(\\theta) = \\frac 1 2 \\sum_{i=i}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$ ​ 当然我们也可以从概率的角度来理解这个问题$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$​ 这里的ϵ是我们预测值与实际值之间的误差，这个问题我们会留到后面重点讲解。 0-1 Loss Function Absolute Loss Function Log Loss Function 1.3 Algorithm至于怎么使得上面的cost function最小呢，因为对于某些数据，其features有成千上百个，我们很难去找到这个最小的极值点，使得cost function最下，所以这个Algorithm就是用来找cost function的最小值的。常用的方法有如下 Gradient Descent 在梯度下降中，我们采用的是 LMS update rules(Least Mean Squares)$$\\theta_j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}= \\alpha e^{(i)} x^{(i)}$$ 当我们的预测值与实际值之前的误差ϵ很小时，我们就只需要对θ做出很小的调整，反之，说明当前的θ不对，需要调整的幅度比较大。直到最后收敛为止。 上面Repeat中的步骤实际是等同于cost function对θ求导的过程，所以为了保证收敛的效果，cost function应该是要 convex fuction，就不会导致停留在了local optim点。 推导过程如下： ​ 可视化后大概的过程如下： 根据对于哪些θ求导，Gradient Descent还可以继续分成不同的几种方法 Batch Gradient Descent Stochastic Gradient Descent Mini Batch Gradient Descent SGD with mini-batch 问题：为什么下降是 - ，θ大J一定大吗？ Normal Equation (linear regression) 推导过程比较复杂，需要的数学知识比较多，这里只给出结论。想要看具体推导过程的还说看cs229的第二节课吧 ：）(cs229-notes-1, p11)$$\\theta = (X^TX)^{-1}X^T\\vec y$$ ​ Newton BGFS Simulated Annealing 2. Generalized Linear Model (GLM) 广义线性模型之前我们在cost function中提到过，我们可以从概率的角度来理解误差这个问题。对于Linear Regression和Logistic Regression，我们都可以假设：$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$这里误差ϵ假设为IID (independently and identically distributed) 2.1 Linear Regression在Linear Regression中，y是连续的值，所以误差ϵ也是一个连续的值。假设误差ϵ是符合Gaussian Distribution (Normal Distribution)，所以有 Gaussian Distribution $$y | x; \\theta ∼ N (μ, σ^2)$$ Probability of error ​ σ实际上是不影响概率的分布的，所以假设σ = 1，所以这里可以忽略了。因此也就是等同于如下 Likelihood$$L(\\theta) = L(\\theta; X, \\vec y) = p(\\vec y | X; \\theta)$$ ​ 以上是在给定输入x和权重θ下，我们的预测值是真实值y的概率，所以这个概率呢，当然是越高越好啦。我们就是要想办法去 maximum likelihood。 ​ 对这个概率取个log（不影响结果），有 ​ 可以看到最终的式子里面，我们就是要求cost function的最小值。 2.2 Logistic Regression在Logistic Regression中， y是离散的值 {0, 1}，所以误差ϵ也是一个离散的值 {0, 1}。假设误差ϵ是符合Bernoulli Distribution，所以有 Bernoulli distribution Probability of error 把误差ϵ代入到上面的bernoulli function，可得 Likelihood ​ 同样，对上面去log之后有 ​ 同样，我们尽量要maximize the likelihood，就相当于要最小化后面的那部分（cost function）。这里可以用Gradient Ascent算法来求最大值，但是和Gradient Descent不一样（为什么）$$\\theta := \\theta + \\alpha∇_\\theta l(\\theta)$$​ 注意这里是加号，在求cost function的最小值时，用的是减号。 ​ 对其求导可得 ​ 所以有 ​ Digression Perception ？ ？ ？ 2.3 GLM似乎到现在，讲了半天都也没讲什么是广义线性模型，实际上呢，上面我们已经从误差概率的角度上来分析了线性回归和逻辑回归两种特例，因为他们误差服从的概率分布都是属于Exponential Family中的一种。 The Exponential Family η 被称作natural parameter，它是指数分布族唯一的参数T(y) 被称作sufficient statistic，很多情况下T(y)=y a(η) 被称作 log partition functionT函数、a函数、b函数共同确定一种分布 那么这个模型和上面我们提到过的Gaussian Distribution 和Bernoulli Distribution有什么关系呢？其实上面的这几个参数取不同的值的时候，即可得到不同的分布模型 Gaussian Distribution Bernolli Distribution 2.4 如何构建一个GLM模型在上面我们只是看到了一个通用的GLM概率模型 实际上对于构建这么一个概率模型，需要作出三个假设作为前提条件： p(y | x; θ) ∼ ExponentialFamily(η). 对于给定的输入x，θ和输出y需要服从某一种指数分布，这个指数分布由η 决定的 对于给定的输入x，预测T(y)的值，且经常T(y) = y。而我们是预测是H(x) 需要满足 H(x) = E[y|x] 对于自然参数η和输入x之间，需要存在相关性关系的，即：η = θT x","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"字符串搜索算法 - KMP","date":"2017-03-15T06:48:47.000Z","path":"2017/03/15/字符串搜索算法-KMP/","text":"1. 理解一：部分匹配表+已匹配数字1.1 Partial Match Table 上来先上个结论，这个先暂时不管怎么生成，用于KMP表的移动。 移动位数 = 已匹配的字符数 - 对应的部分匹配值 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是6 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位。 匹配到了第三个字符C，C的前一个字符B在上表中的值是0，已经匹配的字符数是2 所以移动的位数是 2 - 0 = 2，将搜索词向后移动2位 因为第一个字符不匹配，就将整个字符串向后移一位 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是6 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位 逐个比较，直到完全匹配 如果还需要继续搜索的话，D在上表中的值为0，匹配到的个数为7，移动的位数= 7 - 0 = 7，将整个字符串往后移动7位。接着就是重复之前的比较步骤了。 1.2 计算 Partial Match Table ​ 这里需要理解两个概念：前缀和后缀 ​ “前缀” 指除了最后一个字符以外，一个字符串的全部头部组合； ​ “后缀” 指除了第一个字符以外，一个字符串的全部尾部组合。 ​ 而我们需要的Partial Match Table就是前缀和后缀的最长共有元素的长度 ​ 继续以上面的例子讲解 “A”的前缀和后缀都为空集，共有元素的长度为0；“AB”的前缀为[A]，后缀为[B]，共有元素的长度为0；“ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；“ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；“ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1；“ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2；“ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。 了解了KMP的原理之后，来看一下代码该怎么写。 举个栗子： Text = a b a c a a b a c c a b a c a b a a b b Pattern = a b a c a b 根据前面的Partial Match Table, 我们可以算出Pattern的这个表 P a b a c a b steps 0 0 1 0 1 2 此时我们用两个指针 i 和 j 来表示 Text 和 Pattern 中的字符。 当 T[ i : i + j ] == P[ 1 : j ] 的时候，就是 Text 中包含了我们需要查找的 Pattern 先让 i 和 j 都从 1 开始（python代码中从0开始） 当T[ i ] = P[ j ]的时候，此时指针在 Text 和 Pattern 上都往前各走一步，即j+1，i+1 当 i = 6，j = 6 的时候，我们可以看出T[ i ] != P[ j ]，此时 j 就不能再继续往前走了，需要退回去几步。 那么到底是几步呢，经过上面查表，此时匹配到5，重复的字符串个数为1，意思是对于这个字符串 abaca，abaca 和 abaca 中有一个重复了，我们就不需要再比较这个，跳过这个字符，移动的个数为 6 - 1 = 5，将字符串 Pattern 向前挪5位，新的 j 就等于1了，然后重复之前的步骤。 1.3 Python代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091DEBUG = TrueDEBUG = False# 生成 partial match tabledef PMT(sent): length = len(sent) result = [0] for i in range(2, length + 1): sub_sent = sent[0 : i] sub_len = len(sub_sent) surfix = [] prefix = [] for j in range(sub_len): prefix.append(sub_sent[0 : j]) surfix.append(sub_sent[j + 1: sub_len ]) prefix.remove('') surfix.remove('') all_fix = list(set(prefix + surfix)) max_len = 0 com_str = '' for fix in all_fix: if fix in prefix and fix in surfix: if len(fix) &gt; max_len: max_len = len(fix) com_str = fix result.append(max_len) return result# KMP算法def KMP(text, pattern): if (len(text) &lt; len(pattern)): return 'Not Match' pmt = PMT(pattern) if DEBUG: print('Current Pattern is = ', pattern) print('Partial Match Table = ', pmt) i = 0 j = 0 while(i &lt; len(text)): if (text[i] == pattern[j]): if (j == (len(pattern) - 1)): return 'Match, the position in text is: &#123;&#125; - &#123;&#125; = &#123;&#125;'.format(i - j, i, text[i-j: i]) i += 1 j += 1 continue else: if j &gt;= 1: j = pmt[j - 1] else: i += 1 return 'Not Match'# 测试函数def test(text, pattern): result = KMP(text, pattern) print('Resutl = ', result) print('Pattern =', pattern) if DEBUG: for i in range(len(text)): print('&#123;:3s&#125;'.format(str(i)), end='-') print('') for i in range(len(text)): print('&#123;:3s&#125;'.format(text[i]), end='-') print('') print('\\n')# 测试部分text = 'BBC ABCDAB ABCDABCDABDE'pattern = 'ABCDABD'test(text, pattern)text = 'a b a c a a b a c c a b a c a b a a b b'pattern = 'a b a c a b'test(text, pattern)text = 'BBC ABCDAB ABCDABCDABE'pattern = 'ABCDABD'test(text, pattern)text = 'BBC AB'pattern = 'ABCDABD'test(text, pattern)text = 'ABCDABD'pattern = 'ABCDABD'test(text, pattern) 1.4 测试结果12345678910111213141516171819202122232425Text = BBC ABCDAB ABCDABCDABDEPattern = ABCDABDResutl = Match, the position in text is: 15 - 21 = ABCDABText = a b a c a a b a c c a b a c a b a a b bPattern = a b a c a bResutl = Match, the position in text is: 20 - 30 = a b a c a Text = BBC ABCDAB ABCDABCDABEPattern = ABCDABDResutl = Not MatchText = BBC ABPattern = ABCDABDResutl = Not MatchText = ABCDABDPattern = ABCDABDResutl = Match, the position in text is: 0 - 6 = ABCDAB[Finished in 0.1s] 2. 理解二：部分匹配表+idx2.1 Partial Match Table首先生成和上面一样的前缀后缀表(0-base 和 1-base) 2.2 Example 0-base 找到原文和搜索词不同的那个字符里面的匹配值，然后把搜索字符串右移到idx=匹配值的位置，过程如下图 1-base 与0-base不同的是，找的不是最后一个不同的字符 而是最后一个相同的字符里面的匹配值，然后把搜索字符串右移到idx=匹配值的位置，过程如下图 3. References 阮一峰 - 字符串匹配的KMP算法","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"Hadoop权威指南笔记（一）","date":"2017-03-06T13:44:33.000Z","path":"2017/03/06/Hadoop权威指南笔记（一）/","text":"1. Hadoop1.1 初识Hadoop非常好的Tutorial 在学习hadoop之前，我觉得有必要了解一下hadoop的基本构成以及一些术语。 Block HDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks. By making a block large enough, the time to transfer the data from the diskcan be made to be significantly larger than the time to seek to the start of the block.Thus the time to transfer a large file made of multiple blocks operates at the disk transferrate.A quick calculation shows that if the seek time is around 10 ms, and the transfer rateis 100 MB/s, then to make the seek time 1% of the transfer time, we need to make theblock size around 100 MB. The default is actually 64 MB, although many HDFS installationsuse 128 MB blocks. This figure will continue to be revised upward as transferspeeds grow with new generations of disk drives.This argument shouldn’t be taken too far, however. Map tasks in MapReduce normallyoperate on one block at a time, so if you have too few tasks (fewer than nodes in thecluster), your jobs will run slower than they could otherwise. Node 简单的说就是一台主机，一台电脑。在hadoop中，有NameNode, DataNode, Secondary NameNode, JobTracker Node, TaskTracker Node, CheckpointNode 和 BackupNode。对一个cluster，NameNode只能有一个，DataNode可以有多个 Rack 中文机柜/机架，就是用来存放node的storage，通常一个rack有几十个nodes组成，这些nodes存放在同一个机柜，连接一个交换机 A Node is simply a computer. This is typically non-enterprise, commodity hardware for nodes that contain data. Storage of Nodes is called as rack. A rack is a collection of 30 or 40 nodes that are physically stored close together and are all connected to the same network switch. Network bandwidth between any two nodes in rack is greater than bandwidth between two nodes on different racks.A Hadoop Cluster is a collection of racks. 1.2 Major Components Distributed Filesystem — hadoop中是HDFS MapReduce 1.5 Install configuration123456# 待整理hadoop fshadoop dfs# fs refers to any file system, it oucld be local or HDFS# but dfs refers to only HDFS file system 2. MapReduce2.1 初识MapReduce 整个过程可以分为三个阶段，Input， MapReduce and Output 在input和output阶段，数据是存在HDFS文件系统中，其系统的block size大小默认是64/128MB。 在MapReduce中，又可以分为两个阶段，Map and Reduce，数据从map function到reduce function是存在local disk中，(soreing in HDFS with replication would be overkill)，然后通过network传输数据. 在每个阶段中，input和output的数据都是以 (key, values) 格式进行处理的，然后通过 map function 和 reduce function 进行处理。在本例中，input data的key是从数据文件开始处的行数的偏移量，但是map function输出的key是年份数据，以及reduce function输出的key是也不同的。所以这三个key-value pairs是不同的。 原始数据 Key-Values 以上为原始数据中input进来后的key-values的数据。然后map function阶段，提取出上面文件中的 1950 和 0001 之类的数据，组成新的key-values作为输出给下一阶段。 Key-Values in Map Function 在将Map Function的输出传给Reduce Function之前，实际上MapReduce Framework还是有对数据进行一个处理步骤。从最上的图一中，我们仍然可以看到Map和Reduce之间有一个 Shuffle 的过程。因为之前我们提到了，Map的过程中，只是实现了一个key-value匹配的过程，所有出来的数据也是无序的，而 Shuffle 就是对这个输出 sort &amp; group 的过程，然后将输出传给 Reduce Function 进行处理 当数据从Reduce Function中处理完后出来的大概如下，注意这个reduce只是选择最大值，其他reduce function可能做的是统计或者实现其他功能。 现在再看另外一个经典的WordCount的例子 在Hadoop系统中，处理一个wordcount的任务可以大致分成四个主要阶段，input，map，reduce，output。其中 Map 和 Reduce 可以继续细分，即分成多个 map tasks 和 reduce tasks。 这些tasks然后被 YARN 给分配集群中多台不同的机器处理。这其中的细节等到往后再讨论。 上面提到的分成多个tasks时，应该是input data切片分给多个maps（而不是一个大的map分成多个小的tasks）， 每个MapReduce分到一个fixed-sized 的数据，通常是64/128MB，这个过程叫做 input splits。然后每个split分配一个map task，同时运行在不同的机器上处理。这样划分的好处是有利于load-balancing，对于性能较好的机器可以处理更过是splits。 2.2 Data Flow 上图可以看出hadoop的整个数据流向，其中虚线代表是在一个node，实线代表的是不同node之间。在同一个node之间，数据的读取存储就有速度上的优势，不同node之间，也就是不同主机之间，就必须通过network进行传输，速度较慢。 Partition 当只有一个reduce的时候，map function的output当然就直接传给这个reduce了。但是当有多个reduce的时候，怎么办呢？此时map会将其输出进行partition(分区)，每一个reduce的任务都会创建一个分区，且每一个reduce task都会有一个partition (There can be many keys (and their associated values)in each partition, but the records for any given key are all in a single partition)，也就是说同一个key会在同一个partition中。 Shuffle and Sort 在map和reduce之间的data flow是Shuffle，从上图可以看出，一个reduce可以接受来自多个不同的map的output，其中包含了sort，partition等过程。 Combiner Functions 之前我们讨论过，data flow在map和reduce之间是通过network进行传输的，但我们知道map function的output是一个个key-value的键值对的，这些key-value paris中，有些是可以通过combiner function进行combine的，这样做的目的是减小map和reduce之间传输的数据大小，加快传输数据。 Combiner Function在许多情况和 Reduce Function是很像的，因为做的工作和reduce是比较类似的，只是处理的是局部map的output(因此Combiner是运行在map output端)，减少data flow的size。但是对于是否调用combiner function，这个是不确定的。因为有些情况下的output是不适合进行combine，有些则又是要多次调用进行合并。因为这个，Combiner是可选的，即可以调用，也可以不调用，当不调用的时候，就必需不能影响程序的正常运行。所以Combiner的input和output是一样的，和Mapper的output、Reducer的input一样。 对于有些特殊情况，甚至连reduce function都不需要。 举个栗子： 适用情况（Commutative &amp; Associative） ​ Reduce Function Combiner Function ​ Commutative: max(a, b) = max(b, a) ​ Associative: max(max(a, b), c) = max(a, max(b, c)) 不适用情况： 伪代码 In-Combiner Function Advantage 相比Combiner，In-Combiner的效率更高。 可以减少一些Mapper和Reducer之间的key-value pairs，可以减少处理这部分的开销。因为Combiner只是减少了一些Mapper和Reducer之间的intermediate data，但是并没减少从Mapper的output出来的key-value pairs的数量。但是In-Combiner是是Mapper 的一部分，也就是说key-value pairs在Mapper 输出前就已经减少了。 减少了key-value pairs可以减少系统的object serialization and deserialization 的开销，即垃圾回收机制 Disadvantage 内存使用，因为要保存一个array在内存中，当数据量很大的时候有可能会爆了。解决方案 有两个，第一是限制array的个数，第二是限制内存的使用。当这俩到达某一个阈值的时候，就发送给Reducer。 第二是讲一个Map的过程分成几个部分，导致debug中可能出现oedering-dependent bugs，调试可能比较困难。 ​ ​ ​","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Recommender-Systems/","text":"1. What is Recommender Systems对于推荐系统的定义，我们先举几个例子来理解一下。 电影网站给用户推荐电影，可以根据该用户以往的评分，比如给浪漫爱情电影评分高，给动作片评分较低，那么系统可以根据这些信息，给用户推荐偏向浪漫爱情的电影 如果是新用户呢？我们没有该用户的评分信息。那么我们可以根据整个系统中，某些电影评分较高进行推荐 那么如果是新网站，新用户呢？ 以上例子，我们可以把推荐系统分成两类。 Content-based systems Content-based，就是基于已有的信息进行推荐。具体哪些信息呢？在上面的电影推荐系统中，有两类信息需要分析。 第一，是User的评分信息，比如给爱情片评分高，给动作片评分低。 第二，是Movie的特征信息，比如这部电影偏向爱情片多一些，但也有一部分搞笑。所以在A（爱情片）和B（搞笑片）中， A的权重更高，B的较低 基于以上两部分信息，我们可以给用户推荐他所喜欢的电影。 Collaborative filterring systems 协同过滤器，则是基于用户/物品之间的相似度进行推荐的。即用户A和用户B都喜欢爱情、浪漫电影，我们就可以把用户A评分过的爱情浪漫电影，推荐给用户B。 2. Content-based systems2.1 Problem Analysis以电影推荐系统为例，假设我们已经对系统中的电影特征有了较为完善，即我们知道某部电影属于爱情片多少分，属于动作片多少分。 那么我们现在以Alice为例，她对两部爱情片评分比较高，对于两部动作片评分为0。那么系统就可以给Alice推荐偏向爱情浪漫的，且不怎么属于动作片的电影。 Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love ？ 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 2.2 Optimization Objective实际上我们已经假设之前对所有电影的特征进行了统计，所以此时有电影特征向量X，以及用户对于电影的评分Y向量。根据此时已有的信息，我们需要求出theta的值。所以能够对于那么没有评分过的电影，根据theta和x求出分数y。 因为一开始theat的值是随机的，所以我们用Linear Regression的方法，不断减少cost function的值求出theta。 值得注意的是，因为这里是多个用户，每一个用户我们求出一个theta值。最后对于多个用户，我们需要求出多个theta值。 Actually, we can assume that we have known all features about the all movies, that is x1, x2, …, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 2.2 Gradient descent update 2.3 Gradient descent in Logistic Regression Question: Why we don’t need $\\frac 1 m$? Anwser: As there are only one user However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to learn theta and features at the same time 3. Collaborative filtering3.1 Proble motivation Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love ? 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 3.2 How to do 在之前部分中，我们了解到了content-based，是已知 x 和 y，求 theta。 Assume:$$\\theta^{(1)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(2)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(3)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\space\\theta^{(4)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\spacex^{(1)} = \\begin{bmatrix} 1 \\ 1.0 \\ 0.0 \\end{bmatrix}$$For Movie 1, we can calculate the result of Movie1 rating by all users.$$\\theta^{(1)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(2)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(3)} * x^{(1)} \\approx 0 \\\\\\\\theta^{(4)} * x^{(1)} \\approx 0$$ 但是对于有些情况，我们并不知道x的特征值，该怎么办呢？ 逆向思考，我们也可以通过 theat 和 y，来求 x 的值。 那么对于 theta和x的值都不知道的情况下呢？ 对比特征 Linear Regression Collaborative filtering 特性向量X 已知数据 待求解数据 权重 θ 待求解数据 待求解数据 y值 已知数据 已知数据 3.3 Optimization Algorithm For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. $$θ -&gt; x -&gt; θ -&gt; x -&gt; θ -&gt; x -&gt; θ -&gt; x -&gt; …$$ Actually, these two steps are Linear Regression, we shoud do that simultaneously to update theta and x. 3.4 Collaborative filtering Optimization Algorithm 实际上，上面是两个 LR的问题，我们可以将上面两步合并到一起，这个就是collaborative filterring， 此时的optimizatino object 就从 J(theta) 和 J(X) 变为了 J(theta, X)。 具体步骤如下 3.5 Vectorization: Low rank matrix factorization首先，我们先把评分Y用向量表示出来，同时表示为Theta和X两个矩阵的乘积$$Y= \\begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \\ 5 &amp; ? &amp; ?&amp; 0 \\ ? &amp; 4 &amp; 0 &amp; ? \\ 0 &amp; 0 &amp; 5 &amp; 4 \\ 0 &amp; 0 &amp; 5 &amp; 0\\end{bmatrix} =\\begin{bmatrix}(\\theta^{(1)})^T(x^{(1)}) &amp; (\\theta^{(2)})^T(x^{(1)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(1)}) \\\\(\\theta^{(1)})^T(x^{(2)}) &amp; (\\theta^{(2)})^T(x^{(2)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(2)}) \\\\… &amp; … &amp; … &amp; … \\\\(\\theta^{(1)})^T(x^{(n_m)}) &amp; (\\theta^{(2)})^T(x^{(n_m)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(n_m)})\\end{bmatrix} = X * \\Theta’, R \\in (n_m × n_u)$$ $$X = \\begin{bmatrix}—(x^{(1)})^T— \\\\—(x^{(2)})^T— \\\\… \\\\—(x^{(n_m)})^T—\\end{bmatrix},x^{(n_m)} = \\begin{bmatrix}x^{(n_m)}_1 \\ x^{(n_m)}_2 \\ … \\ x^{(n_m)}_n\\end{bmatrix}, R \\in (n_m × n)$$ $$\\Theta = \\begin{bmatrix}—(\\theta^{(1)})^T— \\\\—(\\theta^{(2)})^T— \\\\… \\\\—(\\theta^{(n_u)})^T—\\end{bmatrix},\\theta^{(n_u)} = \\begin{bmatrix}\\theta^{(n_u)}_1 \\ \\theta^{(n_u)}_2 \\ … \\ \\theta^{(n_u)}_n\\end{bmatrix}, R \\in (n_u × n)$$ 3.6 Mean Normalization对于那些新注册用户，系统中没有记录他们的偏好，则采用以下方法。 先计算出每部电影评分的平均值mu，然后把所有的评分都减去平均值（此后处理过的评分平均值为0）。虽然这样做对有评分记录用户是多余的，但却可以吧没有评分记录的用户给统一进来，避免全是0的情况。 4. Implement Algorithm4.1 Cost Function without Regularization Tips：这里需要计算的只是针对那些已经评分过的电影，对于用户没有评分过的不需要计算。 4.2 Collaborative filtering gradient$$\\frac {\\partial J} {\\partial x_k^{(1)}} , \\frac {\\partial J} {\\partial x_k^{(2)}} , …, \\frac {\\partial J} {\\partial x_k^{(n_m)}} \\space\\space for \\space each \\space movie\\\\\\frac {\\partial J} {\\partial \\theta_k^{(1)}} , \\frac {\\partial J} {\\partial \\theta_k^{(2)}} , …, \\frac {\\partial J} {\\partial \\theta_k^{(n_u)}} \\space\\space for \\space each \\space user$$Tips： 对于使用vectorization方法，最终只有两个for-loop，一个计算$X_{grad}$，一个计算$Theta_{grad}$ 如何对X和Theta求偏导数？ $$(Theta_{grad}(i, :))^T = \\begin{bmatrix}\\frac {\\partial J} {\\partial \\theta^{(i)}_1} \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_2} \\\\… \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_n}\\end{bmatrix}$$ 同样，我们只需考虑用户已经评分过的电影，用其作为训练样本 因为Vectorization非常容易搞乱各个matrix，所以建议先整理一下各个matrix的size，计算时可以根据matrix的size进行计算。 4.3 Implementation注意这里并没有给出完整的代码 (Octave/Matlab)，都只是主要的部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950% Theta : nu x n% X : nm x n% Y : nm x nu% R : nm x nupred = X * Theta'; %nm x nu 'diff = pred - Y;% Cost Function with regularizationJ = 0.5 * sum(sum((diff.^2) .* R));J = J + (lambda * 0.5) * sum(sum(Theta.^2)); % regularized term of theta.J = J + (lambda * 0.5) * sum(sum(X.^2)); % regularized term of x.% calculate Xfor i = 1 : num_movies, % the row vector of all users that have rated movie i idx = find(R(i, :) == 1); % (1 * r) % the list of users who have rated on movie i Theta_temp = Theta(idx, :); % (r * n) Y_temp = Y(i, idx); % (1 * r) X_temp = X(i, :); % (1 * n) % ((1 * n) * (n * r) -(1 * r)) * (r * n) = (1 * n) X_grad(i, :) = (X_temp * Theta_temp' - Y_temp) * Theta_temp; %' % regularization X_grad(i, :) = X_grad(i, :) + lambda * X_temp;end% calculate Thetafor i = 1 : num_users, % the row vector of all movies that user i has rated idx = find(R(:, i) == 1)'; % (1 * r) ' Theta_temp = Theta(i, :); % (1 * n) Y_temp = Y(idx, i); % (r * 1) X_temp = X(idx, :); % (r * n) % ((r * n) * (n * 1) - (r * 1)) * (r * n) = (1 * n) Theta_grad(i, :) = (X_temp * Theta_temp' - Y_temp)' * X_temp; % regularization Theta_grad(i, :) = Theta_grad(i, :) + lambda * Theta_temp;endgrad = [X_grad(:); Theta_grad(:)]; ​​​","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Machine Learning - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-SVM/","text":"1. Optimisation Objective$$h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\\\z = -\\theta^Tx$$ Why we need do that? 2. Hypothesis Function2.1 Logistic Regression$$\\frac 1 m \\sum_{i=1}^m [ y^{(i)} (-log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) (-log(1 - h_\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum_{j=1}^n \\theta_j^2$$ 2.2 Support Vector Machine$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ Analysis： 为了使得cost function取得最小值，我们令C*W + P部分中，C*W为零。即： 当 y = 1时， cost1 = 0，所以 z &gt;= 1 当 y = 0时， cost0 = 0，所以 z &lt;= -1 Note：1. cost0 and cost1 对应的是上图中左右两边的cost function，因为y=0和y=1的目标函数。 常数C取一个很大的值时比较好。因为C*W + P， 所以C大则W会变小，即相对penality就会变大，W会变小 为什么要重新选定一个cost function ？（逻辑回归的临界点为0，但是SVM的临界点是1，所以SVM更加精确。 ） 对应的线性逻辑回归？即次数不大于1的？ Decision Boundary 不是一条直线的情况 3. Large Margin Classifier12结论：常数C取一个比较大的值比较容易获得Large Margin ClassifierC大，则比较容易获得 以上为两类分布比较均匀的时候，Decision Boundary为图中黑色的线，所有点离黑色的距离都相对比较大比较均匀，但是当存在干扰点的时候如下图，Decision Boundary会由黑色变为粉红色。所以C的取值不能太大，也不能太小。需要求出最优解 4. Mathmatics Behind Large Margin Classification4.1 Vector Inner Product Note： 如何求投影p的值？ 当角度 &lt; 90°，p为正数。当角度 &gt; 90°时，p为负数。 向量内积$$u^Tv = ||u|| · ||v|| · cosθ = ||u|| · p_{v,u} = ||v|| · p_{u,v} = u_1v_1+u_2v_2$$ 4.2 SVM Cost Function$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ 当C取一个一个很大的值时，cost function只剩下后面P的部分。 假设θ0 = 0$$\\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + … + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + … + \\theta^2_n)= \\frac 1 2 ||\\theta||^2$$ 所以：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\p^{(i)}是点到向量\\theta的projection，即点到Decision Boundary的距离$$上面我们讨论了，当C取到一个合适的、较大的数值时，SVM的cost function就只剩下后面P的部分，即$$\\frac 1 2 ||\\theta||^2$$我们要减小cost function，所以需要减小θ的值。 当θ取到一个比较小的值的时候，还需要满足上面讨论的：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\$$所以θ比较小时，只能增加p的值去满足p*||θ|| &gt;= 1 或者 p*||θ||&lt;= -1。 这样就保证了p的值比较大，即点到Decision Boundary的大间距。 5. Kernels5.1 Kernels &amp; Similarity首先，我们回想一下之前的logistic regression中对于non-linear 情况的拟合。 Predict y = 1, if$$\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3x_2 + \\theta_4x_1^2 + \\theta_5x_2^2 + … &gt;= 0 \\\\\\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + … &gt;= 0$$即将fn定义为x的幂次项组合，如下：$$f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, …$$ 但是在SVM中，我们要重新定义fn，引入Kernel的概念，即用 kernel function来表示fn。 Note: l 是landmark，且如果training sets里面的数量为n的话，则landmark的数量也为n。 假设training sets数量为n，则对于一个新的example来说，可计算出n个新的特征f1…fn。然后用新的特征，对该example进行判断（低维转为高维的过程） kernel function为guassian function。当x与landmark l越接近时，两点的距离越小，值接近1 5.2 SVM with Kernels 对比之前的cost function，可以发现这里θ和f(x)跟之前的不同。 在logistic regression 中，θ的维度为(n+1) x 1, 包含θ0， 且n为单个example的特征个数 在SVM with kernel中，f(x)的个数为m，其中m是training sets中的个数，所以θ的维度应该是(m+1)x1 Steps 给定一组training sets，根据每个example，选取m个landmark点 计算每一个example与所有landmark的相识度，相同为1，非常不同接近为0。计算相识度的kernel function为Gaussian Function 最终，对于每一个example里面都可以计算出m个新的feature，所以对于这个training sets而言，会得到一个m*m的矩阵？ 将得到的m*m的矩阵，代入到Hypothesis中，计算出θ的值。 5.4 SVM parameters C = 1/λ Large C Small λ Large θ Lower Bias High Variance Over Fitting Small C Large λ Small θ Higher Bias Low Variance Under Fitting σ Large σ more smoothly Higher Bias Lower Variance Under Fitting Small σ less smoothly Lower Bias Higher Variance Over Fitting","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Linux常用指令","date":"2017-02-19T01:16:24.000Z","path":"2017/02/19/Linux常用指令/","text":"有时候记东西老是记不住，虽然是经常使用，所以对常用的linux指令做个简单记录，方便以后使用（不断更新… …）。 1. scp文件传输 (security copy)12# eg. scp chenson@127.0.0.1:~/Home/test . ~/Home/local_test_pathscp PATH_OF_SRC PATH_OF_DST 2. 测内存的使用率12345678valgrind --tool=massif --pages-as-heap=yesms_print massif 文件名valgrind --tool=massif --pages-as-heap=yes ./bwtsearch testfiles/shopping.bwt l.idx &quot;system&quot;ms_print massif.out.3116 1find &lt;PATH TO FOLDER&gt; -ls | tr -s &apos; &apos;|cut -d&apos; &apos; -f 7| paste -sd+ | bc 3. 强制写入123456789# vii commond model, Try the below command:w !sudo tee %Explanation:w – write!sudo – call shell sudo commandtee – the output of write (:w) command is redirected using tee% – current file name 4. 查看二进制文件vim下 1:%!xxd linux下 1xxd -b file_name 5. 查看公网IP1curl http://members.3322.org/dyndns/getip 6. 查看那个进程占用：1lsof -i:9000 # MAC 7. 查看监听端口：12netstat -ntlp # linuxlsof -i:port_num # MAC 8. 测文件夹大小123456find &lt;PATH TO FOLDER&gt; -ls | tr -s &apos; &apos;|cut -d&apos; &apos; -f 7| paste -sd+ |bcdu -ah --max-depth=1，其中a表示显示目录下所有的文件和文件夹（不含子目录），h表示以人类能看懂的方式，max-depth表示目录的深度。du -sh : 查看当前目录总共占的容量。而不单独列出各子项占用的容量 9. 文件系统空间使用情况12345678910//查看系统中文件的使用情况df -h//查看当前目录下各个文件及目录占用空间大小du -sh *//方法一：切换到要删除的目录，删除目录下的所有文件rm -f *//方法二：删除logs文件夹下的所有文件，而不删除文件夹本身rm -rf log/* 10. 加密文件大小12# oJ5A93Fhzip -r -P password utils.zip utils 11. 打包+压缩 压缩 12345678910111213tar czvf my.tar.gz file1 file2 ....fileNtar -cvf jpg.tar *.jpg //将目录里所有jpg文件打包成tar.jpg tar -czf jpg.tar.gz *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz tar -cjf jpg.tar.bz2 *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2tar -cZf jpg.tar.Z *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Zrar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linuxzip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux 解压 1234567891011tar -xvf file.tar //解压 tar包tar -xzvf file.tar.gz //解压tar.gztar -xjvf file.tar.bz2 //解压 tar.bz2tar -xZvf file.tar.Z //解压tar.Zunrar e file.rar //解压rarunzip file.zip //解压zip 12. 查找文件12345678910111213141516171819202122$find ~ -name &quot;*.txt&quot; -print #在$HOME中查.txt文件并显示$find . -name &quot;*.txt&quot; -print$find . -name &quot;[A-Z]*&quot; -print #查以大写字母开头的文件$find /etc -name &quot;host*&quot; -print #查以host开头的文件$find . -name &quot;[a-z][a-z][0–9][0–9].txt&quot; -print #查以两个小写字母和两个数字开头的txt文件$find . -perm 755 -print$find . -perm -007 -exec ls -l &#123;&#125; \\; #查所有用户都可读写执行的文件同-perm 777$find . -type d -print$find . ! -type d -print $find . -type l -print$find . -size +1000000c -print #查长度大于1Mb的文件$find . -size 100c -print # 查长度为100c的文件$find . -size +10 -print #查长度超过期作废10块的文件（1块=512字节）$cd /$find etc home apps -depth -print | cpio -ivcdC65536 -o /dev/rmt0$find /etc -name &quot;passwd*&quot; -exec grep &quot;cnscn&quot; &#123;&#125; \\; #看是否存在cnscn用户$find . -name &quot;yao*&quot; | xargs file$find . -name &quot;yao*&quot; | xargs echo &quot;&quot; &gt; /tmp/core.log$find . -name &quot;yao*&quot; | xargs chmod o-w 13. 递归删除所有文件夹下的特定类型文件1find . -name &apos;*.ttteset&apos; -type f -print -exec rm -rf &#123;&#125; \\; 14. 后台运行指定程序（不挂断地运行命令）123456789101112131415161718nohup python main.py 80 &gt; ~/log/WeChatAccountLog.file 2&gt;&amp;1 &amp;该命令的一般形式为：nohup command &amp;如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件：command&gt;out.file是将command的输出重定向到out.file文件，即输出内容不打印到屏幕上，而是输出到out.file文件中。2&gt;&amp;1 是将标准出错重定向到标准输出，这里的标准输出已经重定向到了out.file文件，即将标准出错也输出到out.file文件中。最后一个&amp;， 是让该命令在后台执行。试想2&gt;1代表什么，2与&gt;结合代表错误重定向，而1则代表错误重定向到一个文件1，而不代表标准输出；换成2&gt;&amp;1，&amp;与1结合就代表标准输出了，就变成错误重定向到标准输出.ctrl + z 可以将一个正在前台执行的命令放到后台，并且处于暂停状态。Ctrl+c 终止前台命令。jobs 查看当前有多少在后台运行的命令使用 fg %n 关闭 15. 后台上传/下载数据1另外有两个常用的ftp工具ncftpget和ncftpput，可以实现后台的ftp上传和下载，这样就可以利用这些命令在后台上传和下载文件了。 16. Docker 加速器地址12https://cr.console.aliyun.com/cn-qingdao/mirrorshttps://q90v31vp.mirror.aliyuncs.com 17. 查找指定内容1find . | xargs grep &quot;keyword&quot; | egrep &quot;keyword&quot; # 当前文件目录下 vsftpd, kaiqi ftp fuwuchoongqi dashuju taojianambari-agent restart","tags":[{"name":"Linux","slug":"Linux","permalink":"http://chenson.com/tags/Linux/"}]},{"title":"Python数据分析笔记（一）","date":"2016-12-15T11:37:30.000Z","path":"2016/12/15/Python数据分析笔记（一）/","text":"1. 常见问题 Pandas.dataframe里面 .values, .iloc, .ix, .loc 的区别 Different Choices for Indexing loc: only work on index / labeliloc: work on position, from 0, 1, 2, … …ix: You can get data from dataframe without it being in the indexat: get scalar values. It’s a very fast lociat: Get scalar values. It’s a very fast iloc 12345678910111213141516171819202122232425262728&gt;&gt;&gt; df = pd.DataFrame(&#123;'A':['a', 'b', 'c'], 'B':[54, 67, 89]&#125;, index=[100, 200, 300])&gt;&gt;&gt; df A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df.iloc[0] # 用位置来索引A aB 54&gt;&gt;&gt; df.loc[100] # 用初始化时设置的index来索引，也就是自己给row设置的labelA aB 54&gt;&gt;&gt; df.loc[100:300] A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df['A'] # 索引 columns100 a200 b300 cName: A, dtype: object&gt;&gt;&gt; df.A # 索引 columns100 a200 b300 cName: A, dtype: object Pandas 和 Numpy之间的转换 np.ndarray 转化为 pd.dataframe 1pd.DataFrame(example) pd.dataframe 转化为 np.ndarray 1example.values[:, :] 读写效率的对比 npy读写效率最高，但最费硬盘空间，比如np.load(), np.save() csv其次，比如pd.Dataframe.to_csv()，pd.load_csv() txt读写，当然也可以很快，但是需要频繁的split，对格式规范的数据比较麻烦 至于简单的excel和word，可以用xlrd,xlwt来操作 2. Numpy N维数组对象，可以利用这种数组对象对整块数据进行一些科学运算，就是把array当做一种对象里操作。这和Python中的array是不同的。 举个栗子： 在Python中 123&gt;&gt;&gt; data = [1, 2, 3]&gt;&gt;&gt; data * 10[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3] Numpy的ndarray 12345&gt;&gt;&gt; data2 = np.array([1, 2, 3])&gt;&gt;&gt; data2array([1, 2, 3])&gt;&gt;&gt; data2 * 10array([10, 20, 30]) 轴(axes) 和 秩(rank) 轴表示的是一种维度，如一维的数据，二维的数据，三维的数据等 12345678910111213141516171819202122&gt;&gt;&gt; data3 = np.array([1, 2, 3]) # 注意这里的**方括号**&gt;&gt;&gt; data3.ndim # 查看维度1&gt;&gt;&gt; data3 = np.array([[1,2,3], [1,2,3], [1,2,3]])&gt;&gt;&gt; data3.ndim2&gt;&gt;&gt; data3 = np.array([[[1], [1], [1]], [[1], [1], [1]],[[1], [1], [1]]])&gt;&gt;&gt; data3.ndim3&gt;&gt;&gt; data3.shape(3, 3, 1) # 维度从最外层到里层&gt;&gt;&gt; data3.size9 # 3 * 3 * 1 = 9#一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。&gt;&gt;&gt; data3.dtype# 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8).&gt;&gt;&gt; data3.itermsize# 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。&gt;&gt;&gt; data3.data 常用的数组创建函数 打印数组 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(6) # 1d array&gt;&gt;&gt; print(a)[0 1 2 3 4 5]&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(12).reshape(4,3) # 2d array&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]&gt;&gt;&gt;&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4) # 3d array&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 如果一个数组用来打印太大了，NumPy自动省略中间部分而只打印角落 1234567891011&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt;&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] 禁用这种reshape来打印整个数组，需要对printoption参数进行设置 1&gt;&gt;&gt; np.set_printoptions(threshold='nan') 基本的数据运算 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; b = np.array(list(range(5, 10)))&gt;&gt;&gt; a + barray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; a - barray([-5, -5, -5, -5, -5])&gt;&gt;&gt; a * barray([ 0, 6, 14, 24, 36])&gt;&gt;&gt; a += b&gt;&gt;&gt; aarray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; c = np.ones(5)&gt;&gt;&gt; carray([ 1., 1., 1., 1., 1.])&gt;&gt;&gt; c + aarray([ 6., 8., 10., 12., 14.])&gt;&gt;&gt; c = np.ones(5, dtype=int)&gt;&gt;&gt; c + aarray([ 6, 8, 10, 12, 14]) 数组的运算 这就类似在Matlab/Octave中，对matrix/array中的数据执行批量运算，即Vectorization，前提是matrix/array的大小必须满足对应的要求。 用数组表达式可以代替循环操作，矢量化的运算是Numpy的优势。 数组转置和轴对换 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.T # 数组转置，轴对换array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]])&gt;&gt;&gt; np.dot(a.T, a) # 内积array([[125, 140, 155, 170, 185], [140, 158, 176, 194, 212], [155, 176, 197, 218, 239], [170, 194, 218, 242, 266], [185, 212, 239, 266, 293]])&gt;&gt;&gt; b = np.arange(16).reshape((2, 2, 4))&gt;&gt;&gt; barray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt;&gt;&gt; b.transpose((1, 0 , 2)) # 对高维数组，transpose需要得要一个由轴编号组成的元组才能对这些轴进行转置array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) 索引和切片 1234567891011121314151617181920212223242526272829303132333435# 一维数据&gt;&gt;&gt; a = np.arange(5)array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:]array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[1 : 4]array([1, 2, 3])&gt;&gt;&gt; a[1]1# 二维数据&gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6]])array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:, 1:3]array([[2, 3], [5, 6]])&gt;&gt;&gt; b[1][2]6&gt;&gt;&gt; b[1, 2]6# 三维数组&gt;&gt;&gt; c = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; c[1]array([[ 7, 8, 9], [10, 11, 12]]) 通用函数 P111 给 array 添加 columns 和 rows 123456789101112# 方法一np.c_[array1, array2] # 添加 columnsnp.r_[array1, array2] # 添加 row# 方法二 被插入的行np.insert(a, 2, values=b, axis=1) # 添加 columns# 方法三a = np.concatenate((a, -np.ones((a.shape[0], 1))), axis=1) # 添加 columns# 方法四np.column_stack((a,b)) 3. Pandas在Pandas中，Series和DataFrame是两个主要的数据结构 Series 类似一维数组，由一组数据（各种Numpt数据类型 ( list, dict等 )）和一组对于的数据标签组成 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&gt;&gt;&gt; obj = pd.Series(list(range(3, 8)))&gt;&gt;&gt; obj0 31 42 53 64 7dtype: int64&gt;&gt;&gt; obj.valuesarray([3, 4, 5, 6, 7])&gt;&gt;&gt; obj.index # 这里就是 index 索引，没有设置的是时候系统会自动设置为integer indexRangeIndex(start=0, stop=5, step=1)&gt;&gt;&gt; obj[1] # 可以用这索引，跟list用法类似，但是list中只能是数字，但pandas中可以自定义index的索引4# 自定义索引，普通的list和Numpy的数组就不行&gt;&gt;&gt; obj2 = pd.Series([1,3,4], index=['a', 'b', 'c'])&gt;&gt;&gt; obj2['a']1# 索引可以直接用来数组运算，这些在数据清洗的时候比较常用&gt;&gt;&gt; index = obj2 &gt; 2&gt;&gt;&gt; indexa Falseb Truec Truedtype: bool &gt;&gt;&gt; obj2[index]b 3c 4dtype: int64# 其他操作&gt;&gt;&gt; np.log(obj2)a 0.000000b 1.098612c 1.386294dtype: float64 # 当做字典来index&gt;&gt;&gt; 2 in obj2False &gt;&gt;&gt; 3 in obj2False&gt;&gt;&gt; 'a' in obj2 # 只能是index (key)，不能是valuesTrue# 用字典来初始化 Series&gt;&gt;&gt; obj3 = &#123;'one': 1, 'two': 2, 'three' : 3&#125;&gt;&gt;&gt; obj3&#123;'one': 1, 'three': 3, 'two': 2&#125;&gt;&gt;&gt; index = ['One', 'Two', 'Three']&gt;&gt;&gt; value = [1, 2, 3]&gt;&gt;&gt; obj4 = pd.Series(value, index)&gt;&gt;&gt; objOne 1Two 2Three 3dtype: int64 # 手动修改索引，且索引的值是不能重复的&gt;&gt;&gt; obj4.index = ['one', 'two', 'Three']&gt;&gt;&gt; obj4one 1two 2Three 3dtype: int64 # 重建索引，如果索引不存在的值，则引入NaN&gt;&gt;&gt; obj4.reindex(['one', 'two', 'Threeee']) # fill_value=0one 1.0two 2.0Threeee NaNdtype: float64# 可选 ffill/pad 向前填充或者bfill/backfill 向后填充&gt;&gt;&gt; obj4.ffill()one 1.0two 2.0Threeee 2.0dtype: float64# 当然也可以drop columns的内容&gt;&gt;&gt; obj4.drop('Threeee')one 1.0two 2.0dtype: float64 # 但是不能这么修改数值&gt;&gt;&gt; obj4.values = [4, 5, 6]---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2702 else:-&gt; 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):AttributeError: can't set attributeDuring handling of the above exception, another exception occurred:AttributeError Traceback (most recent call last)&lt;ipython-input-41-b4392e8960b0&gt; in &lt;module&gt;()----&gt; 1 obj4.values = [4, 5, 6]/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):-&gt; 2705 object.__setattr__(self, name, value) 2706 2707 # ----------------------------------------------------------------------AttributeError: can't set attribute ​ DataFrame DataFrame是一个表格型的数据结构，包含了一组有序的列，每列可以是不同的值类型。所有可以看做这是一个二维的数组，有行索引和列索引 创建DataFrame和基础操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; dates = pd.date_range('20170305', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6, 4), index=dates, &gt;&gt;&gt; columns=list('ABCD'))&gt;&gt;&gt; df A B C D2017-03-05 -1.616361 0.027641 0.328074 1.0386272017-03-06 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 -0.350594 0.231137 -0.004755 0.6014602017-03-08 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 -0.227631 -0.186816 -0.370522 0.343896# 当然也可以手动传进来创建df&gt;&gt;&gt; df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)&gt;&gt;&gt; df2Ones A B C D2017-03-05 1 -1.616361 0.027641 0.328074 1.0386272017-03-06 1 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 1 -0.350594 0.231137 -0.004755 0.6014602017-03-08 1 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 1 -0.227631 -0.186816 -0.370522 0.343896# 看DataFrame的attributes&gt;&gt;&gt; df.&lt;TAB&gt;df.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D 查看DataFrame里面的数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt;&gt; df.head() # 默认5行 A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401&gt;&gt;&gt; df.tail(3) # 手动设置打印的行数 A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988&gt;&gt;&gt; df.index # index 是 row 的 索引DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df.columnsIndex([u'A', u'B', u'C', u'D'], dtype='object')&gt;&gt;&gt; df.valuesarray([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804&gt;&gt;&gt; df.sort_index(axis=1, ascending=False) # 是否按照 columns的值下降来排序 D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690&gt;&gt;&gt; df.sort_values(by='B') # 按照columns B 升序来 A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 索引columns和rows 4. References python，numpy，pandas数据处理之小技巧 ####","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.com/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.com/tags/Pandas/"}]}]