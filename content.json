[{"title":"Hadoop权威指南笔记（三）","date":"2017-06-09T06:08:52.000Z","path":"2017/06/09/Hadoop权威指南笔记（三）/","text":"1. 为什么要使用Secondary Sort在Hadoop中，从Map到Reduce的过程中，key是不断被sort的。所以从map出来的时候写入到一个intermediate output file的时候，key是有序的。然后reduce不断的从不同的cluster里面fetch里面的key-value pairs的时候，仍然多次sort这些pairs。所以最终进入到reducer的key是有序的，但是value是无序的。如果我们需要对value也进行排序呢？Google的MR内置了函数对value也可以排序，但是Hadoop不行，我们需要自己去定制partitioner等去实现这个功能。 举个栗子： 输入文件格式如下 12345678910112015,1,242015,3,542015,1,32015,2,-432015,4,52015,3,462014,2,642015,1,42015,1,212015,2,352015,2,20 ​ 期望的输出格式如下（value是有序的） 123452014-2 642015-1 3，4，21，242015-2 -43，0，352015-3 46，562015-4 5 ​ Hadoop默认的输出格式如下（value是无序的） 123452014-2 642015-1 21，4，3，242015-2 0，35，-432015-3 56，462015-4 5 ​ 2. 解决方案 传统的解决方法 就是进入到同一个reducer的时候，这些同一个可以的values是在一个list里面的，那么我们就可以先把这个list里面的value存到内存中去，然后在内存中将这些value排序。这个方法只适用于数据量较小的时候，当数据量很大的时候，内存并不能同时存入这些values，程序就报错无法正常运行。 ​ 利用Hadoop特点的方法 既然Hadoop可以对Key进行排序，那么可以利用这点，将之前的key-value pairs 组成一个新的key，让Hadoop对这些Key进行排序，栗子如下 12345// 原先的key-value pair(2015,1) 21// 新的composite-key-value pair((2015,1),21) 21 这里的(2015,1)是我们原先就有的key，为了区分新的key，将这个原有的key (k, v1) ,称为natural key，将新的key ((k, v1), v2)称为composite key。将v2称为natural value。具体如下图 ​ 为了实现Hadoop对新的composite key进行排序，我们需要自定义partitioner和grouping来确保这些composite key中natural key相同的会被分配到同一个reducer。因为在composite key中，即使(k, v1)相同，只要v2不同，默认的partitioner就会认为这是俩个不同的key，就很有可能讲这个composite key分配到不同的reducer里去。 ​ ##### 3.实现过程 partitioner: 将natural key相同的发送到同一个reducer里去 在同一个[]里面说明是一个reducer，value任然是无序的 123456789[((2014-2,64),64)][((2015-1,24),24), ((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,56),56), ((2015-3,46),46)][((2015-4,5),5)] ​ grouping comparator: 将natural key相同的作为一个group排序 123456789[((2014-2,64),64)][((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21), ((2015-1,24),24)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,46),46), ((2015-3,56),56)][((2015-4,5),5)] ​ 进入到Reducer的格式 (有点里不理解此时composite key里面的natural value是如何确定的) 123456789((2014-2,64), (64))((2015-1,24), (2，4，21，24))((2015-2,35), (-43,0,35))((2015-3,46), (46,56))((2015-4,5), (5)) ​ 最终的输出 12342014-2 642015-1 3,4,21,242015-2 -43,0,352015-4 5 整个流程图如下（图内数据与上面数据不符合） 4. 代码 composite key 将旧的Key（natural key）和Value组合成新的Key（composite key）的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.secondarySort;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class Entry implements WritableComparable&lt;Entry&gt; &#123; private String yearMonth; private int count; public Entry() &#123; &#125; @Override public int compareTo(Entry entry) &#123; int result = this.yearMonth.compareTo(entry.getYearMonth()); if (result == 0) &#123; result = compare(count, entry.getCount()); &#125; return result; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(yearMonth); dataOutput.writeInt(count); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.yearMonth = dataInput.readUTF(); this.count = dataInput.readInt(); &#125; public String getYearMonth() &#123; return yearMonth; &#125; public void setYearMonth(String yearMonth) &#123; this.yearMonth = yearMonth; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public static int compare(int a, int b) &#123; return a &lt; b ? -1 : (a &gt; b ? 1 : 0); &#125; @Override public String toString() &#123; return yearMonth; &#125;&#125; ​ Partitioner 1234567891011package com.secondarySort; import org.apache.hadoop.mapreduce.Partitioner; public class EntryPartitioner extends Partitioner&lt;Entry, Integer&gt; &#123; @Override public int getPartition(Entry entry, Integer integer, int numberPartitions) &#123; return Math.abs((entry.getYearMonth().hashCode() % numberPartitions)); &#125;&#125; ​ Grouping Compartor 1234567891011121314151617package com.secondarySort; import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class EntryGroupingComparator extends WritableComparator &#123; public EntryGroupingComparator() &#123; super(Entry.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; Entry a1 = (Entry) a; Entry b1 = (Entry) b; return a1.getYearMonth().compareTo(b1.getYearMonth()); &#125;&#125; ​ Mapper 1234567891011121314151617181920212223public class SecondarySortMapper extends Mapper&lt;LongWritable, Text, Entry, Text&gt; &#123; private Entry entry = new Entry(); private Text value = new Text(); @Override protected void map(LongWritable key, Text lines, Context context) throws IOException, InterruptedException &#123; String line = lines.toString(); String[] tokens = line.split(\",\"); // YYYY = tokens[0] // MM = tokens[1] // count = tokens[2] String yearMonth = tokens[0] + \"-\" + tokens[1]; int count = Integer.parseInt(tokens[2]); entry.setYearMonth(yearMonth); entry.setCount(count); value.set(tokens[2]); context.write(entry, value); &#125;&#125; ​ Reducer 123456789101112public class SecondarySortReducer extends Reducer&lt;Entry, Text, Entry, Text&gt; &#123; @Override protected void reduce(Entry key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder builder = new StringBuilder(); for (Text value : values) &#123; builder.append(value.toString()); builder.append(\",\"); &#125; context.write(key, new Text(builder.toString())); &#125;&#125; ​ Deriver 123456789101112131415Configuration conf = new Configuration();Job job = Job.getInstance(conf);job.setJarByClass(Iteblog.class);job.setJobName(\"SecondarySort\"); FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setOutputKeyClass(Entry.class);job.setOutputValueClass(Text.class); job.setMapperClass(SecondarySortMapper.class);job.setReducerClass(SecondarySortReducer.class);job.setPartitionerClass(EntryPartitioner.class);job.setGroupingComparatorClass(EntryGroupingComparator.class); 5. 常用的Secondary Sort代码 IntPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; int first, second; // public IntPair() &#123;&#125; // // public IntPair(int left, int right) &#123; // set(left, right); // &#125; public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; public String toString()&#123; return \"(\" + first + \",\" + second + \")\"; &#125; @Override public void readFields(DataInput arg0) throws IOException &#123; // TODO Auto-generated method stub first = arg0.readInt(); second = arg0.readInt(); &#125; @Override public void write(DataOutput arg0) throws IOException &#123; // TODO Auto-generated method stub arg0.writeInt(first); arg0.writeInt(second); &#125; // 关键：自定义类型的比较方法 @Override public int compareTo(IntPair arg0) &#123; // TODO Auto-generated method stub if (first != arg0.first) &#123; return first &lt; arg0.first ? -1 : 1; &#125; else if (second != arg0.second) &#123; return second &lt; arg0.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; public int hashCode() &#123; return first * 157 + second; &#125; public boolean equals(Object right) &#123; if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125;&#125;public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; &#123; // 类型要和Mapper输出的一样 @Override public int getPartition(IntPair arg0, IntWritable arg1, int arg2) &#123; // TODO Auto-generated method stub return Math.abs((arg0.getFirst() * 127) % arg2); &#125;&#125;/* * 第一种方法，实现接口RawComparator 数据类型的比较在MapReduce中式及其重要的, * Mapreduce中有一个排序阶段，key和其他的key相比较。 针对此，Hadoop 提供的一个优化是 RawComparator * * public static class GroupingComparator implements RawComparator&lt;IntPair&gt;&#123; * * @Override public int compare(IntPair arg0, IntPair arg1) &#123; // TODO * Auto-generated method stub int l = arg0.getFirst(); int r = * arg0.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; * * @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int * s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, * Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; &#125; */// 方法二public static class GroupingComparator extends WritableComparator &#123; protected GroupingComparator() &#123; super(IntPair.class, true);// 调用父类的构造函数 &#125; public int compare(WritableComparable w1, WritableComparable w2) &#123; IntPair i1 = (IntPair) w1; IntPair i2 = (IntPair) w2; int l = i1.getFirst(); int r = i2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125;&#125; ​ TextPair 1// 待整理","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems(2)","date":"2017-05-15T08:43:49.000Z","path":"2017/05/15/Machine-Learning-Recommender-Systems-2/","text":"1. 协同过滤，给用户推荐物品1.1 基于用户的协同过滤算法UserCF 主要通过分析用户的行为记录，计算物品之间的相似度 该算法认为物品A和物品B具有很大的相似度是因为喜欢物品A的用户大部分也喜欢物品B 1.1.1 步骤 找到和目标用户相似的用户集合 找到这个集合中的用户喜欢，且和目标用户没有听说过的的物品推荐给目标用户 1.1.2 计算相似度 得到用户之间的兴趣相似度之后，寻找最相近的K个用户 1.2基于物品的协同过滤算法1.2.1 计算步骤 计算物品之间的相似度 根据物品的相似度和用户的历史行为给用户生成推荐列表 1.2.2 计算相似度 1.3 隐语义模型 UserCF 找到和他们看了同样书的其他用户，即是兴趣相似用户。然后给用户推荐这些用户相似用户所喜欢的其他书籍 ItemCF 在已经看过的书中，寻找相似的书，即和这些看过的书，同时出现在其他用户的看过的书中，然后推荐这些其他的书 其他方法 可以对物品的兴趣进行分类，对于某个用户，首先得到他的兴趣分类，然后从这些分类中挑出他可能喜欢的物品 如何给物品进行分类 隐含语义分析技术(latent variable analysis) 如何确定用户的兴趣，即对哪些类的物品感兴趣，以及感兴趣的程度 对于一个给定的类，选择哪些属于这个类的物品推荐给用户？以及如何确定这些物品在这个中的权重。即如何在这个类中，挑选出合适的物品推荐给用户 1.3.1 LFM (latent factor model)1.4 用户标签数据 User Generated Content2. 需要解决的问题 - 评分预测（用户A对电影x的评分预测） 2.1 实验方法2.1.1 划分训练集 与时间无关，可以均匀分布随机换分数据集。即对每一个用户，随机选取一些评分记录作为训练集，剩下的作为测试集 与时间相关，那么需要将用户的旧行为作为训练集，讲用户的新行为作为测试集 例子 Netflix的评分预测系统中，将每个用户的评分记录按照从早到晚进行排序，然后将用户最后的10%的评分记录作为测试集，90%的评分记录作为训练集。 2.1.2 评分标准$$RMSE = \\frac {\\sqrt {\\sum_{(u, i) \\in T} (r_{ui} - \\hat r_{ui})^2 }} {|Test|}$$ 2.1.3 评分预测算法 平均值 最简单的方法：利用平均值预测用户对物品的评分 全局平均值 计算在整个训练集中，所有评分记录的评分平均值 用户评分平均值 计算用户u在训练集中所给出的评分的平均值 物品评分平均值计算该物品w在训练集中被评价了的评分的平均值 用户对物品分类的平均值 假设这里有两个分类，一个是用户分类函数U，一个是物品分类W，U(u)定义了用户u所属的分类，W(w)定义了物品w所属的分类。那么我们可以利用训练集中同类用户对同类物品评分的平均值预测用户对物品的评分 之前是三种平均值其实是用户对物品分类的平均值的一种特例 U(u) = 0，W(w) = 0，那么就是全局平均值 U(u) = u，W(w) = 0，那么就是用户评分平均值 U(u) = 0，W(w) = w，那么就是物品评分平均值 在以上的方法中，我们并没有考虑到用户的活跃度和物品的流行程度。实际上可以将这两点考虑进去，对活跃用户和流行的物品给定一点penalty 2.1.4 基于领域的方法（基于用户的领域和基于物品的领域算法） 基于用户的领域算法$$\\hat r_{ui} = \\overline r_u +\\frac {\\sum_{v \\in S(u, K) \\bigcap N(i)} w_{uv}(r_{vi} - \\overline r_v)}{\\sum_{v \\in S(u, K) \\bigcap N(i)} |w_{uv}|}$$*E(r_u) 是用户u对他点评过的所有物品评分的平均值 S(u, K) 是和用户u兴趣最相似的K个用户的集合 N(i) 是对物品i点评过分数的用户集合 r_vi 是用户v对物品i的评分 E(r_v) 是用户v对他评分过的所有物品评分的平均值 w_uv 是用户u和v之间的相似度 ​ 基于物品的领域算法$$\\hat r_{ui} = \\overline r_i +\\frac {\\sum_{j \\in S(i, K) \\bigcap N(u)} w_{ij}(r_{uj} - \\overline r_i)}{\\sum_{j \\in S(i, K) \\bigcap N(u)} |w_{ij}|}$$*E(r_i) 是物品i的平均分，是所有用户对物品i点评过的分数的平均值 S(i, K) 是和物品i最相似的K个物品的集合 N(u) 是用户u点评多分数的物品集合 r_uj 是用户u对物品j的评分 w_ij 是物品i和j之间的相似度 2.1.5 计算相似度 余弦相似度（cosine similarity)$$w_{ij} = \\frac {\\sum_{u \\in U} r_{ui} r_{uj}} {\\sqrt{\\sum_{u \\in U}r_{ui}^2 \\sum_{u \\in U}r_{uj}^2}}$$ 皮尔逊系数（pearson correlation）$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_i) (r_{uj} - \\overline r_j)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_i)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_j)^2}}$$ 修正余弦相似度（adjust cosine similarity）（在MovieLens数据集上效果最好）$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_u) (r_{uj} - \\overline r_u)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_u)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_u)^2}}$$ 2.1.6 隐语义模型的矩阵分解模型 （Latent Factor Model)隐含类别模型、隐语义模型等，在本质上都是为了找出某一东西的潜在的主题或者分类。在推荐系统中，可以基于用户的行为利用隐语义模型，对item进行自动聚类，这样可以避免了人为分类的偏差。 举例说明： ​ 用户A喜欢看数学，历史和计算机的书籍 ​ 用户B喜欢看机器学习，编程语言和离散数学方面的书籍 ​ 用户C喜欢看大师的作品，比如专门看Knuth或者Jiawei Han的书籍 那么系统在对用户的喜好进行推荐的时候，需要找出同属于用户兴趣圈子的书籍。对于之前提到过的三个用户来说 ​ 用户A的圈子：数学、计算机、历史 ​ 用户B的圈子：这三本书可以同时分到计算机的圈子，但是离散数学却又可以分到数学圈子去 ​ 用户C的圈子：根据作者不同来划分圈子，那么这个圈子就和之前用户A、B的角度是不同的。 假设让人工来完成之前书籍的分类，那么经常会碰到划分的粒度不同，角度不同等情况。 同时，需要注意一下两点： 用户A对这三个类别书籍感兴趣，不代表不对其他类别的数据感兴趣 同一本书可以属于多个类别，所以每本书在每个类别里面都有一个权重，权重值越大，说明属于这个类别的可能性越高 那么，LFM是如何解决上面的几个问题的呢？ 回答上面的问题前，我们可以思考一下我们需要做的哪些工作。 实际上，我们可以将所有的User看做列，把所有的Item看做行，构建一个mxn的二维矩阵，如下图 最左边的R矩阵是一个user-item矩阵，矩阵值Rij表示的是用户i对item j的兴趣度，或者是评分。那么我们的评分预测，就可以转换成对这个矩阵中的某些值（缺失值）的预测，同时需要保证我们的预测值对于这个矩阵的扰动的最小的。（即补全之后矩阵的特征值和补全之前的特征值相差不大，具体见SVD分解） 而右边的两个P和Q矩阵就是LFM所做的，LFM算法从数据集汇总抽出若干个class，计算出所有user对这些class的感兴趣长度，即P矩阵。同时计算出所有item在这些class中的权重值，即Q矩阵。P矩阵作为user和item之间连接的桥梁，所以R可以表示为P矩阵和Q矩阵相乘。$$R_{UI} = P_U Q_I = \\sum_{k=1}^K = P_{U,k}Q_{k,I}$$以下是LFM的优点： 不需要关心矩阵P是怎么构建的，即不需要如何给物品进行聚类、划分等（角度，粒度等） Q矩阵中，对于一个item并不是明确给划分到某一个分类，而是计算这个item属于这些所有类别的概率，值越大可能性越高 同理，P矩阵中，对于一个user并没有限定在某些class中，而是计算这个user对于这些classes的感兴趣程度 虽然我们知道了LFM为我们做了哪些工作，但是我们还是不知道该如何求解出矩阵P和矩阵Q中的参数值，一般的做法是最优损失函数来求参数。 传统的SVD分解 传统方法中，给定一个user-item的矩阵R。 首先需要对评分矩阵R中的缺失值进行简单的补全，比如用全局平均值，或者用户/物品的平均值补全，得到补全后的矩阵R’ 得到补全后的矩阵R’，接着可以利用SVD分解，将R’分解成如下形式$$R’ = U^TSV \\\\R \\in R^{m n} \\\\U \\in R^{k m} \\\\V \\in R^{k n} \\\\S \\in R^{k k}$$U和V是两个正交矩阵，S是对角矩阵，对角线上的每一个元素都是矩阵的奇异值。 为了对R’进行降维，可以取最大的f个奇异值组成对焦矩阵Sf，并且找到这个f个奇异值汇中每个值在U、V矩阵中对应的行和列，得到Uf、Vf，从而得到一个降维后的评分矩阵：$$R_f’=U_f^TS_fV_f$$该方法的一些缺点： 在现实中，R矩阵基本上会是一个稀疏矩阵，即95%的数据是缺失的，同时该矩阵非常的大。一经补全，该矩阵就是一个稠密矩阵，储存开销非常的大 计算复杂度非常的高，特别是对于补全之后的稠密矩阵 ​ Funk-SVD分解，即 Latent Factor Model（LFM） http://sifter.org/~simon/journal/20061211.html 从矩阵的角度，将评分矩阵R分解成两个低纬度相乘：$$\\hat R = P^TQ \\\\R \\in R^{mn} \\\\P \\in R^{fm} \\\\Q \\in R^{fn}$$P、Q是两个降维后的矩阵，那么对于用户u对于物品i的评分的预测值可以^R(u, i) = ^r_ui，可以通过如下公式计算：$$\\hat r_{ui} =b_{ui} + \\sum_fp_{uf}q_{if} \\\\p_{uf} = P(u, f) \\\\p_{if} = Q(i, f)$$Simon Funk-SVD的思想是直接通过训练集中的观察值，利用*最小化RMSE学习P、Q矩阵。 损失函数的计算：$$C(p, q) = \\sum_{(u, i) \\in Train} (r_{ui} - \\hat r_{ui})^2= \\sum_{(u, i) \\in Train}(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})^2 + \\lambda(||p_u||^2 + ||q_i||^2) \\\\\\hat r_{ui} = \\mu + b_u + b_i + p_u^Tq_i$$要最小化上面的损失函数，可以利用随机梯度下降法。以下是简单的推导该公式 上面的cost function中欧两个参数p和q，首先 对他们分别求偏导，求出最快下降的方向$$\\frac {∂C} {∂p_{uf}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})q_{ik} + 2\\lambda p_{uk} \\\\\\frac {∂C} {∂q_{if}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})p_{uk} + 2\\lambda q_{ik}$$ 然后根据随机梯度下降法，需要将参数沿最快的下降方向前进，即可得到如下的递推公式：$$p_{uf} = p_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) q_{ik} - \\lambda p_{uk}) \\\\q_{if} = q_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) p_{uk} - \\lambda q_{ik})$$ 所以，执行LFM需要： 根据数据集初始化P和Q矩阵（如何初始化） 确定四个参数：分类书F，迭代次数N，学习速率α（α = 0.9） 和正则化参数*λ ​ 伪代码 1234567891011121314151617181920def LFM(user_items, F, N, alpha, lambda): #初始化P,Q矩阵 [P, Q] = InitModel(user_items, F) #开始迭代 For step in range(0, N): #从数据集中依次取出user以及该user喜欢的iterms集 for user, items in user_item.iterms(): #随机抽样，为user抽取与items数量相当的负样本，并将正负样本合并，用于优化计算 samples = RandSelectNegativeSamples(items) #依次获取item和user对该item的兴趣度 for item, rui in samples.items(): #根据当前参数计算误差 eui = eui - Predict(user, item) #优化参数 for f in range(0, F): P[user][f] += alpha * (eui * Q[f][item] - lambda * P[user][f]) Q[f][item] += alpha * (eui * P[user][f] - lambda * Q[f][item]) #每次迭代完后，都要降低学习速率。一开始的时候由于离最优值相差甚远，因此快速下降； #当优化到一定程度后，就需要放慢学习速率，慢慢的接近最优值。 alpha *= 0.9 Baseline Estimats 对比基线考虑到重口难调，有些user会给出比较高的分数，有些要去严格的users会给出比较低的分数，而有些质量好的商品会得到比较高的分数，质量差的分数较低。所以为了调整这些，引入了baseline estimate。比如为了估计某个用户u会给电影i打的评分：$$b_{ui} = u + b_u + b_i$$ $\\mu$ 是该物品的整体平均值 $b_u$ 是用户打分相对整体用户打分平均值的偏差 $b_i$ 是该物品相对整体平均值的偏差 举个例子：预测豆瓣用户小明给电影泰坦尼克号的评分 泰坦尼克号在豆瓣上的平均分数是3.7分(u) 泰坦尼克号的平均分数又比所有电影在豆瓣上的平均分数高0.5分($b_i$) 但是小明是个电影爱好者，比平均用户打分偏低0.3分($b_u$) 所以不考虑regularized的话，预测小明给泰坦尼克号的评分应该是 3.7 - 0.3 + 0.5 = 3.9 所以这里，得到 $b_u$ 和 $b_i$ 的值很重要 这里对$b_u$和$b_i$加入了penality，为了防止overfitting。 这里$r_ui$是我们训练数据中的已知rating，可以直接使用 $\\mu$是整体平均值，也可以根据训练数据计算出来 $\\lambda_2$和$\\lambda_3$是我们手动设置的参数，MovieLens数据上，20比较合适 R(u)和R(i)为用户urating过的物品的集合，和物品i被rating过用户的集合 所以我们可以根据这些数据，计算出$b_u$和$b_i$b_i$（注意维度）$$b_i = \\frac {\\sum_{u \\in R(u)}(r_{ui} - \\mu)} {\\lambda_2 + |R(i)|} \\\\b_u = \\frac {\\sum_{i \\in R(u)}(r_{ui} - \\mu - b_i)} {\\lambda_3 + |R(u)|}$$除了上面的方法，还有一种更为简便的方法计算$b_u$和$b_i$，就是直接使用user，item的rating的平均值估计$$b_u = \\frac {\\sum R(u)} {len(R(u))} \\\\b_i = \\frac {\\sum R(i)} {len(R(i))}$$ Neighborhood Models item-oriented algorithm: a rating is estimated using known rating made by the same user on similarity items. user-oriented algorithm: estimate unknown ratings based on recorded ratings of like minded users. Similarity measure between items Pearson correlation 计算物品i和j的相似度$$s_{ij} = \\frac {n_{ij}} {n_{ij} + \\lambda_2}ρ_{ij}$$ n_ij 表示都对物品i和j评分过的用户的数量 ρij是皮尔逊系数，通常取？？？ λ2 通常取100 12345678910111213预测评分$$\\hat r_&#123;ui&#125; = b_&#123;ui&#125; +\\frac&#123;\\sum_&#123;j \\in S^k(i; u)&#125; s_&#123;ij&#125;(r_&#123;ui&#125; - b_&#123;ui&#125;)&#125;&#123;\\sum_&#123;j \\in S^k(i; u)&#125;s_&#123;ij&#125;&#125;$$在用户u所有评分过的物品中，找到相似度和i最高的k个物品（k-neighbors），用$S^k(i; u)$表示但是这种算法还是有一些局限性，比如对于两个完全没有关系的物品之间的预测。或者是对于某些物品，最为相似的k个物品缺失，所以可以修正以上的公式（不是很明白这里）$$\\hat r_&#123;ui&#125; = b_&#123;ui&#125; + \\sum_&#123;j \\in S^k(i; u)&#125; \\theta^u_&#123;ij&#125;(r_&#123;ui&#125; - b_&#123;ui&#125;) \\\\\\&#123;\\theta^u_&#123;ij&#125; | j \\in S^k(i; u)\\&#125;$$","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Hadoop权威指南笔记（二）","date":"2017-05-04T14:13:45.000Z","path":"2017/05/05/Hadoop权威指南笔记（二）/","text":"1. Mapper 2. Reducer 3. MapReduce数据都是以 key-values pairs 的形式在Mapper和Reduce之间传递的 Mapper输出的 key-value pairs 应该和 Reducer输入的 key-value pairs 类型是一样的 在Reducer中，是 key-valueLists pairs 的形式 4. Deriver也就是初始化配置MR然后调用执行，一般可以写成如下形式： 123456789101112131415161718192021222324252627282930313233// 老APIpublic void run(String inputPath, String outputPath) throws Exception &#123; JobConf conf = new JobConf(WordCount.class); conf.setJobName(\"wordcount\"); // the keys are words (strings) conf.setOutputKeyClass(Text.class); // the values are counts (ints) conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(MapClass.class); conf.setReducerClass(Reduce.class); FileInputFormat.addInputPath(conf, new Path(inputPath)); FileOutputFormat.setOutputPath(conf, new Path(outputPath)); JobClient.runJob(conf);&#125;// 新APIpublic void run(String IN, String OUT) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1);&#125; 5. Data Flow在hadoop中所有的Mapper和Reducer都是独立工作的，这也是hadoop分布式能够稳定运行的原因之一(有利于容错处理)。在MapReduce整个过程中，只有一次数据相互交互，就是Mapper到Reducer这个过程。从Mapper输出的所有的intermediate data会被统一shuffle(必须要等所有MR执行完毕吗？)，然后同一个key的key-value pairs 会被分配到同一个reducer中去。 6. A Closer Look第五部分中，我们看到的是MR的宏观流程，具体的流程具体可以分为 Map -&gt; Combiner -&gt; Partitioner -&gt; Sort -&gt; Shuffle -&gt; Sort -&gt; Reduce(这里对shuffle的定义有点不同，个人认为从map的输出到reduce的输入这段过程可以称之为shuffle。同时根据官方文档，combiner是在mapper最终输出前多次调用的，以及在reducer里也有调用) 下图中，input files在进入到Mapper之前时，会对这些文件split，因为一个mapper一般是64MB或者128MB，当大于的时候需要对输入文件处理，然后传给RecordReaders，以key-value pairs的形式传给mapper。 对于所有Mapper的output，先会对其进行parition操作，也就是决定去哪一个Reducer。当确定好哪些Reducer，这些key-value paris 就会传入到该Reducer相应的分区，然后对齐进行排序。最后将sort好的key-valueLists 传入到Reducer进行处理。 7. ShuffleHadoop The Definitive Guide P197 Mapper 根据官方图，Mapper的output出来的key-values pairs会先进入到buffer memory(默认100MB大小)，但buffer到80%容量的时候，buffer里面的内容会spill到disk去，如果此时buffer还未慢的情况下，mapper继续输出到buffer，如果满了的话mapper会被block，直到可以写入。 在buffer中的内容spill到disk之前，还有一个partitioner的步骤。对这些即将写入到disk的内容分组，同一个key和同一个reducer的ker-value pairs会在一起。然后这些key-value pairs排序(sort)。如果此时我们定义了combiner function，在输出前，这些pairs会先combine，然后输出。也就是说combiner function是在partitioner之后? (Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer) 每当buffer达到那个threshold的时候，buffer里面的内容写入到disk中，此时会新建一个临时的spill文件，每次写入都会新建一个，然后这个map结束前这些spill files会被merge到一个partitioned和sorted的文件里去。除了之前partition的输出后调用了一次combiner一次之外，但合并这些个spill files( &gt;=3 )的时候，会继续调用combiner去合并同一个reducer里的同一个key的value，所以combiner在output file被生成之前，会被调用许多次，以减少之后io的次数。但当spill files只是1个或则两个的时候，并不会调用combiner。注意有些情况下combiner并不适合使用，比如求平均值。 而不同mapper生成的spill文件最终会被merge成 {key:[v1, v2, v3…], …}这种形式。(这里有疑问，博客图和官方图有点不一样。按照官方图的理解，一个mapper对应的是一个spill file，所以最终是多个spill files？还是这些spill files在传给Reducer之前会被merge成上述的list形式？一个cluster一个最终的output file)​ Reducer 之前mapper端的所有工作已经完成了，所有的mapper的output都已经被写入到了一个output文件里面去了。那么Reducer就是要把这个文件里面的key-valueLists pairs 分给不同的reducers，而这个过程称之为Fetch，就是将相应的key-valueList pairs 拉取到相应的reducers里面去。 在Reducer阶段，每个reducer会调用线程从多个不同的cluster的output file里面fetch数据，然后对这些数据merge。这里的过程和之前的mapper有点像。reducer也有一个buffer memory(通过JVM的heap size来设置)，fetch的数据会被传到里面(如果放得下)，当buffer达到threshold的值的时候，buffer里面的内容会被merge然后spill到disk里面去(实际上有多种形式存放这个文件，这里不讨论)，如果之前我们已经定义了combiner，这里combiner也会被调用。直到所有的mapper的output file都被fetch到一个文件里去，reducer会在输入前sort里面的内容(实际上merge的时候已经sort了)，然后这个已经排好序的file就会被传到reducer里面执行，最终输出到HDFS上。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"PostgreSQL复习笔记","date":"2017-03-23T23:38:55.000Z","path":"2017/03/24/PostgreSQL复习笔记/","text":"1 常见问题1.1 匹配一致的编码规则123SET character_set_client=&apos;utf8&apos;; -- gbkSET character_set_connection=&apos;utf8&apos;; -- gbkSET character_set_results=&apos;utf8&apos;; -- gbk 1234-- 创建数据库时候设置CREATE DATABASE `test`CHARACTER SET &apos;utf8&apos;COLLATE &apos;utf8_general_ci&apos;; 1.2 如何理解索引索引字面上理解就是对数据所建立目录，它可以加快我们的查询速度，但是同时也降低了增删改的速度。 创建原则 不要过度使用索引 最好在查询频繁的列上使用索引 如果构建索引，这一列尽量是离散值，而不要过于连续的区间 索引的类型 普通的索引 index 唯一的索引 unique index 一张表上，只能有一个主键，但是可以有一个或是多个唯一索引 主键索引 primary key 不能重复 12-- 查看一张表上的所有索引show index from TABLE_NAMES; 1.3 模糊查询 % 匹配任意字符 _ 匹配单个字符 举个栗子 1234567891011SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC%&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC_&apos;; 1.4 理解 COUNT 见3.1 聚集函数 1.5 理解 UNION 和 UNION ALL UNION 用于合并两个或是多个SELECT语句的结果集 注意： SELECT语句必须拥有相同的数量的列 列的需要拥有相似的数据类型 每条SELECT语句中的列的顺序必须是一致的 结果不允许有重复，否则使用 UNION ALL 123456SELECT mid, sex, ageFROM TABLE_1UNION -- UNIAON ALL -- 允许有重复的值出现在结果集中SELECT mid, sex, ageFROM TABLE_2;-- ORDER BY mid; -- 可以对其结果进行排序，注意的是排序只是针对合并后的结果集排序 1.6 理解 JOIN（左链接，内链接和外链接） 不同的几种JOIN类型，以及之间的差异 JOIN：如果表中至少有一个匹配，则返回行 LEFT JOIN：即使右表中没有匹配，也从左边返回所有的行 RIGHT JOIN：即使左表中没有匹配，也从右表中返回所有的行 FULL JOIN：只要其中一个表存在匹配，就返回行 INNER JOIN 平常我们需要链接两个表的时候，可以用以下方法 123SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P 同时，我们也可以使用JOIN来实现上面的语句 12345SELECT Person.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName LEFT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意到上面的，左边所有的行都返回了，即使没有出现在右表当中，没有的值为NULL RIGHT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意，即使左边没有全部匹配到右边，依然在最后的OrderNo中，返回了右表所有的行数，没有的值为NULL 1.7 理解 HAVING 见3.2 在SQL中增加HAVING子句的原因是由于WHERE中无法使用聚合函数 2 数据库的基本操作2.1 表的使用 使用列约束建表 12345678CREATE [TEMPORARY] TABLE 表名 ( -- TEMPORARY 建立一张临时的表col_not_null INT NOT NULL, -- 列名 类型 &#123;约束1 约束2 ...&#125;col_unique INT UNIQUE,col_prikey INT PRIMARY KEY, -- NOT NULL + UNIQUE 主键col_default INT DEFAULT 42,col_check INT CHECK(col_check &lt; 42)col_ref INT REFERENCES -- 约束这个值必须是另一独立的表的某个列中的某个值); 使用表级约束建表 1234567CREATE TABLE 表名 ( myKey_1 INT, myKey_2 INT, myString varchar(15), CONSTRAINT cs1 CHECK (myString &lt;&gt; &apos;&apos;), -- 不能位空字符串 CONSTRAINT CS2 PRIMARY KEY (myKey_1, myKey_2)); -- 修改表结构 添加新列 1ALTER TABLE 表名 ADD COLUMN 列名 类型; 重命名新添加的列 1ALTER TABLE 表名 RENAME COLUMN 列名 TO 新列名; 改变一些约束和其他规则 12ALTER TABLE 表名 DROP CONSTRAINT cs1; -- DROP约束ALTER TABLE 表名 ADD CONSTRAINT cs3 UNIQUE(列名); --添加新的约束 修改列的类型 1ALTER TABLE 表名 ALTER 列名 TYPE 新类型; 重命名表名 1ALTER TABLE 表名 RENAME TO 新表名; 使用临时表 临时表的功能基本和表是差不多的，区别在于当你的会话结束时，你与数据库连接断开后，临时表会自动被删除。 键的约束 作为一个列的约束的外键（列约束） 1234567CREATE TABLE 表名 ( ... ... customer_id INTEGER NOT NUll REFERENCES customer(customer_id), -- 关联到customer表 ... ...);-- REFERENCES 外表名(外表名中的列) 表级约束 123456CREATE TABLE 表名( ... ... customer_id INTEGER NOT NULL, ... ... CONSTRAINT 表名_列名_fk FOREIGN KEY(列名) REFERENCES 外表名(外面中的列)) 注意：比较推荐的是使用表级约束，而不是混和私用表级和列级约束 ​ 约束名表名_列名_fk允许外面更容易定位错误资源 2.2 视图 建立视图 123CREATE VIEW 视图的名字 AS select系列语句;-- 例子CREATE VIEW item_price AS SELECT item_id, description, sell_price FROM item; 当视图建立好的时候，我们可以像使用表一样来查询这个视图，可以使用SELECT或WHERE语句等。 每次在视图中执行SELECT时，数据都会被重建，所以数据总是最新的，而不是一个在视图被建立的时候冻结的拷贝。也就是当与之相关的表的数据发生该表的时候，VIEW里面的数据也随之改变，而不是储存了建立VIEW的时候的拷贝对象。或者也可以理解类似指针指向原先的表，当原先的表发生变化，这边的数据自然而然的就能够读取出来。 当然，SELECT语句是可以在多个不同的表中提取数据的。 删除和替换VIEW 12DROP VIEW 名字; -- 不影响我们已有的数据CREATE OR REPLACE VIEW 名字 AS 新的select系列语句; 一些与VIEW常用的指令 12\\dv -- 查看当前数据库中的所有的VIEW\\d VIEW的名字 -- 查看具体的某一个VIEW的结构 2.3 INSERT语句 基本插入语句 123INSERT INTO 表名 VALUES (每列的值的列表);INSERT INTO customer VALUES(18, &apos;Mr&apos;, &apos;Jeff&apos;, &apos;Baggott&apos;, &apos;Midtown Street A\\\\33&apos;, &apos;Milltown&apos;, &apos;MT9 8NQ&apos;, &apos;746 3956&apos;); PS：这种操作很危险，SQL注入攻击 推荐的安全方法 12345INSERT INTO 表名(列名的列表) VALUES(跟之前的列的列表对应列的数值);INSERT INTO customer(customer_id, title, fname, lname, addressline, ...)VALUES(19, &apos;Mrs&apos;, &apos;Sarah&apos;, &apos;Harvey&apos;, &apos;84 Willow Way&apos;, ...); PS：避免在插入数据的时候为serial类型的数据提供数值，因为这个是系统自动添加的 访问序列生成器 序列生成器总 是被命名为&lt;表名&gt;_&lt;列名&gt;_seq 123currval(&apos;序列生成器名&apos;);nextval(&apos;序列生成器名&apos;);setval(&apos;序列生成器名&apos;, 新的值); 插入空值 123INSERT INTO customer VALUES(16, &apos;Mr&apos;, NULL, &apos;Smith&apos;,&apos;23 Harlestone&apos;, &apos;Milltown&apos;, &apos;MT7 7HI&apos;, &apos;746 3725&apos;); 使用 \\copy 命令 步骤 先生成如下格式的数据 ​ 再生成如下格式的数据，保存成.sql拓展名的文本文件 ​ 使用\\copy命令导入数据 ​ PS：SQL 里头的 COPY 命令有一个优点：它明显比\\copy 命令快，因为它直接通过服务器进程执行。\\copy 命令是在客户进程中执行，有可能需要通过网络传输所有数据。而且 COPY 在发生错误的时候会更可靠。除非你有大量的数据，否则区别不会太明显。 2.5 从数据库中删除数据 DELETE语句 语法类似于UPDATE语句 1DELETE FROM 表名 WHERE 条件; TRUNCATE语句（不推荐，因为不安全） TRUNCATE语句是把表中所有的数据都删除，但是保留这张表的结构，也就是说最后剩下了一张空表，所有的行都被删除了。 1TRUNCATE TABLE 表名; DROP语句 DROP语句就是删除了整张表的内容，包括表的结构。DROP完之后，这张表就是不存在的了 1DROP TABLE 表名; 2.6 修改数据库中的数据 UPDATE语句 1UPDATE 表名 SET 列名 = 值 WHERE 条件; 1UPDATE customer SET town = &apos;Leicester&apos;, zipcode = &apos;LE4 2WQ&apos; WHERE 一些条件; 如果没有WHERE子句的话，会导致表中的很多甚至是所有的行都被同时更新了 通过另一个表更新 1UPDATE 表名 FROM 表名 WHERE 条件; 3 高级数据选择3.1 聚集函数 Group By and count(*) 错误使用 1SELECT count(*), town FROM customer; 正确使用 1SELECT count(*), town FROM customer GROUP BY town; 结果是获得一个城镇的列表以及每个城镇的客户数量（count(*)) 同时我们也可以用两个columns name在GROUP BY中，然后用ORDER BY指定排列顺序。没有GROUP BY的话按照GROUP BY中的town，lname排序 Having Having是一种用于聚集函数的WHERE从句，我们使用HAVING来约束返回的结果为针对特定的聚集的条件为真的行，比如count(*) &gt; 1 PS：聚集函数无法在WHERE从句中使用，只能用在HAVING从句中 举个栗子： 选出有超过一个客户的城镇，在里使用一个HAVING从句来约束大一的行 SELECT中从句的优先度 1234567SELECTFROM WHEREGROUP BYHAVINGORDER BY -- DESC 下降LIMIT -- 用于限制rows是行数 mysql count(Column_Name) count(*) 统计所有的行 count(column_name) 统计所这个列中值不是NULL的行 count(Distinct column) 只统计这个列中唯一的情况，不重复统计 min min 函数使用一个列名做参数且返回这个列中最小的值。对于 numeric 类型的列，结果应该和预期一 样。对于时态类型，例如 date 的值，它返回最小的日期，日期既可以是过去也可以是未来。对于变长的字符串（varchar 类 型），结果可能和预期有点不同：它在字符串右边添加空白后再进行比较。 min 函数忽略 NULL 值。忽略 NULL 值 是所有的聚集函数的一个特点，除了 count(*)（当然，是否一个电话号码是最小值又是另一个问题了 PS：小心在 varchar 类型的列中使用 min 或者 max，因为结果可能不是你预期的。 max sum Sum 函数使用一个列名作为参数并提供列的内容的合计。和 min 和 max 一样，NULL 值被忽略。 和 count 一样，sum 函数支持 DISTINCT 变体。你可以让它只统计不重复值的和，所以多条值相同的行只会被加一 次 avg 我们要看的最后一个聚集函数是 avg，它使用一个列名做参数并返回这个列数值的平均值。和 sum 一样，它忽略 NULL 值。这里是一个示例 ​ 3.2 子查询 问题一 找到价格比平均价格高的商品项目 方法一（土方法） 方法二（用嵌套WHERE从句） 问题二 找到那些成本高于平均成本但售价低于平售价的产品 方法一（土方法） 方法二（用嵌套WHERE从句） 问题三 - 返回多行记录的子查询 之前的两个问题中，WHERE中的子查询中的SELECT字句返回的最后只有一个值——因为用了count()聚集函数。如果WHERE中的SELECT子句返回多个结果值呢？ 答案是用 WHERE column_name IN (RESULTS) 当然也可以使用NOT IN 来排出选项 3.3 相关子查询​ 在之前的例子中，这里的两个SELECT实际上是不相关的，也就是在内部的SELECT的结果基础上，外部SELECT再做继续查询 ​ 但是相关子查询则是内外的SELECT中，表与表之间是有关系的 格式 PS：建议在相关子查询中使用表的别名 3.4 UNION链接 格式 12345SELECT town FROM tcustUNIONSELECT town FROM customer;SELECT town, zipcode FROM tcust UNION SELECT town, zipcode FROM customer; PS：UNION 连接的使用有一些限制。你要连接的两个从两个表中查找列表的列必须有相同列数，而且选择的每个列必须都有相兼容的类型。 这个查询，虽然非常无意义，但是是有效的，因为 PostgreSQL 可以连接这两个列，即使 title 是一个固定长度的列而 town 是一个变长的列，因为他们都是字符串类型。例如如果我们尝试连接 customer_id 和 town，PostgreSQL 会告诉我们 无法做到，因为这两个列的类型不同。 3.5 自连接 3.6 外链接4. 表的管理​ 5. 事务和锁","tags":[{"name":"SQL","slug":"SQL","permalink":"http://chenson.com/tags/SQL/"}]},{"title":"Machine Learning - Linear Regression and Logistic Regression","date":"2017-03-17T01:06:30.000Z","path":"2017/03/17/Machine-Learning-Linear-Regression-and-Logistic-Regression/","text":"1. 三要素当一开始接触Andrew在Coursera上的ML公开课的时候，对线性回归和逻辑回归这两种模型有个大体的认识。但是在上完cs229的前三节课，初步了解了这两种模型背后的数学模型，Linear Regression和Logistic Regression背后的概率分布，了解到了这两种概率分布其实只是exponential family中的特例。但同时也开始对一些概念性的东西感觉很模糊，所以觉得有必要好好整理一下这部分的内容。 1. 1 Hypothesis首先对于样本数据，输入x和输出y之间是通过Target function在转换的，也就是 Target function f(x) = y。但是我们并不知道这个f(x)都是怎样的，所以我们假设了这么一个Hypothesis function去模拟这个Target function，使得我们用同样的输入x会的一个预测值y’，使得这个y’不断逼近真实值y。 Linear Regression$$H(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx$$ Logistic Regression$$H(x) = g(\\theta^Tx) = \\frac 1 {1 + e^{-\\theta^Tx}}$$等价于（即log odds，logit）$$ln \\frac y {1 - y} = \\theta^T x = ln \\frac {p(y=1| x; \\theta)} {p(y=0| x; \\theta)}$$ 1. 2 Cost functionCost function呢，实际上也可以叫做Error function，就是用我们上面假设的Hypothe function所预测出来的值y’和真实值y之间的误差。而我们需要做的是根据假设出的Hypothesis function，取一个合适的权重值，即theta的值，使其取的一个较低的cost，也就是这预测值与真实值之间的误差最小。 Ordinary Least Squares (Square Loss Function) 常用的方法是最小二乘法 $$J(\\theta) = \\frac 1 2 \\sum_{i=i}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$ ​ 当然我们也可以从概率的角度来理解这个问题$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$​ 这里的ϵ是我们预测值与实际值之间的误差，这个问题我们会留到后面重点讲解。 0-1 Loss Function Absolute Loss Function Log Loss Function 1.3 Algorithm至于怎么使得上面的cost function最小呢，因为对于某些数据，其features有成千上百个，我们很难去找到这个最小的极值点，使得cost function最下，所以这个Algorithm就是用来找cost function的最小值的。常用的方法有如下 Gradient Descent 在梯度下降中，我们采用的是 LMS update rules(Least Mean Squares)$$\\theta_j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}= \\alpha e^{(i)} x^{(i)}$$ 当我们的预测值与实际值之前的误差ϵ很小时，我们就只需要对θ做出很小的调整，反之，说明当前的θ不对，需要调整的幅度比较大。直到最后收敛为止。 上面Repeat中的步骤实际是等同于cost function对θ求导的过程，所以为了保证收敛的效果，cost function应该是要 convex fuction，就不会导致停留在了local optim点。 推导过程如下： ​ 可视化后大概的过程如下： 根据对于哪些θ求导，Gradient Descent还可以继续分成不同的几种方法 Batch Gradient Descent Stochastic Gradient Descent Mini Batch Gradient Descent SGD with mini-batch 问题：为什么下降是 - ，θ大J一定大吗？ Normal Equation (linear regression) 推导过程比较复杂，需要的数学知识比较多，这里只给出结论。想要看具体推导过程的还说看cs229的第二节课吧 ：）(cs229-notes-1, p11)$$\\theta = (X^TX)^{-1}X^T\\vec y$$ ​ Newton BGFS Simulated Annealing 2. Generalized Linear Model (GLM) 广义线性模型之前我们在cost function中提到过，我们可以从概率的角度来理解误差这个问题。对于Linear Regression和Logistic Regression，我们都可以假设：$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$这里误差ϵ假设为IID (independently and identically distributed) 2.1 Linear Regression在Linear Regression中，y是连续的值，所以误差ϵ也是一个连续的值。假设误差ϵ是符合Gaussian Distribution (Normal Distribution)，所以有 Gaussian Distribution $$y | x; \\theta ∼ N (μ, σ^2)$$ Probability of error ​ σ实际上是不影响概率的分布的，所以假设σ = 1，所以这里可以忽略了。因此也就是等同于如下 Likelihood$$L(\\theta) = L(\\theta; X, \\vec y) = p(\\vec y | X; \\theta)$$ ​ 以上是在给定输入x和权重θ下，我们的预测值是真实值y的概率，所以这个概率呢，当然是越高越好啦。我们就是要想办法去 maximum likelihood。 ​ 对这个概率取个log（不影响结果），有 ​ 可以看到最终的式子里面，我们就是要求cost function的最小值。 2.2 Logistic Regression在Logistic Regression中， y是离散的值 {0, 1}，所以误差ϵ也是一个离散的值 {0, 1}。假设误差ϵ是符合Bernoulli Distribution，所以有 Bernoulli distribution Probability of error 把误差ϵ代入到上面的bernoulli function，可得 Likelihood ​ 同样，对上面去log之后有 ​ 同样，我们尽量要maximize the likelihood，就相当于要最小化后面的那部分（cost function）。这里可以用Gradient Ascent算法来求最大值，但是和Gradient Descent不一样（为什么）$$\\theta := \\theta + \\alpha∇_\\theta l(\\theta)$$​ 注意这里是加号，在求cost function的最小值时，用的是减号。 ​ 对其求导可得 ​ 所以有 ​ Digression Perception ？ ？ ？ 2.3 GLM似乎到现在，讲了半天都也没讲什么是广义线性模型，实际上呢，上面我们已经从误差概率的角度上来分析了线性回归和逻辑回归两种特例，因为他们误差服从的概率分布都是属于Exponential Family中的一种。 The Exponential Family η 被称作natural parameter，它是指数分布族唯一的参数T(y) 被称作sufficient statistic，很多情况下T(y)=y a(η) 被称作 log partition functionT函数、a函数、b函数共同确定一种分布 那么这个模型和上面我们提到过的Gaussian Distribution 和Bernoulli Distribution有什么关系呢？其实上面的这几个参数取不同的值的时候，即可得到不同的分布模型 Gaussian Distribution Bernolli Distribution 2.4 如何构建一个GLM模型在上面我们只是看到了一个通用的GLM概率模型 实际上对于构建这么一个概率模型，需要作出三个假设作为前提条件： p(y | x; θ) ∼ ExponentialFamily(η). 对于给定的输入x，θ和输出y需要服从某一种指数分布，这个指数分布由η 决定的 对于给定的输入x，预测T(y)的值，且经常T(y) = y。而我们是预测是H(x) 需要满足 H(x) = E[y|x] 对于自然参数η和输入x之间，需要存在相关性关系的，即：η = θT x","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"KMP算法","date":"2017-03-15T06:48:47.000Z","path":"2017/03/15/KMP算法/","text":"参考内容 阮一峰 - 字符串匹配的KMP算法 Partial Match Table 上来先上个结论，这个先暂时不管怎么生成，用于KMP表的移动。 移动位数 = 已匹配的字符数 - 对应的部分匹配值 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是6 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位。 匹配到了第三个字符C，C在上表中的值是0，已经匹配的字符数是2 所以移动的位数是 2 - 0 = 2，将搜索词向后移动2位 因为第一个字符不匹配，就将整个字符串向后移一位 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是2 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位 逐个比较，直到完全匹配 如果还需要继续搜索的话，D在上表中的值为0，匹配到的个数为7，移动的位数= 7 - 0 = 7，将整个字符串往后移动7位。接着就是重复之前的比较步骤了。 计算Partial Match Table ​ 这里需要理解两个概念：前缀和后缀 ​ “前缀” 指除了最后一个字符以外，一个字符串的全部头部组合； ​ “后缀” 指除了第一个字符以外，一个字符串的全部尾部组合。 ​ 而我们需要的Partial Match Table就是前缀和后缀的最长共有元素的长度 ​ 继续以上面的例子讲解 “A”的前缀和后缀都为空集，共有元素的长度为0；“AB”的前缀为[A]，后缀为[B]，共有元素的长度为0；“ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；“ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；“ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1；“ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2；“ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。 了解了KMP的原理之后，来看一下代码该怎么写。 举个栗子： Text = a b a c a a b a c c a b a c a b a a b b Pattern = a b a c a b 根据前面的Partial Match Table, 我们可以算出Pattern的这个表 P a b a c a b steps 0 0 1 0 1 2 此时我们用两个指针 i 和 j 来表示 Text 和 Pattern 中的字符。 当 T[ i : i + j ] == P[ 1 : j ] 的时候，就是 Text 中包含了我们需要查找的 Pattern 先让 i 和 j 都从 1 开始（python代码中从0开始） 当T[ i ] = P[ j ]的时候，此时指针在 Text 和 Pattern 上都往前各走一步，即j+1，i+1 当 i = 6，j = 6 的时候，我们可以看出T[ i ] != P[ j ]，此时 j 就不能再继续往前走了，需要退回去几步。 那么到底是几步呢，经过上面查表，此时匹配到5，重复的字符串个数为1，意思是对于这个字符串 abaca，abaca 和 abaca 中有一个重复了，我们就不需要再比较这个，跳过这个字符，移动的个数为 6 - 1 = 5，将字符串 Pattern 向前挪5位，新的 j 就等于1了，然后重复之前的步骤。 Python代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091DEBUG = TrueDEBUG = False# 生成 partial match tabledef PMT(sent): length = len(sent) result = [0] for i in range(2, length + 1): sub_sent = sent[0 : i] sub_len = len(sub_sent) surfix = [] prefix = [] for j in range(sub_len): prefix.append(sub_sent[0 : j]) surfix.append(sub_sent[j + 1: sub_len ]) prefix.remove('') surfix.remove('') all_fix = list(set(prefix + surfix)) max_len = 0 com_str = '' for fix in all_fix: if fix in prefix and fix in surfix: if len(fix) &gt; max_len: max_len = len(fix) com_str = fix result.append(max_len) return result# KMP算法def KMP(text, pattern): if (len(text) &lt; len(pattern)): return 'Not Match' pmt = PMT(pattern) if DEBUG: print('Current Pattern is = ', pattern) print('Partial Match Table = ', pmt) i = 0 j = 0 while(i &lt; len(text)): if (text[i] == pattern[j]): if (j == (len(pattern) - 1)): return 'Match, the position in text is: &#123;&#125; - &#123;&#125; = &#123;&#125;'.format(i - j, i, text[i-j: i]) i += 1 j += 1 continue else: if j &gt;= 1: j = pmt[j - 1] else: i += 1 return 'Not Match'# 测试函数def test(text, pattern): result = KMP(text, pattern) print('Resutl = ', result) print('Pattern =', pattern) if DEBUG: for i in range(len(text)): print('&#123;:3s&#125;'.format(str(i)), end='-') print('') for i in range(len(text)): print('&#123;:3s&#125;'.format(text[i]), end='-') print('') print('\\n')# 测试部分text = 'BBC ABCDAB ABCDABCDABDE'pattern = 'ABCDABD'test(text, pattern)text = 'a b a c a a b a c c a b a c a b a a b b'pattern = 'a b a c a b'test(text, pattern)text = 'BBC ABCDAB ABCDABCDABE'pattern = 'ABCDABD'test(text, pattern)text = 'BBC AB'pattern = 'ABCDABD'test(text, pattern)text = 'ABCDABD'pattern = 'ABCDABD'test(text, pattern) 测试结果 12345678910111213141516171819202122232425Text = BBC ABCDAB ABCDABCDABDEPattern = ABCDABDResutl = Match, the position in text is: 15 - 21 = ABCDABText = a b a c a a b a c c a b a c a b a a b bPattern = a b a c a bResutl = Match, the position in text is: 20 - 30 = a b a c a Text = BBC ABCDAB ABCDABCDABEPattern = ABCDABDResutl = Not MatchText = BBC ABPattern = ABCDABDResutl = Not MatchText = ABCDABDPattern = ABCDABDResutl = Match, the position in text is: 0 - 6 = ABCDAB[Finished in 0.1s]","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"Hadoop权威指南笔记（一）","date":"2017-03-06T13:44:33.000Z","path":"2017/03/07/Hadoop权威指南笔记（一）/","text":"1. Hadoop1.1 初识Hadoop非常好的Tutorial 在学习hadoop之前，我觉得有必要了解一下hadoop的基本构成以及一些术语。 Block HDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks. By making a block large enough, the time to transfer the data from the diskcan be made to be significantly larger than the time to seek to the start of the block.Thus the time to transfer a large file made of multiple blocks operates at the disk transferrate.A quick calculation shows that if the seek time is around 10 ms, and the transfer rateis 100 MB/s, then to make the seek time 1% of the transfer time, we need to make theblock size around 100 MB. The default is actually 64 MB, although many HDFS installationsuse 128 MB blocks. This figure will continue to be revised upward as transferspeeds grow with new generations of disk drives.This argument shouldn’t be taken too far, however. Map tasks in MapReduce normallyoperate on one block at a time, so if you have too few tasks (fewer than nodes in thecluster), your jobs will run slower than they could otherwise. Node 简单的说就是一台主机，一台电脑。在hadoop中，有NameNode, DataNode, Secondary NameNode, JobTracker Node, TaskTracker Node, CheckpointNode 和 BackupNode。对一个cluster，NameNode只能有一个，DataNode可以有多个 Rack 中文机柜/机架，就是用来存放node的storage，通常一个rack有几十个nodes组成，这些nodes存放在同一个机柜，连接一个交换机 A Node is simply a computer. This is typically non-enterprise, commodity hardware for nodes that contain data. Storage of Nodes is called as rack. A rack is a collection of 30 or 40 nodes that are physically stored close together and are all connected to the same network switch. Network bandwidth between any two nodes in rack is greater than bandwidth between two nodes on different racks.A Hadoop Cluster is a collection of racks. 1.2 Major Components Distributed Filesystem — hadoop中是HDFS MapReduce 1.5 Install configuration123456# 待整理hadoop fshadoop dfs# fs refers to any file system, it oucld be local or HDFS# but dfs refers to only HDFS file system 2. MapReduce2.1 初识MapReduce 整个过程可以分为三个阶段，Input， MapReduce and Output 在input和output阶段，数据是存在HDFS文件系统中，其系统的block size大小默认是64/128MB。 在MapReduce中，又可以分为两个阶段，Map and Reduce，数据从map function到reduce function是存在local disk中，(soreing in HDFS with replication would be overkill)，然后通过network传输数据. 在每个阶段中，input和output的数据都是以 (key, values) 格式进行处理的，然后通过 map function 和 reduce function 进行处理。在本例中，input data的key是从数据文件开始处的行数的偏移量，但是map function输出的key是年份数据，以及reduce function输出的key是也不同的。所以这三个key-value pairs是不同的。 原始数据 Key-Values 以上为原始数据中input进来后的key-values的数据。然后map function阶段，提取出上面文件中的 1950 和 0001 之类的数据，组成新的key-values作为输出给下一阶段。 Key-Values in Map Function 在将Map Function的输出传给Reduce Function之前，实际上MapReduce Framework还是有对数据进行一个处理步骤。从最上的图一中，我们仍然可以看到Map和Reduce之间有一个 Shuffle 的过程。因为之前我们提到了，Map的过程中，只是实现了一个key-value匹配的过程，所有出来的数据也是无序的，而 Shuffle 就是对这个输出 sort &amp; group 的过程，然后将输出传给 Reduce Function 进行处理 当数据从Reduce Function中处理完后出来的大概如下，注意这个reduce只是选择最大值，其他reduce function可能做的是统计或者实现其他功能。 现在再看另外一个经典的WordCount的例子 在Hadoop系统中，处理一个wordcount的任务可以大致分成四个主要阶段，input，map，reduce，output。其中 Map 和 Reduce 可以继续细分，即分成多个 map tasks 和 reduce tasks。 这些tasks然后被 YARN 给分配集群中多台不同的机器处理。这其中的细节等到往后再讨论。 上面提到的分成多个tasks时，应该是input data切片分给多个maps（而不是一个大的map分成多个小的tasks）， 每个MapReduce分到一个fixed-sized 的数据，通常是64/128MB，这个过程叫做 input splits。然后每个split分配一个map task，同时运行在不同的机器上处理。这样划分的好处是有利于load-balancing，对于性能较好的机器可以处理更过是splits。 2.2 Data Flow 上图可以看出hadoop的整个数据流向，其中虚线代表是在一个node，实线代表的是不同node之间。在同一个node之间，数据的读取存储就有速度上的优势，不同node之间，也就是不同主机之间，就必须通过network进行传输，速度较慢。 Partition 当只有一个reduce的时候，map function的output当然就直接传给这个reduce了。但是当有多个reduce的时候，怎么办呢？此时map会将其输出进行partition(分区)，每一个reduce的任务都会创建一个分区，且每一个reduce task都会有一个partition (There can be many keys (and their associated values)in each partition, but the records for any given key are all in a single partition)，也就是说同一个key会在同一个partition中。 Shuffle and Sort 在map和reduce之间的data flow是Shuffle，从上图可以看出，一个reduce可以接受来自多个不同的map的output，其中包含了sort，partition等过程。 Combiner Functions 之前我们讨论过，data flow在map和reduce之间是通过network进行传输的，但我们知道map function的output是一个个key-value的键值对的，这些key-value paris中，有些是可以通过combiner function进行combine的，这样做的目的是减小map和reduce之间传输的数据大小，加快传输数据。 Combiner Function在许多情况和 Reduce Function是很像的，因为做的工作和reduce是比较类似的，只是处理的是局部map的output(因此Combiner是运行在map output端)，减少data flow的size。但是对于是否调用combiner function，这个是不确定的。因为有些情况下的output是不适合进行combine，有些则又是要多次调用进行合并。因为这个，Combiner是可选的，即可以调用，也可以不调用，当不调用的时候，就必需不能影响程序的正常运行。所以Combiner的input和output是一样的，和Mapper的output、Reducer的input一样。 对于有些特殊情况，甚至连reduce function都不需要。 举个栗子： 适用情况（Commutative &amp; Associative） ​ Reduce Function Combiner Function ​ Commutative: max(a, b) = max(b, a) ​ Associative: max(max(a, b), c) = max(a, max(b, c)) 不适用情况： 伪代码 In-Combiner Function Advantage 相比Combiner，In-Combiner的效率更高。 可以减少一些Mapper和Reducer之间的key-value pairs，可以减少处理这部分的开销。因为Combiner只是减少了一些Mapper和Reducer之间的intermediate data，但是并没减少从Mapper的output出来的key-value pairs的数量。但是In-Combiner是是Mapper 的一部分，也就是说key-value pairs在Mapper 输出前就已经减少了。 减少了key-value pairs可以减少系统的object serialization and deserialization 的开销，即垃圾回收机制 Disadvantage 内存使用，因为要保存一个array在内存中，当数据量很大的时候有可能会爆了。解决方案 有两个，第一是限制array的个数，第二是限制内存的使用。当这俩到达某一个阈值的时候，就发送给Reducer。 第二是讲一个Map的过程分成几个部分，导致debug中可能出现oedering-dependent bugs，调试可能比较困难。 ​ ​ ​","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Recommender-Systems/","text":"1. What is Recommender Systems对于推荐系统的定义，我们先举几个例子来理解一下。 电影网站给用户推荐电影，可以根据该用户以往的评分，比如给浪漫爱情电影评分高，给动作片评分较低，那么系统可以根据这些信息，给用户推荐偏向浪漫爱情的电影 如果是新用户呢？我们没有该用户的评分信息。那么我们可以根据整个系统中，某些电影评分较高进行推荐 那么如果是新网站，新用户呢？ 以上例子，我们可以把推荐系统分成两类。 Content-based systems Content-based，就是基于已有的信息进行推荐。具体哪些信息呢？在上面的电影推荐系统中，有两类信息需要分析。 第一，是User的评分信息，比如给爱情片评分高，给动作片评分低。 第二，是Movie的特征信息，比如这部电影偏向爱情片多一些，但也有一部分搞笑。所以在A（爱情片）和B（搞笑片）中， A的权重更高，B的较低 基于以上两部分信息，我们可以给用户推荐他所喜欢的电影。 Collaborative filterring systems 协同过滤器，则是基于用户/物品之间的相似度进行推荐的。即用户A和用户B都喜欢爱情、浪漫电影，我们就可以把用户A评分过的爱情浪漫电影，推荐给用户B。 2. Content-based systems2.1 Problem Analysis以电影推荐系统为例，假设我们已经对系统中的电影特征有了较为完善，即我们知道某部电影属于爱情片多少分，属于动作片多少分。 那么我们现在以Alice为例，她对两部爱情片评分比较高，对于两部动作片评分为0。那么系统就可以给Alice推荐偏向爱情浪漫的，且不怎么属于动作片的电影。 Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love ？ 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 2.2 Optimization Objective实际上我们已经假设之前对所有电影的特征进行了统计，所以此时有电影特征向量X，以及用户对于电影的评分Y向量。根据此时已有的信息，我们需要求出theta的值。所以能够对于那么没有评分过的电影，根据theta和x求出分数y。 因为一开始theat的值是随机的，所以我们用Linear Regression的方法，不断减少cost function的值求出theta。 值得注意的是，因为这里是多个用户，每一个用户我们求出一个theta值。最后对于多个用户，我们需要求出多个theta值。 Actually, we can assume that we have known all features about the all movies, that is x1, x2, …, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 2.2 Gradient descent update 2.3 Gradient descent in Logistic Regression Question: Why we don’t need $\\frac 1 m$? Anwser: As there are only one user However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to learn theta and features at the same time 3. Collaborative filtering3.1 Proble motivation Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love ? 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 3.2 How to do 在之前部分中，我们了解到了content-based，是已知 x 和 y，求 theta。 Assume:$$\\theta^{(1)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(2)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(3)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\space\\theta^{(4)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\spacex^{(1)} = \\begin{bmatrix} 1 \\ 1.0 \\ 0.0 \\end{bmatrix}$$For Movie 1, we can calculate the result of Movie1 rating by all users.$$\\theta^{(1)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(2)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(3)} * x^{(1)} \\approx 0 \\\\\\\\theta^{(4)} * x^{(1)} \\approx 0$$ 但是对于有些情况，我们并不知道x的特征值，该怎么办呢？ 逆向思考，我们也可以通过 theat 和 y，来求 x 的值。 那么对于 theta和x的值都不知道的情况下呢？ 对比特征 Linear Regression Collaborative filtering 特性向量X 已知数据 待求解数据 权重 θ 待求解数据 待求解数据 y值 已知数据 已知数据 3.3 Optimization Algorithm For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. $$θ -&gt; x -&gt; θ -&gt; x -&gt; θ -&gt; x -&gt; θ -&gt; x -&gt; …$$ Actually, these two steps are Linear Regression, we shoud do that simultaneously to update theta and x. 3.4 Collaborative filtering Optimization Algorithm 实际上，上面是两个 LR的问题，我们可以将上面两步合并到一起，这个就是collaborative filterring， 此时的optimizatino object 就从 J(theta) 和 J(X) 变为了 J(theta, X)。 具体步骤如下 3.5 Vectorization: Low rank matrix factorization首先，我们先把评分Y用向量表示出来，同时表示为Theta和X两个矩阵的乘积$$Y= \\begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \\ 5 &amp; ? &amp; ?&amp; 0 \\ ? &amp; 4 &amp; 0 &amp; ? \\ 0 &amp; 0 &amp; 5 &amp; 4 \\ 0 &amp; 0 &amp; 5 &amp; 0\\end{bmatrix} =\\begin{bmatrix}(\\theta^{(1)})^T(x^{(1)}) &amp; (\\theta^{(2)})^T(x^{(1)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(1)}) \\\\(\\theta^{(1)})^T(x^{(2)}) &amp; (\\theta^{(2)})^T(x^{(2)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(2)}) \\\\… &amp; … &amp; … &amp; … \\\\(\\theta^{(1)})^T(x^{(n_m)}) &amp; (\\theta^{(2)})^T(x^{(n_m)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(n_m)})\\end{bmatrix} = X * \\Theta’, R \\in (n_m × n_u)$$ $$X = \\begin{bmatrix}—(x^{(1)})^T— \\\\—(x^{(2)})^T— \\\\… \\\\—(x^{(n_m)})^T—\\end{bmatrix},x^{(n_m)} = \\begin{bmatrix}x^{(n_m)}_1 \\ x^{(n_m)}_2 \\ … \\ x^{(n_m)}_n\\end{bmatrix}, R \\in (n_m × n)$$ $$\\Theta = \\begin{bmatrix}—(\\theta^{(1)})^T— \\\\—(\\theta^{(2)})^T— \\\\… \\\\—(\\theta^{(n_u)})^T—\\end{bmatrix},\\theta^{(n_u)} = \\begin{bmatrix}\\theta^{(n_u)}_1 \\ \\theta^{(n_u)}_2 \\ … \\ \\theta^{(n_u)}_n\\end{bmatrix}, R \\in (n_u × n)$$ 3.6 Mean Normalization对于那些新注册用户，系统中没有记录他们的偏好，则采用以下方法。 先计算出每部电影评分的平均值mu，然后把所有的评分都减去平均值（此后处理过的评分平均值为0）。虽然这样做对有评分记录用户是多余的，但却可以吧没有评分记录的用户给统一进来，避免全是0的情况。 4. Implement Algorithm4.1 Cost Function without Regularization Tips：这里需要计算的只是针对那些已经评分过的电影，对于用户没有评分过的不需要计算。 4.2 Collaborative filtering gradient$$\\frac {\\partial J} {\\partial x_k^{(1)}} , \\frac {\\partial J} {\\partial x_k^{(2)}} , …, \\frac {\\partial J} {\\partial x_k^{(n_m)}} \\space\\space for \\space each \\space movie\\\\\\frac {\\partial J} {\\partial \\theta_k^{(1)}} , \\frac {\\partial J} {\\partial \\theta_k^{(2)}} , …, \\frac {\\partial J} {\\partial \\theta_k^{(n_u)}} \\space\\space for \\space each \\space user$$Tips： 对于使用vectorization方法，最终只有两个for-loop，一个计算$X_{grad}$，一个计算$Theta_{grad}$ 如何对X和Theta求偏导数？ $$(Theta_{grad}(i, :))^T = \\begin{bmatrix}\\frac {\\partial J} {\\partial \\theta^{(i)}_1} \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_2} \\\\… \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_n}\\end{bmatrix}$$ 同样，我们只需考虑用户已经评分过的电影，用其作为训练样本 因为Vectorization非常容易搞乱各个matrix，所以建议先整理一下各个matrix的size，计算时可以根据matrix的size进行计算。 4.3 Implementation注意这里并没有给出完整的代码 (Octave/Matlab)，都只是主要的部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950% Theta : nu x n% X : nm x n% Y : nm x nu% R : nm x nupred = X * Theta'; %nm x nu 'diff = pred - Y;% Cost Function with regularizationJ = 0.5 * sum(sum((diff.^2) .* R));J = J + (lambda * 0.5) * sum(sum(Theta.^2)); % regularized term of theta.J = J + (lambda * 0.5) * sum(sum(X.^2)); % regularized term of x.% calculate Xfor i = 1 : num_movies, % the row vector of all users that have rated movie i idx = find(R(i, :) == 1); % (1 * r) % the list of users who have rated on movie i Theta_temp = Theta(idx, :); % (r * n) Y_temp = Y(i, idx); % (1 * r) X_temp = X(i, :); % (1 * n) % ((1 * n) * (n * r) -(1 * r)) * (r * n) = (1 * n) X_grad(i, :) = (X_temp * Theta_temp' - Y_temp) * Theta_temp; %' % regularization X_grad(i, :) = X_grad(i, :) + lambda * X_temp;end% calculate Thetafor i = 1 : num_users, % the row vector of all movies that user i has rated idx = find(R(:, i) == 1)'; % (1 * r) ' Theta_temp = Theta(i, :); % (1 * n) Y_temp = Y(idx, i); % (r * 1) X_temp = X(idx, :); % (r * n) % ((r * n) * (n * 1) - (r * 1)) * (r * n) = (1 * n) Theta_grad(i, :) = (X_temp * Theta_temp' - Y_temp)' * X_temp; % regularization Theta_grad(i, :) = Theta_grad(i, :) + lambda * Theta_temp;endgrad = [X_grad(:); Theta_grad(:)]; ​​​","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Machine Learning - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-SVM/","text":"1. Optimisation Objective$$h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\\\z = -\\theta^Tx$$ Why we need do that? 2. Hypothesis Function2.1 Logistic Regression$$\\frac 1 m \\sum_{i=1}^m [ y^{(i)} (-log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) (-log(1 - h_\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum_{j=1}^n \\theta_j^2$$ 2.2 Support Vector Machine$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ Analysis： 为了使得cost function取得最小值，我们令C*W + P部分中，C*W为零。即： 当 y = 1时， cost1 = 0，所以 z &gt;= 1 当 y = 0时， cost0 = 0，所以 z &lt;= -1 Note：1. cost0 and cost1 对应的是上图中左右两边的cost function，因为y=0和y=1的目标函数。 常数C取一个很大的值时比较好。因为C*W + P， 所以C大则W会变小，即相对penality就会变大，W会变小 为什么要重新选定一个cost function ？（逻辑回归的临界点为0，但是SVM的临界点是1，所以SVM更加精确。 ） 对应的线性逻辑回归？即次数不大于1的？ Decision Boundary 不是一条直线的情况 3. Large Margin Classifier12结论：常数C取一个比较大的值比较容易获得Large Margin ClassifierC大，则比较容易获得 以上为两类分布比较均匀的时候，Decision Boundary为图中黑色的线，所有点离黑色的距离都相对比较大比较均匀，但是当存在干扰点的时候如下图，Decision Boundary会由黑色变为粉红色。所以C的取值不能太大，也不能太小。需要求出最优解 4. Mathmatics Behind Large Margin Classification4.1 Vector Inner Product Note： 如何求投影p的值？ 当角度 &lt; 90°，p为正数。当角度 &gt; 90°时，p为负数。 向量内积$$u^Tv = ||u|| · ||v|| · cosθ = ||u|| · p_{v,u} = ||v|| · p_{u,v} = u_1v_1+u_2v_2$$ 4.2 SVM Cost Function$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ 当C取一个一个很大的值时，cost function只剩下后面P的部分。 假设θ0 = 0$$\\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + … + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + … + \\theta^2_n)= \\frac 1 2 ||\\theta||^2$$ 所以：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\p^{(i)}是点到向量\\theta的projection，即点到Decision Boundary的距离$$上面我们讨论了，当C取到一个合适的、较大的数值时，SVM的cost function就只剩下后面P的部分，即$$\\frac 1 2 ||\\theta||^2$$我们要减小cost function，所以需要减小θ的值。 当θ取到一个比较小的值的时候，还需要满足上面讨论的：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\$$所以θ比较小时，只能增加p的值去满足p*||θ|| &gt;= 1 或者 p*||θ||&lt;= -1。 这样就保证了p的值比较大，即点到Decision Boundary的大间距。 5. Kernels5.1 Kernels &amp; Similarity首先，我们回想一下之前的logistic regression中对于non-linear 情况的拟合。 Predict y = 1, if$$\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3x_2 + \\theta_4x_1^2 + \\theta_5x_2^2 + … &gt;= 0 \\\\\\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + … &gt;= 0$$即将fn定义为x的幂次项组合，如下：$$f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, …$$ 但是在SVM中，我们要重新定义fn，引入Kernel的概念，即用 kernel function来表示fn。 Note: l 是landmark，且如果training sets里面的数量为n的话，则landmark的数量也为n。 假设training sets数量为n，则对于一个新的example来说，可计算出n个新的特征f1…fn。然后用新的特征，对该example进行判断（低维转为高维的过程） kernel function为guassian function。当x与landmark l越接近时，两点的距离越小，值接近1 5.2 SVM with Kernels 对比之前的cost function，可以发现这里θ和f(x)跟之前的不同。 在logistic regression 中，θ的维度为(n+1) x 1, 包含θ0， 且n为单个example的特征个数 在SVM with kernel中，f(x)的个数为m，其中m是training sets中的个数，所以θ的维度应该是(m+1)x1 Steps 给定一组training sets，根据每个example，选取m个landmark点 计算每一个example与所有landmark的相识度，相同为1，非常不同接近为0。计算相识度的kernel function为Gaussian Function 最终，对于每一个example里面都可以计算出m个新的feature，所以对于这个training sets而言，会得到一个m*m的矩阵？ 将得到的m*m的矩阵，代入到Hypothesis中，计算出θ的值。 5.4 SVM parameters C = 1/λ Large C Small λ Large θ Lower Bias High Variance Over Fitting Small C Large λ Small θ Higher Bias Low Variance Under Fitting σ Large σ more smoothly Higher Bias Lower Variance Under Fitting Small σ less smoothly Lower Bias Higher Variance Over Fitting","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Python数据分析笔记（一）","date":"2016-12-15T11:37:30.000Z","path":"2016/12/15/Python数据分析笔记（一）/","text":"1. 常见问题 Pandas.dataframe里面 .values, .iloc, .ix, .loc 的区别 Different Choices for Indexing loc: only work on index / labeliloc: work on position, from 0, 1, 2, … …ix: You can get data from dataframe without it being in the indexat: get scalar values. It’s a very fast lociat: Get scalar values. It’s a very fast iloc 12345678910111213141516171819202122232425262728&gt;&gt;&gt; df = pd.DataFrame(&#123;'A':['a', 'b', 'c'], 'B':[54, 67, 89]&#125;, index=[100, 200, 300])&gt;&gt;&gt; df A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df.iloc[0] # 用位置来索引A aB 54&gt;&gt;&gt; df.loc[100] # 用初始化时设置的index来索引，也就是自己给row设置的labelA aB 54&gt;&gt;&gt; df.loc[100:300] A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df['A'] # 索引 columns100 a200 b300 cName: A, dtype: object&gt;&gt;&gt; df.A # 索引 columns100 a200 b300 cName: A, dtype: object Pandas 和 Numpy之间的转换 np.ndarray 转化为 pd.dataframe 1pd.DataFrame(example) pd.dataframe 转化为 np.ndarray 1example.values[:, :] 读写效率的对比 npy读写效率最高，但最费硬盘空间，比如np.load(), np.save() csv其次，比如pd.Dataframe.to_csv()，pd.load_csv() txt读写，当然也可以很快，但是需要频繁的split，对格式规范的数据比较麻烦 至于简单的excel和word，可以用xlrd,xlwt来操作 2. Numpy N维数组对象，可以利用这种数组对象对整块数据进行一些科学运算，就是把array当做一种对象里操作。这和Python中的array是不同的。 举个栗子： 在Python中 123&gt;&gt;&gt; data = [1, 2, 3]&gt;&gt;&gt; data * 10[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3] Numpy的ndarray 12345&gt;&gt;&gt; data2 = np.array([1, 2, 3])&gt;&gt;&gt; data2array([1, 2, 3])&gt;&gt;&gt; data2 * 10array([10, 20, 30]) 轴(axes) 和 秩(rank) 轴表示的是一种维度，如一维的数据，二维的数据，三维的数据等 12345678910111213141516171819202122&gt;&gt;&gt; data3 = np.array([1, 2, 3]) # 注意这里的**方括号**&gt;&gt;&gt; data3.ndim # 查看维度1&gt;&gt;&gt; data3 = np.array([[1,2,3], [1,2,3], [1,2,3]])&gt;&gt;&gt; data3.ndim2&gt;&gt;&gt; data3 = np.array([[[1], [1], [1]], [[1], [1], [1]],[[1], [1], [1]]])&gt;&gt;&gt; data3.ndim3&gt;&gt;&gt; data3.shape(3, 3, 1) # 维度从最外层到里层&gt;&gt;&gt; data3.size9 # 3 * 3 * 1 = 9#一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。&gt;&gt;&gt; data3.dtype# 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8).&gt;&gt;&gt; data3.itermsize# 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。&gt;&gt;&gt; data3.data 常用的数组创建函数 打印数组 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(6) # 1d array&gt;&gt;&gt; print(a)[0 1 2 3 4 5]&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(12).reshape(4,3) # 2d array&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]&gt;&gt;&gt;&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4) # 3d array&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 如果一个数组用来打印太大了，NumPy自动省略中间部分而只打印角落 1234567891011&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt;&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] 禁用这种reshape来打印整个数组，需要对printoption参数进行设置 1&gt;&gt;&gt; np.set_printoptions(threshold='nan') 基本的数据运算 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; b = np.array(list(range(5, 10)))&gt;&gt;&gt; a + barray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; a - barray([-5, -5, -5, -5, -5])&gt;&gt;&gt; a * barray([ 0, 6, 14, 24, 36])&gt;&gt;&gt; a += b&gt;&gt;&gt; aarray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; c = np.ones(5)&gt;&gt;&gt; carray([ 1., 1., 1., 1., 1.])&gt;&gt;&gt; c + aarray([ 6., 8., 10., 12., 14.])&gt;&gt;&gt; c = np.ones(5, dtype=int)&gt;&gt;&gt; c + aarray([ 6, 8, 10, 12, 14]) 数组的运算 这就类似在Matlab/Octave中，对matrix/array中的数据执行批量运算，即Vectorization，前提是matrix/array的大小必须满足对应的要求。 用数组表达式可以代替循环操作，矢量化的运算是Numpy的优势。 数组转置和轴对换 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.T # 数组转置，轴对换array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]])&gt;&gt;&gt; np.dot(a.T, a) # 内积array([[125, 140, 155, 170, 185], [140, 158, 176, 194, 212], [155, 176, 197, 218, 239], [170, 194, 218, 242, 266], [185, 212, 239, 266, 293]])&gt;&gt;&gt; b = np.arange(16).reshape((2, 2, 4))&gt;&gt;&gt; barray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt;&gt;&gt; b.transpose((1, 0 , 2)) # 对高维数组，transpose需要得要一个由轴编号组成的元组才能对这些轴进行转置array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) 索引和切片 1234567891011121314151617181920212223242526272829303132333435# 一维数据&gt;&gt;&gt; a = np.arange(5)array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:]array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[1 : 4]array([1, 2, 3])&gt;&gt;&gt; a[1]1# 二维数据&gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6]])array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:, 1:3]array([[2, 3], [5, 6]])&gt;&gt;&gt; b[1][2]6&gt;&gt;&gt; b[1, 2]6# 三维数组&gt;&gt;&gt; c = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; c[1]array([[ 7, 8, 9], [10, 11, 12]]) 通用函数 P111 给 array 添加 columns 和 rows 123456789101112# 方法一np.c_[array1, array2] # 添加 columnsnp.r_[array1, array2] # 添加 row# 方法二 被插入的行np.insert(a, 2, values=b, axis=1) # 添加 columns# 方法三a = np.concatenate((a, -np.ones((a.shape[0], 1))), axis=1) # 添加 columns# 方法四np.column_stack((a,b)) 3. Pandas在Pandas中，Series和DataFrame是两个主要的数据结构 Series 类似一维数组，由一组数据（各种Numpt数据类型 ( list, dict等 )）和一组对于的数据标签组成 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&gt;&gt;&gt; obj = pd.Series(list(range(3, 8)))&gt;&gt;&gt; obj0 31 42 53 64 7dtype: int64&gt;&gt;&gt; obj.valuesarray([3, 4, 5, 6, 7])&gt;&gt;&gt; obj.index # 这里就是 index 索引，没有设置的是时候系统会自动设置为integer indexRangeIndex(start=0, stop=5, step=1)&gt;&gt;&gt; obj[1] # 可以用这索引，跟list用法类似，但是list中只能是数字，但pandas中可以自定义index的索引4# 自定义索引，普通的list和Numpy的数组就不行&gt;&gt;&gt; obj2 = pd.Series([1,3,4], index=['a', 'b', 'c'])&gt;&gt;&gt; obj2['a']1# 索引可以直接用来数组运算，这些在数据清洗的时候比较常用&gt;&gt;&gt; index = obj2 &gt; 2&gt;&gt;&gt; indexa Falseb Truec Truedtype: bool &gt;&gt;&gt; obj2[index]b 3c 4dtype: int64# 其他操作&gt;&gt;&gt; np.log(obj2)a 0.000000b 1.098612c 1.386294dtype: float64 # 当做字典来index&gt;&gt;&gt; 2 in obj2False &gt;&gt;&gt; 3 in obj2False&gt;&gt;&gt; 'a' in obj2 # 只能是index (key)，不能是valuesTrue# 用字典来初始化 Series&gt;&gt;&gt; obj3 = &#123;'one': 1, 'two': 2, 'three' : 3&#125;&gt;&gt;&gt; obj3&#123;'one': 1, 'three': 3, 'two': 2&#125;&gt;&gt;&gt; index = ['One', 'Two', 'Three']&gt;&gt;&gt; value = [1, 2, 3]&gt;&gt;&gt; obj4 = pd.Series(value, index)&gt;&gt;&gt; objOne 1Two 2Three 3dtype: int64 # 手动修改索引，且索引的值是不能重复的&gt;&gt;&gt; obj4.index = ['one', 'two', 'Three']&gt;&gt;&gt; obj4one 1two 2Three 3dtype: int64 # 重建索引，如果索引不存在的值，则引入NaN&gt;&gt;&gt; obj4.reindex(['one', 'two', 'Threeee']) # fill_value=0one 1.0two 2.0Threeee NaNdtype: float64# 可选 ffill/pad 向前填充或者bfill/backfill 向后填充&gt;&gt;&gt; obj4.ffill()one 1.0two 2.0Threeee 2.0dtype: float64# 当然也可以drop columns的内容&gt;&gt;&gt; obj4.drop('Threeee')one 1.0two 2.0dtype: float64 # 但是不能这么修改数值&gt;&gt;&gt; obj4.values = [4, 5, 6]---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2702 else:-&gt; 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):AttributeError: can't set attributeDuring handling of the above exception, another exception occurred:AttributeError Traceback (most recent call last)&lt;ipython-input-41-b4392e8960b0&gt; in &lt;module&gt;()----&gt; 1 obj4.values = [4, 5, 6]/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):-&gt; 2705 object.__setattr__(self, name, value) 2706 2707 # ----------------------------------------------------------------------AttributeError: can't set attribute ​ DataFrame DataFrame是一个表格型的数据结构，包含了一组有序的列，每列可以是不同的值类型。所有可以看做这是一个二维的数组，有行索引和列索引 创建DataFrame和基础操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; dates = pd.date_range('20170305', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6, 4), index=dates, &gt;&gt;&gt; columns=list('ABCD'))&gt;&gt;&gt; df A B C D2017-03-05 -1.616361 0.027641 0.328074 1.0386272017-03-06 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 -0.350594 0.231137 -0.004755 0.6014602017-03-08 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 -0.227631 -0.186816 -0.370522 0.343896# 当然也可以手动传进来创建df&gt;&gt;&gt; df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)&gt;&gt;&gt; df2Ones A B C D2017-03-05 1 -1.616361 0.027641 0.328074 1.0386272017-03-06 1 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 1 -0.350594 0.231137 -0.004755 0.6014602017-03-08 1 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 1 -0.227631 -0.186816 -0.370522 0.343896# 看DataFrame的attributes&gt;&gt;&gt; df.&lt;TAB&gt;df.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D 查看DataFrame里面的数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt;&gt; df.head() # 默认5行 A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401&gt;&gt;&gt; df.tail(3) # 手动设置打印的行数 A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988&gt;&gt;&gt; df.index # index 是 row 的 索引DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df.columnsIndex([u'A', u'B', u'C', u'D'], dtype='object')&gt;&gt;&gt; df.valuesarray([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804&gt;&gt;&gt; df.sort_index(axis=1, ascending=False) # 是否按照 columns的值下降来排序 D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690&gt;&gt;&gt; df.sort_values(by='B') # 按照columns B 升序来 A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 索引columns和rows 4. Reference python，numpy，pandas数据处理之小技巧 ####","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.com/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.com/tags/Pandas/"}]}]