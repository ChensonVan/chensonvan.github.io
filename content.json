[{"title":"å›¾æ•°æ®åº“-åæ¬ºè¯ˆç½‘ç»œè®¾è®¡","date":"2018-08-18T06:45:01.000Z","path":"2018/08/18/å›¾æ•°æ®åº“-åæ¬ºè¯ˆç½‘ç»œè®¾è®¡/","text":"","tags":[]},{"title":"å›¾æ•°æ®åº“-Neo4j-å¸¸ç”¨ç®—æ³•","date":"2018-08-18T06:03:23.000Z","path":"2018/08/18/å›¾æ•°æ®åº“-Neo4j-å¸¸ç”¨ç®—æ³•/","text":"æœ¬æ¬¡ä¸»è¦å­¦ä¹ å›¾æ•°æ®åº“ä¸­å¸¸ç”¨åˆ°çš„ä¸€äº›ç®—æ³•ï¼Œä»¥åŠå¦‚ä½•åœ¨Neo4jä¸­è°ƒç”¨ï¼Œæ‰€ä»¥è¿™ä¸€ç¯‡åå®æˆ˜ï¼Œæ¯ä¸ªç®—æ³•çš„åŸç†å°±ç®€å•çš„æä¸€ä¸‹ã€‚ 1. å›¾æ•°æ®åº“ä¸­å¸¸ç”¨çš„ç®—æ³• PathFinding &amp; Search ä¸€èˆ¬ç”¨æ¥å‘ç°Nodesä¹‹é—´çš„æœ€çŸ­è·¯å¾„ï¼Œå¸¸ç”¨ç®—æ³•æœ‰å¦‚ä¸‹å‡ ç§ Google Search Results Dijkstra - è¾¹ä¸èƒ½ä¸ºè´Ÿå€¼ Folyd - è¾¹å¯ä»¥ä¸ºè´Ÿå€¼ï¼Œæœ‰å‘å›¾ã€æ— å‘å›¾ Bellman-Ford SPFA Centrality ä¸€èˆ¬ç”¨æ¥è®¡ç®—è¿™ä¸ªå›¾ä¸­èŠ‚ç‚¹çš„ä¸­å¿ƒæ€§ï¼Œç”¨æ¥å‘ç°æ¯”è¾ƒé‡è¦çš„é‚£äº›Nodesã€‚è¿™äº›ä¸­å¿ƒæ€§å¯ä»¥æœ‰å¾ˆå¤šç§ï¼Œæ¯”å¦‚ Degree Centrality - åº¦ä¸­å¿ƒæ€§ Weighted Degree Centrality - åŠ æƒåº¦ä¸­å¿ƒæ€§ Betweenness Centrality - ä»‹æ•°ä¸­å¿ƒæ€§ Closeness Centrality - ç´§åº¦ä¸­å¿ƒæ€§ Community Detection åŸºäºç¤¾åŒºå‘ç°ç®—æ³•å’Œå›¾åˆ†æNeo4jè§£è¯»ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹ ç”¨äºå‘ç°è¿™ä¸ªå›¾ä¸­å±€éƒ¨è”ç³»æ¯”è¾ƒç´§å¯†çš„Nodesï¼Œç±»ä¼¼æˆ‘ä»¬å­¦è¿‡çš„èšç±»ç®—æ³•ã€‚ Strongly Connected Components Weakly Connected Components (Union Find) Label Propagation Lovain Modularity Triangle Count and Average Clustering Coefficient 2. è·¯å¾„æœç´¢ç®—æ³• Shortest Path 1234567MATCH (start:Loc&#123;name:&quot;A&quot;&#125;), (end:Loc&#123;name:&quot;F&quot;&#125;)CALL algo.shortestPath.stream(start, end, &quot;cost&quot;)YIELD nodeId, costMATCH (other:Loc) WHERE id(other) = nodeIdRETURN other.name AS name, cost Single Source Shortest Path 123456MATCH (n:Loc &#123;name:&quot;A&quot;&#125;)CALL algo.shortestPath.deltaStepping.stream(n, &quot;cost&quot;, 3.0YIELD nodeId, distanceMATCH (destination) WHERE id(destination) = nodeIdRETURN destination.name AS destination, distance All Pairs Shortest Path 1234567891011CALL algo.allShortestPaths.stream(&quot;cost&quot;,&#123;nodeQuery:&quot;Loc&quot;,defaultValue:1.0&#125;)YIELD sourceNodeId, targetNodeId, distanceWITH sourceNodeId, targetNodeId, distanceWHERE algo.isFinite(distance) = trueMATCH (source:Loc) WHERE id(source) = sourceNodeIdMATCH (target:Loc) WHERE id(target) = targetNodeIdWITH source, target, distance WHERE source &lt;&gt; targetRETURN source.name AS source, target.name AS target, distanceORDER BY distance DESCLIMIT 10 Minimum Weight Spanning Tree 12345MATCH (n:Place &#123;id:&quot;D&quot;&#125;)CALL algo.spanningTree.minimum(&quot;Place&quot;, &quot;LINK&quot;, &quot;cost&quot;, id(n), &#123;write:true, writeProperty:&quot;MINST&quot;&#125;)YIELD loadMillis, computeMillis, writeMillis, effectiveNodeCountRETURN loadMillis, computeMillis, writeMillis, effectiveNodeCount; CASE 123456789101112131415MERGE (a:Loc &#123;name:&quot;A&quot;&#125;)MERGE (b:Loc &#123;name:&quot;B&quot;&#125;)MERGE (c:Loc &#123;name:&quot;C&quot;&#125;)MERGE (d:Loc &#123;name:&quot;D&quot;&#125;)MERGE (e:Loc &#123;name:&quot;E&quot;&#125;)MERGE (f:Loc &#123;name:&quot;F&quot;&#125;)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(b)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(c)MERGE (a)-[:ROAD &#123;cost:100&#125;]-&gt;(d)MERGE (b)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:80&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:30&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:80&#125;]-&gt;(f)MERGE (e)-[:ROAD &#123;cost:40&#125;]-&gt;(f); 3. ä¸­å¿ƒæ€§ç®—æ³• PageRank 123456CALL algo.pageRank.stream(&quot;Page&quot;, &quot;LINKS&quot;,&#123;iterations:20&#125;)YIELD nodeId, scoreMATCH (node) WHERE id(node) = nodeIdRETURN node.name AS page,scoreORDER BY score DESC Degree Centrality Betweenness Centrality 12345CALL algo.betweenness.stream(&quot;User&quot;, &quot;MANAGES&quot;, &#123;direction:&quot;out&quot;&#125;)YIELD nodeId, centralityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user,centralityORDER BY centrality DESC; Closeness Centrality 123456CALL algo.closeness.stream(&quot;Node&quot;, &quot;LINK&quot;)YIELD nodeId, centralityMATCH (n:Node) WHERE id(n) = nodeIdRETURN n.id AS node, centralityORDER BY centrality DESCLIMIT 20; CASE 12345678910111213141516171819202122MERGE (home:Page &#123;name:&quot;Home&quot;&#125;)MERGE (about:Page &#123;name:&quot;About&quot;&#125;)MERGE (product:Page &#123;name:&quot;Product&quot;&#125;)MERGE (links:Page &#123;name:&quot;Links&quot;&#125;)MERGE (a:Page &#123;name:&quot;Site A&quot;&#125;)MERGE (b:Page &#123;name:&quot;Site B&quot;&#125;)MERGE (c:Page &#123;name:&quot;Site C&quot;&#125;)MERGE (d:Page &#123;name:&quot;Site D&quot;&#125;)MERGE (home)-[:LINKS]-&gt;(about)MERGE (about)-[:LINKS]-&gt;(home)MERGE (product)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(product)MERGE (links)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(links)MERGE (links)-[:LINKS]-&gt;(a)MERGE (a)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(b)MERGE (b)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(c)MERGE (c)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(d)MERGE (d)-[:LINKS]-&gt;(home) 4. ç¤¾åŒºå‘ç°ç®—æ³• Strongly Connected Components 1234CALL algo.scc.stream(&quot;User&quot;,&quot;FOLLOWS&quot;)YIELD nodeId, partitionMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS name, partition Weakly Connected Components (Union Find) 1234CALL algo.unionFind.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId,setIdMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS user, setId Label Propagation 12CALL algo.labelPropagation.stream(&quot;User&quot;, &quot;FOLLOWS&quot;, &#123;direction: &quot;OUTGOING&quot;, iterations: 10&#125;) Lovain Modularity 12345CALL algo.louvain.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId, communityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user, communityORDER BY community; Triangle Count and Average Clustering Coefficient 123456CALL algo.triangle.stream(&quot;Person&quot;,&quot;KNOWS&quot;)YIELD nodeA,nodeB,nodeCMATCH (a:Person) WHERE id(a) = nodeAMATCH (b:Person) WHERE id(b) = nodeBMATCH (c:Person) WHERE id(c) = nodeCRETURN a.id AS nodeA, b.id AS nodeB, c.id AS node 5. Tutorial Neo4j in deep å®˜æ–¹æ–‡æ¡£ï¼šComprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.com/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.com/tags/Neo4j/"}]},{"title":"å›¾æ•°æ®åº“-Neo4j-åˆæ¢","date":"2018-08-17T05:15:46.000Z","path":"2018/08/17/å›¾æ•°æ®åº“-Neo4j-åˆæ¢/","text":"æœ¬æ¬¡åˆæ¢ä¸»è¦å­¦ä¹ å¦‚ä½•å®‰è£…Neo4jï¼Œä»¥åŠCypherçš„åŸºæœ¬è¯­æ³•ã€‚ 1. å®‰è£…Neo4j Desktopç‰ˆæœ¬ neo4j-desktop Serverç‰ˆæœ¬ï¼ˆCommunityç‰ˆ) æ¯”è¾ƒå»ºè®®å®‰è£…è¿™ä¸ªç‰ˆæœ¬ï¼Œå› ä¸ºDesktopç‰ˆæœ¬çš„è€æ˜¯é—ªé€€ï¼Œä¸”è¦æ¿€æ´»ä¹‹ç±»çš„ã€‚ ä¸‹è½½Neo4jæ•°æ®åº“ neo4j-server-community ä¸‹è½½å¸¸ç”¨ç®—æ³•çš„æ’ä»¶ graph-algorithms neo4j-graph-algorithms apoc-procedures neo4j-apoc-procedures å°†ä¸‹è½½ä¸‹æ¥çš„ç®—æ³•æ’ä»¶æ”¾å…¥åˆ°$NEO4J_HOME/pluginsæ–‡ä»¶å¤¹ä¸‹ Serviceç‰ˆä¿®æ”¹é…ç½®æ–‡ä»¶$NEO4J_HOME/conf/neo4j.conf 12345# è§£å†³ç™»å…¥çš„æ—¶å€™æŠ¥æ²¡æœ‰æˆæƒçš„é”™è¯¯dbms.security.auth_enabled=false# æ·»åŠ ä¸‹è½½çš„ç®—æ³•æ’ä»¶dbms.security.procedures.unrestricted=apoc.trigger.*,apoc.meta.*,algo.*dbms.security.procedures.whitelist=apoc.coll.*,apoc.load.* å¯åŠ¨/åœæ­¢ (æŠŠserveræ‰€åœ¨çš„è·¯å¾„æ·»åŠ åˆ°ç³»ç»Ÿçš„PATH) 1234567# å»ºè®®å°†neo4jæ‰€åœ¨çš„è·¯å¾„æ¡ä»¶åˆ°ç³»ç»Ÿ$PATHå½“ä¸­ï¼Œ# export NEO4J_HOME=&quot;path-to-neo4j&quot;$NEO4J_HOME/bin/neo4j start$NEO4J_HOME/bin/neo4j console$NEO4J_HOME/bin/neo4j stop$NEO4J_HOME/bin/neo4j start -u neo4j -p neo4j$NEO4J_HOME/bin/cypher-shell 1CALL dbms.procedures() // æŸ¥çœ‹neo4jå¯ç”¨çš„è¿›ç¨‹ï¼ŒåŒ…æ‹¬åˆšåˆšå®‰è£…çš„æ’ä»¶ 2. CypheråŸºæœ¬è¯­æ³• NodesåŸºæœ¬è¯­æ³• åœ¨Cypheré‡Œé¢é€šè¿‡ä¸€å¯¹å°æ‹¬å·ä»£è¡¨ä¸€ä¸ªèŠ‚ç‚¹ () ä»£è¡¨åŒ¹é…ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ (node1) ä»£è¡¨åŒ¹é…ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¹¶ç»™å®ƒèµ·äº†ä¸€ä¸ªåˆ«å (:Lable) ä»£è¡¨æŸ¥è¯¢ä¸€ä¸ªç±»å‹çš„æ•°æ® (person:Lable) ä»£è¡¨æŸ¥è¯¢ä¸€ä¸ªç±»å‹çš„æ•°æ®ï¼Œå¹¶ç»™å®ƒèµ·äº†ä¸€ä¸ªåˆ«å (person:Lable {name:â€å°ç‹â€}) æŸ¥è¯¢æŸä¸ªç±»å‹ä¸‹ï¼ŒèŠ‚ç‚¹å±æ€§æ»¡è¶³æŸä¸ªå€¼çš„æ•°æ® (person:Lable {name:â€å°ç‹â€,age:23}) èŠ‚ç‚¹çš„å±æ€§å¯ä»¥åŒæ—¶å­˜åœ¨å¤šä¸ªï¼Œæ˜¯ä¸€ä¸ªANDçš„å…³ç³» RelationshipåŸºæœ¬è¯­æ³• ç³»ç”¨ä¸€å¯¹-ç»„æˆï¼Œå…³ç³»åˆ†æœ‰æ–¹å‘çš„è¿›å’Œå‡ºï¼Œå¦‚æœæ˜¯æ— æ–¹å‘å°±æ˜¯è¿›å’Œå‡ºéƒ½æŸ¥è¯¢ â€“&gt; æŒ‡å‘ä¸€ä¸ªèŠ‚ç‚¹ -[role]-&gt; ç»™å…³ç³»åŠ ä¸ªåˆ«å -[:acted_in]-&gt; è®¿é—®æŸä¸€ç±»å…³ç³» -[role:acted_in]-&gt; è®¿é—®æŸä¸€ç±»å…³ç³»ï¼Œå¹¶åŠ äº†åˆ«å -[role:acted_in {roles:[â€œneoâ€,â€Hadoopâ€œ]}]-&gt; åˆ›å»º/åˆ é™¤èŠ‚ç‚¹ 1234567891011121314151617181920212223// æ’å…¥ä¸€ä¸ªArtistç±»åˆ«çš„èŠ‚ç‚¹ï¼Œè€Œä¸”è¿™ä¸ªèŠ‚ç‚¹æœ‰ä¸€ä¸ªå±æ€§ä¸ºNameï¼Œå€¼ä¸ºLady GagaCREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;&#125;)// åˆ›å»ºå¹¶è¿”å›CREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;, Gemder:&quot;Femal&quot;&#125;) return a// ä¸€æ¬¡æ€§åˆ›å»ºå¤šä¸ªCREATE (a:Album &#123; Name: &quot;Killers&quot;&#125;), (b:Album &#123; Name: &quot;Fear of the Dark&quot;&#125;) RETURN a, bCREATE (a:Album &#123; Name: &quot;Piece of Mind&quot;&#125;) CREATE (b:Album &#123; Name: &quot;Somewhere in Time&quot;&#125;) RETURN a, b// åˆ é™¤èŠ‚ç‚¹ï¼Œå¦‚æœè¿™ä¸ªèŠ‚ç‚¹å’Œå…¶ä»–èŠ‚ç‚¹æœ‰è¿æ¥çš„è¯ï¼Œä¸èƒ½å•å•åˆ é™¤è¿™ä¸ªèŠ‚ç‚¹MATCH (a:Album &#123;Name: &quot;Killers&quot;&#125;) DELETE a// ä¸€æ¬¡æ€§åˆ é™¤å¤šä¸ªèŠ‚ç‚¹MATCH (a:Artist &#123;Name: &quot;Iron Maiden&quot;&#125;), (b:Album &#123;Name: &quot;Powerslave&quot;&#125;) DELETE a, b // åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹MATCH (n) DELETE n åˆ›å»º/åˆ é™¤å…³ç³» 123456789101112131415161718192021222324252627282930// å¯¹Lady Gagaå’Œä¸“è¾‘PieceOfMindä¹‹é—´åˆ›å»ºä¸€ä¸ªreleasedçš„å…³ç³»MATCH (a:Artist), (b:Album)WHERE a.Name = &quot;Lady Gaga&quot; AND b.Name = &quot;Piece of Mind&quot;CREATE (a)-[r:RELEASED]-&gt;(b)RETURN rMATCH (a:Artist), (b:Album), (p:Person)WHERE a.Name = &quot;Strapping Young Lad&quot; AND b.Name = &quot;Heavy as a Really Heavy Thing&quot; AND p.Name = &quot;Devin Townsend&quot; CREATE (p)-[pr:PRODUCED]-&gt;(b), (p)-[pf:PERFORMED_ON]-&gt;(b), (p)-[pl:PLAYS_IN]-&gt;(a)RETURN a, b, p // åˆ é™¤æŒ‡å®šçš„å…³ç³»MATCH (:Artist)-[r:RELEASED]-(:Album) DELETE r MATCH (:Artist &#123;Name: &quot;Strapping Young Lad&quot;&#125;)-[r:RELEASED]-(:Album &#123;Name: &quot;Heavy as a Really Heavy Thing&quot;&#125;) DELETE r // åˆ é™¤æ‰€æœ‰çš„å…³ç³»MATCH ()-[r:RELEASED]-() DELETE r // æ¸…é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³» MATCH (n)OPTIONAL MATCH(n)-[r]-()DELETE n,r // åˆ é™¤æ•´ä¸ªæ•°æ®åº“MATCH (n) DETACH DELETE n åˆ›å»º/åˆ é™¤çº¦æŸ åŒSQLä¸€æ ·ï¼ŒNeo4jæ•°æ®åº“æ”¯æŒå¯¹Nodeæˆ–relationshipçš„å±æ€§çš„UNIQUEçº¦æŸ 123CREATE CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUEDROP CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUE åˆ›å»º/åˆ é™¤ç´¢å¼• 123456CREATE INDEX ON :Album(Name) // View the schema:schemaDROP INDEX ON :Album(Name) æ›´æ–°ä¸€ä¸ªèŠ‚ç‚¹/è¾¹ 12MATCH (n:Person &#123; name: &quot;Andres&quot; &#125;)SET n.name = &quot;Taylor&quot;; ç­›é€‰è¿‡æ»¤ 123456789// WHEREMATCH (p1: Person)-[r:friend]-&gt;(p2: Person) WHERE p1.name=~&quot;K.+&quot; or p2.age=24 or &quot;neo&quot; in r.rels RETURN p1, r, p2 // NOT MATCH (p:Person)-[:ACTED_IN]-&gt;(m)WHERE NOT (p)-[:DIRECTED]-&gt;()RETURN p, m ç»“æœé›†è¿”å› 12345MATCH (p:Person)RETURN p, p.name AS name, upper(p.name), coalesce(p.nickname,&quot;n/a&quot;) AS nickname, &#123; name: p.name, label:head(labels(p))&#125; AS person MATCH (n) RETURN DISTINCT n.name; èšåˆå‡½æ•° Cypheræ”¯æŒcount, sum, avg, min, max èšåˆçš„æ—¶å€™nullä¼šè¢«è·³è¿‡ count è¯­æ³• æ”¯æŒ count( distinct role ) 123456MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations// æ”¶é›†èšåˆç»“æœMATCH (m:Movie)&lt;-[:ACTED_IN]-(a:Person)RETURN m.title AS movie, collect(a.name) AS cast, count(*) AS actors æ’åºå’Œåˆ†é¡µ 123MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a, count(*) AS appearancesORDER BY appearances DESC SKIP 3 LIMIT 10; Union è”åˆ 12345MATCH (actor:Person)-[r:ACTED_IN]-&gt;(movie:Movie)RETURN actor.name AS name, type(r) AS acted_in, movie.title AS titleUNION ï¼ˆALLï¼‰MATCH (director:Person)-[r:DIRECTED]-&gt;(movie:Movie)RETURN director.name AS name, type(r) AS acted_in, movie.title AS title Withè¯­å¥ withè¯­å¥ç»™Cypheræä¾›äº†å¼ºå¤§çš„pipelineèƒ½åŠ›ï¼Œå¯ä»¥ä¸€ä¸ªæˆ–è€…queryçš„è¾“å‡ºï¼Œæˆ–è€…ä¸‹ä¸€ä¸ªqueryçš„è¾“å…¥ å’Œreturnè¯­å¥éå¸¸ç±»ä¼¼ï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œwithçš„æ¯ä¸€ä¸ªç»“æœï¼Œå¿…é¡»ä½¿ç”¨åˆ«åæ ‡è¯†ã€‚ ä½¿ç”¨withæˆ‘ä»¬å¯ä»¥åœ¨æŸ¥è¯¢ç»“æœé‡Œé¢åœ¨ç»§ç»­åµŒå¥—æŸ¥è¯¢ã€‚ 1234MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)WITH p, count(*) AS appearances, COLLECT(m.Title) AS moviesWHERE appearances &gt; 1RETURN p.name, appearances, movies æœ‰ç‚¹ç±»ä¼¼SQLä¸­çš„havingï¼Œè¿™é‡Œæ˜¯with + whereä¸¤ä¸ªä¸€èµ·æ¥å®ç°çš„ã€‚ æŸ¥è¯¢æœ€çŸ­è·¯å¾„ 12MATCH (ms:Person &#123; name: &quot;Node A&quot; &#125;),(cs:Person &#123; name:&quot;Node B&quot; &#125;), p = shortestPath((ms)-[r:Follow]-(cs)) RETURN p; åŠ è½½æ•°æ® Cypher Neo4j Couldnâ€™t load the external resource neo4jåˆæ¢ åŠ è½½å­˜åœ¨æœ¬åœ°serverä¸Šçš„æ•°æ®ï¼Œä¼šåœ¨è·¯å¾„å‰é¢è‡ªåŠ¨åŠ ä¸ªå‰ç¼€ /path-to-neo4j/neo4j-community-3.4.5/importï¼Œå³Serverå¯¹åº”æ‰€åœ¨çš„è·¯å¾„ä¸‹çš„import 12345678910111213141516// åŠ è½½addressLOAD CSV WITH HEADERS FROM &quot;file:///data/addresses.csv&quot; AS csvLineCREATE (p:Person &#123;id: toInt(csvLine.id), email: csvLine.address &#125;)// åŠ è½½emailLOAD CSV WITH HEADERS FROM &quot;file:///data/emails.csv&quot; AS csvLineCREATE (e:Email &#123;id: toInt(csvLine.id), time: csvLine.time, content: csvLine.content &#125;) // åˆ›å»ºæ”¶å‘å…³ç³»USING PERIODIC COMMIT 500 // åˆ†æ®µåŠ è½½LOAD CSV WITH HEADERS FROM &quot;file:///data/relations.csv&quot; AS csvLineMATCH (p1:Person &#123;id: toInt(csvLine.fromId)&#125;),(e:Email &#123; id: toInt(csvLine.emailId)&#125;),(p2:Person&#123; id: toInt(csvLine.toId)&#125;)CREATE UNIQUE (p1)-[:FROM]-&gt;(e)CREATE(e)-[:TO]-&gt;(p2) å¦‚æœéœ€è¦å¯¼å…¥å…¶ä»–åœ°æ–¹çš„ï¼Œå¯ä»¥ä½¿ç”¨ 123456789LOAD CSV FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine[0], Name: csvLine[1]&#125;)// ä½¿ç”¨csvä¸­çš„header LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine.Id, Name: csvLine.Track, Length: csvLine.Length&#125;) // è‡ªå®šä¹‰csvæ–‡ä»¶ä¸­çš„åˆ†éš”ç¬¦LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLine FIELDTERMINATOR &quot;;&quot; ä½¿ç”¨ neo4j-import å¯¼å…¥æ•°æ® ä½¿ç”¨neo4j-importå¯¼å…¥æ•°æ® ä½¿ç”¨æ¡ä»¶ éœ€è¦å…ˆå…³é—­neo4j æ— æ³•å†åŸæœ‰çš„æ•°æ®åº“æ·»åŠ ï¼Œåªèƒ½é‡æ–°ç”Ÿæˆä¸€ä¸ªæ•°æ®åº“ å¯¼å…¥æ–‡ä»¶æ ¼å¼ä¸ºcsv å‚æ•° â€“intoï¼šæ•°æ®åº“åç§° â€“bad-toleranceï¼šèƒ½å®¹å¿çš„é”™è¯¯æ•°æ®æ¡æ•°ï¼ˆå³è¶…è¿‡æŒ‡å®šæ¡æ•°ç¨‹åºç›´æ¥æŒ‚æ‰ï¼‰ï¼Œé»˜è®¤1000 â€“multiline-fieldsï¼šæ˜¯å¦å…è®¸å¤šè¡Œæ’å…¥ï¼ˆå³æœ‰äº›æ¢è¡Œçš„æ•°æ®ä¹Ÿå¯è¯»å–ï¼‰ â€“nodesï¼šæ’å…¥èŠ‚ç‚¹ â€“relationshipsï¼šæ’å…¥å…³ç³» æ›´å¤šå‚æ•°å¯å…è®¸å‘½ä»¤bin/neo4j-import 1bin/neo4j-import --multiline-fields=true --bad-tolerance=1000000 --into graph.db --id-type string --nodes:person node.csv --relationships:related relation_header.csv,relation.csv è¿è¡Œå®Œåï¼Œå°†ç”Ÿæˆçš„graph.dbæ”¾å…¥data/databasesï¼Œè¦†ç›–åŸæœ‰æ•°æ®åº“ï¼Œå¯åŠ¨è¿è¡Œå³å¯ 3. Tutorial Neo4jçš„ç®€å•æ­å»ºä¸ä½¿ç”¨ Neo4j Tutorial Neo4jçš„æŸ¥è¯¢è¯­æ³•ç¬”è®° å®˜æ–¹æ–‡æ¡£ï¼šComprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"å›¾æ•°æ®åº“","slug":"å›¾æ•°æ®åº“","permalink":"http://chenson.com/tags/å›¾æ•°æ®åº“/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.com/tags/Neo4j/"}]},{"title":"Macç³»ç»Ÿä¸‹matplotlibæ˜¾ç¤ºä¸­æ–‡","date":"2018-04-10T07:08:20.000Z","path":"2018/04/10/Macç³»ç»Ÿä¸‹matplotlibæ˜¾ç¤ºä¸­æ–‡/","text":"1. Macç³»ç»Ÿä¸‹è®©matplotlibæ˜¾ç¤ºä¸­æ–‡ç°åœ¨çš„æ—¶é—´æ˜¯2018.04.10ï¼Œæ›´æ–°ä¸€ä¸‹Macç³»ç»Ÿä¸‹çš„è§£å†³æ–¹æ³• æˆ‘çš„ç¯å¢ƒï¼šanaconda3 + Python 3.6.3 æ·»åŠ å­—ä½“ æ·»åŠ  SimHei å­—ä½“ï¼ˆsimhei.ttfæ–‡ä»¶ï¼‰åˆ° ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf ä¸­ï¼› ä¸‹è½½åœ°å€ï¼šé»‘ä½“å­—ä½“simhei.ttf æˆ‘ç”¨çš„æ˜¯ anaconda3 ä¸‹çš„ python ç¯å¢ƒï¼Œè¿™ä¸ªåœ°å€å¯¹åº”ä½ æ­£åœ¨ä½¿ç”¨çš„ python å®‰è£…åœ°å€ â€‹ ä¿®æ”¹matplotlibé…ç½®æ–‡ä»¶ 12cd ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-datavi matplotlibrc # ç¼–è¾‘é…ç½®æ–‡ä»¶ æ‰¾åˆ° font.sans-serif æ·»åŠ  SimHei åˆ°å­—ä½“åˆ—è¡¨ ï¼ˆå¦‚å›¾ï¼Œå¤§çº¦åœ¨211è¡Œï¼‰ åŒæ—¶ä¿®æ”¹ axes.unicode_minusï¼Œå°† True æ”¹ä¸º Falseï¼Œä½œç”¨å°±æ˜¯è§£å†³è´Ÿå·â€™-â€˜æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜ï¼ˆå¤§çº¦åœ¨330è¡Œï¼‰ åˆ é™¤ç¼“å­˜æ–‡ä»¶ Mac ç³»ç»Ÿä¸‹åˆ é™¤ ~/.matplotlib/ ä¸‹çš„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ 12rm -rf ~/.matplotlib/*.cacherm -rf ~/.matplotlib/fontList.json Linuxã€CentOS åˆ é™¤ ~/.cache/matplotlibç›®å½•ä¸‹çš„ä¸¤ä¸ªç¼“å­˜æ–‡ä»¶ï¼ˆåŒä¸Šï¼‰ æ³¨æ„ rm -rf å‘½ä»¤ï¼Œç¡®è®¤è·¯å¾„æ²¡é”™åœ¨ç”¨ â€‹ ç”»å›¾æµ‹è¯• æœªä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œéœ€è¦æ·»åŠ å¦‚ä¸‹ä»£ç ï¼š 1234567891011121314#coding:utf-8 import matplotlib #æŒ‡å®šé»˜è®¤å­—ä½“ matplotlib.rcParams['font.sans-serif'] = ['SimHei'] matplotlib.rcParams['font.family']='sans-serif' #è§£å†³è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜ matplotlib.rcParams['axes.unicode_minus'] = False from matplotlib.font_manager import _rebuild_rebuild()plt.plot([-1,2,-5,3]) plt.title(u'ä¸­æ–‡',fontproperties=myfont) plt.show() â€‹ å¦‚æœå·²ç»ä¿®æ”¹äº† matplotlib é…ç½®æ–‡ä»¶ï¼Œåˆ™ä¸éœ€è¦ä¸Šè¿°ä»£ç ï¼Œç›´æ¥ç”»å›¾å³å¯ã€‚ â€‹ 2. Tutorial å½»åº•è§£å†³matplotlibä¸­æ–‡ä¹±ç é—®é¢˜ matplotlibå›¾ä¾‹ä¸­æ–‡ä¹±ç ?","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"http://chenson.com/tags/Jupyter/"}]},{"title":"æ·±åº¦å­¦ä¹ ç¬”è®°-CNN","date":"2018-03-20T05:45:37.000Z","path":"2018/03/20/æ·±åº¦å­¦ä¹ ç¬”è®°-CNN/","text":"éå¸¸ç®€å•çš„æ·±åº¦å­¦ä¹ ç¬”è®°ï¼Œæ€»ç»“ä¸€ä¸‹CNNçš„ä¸€äº›æ¦‚å¿µï¼Œä»¥å…è€æ˜¯å¿˜è®°äº†ã€‚ 1. å·ç§¯ç¥ç»ç½‘ç»œ CNN - (Convolutional Neural Network) åŸºæœ¬ç»“æ„ è¾“å…¥å±‚ â€”&gt;å·ç§¯å±‚ â€”&gt; æ¿€æ´»å‡½æ•° â€”&gt;æ± åŒ–å±‚ â€”&gt; å…¨è¿æ¥å±‚ ç¥ç»ç½‘ç»œ - Neural Network å¤§å®¶éƒ½äº†è§£ï¼Œå°±ä¸è§£é‡Šäº†ã€‚ å·ç§¯ - Convolutional è£‚å¢™æ¨èçŸ¥ä¹ä¸Šçš„è¿™ç¯‡æ–‡ç« é€šä¿—ç†è§£ã€å·ç§¯ã€â€”â€”ä»å‚…é‡Œå¶å˜æ¢åˆ°æ»¤æ³¢å™¨ï¼Œä»ä¿¡å·å¤„ç†è¿™ä¸ªæœ¬è´¨çš„è§’åº¦æ¥ç†è§£å·ç§¯ï¼Œéå¸¸æ£’ğŸ‘ å·ç§¯çš„æ„æ€å°±æ˜¯ï¼Œç¥ç»ç½‘ç»œæ˜¯å¯¹å›¾ç‰‡ä¸Šçš„ä¸€å°å—åŒºåŸŸè¿›è¡Œå¤„ç†ï¼Œè¿™å¼ åšæ³•åŠ å¼ºäº†å›¾ç‰‡ä¿¡æ¯çš„è¿ç»­æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥çœ‹åˆ°å›¾ç‰‡ä¸Šçš„å›¾å½¢ï¼Œè€Œéæ˜¯ä¸€ä¸ªä¸ªç¦»æ•£çš„ç‚¹ã€‚ æ¿€æ´»å‡½æ•° å¸¸ç”¨çš„å‡ ä¸ªæ¿€æ´»å‡½æ•°æ˜¯ Reluï¼ŒSigmoidï¼ŒTanhå’ŒSoftplus æ„Ÿå—é‡ - Receptive Field ä¸­æ–‡åçœŸçš„æ˜¯â€¦ â€¦ å¤ªå¤ªå¤ªéš¾ç†è§£äº†ï¼Œç›´æ¥çœ‹è‹±æ–‡åReceptive Fieldæ¯”è¾ƒç›´è§‚ä¸€äº›ã€‚ åŸºæœ¬å®šä¹‰å°±æ˜¯ å·ç§¯ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚è¾“å‡ºçš„ç‰¹å¾å›¾ï¼ˆFeature apï¼‰ä¸Šçš„åƒç´ ç‚¹åœ¨åŸå›¾åƒä¸Šæ˜ å°„çš„åŒºåŸŸå¤§å°ã€‚ ç¬¬ä¸€å±‚å·ç§¯ è¾“å…¥ä¸º10x10çš„å›¾ç‰‡ï¼Œç»è¿‡3x3çš„kernelï¼Œè¾“å‡ºä¸º8x8çš„å¤§å°ï¼ˆè¿™é‡Œåªè€ƒè™‘å•å±‚ï¼‰ ä¸”åœ¨output1ä¸­çš„æ¯ä¸€ä¸ªåƒç´ ç‚¹ï¼Œéƒ½å—åˆ°åŸå§‹å›¾åƒå¯¹åº”çš„3x3åŒºåŸŸå†…çš„å½±å“ï¼Œç¬¬ä¸€åœºçš„æ„Ÿå—é‡ä¸º3ï¼Œç”¨å­—æ¯è¡¨ç¤ºRF1=3 ç¬¬äºŒå±‚å·ç§¯ åœ¨ç¬¬ä¸€å±‚å·ç§¯çš„è¾“å‡ºoutput1ä¸‹ï¼Œç»è¿‡ç¬¬äºŒå±‚3x3çš„kernelå·ç§¯ï¼Œè¾“å‡ºçš„å¤§å°ä¸º6x6 å¦‚æœä»output2å¾€å›æ¨çš„è¯ï¼Œoutput2ä¸Šçš„ä¸€ä¸ªåƒç´ ç‚¹ï¼Œå—åˆ°output1ä¸Šä¸‰ä¸ªåƒç´ ç‚¹çš„å½±å“ï¼Œè€Œè¿™ä¸‰ä¸ªåƒç´ ç‚¹åˆæ€»å…±å—åˆ°è¾“å…¥å±‚äº”ä¸ªåƒç´ ç‚¹çš„å½±å“ï¼Œæ‰€ä»¥ç¬¬äºŒå±‚çš„æ„Ÿå—é‡RF2=5 ç¬¬ä¸‰å±‚å·ç§¯ æ­¤æ—¶kernel3ä¾æ—§ä¸º3x3ï¼Œæ ¹æ®ä¸Šé¢æ¨ï¼ŒRF3=7 æ± åŒ– - Pooling æ± åŒ–å¤§è‡´å°±æ˜¯ä¸€ä¸ªç­›é€‰è¿‡æ»¤çš„è¿‡ç¨‹ï¼Œèƒ½è¿‡å°†å·¦è¾¹çš„layerä¸­æœ‰ç”¨çš„ä¿¡æ¯ç­›é€‰å‡ºæ¥ï¼Œç»™å³è¾¹çš„ä½¿ç”¨åˆ†æã€‚å› ä¸ºåœ¨å·ç§¯çš„è¿‡ç¨‹ä¸­ä¸ºäº†ä¿ç•™æ›´å¤šçš„ä¿¡æ¯ï¼Œä¸å‹ç¼©é•¿å®½æ¯”ã€‚åŸå§‹å›¾ç‰‡å¾ˆå¤§çš„è¯ï¼Œå·ç§¯å‡ºæ¥çš„ç»“æœä¹Ÿæ˜¯å¾ˆå¤§ï¼Œä¸”æ¯”è¾ƒç¨€ç–çš„ã€‚æ‰€ä»¥è¿™æ—¶å€™å¦‚æœç”¨æ± åŒ–å±‚å‹ç¼©çš„è¯ï¼Œå¯ä»¥å°½å¯èƒ½çš„ä¿ç•™ä¿¡æ¯å’Œå‹ç¼©æ•°æ®çš„å¤§å°ã€‚ 2. PyTorchä»£ç å®ç° 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class CNN(nn.Module): def __init__(self, in_dim, n_class): super(CNN, self).__init__() # è¡¨ç¤ºå°†ä¸€ä¸ªæœ‰åºçš„æ¨¡å—å†™åœ¨ä¸€èµ· # ç›¸å½“äºç¥ç»ç½‘ç»œçš„å±‚æŒ‰é¡ºåºæ”¾ä¸€èµ·æ–¹ä¾¿ç»“æ„æ˜¾ç¤º self.conv = nn.Sequential( # å·ç§¯å±‚ï¼Œæœ‰äº”ä¸ªå‚æ•°ï¼š # in_channels: è¡¨ç¤ºçš„æ˜¯è¾“å…¥å·ç§¯å±‚çš„å›¾ç‰‡åšåº¦ # out_channels: æ¯”å¶å¥¥æ•°çš„æ˜¯è¦è¾“å‡ºçš„åšåº¦ # kernel_size: è¡¨ç¤ºçš„æ˜¯å·ç§¯æ ¸çš„å¤§å°ï¼Œä¸€ä¸ªæ•°å­—çš„è¯è¡¨ç¤ºé•¿å®½ç›¸ç­‰çš„å·ç§¯æ ¸ # stride: è¡¨ç¤ºå·ç§¯æ ¸æ»‘åŠ¨çš„æ­¥é•¿ # padding: è¡¨ç¤ºåœ¨å›¾ç‰‡èµ°ä½å¡«å……0çš„å¤šå°‘ï¼Œpadding=0è¡¨ç¤ºä¸å¡«å……ï¼Œpadding=1å››å‘¨éƒ½å¡«å……1ç»´ nn.Conv2d(in_dim, 6, 3, stride=1, padding=1), # æ¿€æ´»å‡½æ•°ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªå‚æ•°inplace # Falseï¼Œè¡¨ç¤ºæ–°åˆ›å»ºä¸€ä¸ªå¯¹è±¡å¯¹å…¶ä¿®æ”¹ # Trueï¼Œè¡¨ç¤ºç›´æ¥å¯¹è¿™ä¸ªå¯¹è±¡è¿›è¡Œä¿®æ”¹ nn.ReLU(True), # æœ€å¤§æ± åŒ–å±‚ï¼Œä¹Ÿæœ‰å¹³å‡æ± åŒ–å±‚ç­‰ï¼Œé‡Œé¢çš„å‚æ•°æœ‰: # kernel_size: è¡¨ç¤ºæ± åŒ–çš„çª—å£çš„å¤§å°ï¼Œå’Œå·ç§¯é‡Œé¢çš„kernel_sizeæ˜¯ä¸€æ ·çš„ # stride: ä¹Ÿå’Œå·ç§¯å±‚é‡Œé¢ä¸€æ ·ï¼Œéœ€è¦è‡ªå·±è®¾ç½®æ»‘åŠ¨æ­¥é•¿ # padding å’Œå·ç§¯å±‚ä¸€æ ·ï¼Œé»˜è®¤æ˜¯0 nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5, stride=1, padding=0), nn.ReLU(True), nn.MaxPool2d(2, 2) ) # self.fc = nn.Sequential( nn.Linear(400, 120), nn.Linear(120, 84), nn.Linear(84, n_class) ) def forward(self, x): out = self.conv(x) # batch_size out = out.view(out.size(0), -1) out = self.fc(out) return out # å®šä¹‰losså’Œoptimizercriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 # å¼€å§‹è®­ç»ƒ for epoch in range(num_epoches): print('epoch &#123;&#125;'.format(epoch + 1)) print('*' * 10) running_loss = 0.0 running_acc = 0.0 for i, data in enumerate(train_loader, 1): img, label = data if use_gpu: img = img.cuda() label = label.cuda() img = Variable(img) label = Variable(label) # å‘å‰ä¼ æ’­ out = model(img) loss = criterion(out, label) running_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() accuracy = (pred == label).float().mean() running_acc += num_correct.data[0] # å‘åä¼ æ’­ optimizer.zero_grad() loss.backward() optimizer.step() # ========================= Log ====================== step = epoch * len(train_loader) + i # (1) Log the scalar values info = &#123;'loss' : loss.data[0], 'accuracy' : accuracy.data[0]&#125; for tag, value in info.items(): logger.scalar_summary(tag, value, step) # (2) Log values and gradients of the parameters (histogram) for tag, value in model.named_parameters(): tag = tag.replace('.', '/') logger.histo_summary(tag, to_np(value), step) logger.histo_summary(tag + '/grad', to_np(value.grad), step) # (3) Log the images info = &#123;'images': to_np(img.view(-1, 28, 28)[:10])&#125; for tag, images in info.items(): logger.image_summary(tag, images, step) if i % 300 == 0: print('[&#123;&#125;/&#123;&#125;] Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, num_epoches, running_loss / (batch_size * i), running_acc / (batch_size * i))) print('Finish &#123;&#125; epoch, Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, running_loss / (len(train_dataset)), running_acc / (len( train_dataset)))) model.eval() eval_loss = 0 eval_acc = 0 for data in test_loader: img, label = data if use_gpu: img = Variable(img, volatile=True).cuda() label = Variable(label, volatile=True).cuda() else: img = Variable(img, volatile=True) label = Variable(label, volatile=True) out = model(img) loss = criterion(out, label) eval_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() eval_acc += num_correct.data[0] print('Test Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format(eval_loss / (len( test_dataset)), eval_acc / (len(test_dataset)))) print()# ä¿å­˜æ¨¡å‹torch.save(model.state_dict(), './model/cnn.pth') 3. Tutorials è«çƒ¦ - ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œ CNN (Convolutional Neural Network) ä»€ä¹ˆæ˜¯æ„Ÿå—é‡ é€šä¿—ç†è§£ã€å·ç§¯ã€â€”â€”ä»å‚…é‡Œå¶å˜æ¢åˆ°æ»¤æ³¢å™¨","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://chenson.com/tags/PyTorch/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.com/tags/Deep-Learning/"}]},{"title":"Hadoop2.7.4å®Œå…¨åˆ†å¸ƒå¼é›†ç¾¤æ­å»ºå’Œæµ‹è¯•","date":"2017-10-10T05:24:58.000Z","path":"2017/10/10/Hadoop2-7-4å®Œå…¨åˆ†å¸ƒå¼é›†ç¾¤æ­å»ºå’Œæµ‹è¯•/","text":"1. ç¯å¢ƒé…ç½®1.1 ç¯å¢ƒè¯´æ˜ 1.2 ä¿®æ”¹æœºå™¨åç§°å’Œhostsç­‰ vi /etc/sysconfig/network 1234HOSTNAME=hadoop-masterHOSTNAME=hadoop-salve1HOSTNAME=hadoop-salve2HOSTNAME=hadoop-salve3 æ‰§è¡Œ reboot åç”Ÿæ•ˆ sudo vi /etc/hostname 1234# ç›¸åº”ä¿®æ”¹ä¸‰å°æœºå™¨hadoop-masterhadoop-slave1hadoop-slave2 sudo vi /etc/hosts 1234567127.0.0.1 localhost localhost.localdomain VM-0-6-ubuntu# Hadoop Cluster# ã€æ³¨æ„ã€‘ï¼šç”¨å†…ç½‘IPï¼Œè‹¥ç”¨å…¬ç½‘IPï¼Œåˆ™æ— æ³•å¯åŠ¨masterä¸Šçš„9000ç›‘å¬ç«¯å£172.17.6 hadoop-master172.17.11 hadoop-salve1 172.17.7 hadoop-salve2 1.5. SSHæ— å¯†ç éªŒè¯é…ç½® å®‰è£… ssh 123sudo apt-get install openssh-serverps -e | grep &quot;ssh&quot;ssh localhost ç”Ÿæˆå¯†é’¥ pair 12345678910# æŸ¥çœ‹æƒé™ls -aldrwxr-x--x 2 root root 4096 Dec 23 2015 .ssh# ç»™ç”¨æˆ·æƒé™# sudo chown ubuntu .sshchmod 700 .ssh # ç”Ÿæˆå¯†é’¥ssh-keygen -t rsa åœ¨ master ä¸Šå¯¼å…¥ authorized_keys 123456789# é‡è¦sudo chmod 700 .ssh sudo chmod 640 .ssh/authorized_keyssudo chown $USER .sshsudo chown $USER .ssh/authorized_keyscat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# åŒæ—¶ç»™å…¶ä»–çš„ salver åœ¨å„ä¸ªæœºå™¨éªŒè¯ä¸€ä¸‹ 2. æ‰€éœ€è½¯ä»¶2.1 JDKè½¯ä»¶tutorial 1 å®‰è£… JDK 123456sudo add-apt-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installersudo apt-get install openjdk-8-jdksudo apt-get install openjdk-8-jre é…ç½®ç¯å¢ƒå˜é‡ vi /etc/profile 123456789101112131415# JAVA# 1. AWS EC2 Ubuntu 16export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64# 2. è…¾è®¯äº‘ Ubuntu 14export JAVA_HOME=/usr/lib/jvm/java-8-oracle# 3. MAC OSexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Homeexport JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar æ‰§è¡Œå‘½ä»¤ä½¿ä¹‹ç”Ÿæ•ˆ source /etc/profile 2.2 Hadoop è½¯ä»¶ ä¸‹è½½è½¯ä»¶ 123wget http://apache.uberglobalmirror.com/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gztar xvf hadoop-2.7.4.tar.gz é…ç½® vi ~/.bashrc è®¾ç½® Hadoop çš„ç¯å¢ƒå˜é‡ 1234export HADOOP_HOMEexport PATHexport HADOOP_CONF_DIRexport YARN_CONF_DIR 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# ~/.bashrc: executed by bash(1) for non-login shells.# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)# for examples# If not running interactively, don&apos;t do anythingcase $- in *i*) ;; *) return;;esac# don&apos;t put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don&apos;t overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern &quot;**&quot; used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)[ -x /usr/bin/lesspipe ] &amp;&amp; eval &quot;$(SHELL=/bin/sh lesspipe)&quot;# set variable identifying the chroot you work in (used in the prompt below)if [ -z &quot;$&#123;debian_chroot:-&#125;&quot; ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we &quot;want&quot; color)case &quot;$TERM&quot; in xterm-color|*-256color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the prompt#force_color_prompt=yesif [ -n &quot;$force_color_prompt&quot; ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it&apos;s compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ &quot;$color_prompt&quot; = yes ]; then PS1=&apos;$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ &apos;else PS1=&apos;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h:\\w\\$ &apos;fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase &quot;$TERM&quot; inxterm*|rxvt*) PS1=&quot;\\[\\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h: \\w\\a\\]$PS1&quot; ;;*) ;;esac# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval &quot;$(dircolors -b ~/.dircolors)&quot; || eval &quot;$(dircolors -b)&quot; alias ls=&apos;ls --color=auto&apos; #alias dir=&apos;dir --color=auto&apos; #alias vdir=&apos;vdir --color=auto&apos; alias grep=&apos;grep --color=auto&apos; alias fgrep=&apos;fgrep --color=auto&apos; alias egrep=&apos;egrep --color=auto&apos;fi# colored GCC warnings and errors#export GCC_COLORS=&apos;error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01&apos;# some more ls aliasesalias ll=&apos;ls -alF&apos;alias la=&apos;ls -A&apos;alias l=&apos;ls -CF&apos;# Add an &quot;alert&quot; alias for long running commands. Use like so:# sleep 10; alertalias alert=&apos;notify-send --urgency=low -i &quot;$([ $? = 0 ] &amp;&amp; echo terminal || echo error)&quot; &quot;$(history|tail -n1|sed -e &apos;\\&apos;&apos;s/^\\s*[0-9]\\+\\s*//;s/[;&amp;|]\\s*alert$//&apos;\\&apos;&apos;)&quot;&apos;# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don&apos;t need to enable# this, if it&apos;s already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi# HADOOPexport HADOOP_HOME=/home/ubuntu/workdir/hadoop-2.7.4export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=$CLASSPATH:$HADOOP_HOME/bin# JAVAexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib# LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/home/ubuntu/workdir/hadoop-2.7.4/lib/native 1export CLASSPATH=$CLASSPATH:/home/ubuntu/workdir/hadoop-2.7.4/etc/hadoop:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/contrib/capacity-scheduler/*.jar åœ¨ $HADOOP_CONF_DIR ä¸­è®¾ç½® JAVA_HOME 123vi /home/hadoop-2.7.4/etc/hadoop/hadoop-env.sh è®¾ç½®JAVA_HOMEvi /home/hadoop-2.7.4/etc/hadoop/mapred-env.sh è®¾ç½®JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 vi slaves è¿™é‡Œé¢å¡«å†™çš„å…¨æ˜¯slaves 12hadoop-slave1hadoop-slave2 vi $HADOOP_CONF_DIR/core-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!-- æ³¨æ„è¿™é‡Œçš„åŒºåˆ« ä¸€ä¸ªé€‚ç”¨äºå•æœº ä¸€ä¸ªé€‚ç”¨äºé›†ç¾¤ --&gt; &lt;!-- å•æœº --&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;!-- é›†ç¾¤ master --&gt; &lt;value&gt;hdfs://hadoop-master:9000&lt;/value&gt; &lt;description&gt;è®¾å®šnamenodeçš„ä¸»æœºååŠç«¯å£(å»ºè®®ä¸è¦æ›´æ”¹ç«¯å£å·)&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;description&gt; è®¾ç½®ç¼“å­˜å¤§å° &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop-2.7.4/tmp&lt;/value&gt; &lt;description&gt; å­˜æ”¾ä¸´æ—¶æ–‡ä»¶çš„ç›®å½• &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/hdfs-site.xml 1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/name&lt;/value&gt; &lt;description&gt; namenode ç”¨æ¥æŒç»­å­˜æ”¾å‘½åç©ºé—´å’Œäº¤æ¢æ—¥å¿—çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/data&lt;/value&gt; &lt;description&gt; DataNode åœ¨æœ¬åœ°å­˜æ”¾å—æ–‡ä»¶çš„ç›®å½•åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš” &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt; è®¾å®š HDFS å­˜å‚¨æ–‡ä»¶çš„å‰¯æœ¬ä¸ªæ•°ï¼Œé»˜è®¤ä¸º3 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; vi $HADOOP_CONF_DIR/mapred-site.xml (å¤åˆ¶mapred-site.xml.template,å†ä¿®æ”¹æ–‡ä»¶å) 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;hadoop-master:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop-master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://hadoop-master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;!-- master node çš„åå­— --&gt; &lt;!-- å•æœºå’Œé›†ç¾¤çš„åŒºåˆ«ï¼Ÿï¼Ÿ --&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop-master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop-master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;hadoop-master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop-master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345dfs.nameservices â€”â€“ HDFS NNçš„é€»è¾‘åç§°ï¼Œä½¿ç”¨ä¸Šé¢è®¾ç½®çš„myhdfsdfs.ha.namenodes.myhdfs â€”â€“ ç»™å®šæœåŠ¡é€»è¾‘åç§°myhdfsçš„èŠ‚ç‚¹åˆ—è¡¨dfs.namenode.rpc-address.myhdfs.nn1 â€”â€“ myhdfsä¸­nn1èŠ‚ç‚¹å¯¹å¤–æœåŠ¡çš„RPCåœ°å€dfs.namenode.http-address.myhdfs.nn1 â€”â€“ myhdfsä¸­nn1èŠ‚ç‚¹å¯¹å¤–æœåŠ¡çš„httpåœ°å€dfs.namenode.shared.edits.dir â€”â€“ è®¾ç½®ä¸€ç»„ journalNode çš„ URI åœ°å€ï¼Œactive NN å°† edit log å†™å…¥è¿™äº› 12345åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•/home/hadoop-2.7.4/hdfs ç”¨æ¥å­˜æ”¾é›†ç¾¤æ•°æ®ã€‚åœ¨ä¸»èŠ‚ç‚¹nodeä¸Šåˆ›å»ºç›®å½•/home/hadoop-2.7.4/hdfs/name ç”¨æ¥å­˜æ”¾æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®ã€‚åœ¨æ¯ä¸ªä»èŠ‚ç‚¹ä¸Šåˆ›å»ºç›®å½•/home/hadoop-2.7.4/hdfs/data ç”¨æ¥å­˜æ”¾çœŸæ­£çš„æ•°æ®ã€‚æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„æ—¥å¿—ç›®å½•ä¸º/home/hadoop-2.7.4/logsæ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„ä¸´æ—¶ç›®å½•ä¸º/home/hadoop-2.7.4/tmp 1ä¸Šé¢çš„é…ç½®åªéœ€è¦åœ¨masterä¸­é…å¥½, ç„¶åå¤åˆ¶åˆ°å…¶ä»–çš„slavesèŠ‚ç‚¹ä¸­ä¸­å» æ ¼å¼åŒ– namenode å’Œ datanodeï¼ˆåœ¨ master ä¸Šæ‰§è¡Œå°±å¯ä»¥äº† ä¸éœ€è¦åœ¨ slaves ä¸Šæ‰§è¡Œï¼‰ 12hdfs namenode -formathdfs datanode -format åˆ†åˆ«åœ¨ master å’Œ slaves ä¸­ç”¨ jps æŸ¥çœ‹è¿›ç¨‹","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"}]},{"title":"Machine Learning - PCA","date":"2017-06-17T12:59:32.000Z","path":"2017/06/17/Machine-Learning-PCA/","text":"1. PCAPCA(Principal Components Analysis)æ˜¯ä¸»æˆæˆåˆ†åˆ†æï¼Œä¹‹å‰ä¹Ÿå«åšPricipal Factor Analysisã€‚é¡¾åæ€ä¹‰å°±æ˜¯åˆ†ææ•°æ®é‡Œé¢çš„ä¸»è¦éƒ¨åˆ†ï¼Œæ˜¯æœ€å¸¸ç”¨çš„ä¸€ç§é™ç»´æ–¹æ³•ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆéœ€è¦é™ç»´å‘¢ï¼Ÿ å› ä¸ºåœ¨çœŸå®çš„è®­ç»ƒæ•°æ®ä¸­ï¼Œæ€»æ˜¯ä¼šå­˜åœ¨å„ç§å†—ä½™çš„æ•°æ® æ¯”å¦‚è¯´æ‹¿åˆ°ä¸€ä¸ªæ±½è½¦çš„æ ·æœ¬ï¼Œé‡Œé¢æ—¢æœ‰ä»¥â€œåƒç±³/æ¯å°æ—¶â€åº¦é‡çš„æœ€å¤§é€Ÿåº¦ç‰¹å¾ï¼Œä¹Ÿæœ‰â€œè‹±é‡Œ/å°æ—¶â€çš„æœ€å¤§é€Ÿåº¦ç‰¹å¾ï¼Œæ˜¾ç„¶è¿™ä¸¤ä¸ªç‰¹å¾æœ‰ä¸€ä¸ªå¤šä½™ æ‹¿åˆ°ä¸€ä¸ªæ•°å­¦ç³»çš„æœ¬ç§‘ç”ŸæœŸæœ«è€ƒè¯•æˆç»©å•ï¼Œé‡Œé¢æœ‰ä¸‰åˆ—ï¼Œä¸€åˆ—æ˜¯å¯¹æ•°å­¦çš„å…´è¶£ç¨‹åº¦ï¼Œä¸€åˆ—æ˜¯å¤ä¹ æ—¶é—´ï¼Œè¿˜æœ‰ä¸€åˆ—æ˜¯è€ƒè¯•æˆç»©ã€‚æˆ‘ä»¬çŸ¥é“è¦å­¦å¥½æ•°å­¦ï¼Œéœ€è¦æœ‰æµ“åšçš„å…´è¶£ï¼Œæ‰€ä»¥ç¬¬äºŒé¡¹ä¸ç¬¬ä¸€é¡¹å¼ºç›¸å…³ï¼Œç¬¬ä¸‰é¡¹å’Œç¬¬äºŒé¡¹ä¹Ÿæ˜¯å¼ºç›¸å…³ã€‚é‚£æ˜¯ä¸æ˜¯å¯ä»¥åˆå¹¶ç¬¬ä¸€é¡¹å’Œç¬¬äºŒé¡¹å‘¢ï¼Ÿ è¿™ä¸ªä¸ç¬¬äºŒä¸ªæœ‰ç‚¹ç±»ä¼¼ï¼Œå‡è®¾åœ¨IRä¸­æˆ‘ä»¬å»ºç«‹çš„æ–‡æ¡£-è¯é¡¹çŸ©é˜µä¸­ï¼Œæœ‰ä¸¤ä¸ªè¯é¡¹ä¸ºâ€œlearnâ€å’Œâ€œstudyâ€ï¼Œåœ¨ä¼ ç»Ÿçš„å‘é‡ç©ºé—´æ¨¡å‹ä¸­ï¼Œè®¤ä¸ºä¸¤è€…ç‹¬ç«‹ã€‚ç„¶è€Œä»è¯­ä¹‰çš„è§’åº¦æ¥è®²ï¼Œä¸¤è€…æ˜¯ç›¸ä¼¼çš„ï¼Œè€Œä¸”ä¸¤è€…å‡ºç°é¢‘ç‡ä¹Ÿç±»ä¼¼ï¼Œæ˜¯ä¸æ˜¯å¯ä»¥åˆæˆä¸ºä¸€ä¸ªç‰¹å¾å‘¢ï¼Ÿ å› ä¸ºè¿™äº›å†—ä½™çš„æ•°æ®ï¼Œå¸¸å¸¸ä¼šå¯¼è‡´æˆ‘ä»¬çš„æ¨¡å‹è¿‡åº¦æ‹Ÿåˆã€‚è¿™ä¸ªæ—¶å€™ï¼Œå°±éœ€è¦ä¸€ç§ç‰¹å¾é™ç»´çš„æ–¹æ³•æ¥å‡å°‘ç‰¹å¾æ•°ï¼Œå‡å°‘å™ªéŸ³å’Œå†—ä½™ï¼Œå‡å°‘è¿‡åº¦æ‹Ÿåˆçš„å¯èƒ½æ€§ã€‚è€ŒPCAå°±æ˜¯é™ç»´çš„ç®—æ³•ä¹‹ä¸€ï¼Œå°†åŸå…ˆçš„æ•°æ®ä»nç»´æ˜ å°„åˆ°kç»´ä¸Š(k&lt;n)ã€‚è¿™é‡Œkç»´æ˜¯å…¨æ–°çš„æ­£äº¤ç‰¹å¾ï¼Œè¿™kç»´ç‰¹å¾æˆä¸ºä¸»å…ƒï¼Œæ˜¯é‡æ–°æ„é€ å‡ºå‡ºæ¥çš„kç»´ç‰¹å¾ï¼Œè€Œä¸æ˜¯ç®€å•çš„ä»nç»´å½“ä¸­ç§»å»äº†(n-k)ç»´ï¼Œç„¶åå‰©ä¸‹ä¸»è¦çš„kç»´ç‰¹å¾ã€‚ é‚£ä¹ˆè¯·æ€è€ƒä¸€ä¸ªé—®é¢˜ï¼šå¯¹äºæ­£äº¤å±æ€§ç©ºé—´ä¸­çš„æ ·æœ¬ç‚¹ï¼Œå¦‚ä½•ç”¨ä¸€ä¸ªè¶…å¹³é¢(ç›´çº¿çš„é«˜ç»´æ¨å¹¿)ï¼Œå¯¹æ‰€æœ‰çš„æ ·æœ¬è¿›è¡Œæ°å½“çš„è¡¨è¾¾ï¼Ÿé‚£ä¹ˆéœ€è¦æ‰¾åˆ°æ€ä¹ˆæ ·çš„ä¸€ä¸ªè¶…å¹³é¢æ¥åˆ†å‰²å‘¢ï¼Ÿä¸€èˆ¬éœ€è¦å…·å¤‡å¦‚ä¸‹ç‰¹å¾ï¼š æœ€è¿‘é‡æ„æ€§ï¼šæ ·æœ¬ç‚¹åˆ°è¿™ä¸ªè¶…å¹³é¢çš„è·ç¦»éƒ½è¶³å¤Ÿè¿‘ æœ€å¤§å¯åˆ†æ€§ï¼šæ ·æœ¬ç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½çš„è¦åˆ†å¼€ï¼Œè€Œä¸æ˜¯é‡å ä¸€èµ· 2. è®¡ç®—è¿‡ç¨‹é‚£ä¹ˆå¦‚ä½•æ¥æ„å»ºå‘¢ï¼Ÿä¸¾ä¸ªæ —å­ï¼š ç°åœ¨æœ‰è¿™ä¹ˆä¸€ç»„æ•°æ®ï¼Œxå’Œyæ˜¯ä¸¤ä¸ªç‰¹å¾ ç¬¬ä¸€æ­¥ï¼šæ±‚å‡ºæ‰€æœ‰ç»´åº¦çš„å¹³å‡å€¼ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œxçš„å¹³å‡å€¼æ˜¯1.81ï¼Œyçš„å¹³å‡å€¼æ˜¯1.91 ç¬¬äºŒæ­¥ï¼šä¸­å¿ƒåŒ–ï¼Œå³$\\sum x_i=0$.å°†æ‰€æœ‰çš„æ ·æœ¬éƒ½å‡å»è¿™ä¿©ä¸ªå¹³å‡å€¼ã€‚æ¯”å¦‚ç¬¬ä¸€ä¸ªæ ·æœ¬(2.5, 2.4) - (1.81, 1.91) = (0.69, 0.49) ç¬¬ä¸‰æ­¥ï¼šæ±‚ç‰¹å¾çš„åæ–¹å·®çŸ©é˜µå¦‚æœæœ‰x, y, zä¸‰ä¸ªç‰¹å¾ï¼Œåˆ†åˆ«éœ€è¦æ±‚cov(x, x), cov(x, y), cov(x, z), cov(y, y), cov(y, z), cov(z, z)è¿™å‡ ä¸ªã€‚ å½“åæ–¹å·®å¤§äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºxå’Œyè‹¥æœ‰ä¸€ä¸ªå¢åŠ ï¼Œå¦ä¸€ä¸ªä¹Ÿä¼šå¢åŠ ã€‚ å½“åæ–¹å·®ä¸‹äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºä¸€ä¸ªå¢åŠ ï¼Œå¦ä¸€ä¸ªä¼šå‡å°‘ å½“åæ–¹å·®ç­‰äº0çš„æ—¶å€™ï¼Œè¡¨ç¤ºæ¥è€…ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ã€‚ åæ–¹å·®çš„ç»å¯¹å€¼è¶Šå¤§ï¼Œä¸¤è€…å¯¹å½¼æ­¤çš„å½±å“ä¹Ÿå°±è¶Šå¤§ å…·ä½“å¦‚ä¸‹å›¾ è€Œæˆ‘ä»¬ä¸Šé¢çš„ä¾‹å­ä¸­åªæœ‰xå’Œyä¸¤ä¸ªå˜é‡ï¼Œå³ ç¬¬å››æ­¥ï¼šæ±‚åæ–¹å·®çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå¾—åˆ° ä¸Šé¢æ˜¯ä¸¤ä¸ªç‰¹å¾å€¼ï¼Œä¸‹é¢æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ æ¯”å¦‚ç‰¹å¾å€¼0.0490833989å¯¹åº”çš„ç‰¹å¾å‘é‡ä¸º(-0.735178656, 0.677873399)Tï¼Œè¿™é‡Œçš„ç‰¹å¾å‘é‡éƒ½å½’ä¸€åŒ–ä¸ºå•ä½å•ä½å‘é‡ æ±‚åæ–¹å·®çš„æ­¥éª¤ï¼š ç¬¬äº”æ­¥ï¼šå°†ç‰¹å¾å€¼æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ï¼Œé€‰æ‹©å…¶ä¸­ä¸ªæœ€å¤§çš„kä¸ªï¼Œç„¶åå°†å…¶å¯¹åº”çš„kä¸ªç‰¹å¾å‘é‡åˆ†åˆ«ä½œä¸ºåˆ—å‘é‡ç»„æˆç‰¹å¾å‘é‡çŸ©é˜µã€‚ ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç‰¹å¾å€¼åªæœ‰ä¸¤ä¸ªï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©å…¶ä¸­æœ€å¤§çš„é‚£ä¸ªã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬é€‰æ‹©1.28402771ï¼Œå¯¹åº”çš„ç‰¹å¾å‘é‡æ˜¯ (-0.677873399, -0.735178656)T. ç¬¬å…­æ­¥ï¼šå°†æ ·æœ¬æŠ•å½±åˆ°é€‰å–çš„ç‰¹å¾å‘é‡ä¸Šå»ã€‚ å‡è®¾æ ·æœ¬æ•°é‡ä¸ºmï¼Œç‰¹å¾æ•°é‡ä¸ºnï¼Œå‡å»å‡å€¼åçš„æ ·æœ¬çŸ©é˜µä¸ºDataAdjust(m*n)ï¼Œåæ–¹å·®çŸ©é˜µæ˜¯n*mï¼Œé€‰å–çš„kä¸ªç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µä¸ºEigenVectors(n*k)ã€‚é‚£ä¹ˆæŠ•å½±åçš„æ•°æ®FinalDataä¸º$$FinalData(m\\ast k) = DataAdjust(m\\ast n) \\times EigenVectors(n \\ast k)$$æ‰€ä»¥ä¸Šé¢çš„ä¾‹å­ä¸­$$FinalData(10\\ast 1) = DataAdjust(10\\ast 2) \\times EigenVectors(-0.677873399, -0.735178656)^T$$å¾—åˆ°çš„ç»“æœæ˜¯ è¿™æ ·å°±å°†åŸå§‹çš„æ ·æœ¬æ•°æ®ä»nç»´ç‰¹å¾å˜æˆäº†kç»´ç‰¹å¾ï¼Œè¿™kç»´å°±æ˜¯åŸå§‹ç‰¹å¾åœ¨kç»´ä¸Šçš„æŠ•å½±ã€‚ ä¸‹é¢çš„å›¾æè¿°äº†ä¸Šé¢çš„è¿‡ç¨‹ï¼š åŸå…ˆæ‰€æœ‰æ•°æ®æ˜¯åˆ†å¸ƒåœ¨x-yè¿™ä¸ªåæ ‡ç³»ä¸Šï¼Œ+è¡¨ç¤ºçš„æ˜¯æ ·æœ¬æ•°æ®ã€‚å·®ä¸å¤šæ˜¯å¯¹è§’çº¿ä¸Šçš„ä¸¤æ¡çº¿åˆ†åˆ«ä»£è¡¨äº†ä¸¤ä¸ªæ­£äº¤çš„ç‰¹å¾å‘é‡ã€‚ç”±äºåæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œå› æ­¤å…¶ç‰¹å¾å‘é‡æ­£äº¤ã€‚ç„¶åæˆ‘ä»¬å°†åŸæœ‰çš„æ ·æœ¬æ•°æ®æŠ•å½±åˆ°è¿™ä¸ªæ–°çš„åæ ‡ç³»ä¸­å»ï¼Œå¾—åˆ°è½¬æ¢åçš„æ•°æ®ï¼š ä»ä¸Šé¢çš„å›¾æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œåœ¨æ–°çš„åæ ‡ç³»ä¸­ï¼Œxè½´åŸºæœ¬å°±å¯ä»¥è¡¨ç¤ºåŸæœ‰çš„æ ·æœ¬æ•°æ®ç‰¹å¾ã€‚è€Œæ•´ä¸ªè¿‡ç¨‹ï¼Œçœ‹èµ·æ¥æœ‰ç‚¹åƒæ˜¯å°†åæ ‡ç³»åšäº†æ—‹è½¬ã€‚å¦‚æœå½“k=1çš„æ—¶å€™ï¼Œä¸Šé¢æ•°æ®å°±åªä¼šå˜æˆä¸€ç»´çš„æ•°æ®ã€‚è€Œå½“åŸæœ‰çš„æ•°æ®ç»´åº¦å¾ˆé«˜çš„æ—¶å€™ï¼Œæœ‰æ—¶å€™ä¸ºäº†å¯è§†åŒ–æ“ä½œï¼Œæˆ‘ä»¬å°±ä¼šå¯¹å…¶è¡ŒPCAé™ç»´ã€‚ 3. PCAç†è®ºåŸºç¡€ä¹‹å‰æˆ‘ä»¬æåˆ°äº†ï¼Œæˆ‘ä»¬å¸Œæœ›æ–°çš„è¶…å¹³é¢å…·å¤‡æœ€å¤§å¯åˆ†æ€§å’Œæœ€è¿‘é‡æ„æ€§è¿™ä¸¤ä¸ªç‰¹å¾ã€‚å®é™…ä¸Šæ ¹æ®è¿™ä¸¤ä¸ªç‰¹å¾ï¼Œèƒ½å¤Ÿåˆ†åˆ«å¾—åˆ°ä¸»æˆåˆ†åˆ†æçš„ä¸¤ç§ç­‰ä»·æ¨å¯¼ã€‚ æœ€å¤§å¯åˆ†æ€§ï¼šæœ€å¤§æ–¹å·®ç†è®º æ ·æœ¬ç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½å°½é‡åˆ†å¼€ï¼Œå³æŠ•å½±åçš„æ ·æœ¬ä¹‹é—´çš„æ–¹å·®è¦æœ€å¤§åŒ–ã€‚å‡è®¾ç°åœ¨æœ‰äº”ä¸ªæ ·æœ¬åˆ†å¸ƒåœ¨x-yåæ ‡ç³»ä¸‹ï¼Œå¦‚ä¸‹å›¾ æŠ•å½±çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š çº¢è‰²ç‚¹è¡¨ç¤ºæŸä¸€ä¸ªæ ·æœ¬ç‚¹$x^{(i)}$ï¼Œè“è‰²ç‚¹è¡¨ç¤º $x^{(i)}$åœ¨è¶…å¹³é¢uä¸Šçš„æŠ•å½±ï¼Œåœ¨è¿™é‡Œuæ˜¯ç›´çº¿çš„æ–œç‡ï¼Œä¹Ÿæ˜¯ç›´çº¿çš„æ–¹å‘å‘é‡ï¼Œä¸”æ˜¯å•ä½å‘é‡ã€‚è“è‰²çš„ç‚¹$x^{(i)}$åœ¨uä¸Šçš„æŠ•å½±å¸¦ä½ ï¼Œç¦»åŸç‚¹çš„è·ç¦»æ˜¯($x^{(i)}$, u)ï¼Œå³$(x^{(i)})^Tu$æˆ–è€…$u^Tx^{(i)}$ï¼Œè€Œè“è‰²ç‚¹åˆ°åŸç‚¹çš„è·ç¦»ï¼Œå°±æ˜¯åœ¨è“è‰²åæ ‡è½´ä¸Šçš„åæ ‡ã€‚ç”±äºè¿™äº›æ ·æœ¬ç‚¹çš„æ¯ä¸€ç»´çš„ç‰¹å¾å‡å€¼éƒ½æ˜¯0ï¼ˆä¹‹å‰å½’ä¸€åŒ–è¿‡ï¼‰ï¼Œå› æ­¤æŠ•å½±åˆ°uä¸Šçš„æ ·æœ¬ç‚¹çš„å‡å€¼ä»ç„¶æ˜¯0. å¦‚æœæˆ‘ä»¬å°†è¿™è¿™äº”ä¸ªæ ·æœ¬æŠ•å½±åˆ°æŸä¸€ä¸ªç»´åº¦ä¸Šï¼Œå³ä»äºŒç»´æŠ•å½±åˆ°ä¸€ç»´ä¸Šã€‚è¿™é‡Œä¸ºäº†æ¯”è¾ƒä¸åŒæ•ˆæœï¼Œåˆ†åˆ«é€‰äº†ä¸€æ¡è¿‡åŸç‚¹çš„ç›´çº¿è¡¨ç¤ºã€‚å¦‚ä¸‹å›¾ ä»ä¸Šé¢ä¸¤å¹…å›¾å¯ä»¥çœ‹å‡ºï¼Œå·¦è¾¹çš„å›¾ä¸Šçš„æ ·æœ¬é—´çš„è·ç¦»æ¯”å³è¾¹çš„è¦å¤§ï¼Œå³å·¦è¾¹æŠ•å½±åçš„æ ·æœ¬ç‚¹ä¹‹é—´çš„æ–¹å·®æœ€å¤§ï¼Œä¸ºï¼š æœ€åçš„ä¸€ä¸ªç­‰å¼ä¸­ï¼Œä¸­é—´é‚£éƒ¨åˆ†æ°å¥½æ˜¯æ±‚åæ–¹å·®çš„å…¬å¼(è¿™é‡Œç”¨mï¼Œè€Œä¸æ˜¯m-1)ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨$\\lambda$è¡¨ç¤ºå·¦è¾¹éƒ¨åˆ†ï¼Œå³æ ·æœ¬ç‚¹ä¹‹é—´çš„æ–¹å·®:$$\\lambda = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2$$ç”¨$\\sum$è¡¨ç¤ºä¸­é—´éƒ¨åˆ†ï¼š$$\\sum = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2$$é‚£ä¹ˆä¸Šé¢çš„å¼å­å¯ä»¥æ”¹å†™ä¸ºï¼š$$\\lambda = u^T\\sum u$$ä¹‹å‰æåˆ°äº†uæ˜¯è¶…å¹³é¢çš„å•ä½å‘é‡ï¼Œå³æœ‰$u^Tu = 1$ï¼Œå°†ä¸Šé¢å·¦å³ä¸¤è¾¹çš„å¼å­éƒ½ä¹˜ä»¥uï¼Œå¯å¾—ï¼š$$u\\lambda = \\lambda u= uu^T\\sum u = \\sum u$$å³$\\sum u = \\lambda u$ï¼Œæ‰€ä»¥$\\lambda$å°±æ˜¯$\\sum$çš„ç‰¹å¾å€¼ï¼Œuæ˜¯ç‰¹å¾å‘é‡ã€‚æœ€ä½³çš„æŠ•å½±ç›´çº¿æ˜¯ç‰¹å¾å€¼$\\lambda$æœ€å¤§æ—¶å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œå…¶æ¬¡æ˜¯$\\lambda$ç¬¬äºŒå¤§å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œä¾æ¬¡ç±»æ¨ã€‚ å› æ­¤åªéœ€è¦å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå¾—åˆ°çš„å‰kå¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å°±æ˜¯æœ€ä½³çš„kç»´æ–°ç‰¹å¾ï¼Œè€Œä¸”è¿™kç»´æ–°ç‰¹å¾æ˜¯æ­£äº¤çš„ã€‚å¾—åˆ°å‰kä¸ªuä»¥åï¼Œå°±å¯å°†æ ·ä¾‹$x^{(i)}$è¡¨ç¤ºä¸ºä¸‹é¢çš„æ–°æ ·æœ¬ï¼š å…¶ä¸­çš„ç¬¬jç»´å°±æ˜¯$x^{(i)}$åœ¨$u_j$ä¸Šçš„æŠ•å½±ã€‚é€šè¿‡é€‰å–æœ€å¤§çš„kä¸ªuï¼Œä½¿å¾—æ–¹å·®è¾ƒå°çš„ç‰¹å¾ï¼ˆå¦‚å™ªå£°ï¼‰è¢«ä¸¢å¼ƒã€‚ æœ€è¿‘é‡æ„æ€§ï¼šæœ€å°å¹³æ–¹è¯¯å·®ç†è®º å‡è®¾ç°åœ¨é€‰çš„è¶…å¹³é¢æ˜¯L(è¿™ä¸ªä¾‹å­ä¸­æ˜¯ç›´çº¿)ï¼Œé‚£ä¹ˆæŸä¸€æ ·æœ¬$x_k$åˆ°Lçš„å‚ç›´è·ç¦»ä¸ºdâ€™ï¼Œé‚£ä¹ˆæœ‰æ‰€æœ‰ç‚¹åˆ°è¯¥ç›´çº¿çš„è·ç¦»ä¸ºï¼š$$\\sum_{k=1}^n||(x_kâ€™ - x_k)||^2$$ä¸Šé¢è¿™ä¸ªå…¬å¼ç§°ä½œæœ€å°å¹³æ–¹è¯¯å·®(Least Squared Erroe)","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"PCA","slug":"PCA","permalink":"http://chenson.com/tags/PCA/"}]},{"title":"Machine Learning - æ”¯æŒå‘é‡æœº","date":"2017-06-16T09:21:11.000Z","path":"2017/06/16/Machine-Learning-æ”¯æŒå‘é‡æœº/","text":"1. Margins Logistic Regression â€‹åœ¨logistic regressionä¸­, å¦‚æœæˆ‘ä»¬è¦åˆ¤æ–­ä¸€ä¸ªç‚¹æ˜¯å±äº0è¿˜æ˜¯1æ¦‚ç‡æ˜¯Î¸ $$p(y=1 | x; \\theta) \\\\h_\\theta(x) = g(\\theta^T x)$$ logistic regressionå’ŒSVMçš„åŒºåˆ« ç”±ä¸Šå›¾å¯çŸ¥, Cæ˜¯éå¸¸é è¿‘Boundaryçš„, è€ŒAæ˜¯ç¦»Boundaryæœ€è¿œçš„ç‚¹. æ‰€ä»¥æˆ‘ä»¬éå¸¸æœ‰ä¿¡å¿ƒçš„è¯´Aæ˜¯å±äº+, è€ŒCæ¯”è¾ƒæœ‰å¯èƒ½å±äº+. æ‰€ä»¥å¯¹äºé‚£äº›è·ç¦»è¾¹ç•Œæ¯”è¾ƒè¿‘çš„ç‚¹æ‰æ˜¯æˆ‘ä»¬éœ€è¦é‡ç‚¹è€ƒè™‘çš„. è€Œè¿™ä¹Ÿæ­£æ˜¯logistic regressionå’ŒSVMä¹‹é—´çš„åŒºåˆ«. logistic regressionè€ƒè™‘å…¨å±€(å¦‚ä½•è€ƒè™‘å…¨å±€ï¼Ÿansï¼šRMSEæœ€å°å€¼)çº¢çº¿ SVMè€ƒè™‘å±€éƒ¨(æœ€å°é—´éš”è¦å¤§äºå¤šå°‘)ç»¿çº¿ 2. SVM ç¬¦å·è¯´æ˜ åœ¨Logsticä¸­, å¯¹äºäºŒåˆ†ç±»é—®é¢˜, y âˆˆ {0, 1} åœ¨SVMä¸­, å¯¹äºäºŒåˆ†ç±»é—®é¢˜, y âˆˆ {-1, +1}, ä¸”é‡æ–°å®šä¹‰å…¬å¼ $$h_{w,b} (x) = g(w^Tx + b) \\\\g(z) = 1 \\space (z\\geq 0) \\\\g(z) = -1 \\space (z &lt; 0)$$ å‡½æ•°é—´éš” Functional Margins ç¬¬ä¸€ä¸ªå…¬å¼è¡¨ç¤ºçš„æ˜¯æŸä¸ªæ ·æœ¬çš„å‡½æ•°é—´éš”. ç¬¬äºŒä¸ªå…¬å¼è¡¨ç¤ºçš„æ˜¯å…¨å±€çš„å‡½æ•°é—´éš”, ä¹Ÿå°±æ˜¯å‡½æ•°é—´éš”ä¸ºæ‰€æœ‰æ ·æœ¬ä¸­å‡½æ•°é—´éš”æœ€å°çš„é‚£ä¸ªå‡½æ•°é—´éš”å†³å®š. ä¹‹å‰ä½¿ç”¨æ­£è´Ÿ1æ¥è¡¨ç¤º y, æ‰€ä»¥è®¡ç®—å‡ºæ¥çš„è·ç¦»éƒ½æ˜¯ä¸€ä¸ªéè´Ÿæ•°, ä¸”è¿™ä¸ªæ•°å€¼çš„å¤§å°è¡¨ç¤ºäº†å¯¹äºé¢„æµ‹ç»“æœçš„confidence. y å€¼è¶Šæ¥è¿‘1, è¡¨ç¤ºå¯¹è¿™ä¸ªåˆ¤æ–­è¶Šè‚¯å®š.$$\\hat {\\gamma}^{(i)} = y^{(i)} (w^Tx^{(i)} + b) \\\\\\hat \\gamma = \\min_{i=1,â€¦,m} \\hat \\gamma^{(i)}$$å‡½æ•°é—´éš”è¶Šå¤§, ä»£è¡¨äº†æˆ‘ä»¬å¯¹äºåˆ†ç±»çš„ç»“æœéå¸¸çš„è‚¯å®š, æ‰€ä»¥å¸Œæœ›å‡½æ•°é—´éš”è¶Šå¤§è¶Šå¥½, ä½†éœ€è¦å¯¹è¿™ä¸ªé—´éš”åŠ ä¸Šä¸€äº›é™åˆ¶æ¡ä»¶(åé¢å…·ä½“è®²)æ‰è¡Œ. å› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ä¸æ”¹å˜è¿™ä¸ªè¶…å¹³é¢çš„æƒ…å†µä¸‹, åªè¦æˆæ¯”ä¾‹å¢åŠ wå’Œb, å°±èƒ½è®©å‡½æ•°é—´éš”ä»»æ„çš„å¤§. å‡ ä½•è·ç¦» Geometrix Margins ä¸‹å›¾ä¸­, å¦‚æœæˆ‘ä»¬çŸ¥é“Bç‚¹æ‰€åœ¨çš„è¶…å¹³é¢(separating hyperplane)çš„è§£æå¼, ä»»ä½•å…¶ä»–ç‚¹åˆ°è¯¥é¢çš„è·ç¦»éƒ½å¯ä»¥ç”¨ä¸Šé¢å®šä¹‰è¿‡çš„å‡½æ•°é—´éš”æ¥è¡¨ç¤º. $$å†³ç­–è¾¹ç•Œï¼šw^Tx + b = 0$$wæ˜¯è¶…å¹³é¢çš„æ³•å‘é‡, å‚ç›´äºå†³ç­–è¾¹ç•Œ, ä¹Ÿå°±æ˜¯è¿™ä¸ªè¶…å¹³é¢. è‹¥Bæ˜¯Aåœ¨åˆ†å‰²é¢ä¸Šçš„æŠ•å½± (ABå‚ç›´äºè¶…å¹³é¢), é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è®¡ç®—Aåˆ°è¶…å¹³é¢çš„è·ç¦» Î³ . å‡è®¾Aç‚¹æ˜¯$x_i$, é‚£ä¹ˆBç‚¹ä¸ºï¼š$$\\overrightarrow {OB} = \\overrightarrow {OA} - \\overrightarrow {BA} \\ x^{(i)} - \\gamma^{(i)} Â· \\frac w {||w||}$$å› ä¸ºBç‚¹åœ¨è¿™ä¸ªè¶…å¹³é¢ä¸Š, æ‰€ä»¥æˆ‘ä»¬å°†è¿™ä¸ªç‚¹å¸¦å›è¶…å¹³é¢å¾—åˆ°ï¼š$$w^T(x^{(i)} - \\gamma^{(i)} Â· \\frac w {||w||}) + b = 0$$é€šè¿‡ä¸Šé¢çš„å¼å­, å¯ä»¥è§£å‡ºÎ³ï¼š$$w^Tx^{(i)} - \\gamma^{(i)} Â· \\frac {w^Tw} {||w||} + b = 0 \\\\w^Tx^{(i)} + b = \\gamma^{(i)} ||w|| \\\\\\gamma^{(i)} = (\\frac w {||w||})^Tx^{(i)} + \\frac b {||w||}$$åŠ ä¸Šå‰é¢çš„$y^{(i)}$, äºæ˜¯æˆ‘ä»¬å°±èƒ½å¾—åˆ°äº†å‡ ä½•é—´éš”ï¼š$$\\gamma^{(i)} = y^{(i)}(\\frac {w^T} {||w||}x^{(i)} + \\frac b {||w||})$$é€šè¿‡ä¸Šé¢çš„å¼å­, å‘ç°å½“||w|| = 1æ—¶, å‡ ä½•é—´éš”å°±æ˜¯å‡½æ•°é—´éš”. è¿™ä¸ªæ—¶å€™, å¦‚æœä»»æ„æ”¾å¤§||w||, å‡ ä½•é—´éš”æ˜¯ä¸ä¼šæ”¹å˜çš„. å› ä¸º||w||ä¹Ÿä¼šéšç€è¢«æ”¾å¤§. å‡ ä½•é—´éš”ä¸å‡½æ•°é—´éš”çš„å…³ç³»ä¸ºï¼š$$\\gamma^{(i)} = \\frac {\\hat \\gamma^{(i)}} {||w||}$$å¯¹äºæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬, å‡ ä½•é—´éš”ä¸ºï¼š$$\\gamma = \\min_{i=1,â€¦,m} \\gamma^{(i)}$$ 2. é—´éš”æœ€å¤§åŒ–æ ¹æ®ä¸Šä¸€èŠ‚, æˆ‘ä»¬å¯ä»¥æ±‚å‡ºå‡ ä½•é—´éš”Î³. å¦‚æœç°åœ¨éœ€è¦æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢S, ä½¿å¾—ç¦»è¶…å¹³é¢æœ€è¿‘çš„ç‚¹çš„å‡ ä½•é—´éš”è¶Šå¤§è¶Šå¥½, å¯ä»¥ç”¨ä¸‹åˆ—ä¼˜åŒ–é—®é¢˜è¡¨ç¤ºï¼š$$\\max_{w,b} \\gamma \\\\s.t. \\space y_i(\\frac w {||w||}Â·x_i + \\frac b {||w||}) \\geq \\gamma, \\space i = 1, 2, â€¦, N$$å³æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¶…å¹³é¢(w, b)å…³äºè®­ç»ƒæ•°æ®é›†çš„å‡ ä½•é—´éš”Î³, çº¦æŸæ¡ä»¶è¡¨ç¤ºè¶…å¹³é¢(w,b)å…³äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ç‚¹çš„å‡ ä½•é—´éš”è‡³å°‘æ˜¯Î³. è€ƒè™‘åˆ°å‡ ä½•é—´éš”ä¸å‡½æ•°é—´éš”çš„å…³ç³»å¼, å¯ä»¥å°†è¿™ä¸ªé—®é¢˜æ”¹å†™æˆå‡½æ•°é—´éš”æ¥è¡¨ç¤º, å³ï¼š$$\\max_{w,b} \\frac {\\hat \\gamma} {||w||} \\\\s.t. \\space y_i(wÂ·x_i + b) \\geq \\hat \\gamma, \\space i = 1, 2, â€¦, N$$ä¸Šé¢å¼å­ä¸­, å‡½æ•°é—´éš”çš„å–å€¼å¹¶ä¸ä¼šå½±å“åˆ°æœ€ä¼˜åŒ–é—®é¢˜çš„è§£. äº‹å®ä¸Š, å‡è®¾å°†wå’ŒbæŒ‰æ¯”ä¾‹æ”¹å˜ä¸ºÎ»wå’ŒÎ»b, è¿™æ—¶å‡½æ•°é—´éš”ä¹Ÿä¼šè¢«å½“å¤§Î»å€, æ‰€ä»¥è¿™ä¸ªå¯¹ä¸Šé¢çš„æœ€ä¼˜åŒ–é—®é¢˜çš„ä¸ç­‰å¼çº¦æŸæ˜¯æ²¡æœ‰å½±å“çš„, å¯¹ç›®æ ‡å‡½æ•°çš„ä¼˜åŒ–ä¹Ÿæ²¡æœ‰å½±å“. è¿™æ ·, å¯ä»¥å»å‡½æ•°é—´éš”Î³=1, ä»£å…¥åˆ°ä¸Šé¢çš„æœ€ä¼˜åŒ–é—®é¢˜, æ³¨æ„åˆ°æœ€å¤§åŒ– $\\frac 1 {||w||}$å’Œæœ€å°åŒ– $\\frac 1 2$ $||w||^2$æ˜¯ç­‰ä»·çš„, ä¹Ÿæ˜¯å°±å¾—åˆ°å¦é—¨çš„çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœºå­¦ä¹ çš„æœ€ä¼˜åŒ–é—®é¢˜ï¼š$$\\min_{w,b} \\frac 1 2 ||w||^2 \\\\s.t. \\space y_i(wÂ·x_i + b) - 1 \\geq 0$$è¿™ä¸ªæ—¶å€™æˆ‘ä»¬çš„é—®é¢˜å°±è½¬åŒ–æˆäº†åœ¨çº¿æ€§çº¦æŸä¸‹çš„äºŒæ¬¡è§„åˆ’, å¯ä»¥ä½¿ç”¨äºŒæ¬¡è§„åˆ’çš„è½¯ä»¶æ¥è§£å†³è¿™ä¸ªä¼˜åŒ–é—®é¢˜,, ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„æœ€ä¼˜é—´éš”åˆ†ç±»å™¨.å®é™…ä¸Š, æˆ‘ä»¬æœ‰æ›´å¥½çš„åŠæ³•å»è§£è¿™ä¸ªä¼˜åŒ–é—®é¢˜. ä½†åœ¨è¿™ä¹‹å‰, æˆ‘ä»¬éœ€è¦è¡¥å……ä¸€ä¸‹å…¶ä»–çš„ç›¸å…³çŸ¥è¯†. 3. æ‹‰æ ¼æœ—æ—¥å¯¹å¶åœ¨çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ä¸­, å¸¸å¸¸åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§(Lagrange dulity)è®²åŸå§‹é—®é¢˜è½¬æ¢ä¸ºå¯¹å¶é—®é¢˜, é€šè¿‡è§£åº¦å¶é—®é¢˜è€Œå¾—åˆ°çš„åŸå§‹é—®é¢˜çš„è§£. è¯¥æ–¹æ³•åº”ç”¨åœ¨è®¸å¤šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ä¸­, ä¾‹å¦‚æœ€å¤§ç†µæ¨¡å‹å’Œæ”¯æŒå‘é‡æœº. åŸå§‹é—®é¢˜$$\\min_w f(w) \\\\s.t. \\space h_i(w) = 0, \\space i = 1,2,â€¦,l$$ä¸‹é¢æ˜¯çº¦æŸæ¡ä»¶, ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•, å°†é—®é¢˜è½¬æ¢ä¸ºï¼š$$L(w, b) = f(w) + \\sum_{i=1}^l\\beta_ih_i(w)$$è¿™é‡Œé¢, Î²iä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜å­(Lagrange Multipliers). ç„¶åä»¤åå¯¼ä¸º0æ¥è§£å¾—wå’ŒÎ²ï¼š$$\\frac {âˆ‚L} {âˆ‚w_i} = 0 \\\\\\frac {âˆ‚L} {âˆ‚\\beta_i} = 0$$æ›´åŠ å¹¿æ³›çš„çº¦æŸæœ€ä¼˜åŒ–é—®é¢˜ï¼š$$\\min_w f(w) \\\\s.t. \\space g_i(x) â‰¤ 0, \\space i = 1, 2, â€¦,k \\\\\\space \\space \\space \\space \\space \\space \\space h_i(w) = 0, \\space i = 1,2,â€¦,l$$æ‰€ä»¥æˆ‘ä»¬å®šä¹‰å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å…¬å¼(Generalized Lagrangian)ä¸ºï¼š$$L(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^k \\alpha_ig_i(w) + \\sum_{i=1}^l \\beta_ih_i(w)$$å…¶ä¸­, Î±i,Î²iä¸ºæ‹‰æ ¼æœ—æ—¥ä¹˜å­(Lagrange Multipliers). ç°åœ¨æˆ‘ä»¬å®šä¹‰ï¼š$$\\theta_p(w) = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} L (w, \\alpha, \\beta)$$ â€‹ å…¶ä¸­ä¸‹è¡¨Pä»£è¡¨äº†â€œprimalâ€. è‹¥ä¸¤ä¸ªçº¦æŸæ¡ä»¶å½“ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå¾—ä¸åˆ°æ»¡è¶³çš„æ—¶å€™, åˆ™å¯ä»¤Î±iä¸ºæ— ç©·å¤§æˆ–Î²iä¸ºæ— ç©·å¤§ä½¿å¾—ï¼š$$\\theta_P = f(w) \\space \\space \\space (wæ»¡è¶³åŸå§‹é—®é¢˜çš„çº¦æŸ)\\\\\\theta_p = \\infty \\space \\space \\space (å…¶ä»–)\\$$â€‹ å¯¹äºæ»¡è¶³åŸå§‹çº¦æŸçš„wæ¥è¯´, Î¸pä¸åŸå§‹é—®é¢˜ä¸­çš„ç›®æ ‡å‡½æ•°ç›¸åŒ. å¯¹äºè¿ååŸå§‹çº¦æŸé—®é¢˜çš„wæ¥è¯´Î¸p = âˆ â€‹ å¦‚æœè€ƒè™‘æœ€å°åŒ–ï¼š â€‹$$\\min_w\\theta_p(w) = \\min_w \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0 } L(w, \\alpha, \\beta)$$â€‹ å®ƒæ˜¯ä¸åŸå§‹æœ€ä¼˜åŒ–é—®é¢˜ç›¸ç­‰ä»·çš„, å³å®ƒä»¬æœ‰ç›¸åŒçš„è§£. å¯¹å¶é—®é¢˜ ç°åœ¨çœ‹å¦å¤–ä¸€ä¸ªé—®é¢˜ï¼š$$\\theta_D(\\alpha, \\beta) = \\min_w L(w, \\alpha, \\beta)$$å…¶ä¸­ä¸‹è¡¨Dä»£è¡¨äº†å¯¹å¶(dual). åœ¨åŸå§‹é—®é¢˜ä¸­, æˆ‘ä»¬å…ˆæœ€å¤§åŒ–å…³äºÎ±å’ŒÎ²çš„å‡½æ•°, å†æœ€å°åŒ–å…³äºwçš„å‡½æ•°ï¼› è€Œå¯¹å¶é—®é¢˜ä¸­, æˆ‘ä»¬å…ˆæœ€å°åŒ–å…³äºwçš„å‡½æ•°, åœ¨æœ€å¤§åŒ–å…³äºÎ±å’ŒÎ²çš„å‡½æ•°ï¼š$$\\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\theta_D(\\alpha, \\beta) = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\min_w L(w, \\alpha, \\beta)$$å®ƒä»¬å”¯ä¸€çš„åŒºåˆ«å°±åœ¨äºminå’Œmaxçš„é¡ºåºä¸åŒ. æˆ‘ä»¬ä»¤:$$d^{*} = \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} \\min_w L(w, \\alpha, \\beta) \\leq \\min_w \\max_{\\alpha, \\beta: \\alpha_i â‰¥ 0} L(w, \\alpha, \\beta) = p^{*}$$ä¹Ÿå°±æ˜¯è¯´, åœ¨æŸç§æƒ…å†µä¸‹, ä¼šæœ‰d = p. è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±å¯æŠŠæ±‚åŸå§‹é—®é¢˜è½¬åŒ–æˆæ±‚å¯¹å¶é—®é¢˜. å‡è®¾få’Œgéƒ½æ˜¯å‡¸å‡½æ•°(convex function), hæ˜¯ä»¿å°„çš„, å¹¶ä¸”å­˜åœ¨wæ˜¯å¯¹æ‰€æœ‰çš„i, èƒ½å¤Ÿä½¿å¾—:$$g_i(w) &lt; 0$$ä¸Šè¿°å‡è®¾æ¡ä»¶ä¸‹, ä¸€å®šå­˜åœ¨w*, Î±*å’ŒÎ²*ä½¿å¾—w*æ˜¯åŸå§‹é—®é¢˜çš„è§£. Î±*å’ŒÎ²*æ˜¯å¯¹å¶é—®é¢˜çš„è§£. å¹¶ä¸”è¿˜æœ‰ p = d = L(w, Î±\\, Î²*). w*, Î±*å’ŒÎ²*æ»¡è¶³KKTæ¡ä»¶(Karush-Kuhn-Tucker conditions)ï¼š å¦‚æœå­˜åœ¨æ»¡è¶³KKTæ¡ä»¶çš„w*,Î±*,Î²*, åˆ™åŸå§‹æ¡ä»¶ä¸å¯¹å¶é—®é¢˜ä¸€å®šæœ‰è§£. å…¬å¼(5)åˆç§°ä¹‹ä¸ºKKTå¯¹å¶äº’è¡¥æ¡ä»¶, è¿™ä¸ªæ¡ä»¶è¡¨æ˜å¦‚æœ Î±* &gt; 0, é‚£ä¹ˆå°±æœ‰g(w*) = 0. å³çº¦æŸæ¡ä»¶ g(w*) &lt;= 0 æ¿€æ´», wå¤„äºå¯è¡ŒåŸŸçš„è¾¹ç•Œä¸Š. è€Œå…¶ä»–è°“è¯­å¯è¡ŒåŸŸå†…éƒ¨g(w*) &lt; 0çš„ç‚¹éƒ½ä¸èµ·çº¦æŸä½œç”¨, å¯¹åº”çš„Î±* = 0. ä¸ºä»€ä¹ˆè¦å¼•å…¥å¯¹å¶ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ ç”¨æ™®é€šçš„åŸºäºæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦å’Œçš„å‡å€¼çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆBGDï¼‰æ˜¯è¡Œä¸é€šçš„ï¼ŒåŸå› åœ¨äºæˆ‘ä»¬çš„æŸå¤±å‡½æ•°é‡Œé¢æœ‰é™å®šï¼Œåªæœ‰è¯¯åˆ†ç±»çš„Mé›†åˆé‡Œé¢çš„æ ·æœ¬æ‰èƒ½å‚ä¸æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç”¨æœ€æ™®é€šçš„æ‰¹é‡æ¢¯åº¦ä¸‹é™(ä¸ºä»€ä¹ˆï¼Ÿï¼Ÿ), åªèƒ½é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æˆ–è€…å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMBGDï¼‰ã€‚ å¯¹å¶çš„ä¼˜åŠ¿ å¯¹å¶å½¢å¼å°†æƒé‡å‘é‡wè½¬åŒ–æˆå®ä¾‹$x_i$å’Œ$y_i$çš„çº¿æ€§ç»„åˆå½¢å¼ã€‚ä¸”å¯¹å¶æ˜¯ä»¥å†…ç§¯çš„å½¢å¼å‡ºç°çš„ï¼Œå¯ä»¥é¢„å…ˆä½¿ç”¨GramçŸ©é˜µå‚¨å­˜ï¼Œç”¨ç©ºé—´æ¢æ—¶é—´çš„æ–¹æ³•æé«˜è®¡ç®—æ•ˆç‡ã€‚åŸå§‹å½¢å¼æ¯æ¬¡åˆ¤æ–­è¯¯åˆ†ç±»ç‚¹æ—¶éƒ½éœ€è¦è¿›è¡Œå‘é‡ç‚¹ä¹˜è¿ç®—ã€‚ è¿™é‡ŒåŒæ—¶ä¹Ÿä¸ºåé¢å¼•å…¥æ ¸å‡½æ•°åšä¼ç¬”ï¼Œå› ä¸ºæ„ŸçŸ¥æœºæ˜¯ç¥ç»ç½‘ç»œå’Œæ”¯æŒå‘é‡æœºçš„åŸºç¡€ã€‚ 4. æœ€ä¼˜é—´éš”åˆ†ç±»å™¨æ ¹æ®ä¸Šé¢çš„å†…å®¹, å›é¡¾SVMçš„é—®é¢˜$$\\min_{\\gamma,w,b} \\frac 1 2 ||w||^2 \\\\s.t. \\space y^{(i)}(w^Tx^{(i)} + b) \\geq 1 \\$$æ ¹æ®æ‹‰æ ¼æœ—æ—¥å¯¹å¶é—®é¢˜, ä¿®æ”¹çº¦æŸæ¡ä»¶ä¸º$$g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0$$é‚£ä¹ˆå°±å’Œæ‹‰æ ¼æœ—æ—¥å…¬å¼æ˜¯ä¸€æ ·çš„äº†. æ ¹æ®ä¸Šé¢çš„KTTæ¡ä»¶å¯çŸ¥åªæœ‰å‡½æ•°é—´éš”æ˜¯1, çº¿æ€§çº¦æŸå¼å‰é¢çš„ç³»æ•°å¤§äº0, g(w)=0, å…¶ä»–çš„ä¸åœ¨çº¿ä¸Šçš„ç‚¹(g(w)&lt;0), æå€¼ä¸ä¼šåœ¨ä»–ä»¬æ‰€åœ¨çš„èŒƒå›´å†…å–å¾—, å› æ­¤å‰é¢çš„ç³»æ•°ç­‰äº0. è€ƒè™‘ä¸‹å›¾, æœ€å¤§é—´éš”åˆ†ç±»è¶…å¹³é¢ä¸ºå®çº¿ï¼š å…¶ä¸­ä¸€ä¸ªæ­£æ ·æœ¬å’Œä¸¤ä¸ªè´Ÿæ ·æœ¬æ­£å¥½åœ¨å¹³è¡Œäºåˆ†ç±»è¶…å¹³é¢çš„è™šçº¿ä¸Š, åªæœ‰è¿™ä¸‰ä¸ªæ ·æœ¬å¯¹åº”çš„ Î±i&lt;0, å…¶ä»–æ ·æœ¬å¯¹åº”çš„ Î±i=0. è¿™ä¸‰ä¸ªæ ·æœ¬å°±å«åšæ”¯æŒå‘é‡æœº. ä»è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹å‡º, æ”¯æŒå‘é‡çš„ä¸ªæ•°è¿œè¿œå°äºé›†è®­é›†çš„å¤§å°. ç°åœ¨æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š$$L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]$$æ‰€ä»¥æ¥ä¸‹å»çš„ä»»åŠ¡å°±æ˜¯æ±‚è§£å¯¹å¶é—®é¢˜, æ ¹æ®ä¸Šé¢çš„çŸ¥è¯†, æœ‰ï¼š$$d* = \\max_{\\alpha:\\alpha_i \\geq 0} \\theta_D(\\alpha) = \\max_{\\alpha:\\alpha_i \\geq 0} \\min_{w, b} L(w, b,\\alpha)$$é¦–å…ˆ, æ±‚L(w,b,Î±)å…³äºw, b çš„æœ€å°å€¼. ä»¤åå¯¼æ•°ä¸º0ï¼š$$\\frac {âˆ‚L} {âˆ‚w} = w - \\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)} = 0 \\\\\\frac {âˆ‚L} {âˆ‚b} = 0 - \\sum_{i=1}^m\\alpha_iy^{(i)}= 0$$å¯å¾—ï¼š$$w = \\sum_{i=1}^m\\alpha_i y^{(i)} x^{(i)} \\\\\\frac {âˆ‚} {âˆ‚b} L(w, b, \\alpha) = \\sum_{i=1}^m\\alpha_i y^{(i)} = 0$$å°†æ±‚å¾—çš„wå¸¦å›æ‹‰æ ¼æœ—æ—¥å‡½æ•°L(w,b,Î±), å¯å¾—åˆ°ï¼š$$L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]$$ è®¡ç®—å‡ºäº†min L(w,b,Î±), ä¾¿å¯ä»¥ç»§ç»­è¿›è¡Œmaxæ“ä½œ, å³ï¼š å¯ä»¥è¯æ˜è¯¥ä¼˜åŒ–é—®é¢˜æ»¡è¶³KKTæ¡ä»¶, æ±‚å¾—$Î±^{*}_i$ä¹‹å(åé¢å°†å¦‚ä½•æ±‚è§£), å¯é€šè¿‡:$$w = \\sum_{i=1}^m \\alpha_i y^{(i)}x^$$æ±‚å¾—w*, æœ€åé€šè¿‡ä¸‹é¢å¼å­æ±‚å¾—b*$$b^{*} = -\\frac {\\max_{i:y^{(i)}=-1}w^{*T}x^{(i)} + \\min_{i:y^{(i)} = 1}w^{*T}x^{(i)}}{2}$$å½“æ±‚å‡ºæ‰€æœ‰çš„å‚æ•°, å°±å¯ä»¥é€šè¿‡$w^T$x + b æ¥è¿›è¡Œåˆ†ç±»äº†ï¼š$$w^T + b = (\\sum_i^m \\alpha_iy_ix_i)^T x + b= \\sum_i^m \\alpha_iy_iâŸ¨x_i, xâŸ© + b$$é€šè¿‡ä¸Šé¢å¼å­å‘ç°, ç°åœ¨æ–°æ¥ä¸€ä¸ªæ–°æ•°æ®, åªéœ€è¦è®¡ç®—å®ƒä¸è®­ç»ƒæ ·æœ¬çš„å†…ç§¯å³å¯. å¹¶é€šè¿‡å‰é¢çš„KKTæ¡ä»¶æˆ‘ä»¬çŸ¥é“, åªæœ‰é™¤äº†æ”¯æŒå‘é‡çš„é‚£äº›åŸæœ¬, éƒ½æœ‰$Î±_i$ = 0. æ‰€ä»¥, æˆ‘ä»¬åªéœ€è¦å°†æ–°æ ·æœ¬ä¸æ”¯æŒå‘é‡æœºåšå†…ç§¯è¿ç®—, å³å¯æ±‚å‡º$w^T$x + b Example å‡è®¾è¿™é‡Œæœ‰ä¸‰ä¸ªæ ·æœ¬ç‚¹ï¼Œæ­£æ ·æœ¬ç‚¹x1=(3,3)^T, x2=(4,3)^T, è´Ÿæ ·æœ¬ç‚¹æ˜¯x3=(1,1)^Tï¼Œè¯•ç”¨æ„ŸçŸ¥æœºå­¦ä¹ ç®—æ³•å¯¹å¶å½¢å¼æ±‚æ„ŸçŸ¥æœºæ¨¡å‹ã€‚ ANSWER å–Î±i = 0ï¼Œè¿™é‡Œi=1ï¼Œ2ï¼Œ3ï¼Œb=0ï¼Œn=1 è®¡ç®—GramçŸ©é˜µ$$G= \\begin {bmatrix} ||x_1||^2 &amp; x_1Â·x2 &amp; x_1Â·x3 \\ x2Â· x1 &amp; ||x2||^2 &amp; x_2Â·x3 \\\\ ||x_3Â· x1 &amp; x_3Â·x2 &amp; ||x_3||^2 \\end{bmatrix} = \\begin {bmatrix} 18 &amp; 21 &amp; 6 \\ 21 &amp; 25 &amp; 7 \\\\ 6 &amp; 7 &amp; 2 \\end{bmatrix} $$ è¯¯åˆ†æ¡ä»¶$$y_i(\\sum_{j=1}^N\\alpha_jy_jx_j Â·x_i + b) \\leq 0$$å‚æ•°æ›´æ–°$$\\alpha_i \\leftarrow \\alpha_i + 1 \\ b \\leftarrow b + y_i$$â€‹ è¿­ä»£è¿‡ç¨‹å¦‚ä¸‹ï¼Œç»“æœåˆ—äºä¸‹è¡¨ | k | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 || :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€“: | :â€”-: || | | x1 | x3 | x3 | x3 | x1 | x3 | x3 || Î±1 | 0 | 1 | 1 | 1 | 2 | 2 | 2 | 2 || Î±2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 || Î±3 | 0 | 0 | 1 | 2 | 2 | 3 | 4 | 5 || b | 0 | 1 | 0 | -1 | 0 | -1 | -2 | -3 | w, båˆ†åˆ«ä¸ºï¼š $$w = 2x_1 + 0x_2 - 5x_3 = (1, 1)^T \\\\b = -3$$ åˆ†ç¦»è¶…å¹³é¢ï¼š$$x^{(1)} + x^{(2)} - 3 = 0$$ 5. Kernelsåœ¨ä¹‹å‰çš„çº¿æ€§å›å½’çš„ç« èŠ‚ä¸­ï¼Œæœ‰æåˆ°è¿‡polynomial regressionã€‚å‡è®¾xæ˜¯æˆ¿å­çš„é¢ç§¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªç‰¹å¾$x$, $x^2$, $x^3$æ¥æ„é€ ä¸€ä¸ªä¸‰æ¬¡å¤šé¡¹å¼ã€‚ è¿™é‡Œé¢æœ‰ä¸¤ä¸ªæ¦‚å¿µè¦åŒºåˆ†ä¸€ä¸‹ã€‚xä¸ºåŸå…ˆæˆ¿å­çš„é¢ç§¯ï¼Œæ˜¯å±æ€§(attributes)ã€‚é€šè¿‡è¿™ä¸ªå±æ€§xæ˜ å°„å‡ºæ¥çš„$x$, $x^2$, $x^3$å«åšç‰¹å¾(features)ã€‚åœ¨è¿™é‡Œä½¿ç”¨Ï•æ¥è¡¨ç¤ºä»å±æ€§åˆ°ç‰¹å¾çš„ç‰¹å¾æ˜ å°„(featuer mapping)ã€‚æ¯”å¦‚ï¼š$$Ï•(x) = \\begin{bmatrix} x \\ x^2 \\ x^3 \\end{bmatrix}$$é‚£ä¹ˆåœ¨SVMä¸­ï¼Œå¦‚ä½•ä½¿ç”¨è¿™ç§ç‰¹å¾æ˜ å°„å‘¢ï¼Ÿ é€šè¿‡ä¸Šé¢çŸ¥è¯†ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ‰€æœ‰å‡ºç°$âŸ¨x^{(i)}, x^{(j)}âŸ©$ æ›¿æ¢ä¸º $âŸ¨Ï•^{(i)}, Ï•^{(j)}âŸ©$ çœ‹ä¸Šå»å¥½åƒæˆ‘ä»¬æ—¢åœ¨SVMä¸­ä½¿ç”¨äº†ç‰¹å¾æ˜ å°„, åˆè§£å†³äº†æ•°æ®åœ¨ä½ç»´ç©ºé—´ä¸­çº¿æ€§ä¸å¯åˆ†çš„æƒ…å†µ. ä½†æ˜¯, è¿™é‡Œæœ‰ä¸ªé—®é¢˜. å¦‚æœæˆ‘ä»¬é€šè¿‡ç‰¹å¾æ˜ å°„å¾—åˆ°çš„$Ï•(x)$æ˜¯ä¸€ä¸ªå¾ˆé«˜ç»´ç”šè‡³æ˜¯æ— ç©·ç»´çš„, é‚£ä¹ˆè®¡ç®—$âŸ¨Ï•(x^{(i)}),Ï•(x^{(j)})âŸ©$å°±ä¸æ˜¯é‚£ä¹ˆç°å®äº†, è®¡ç®—æ—¶é—´ä¼šå¾ˆä¹…. è¿™é‡Œæˆ‘ä»¬å°±è¦å¼•å‡ºä¸€ä¸ªå«kernelsçš„æ¦‚å¿µ, å‡è®¾ Q: zåœ¨è¿™é‡Œä»£è¡¨çš„ä»€ä¹ˆï¼Ÿ $$K(x, z) = (x^Tz)^2 \\space \\space \\space \\space \\space \\space \\space \\space x, z \\in R^b$$ å±•å¼€K(x, z):$$K(x, z) = (\\sum_{i=1}^n x_iz_i) (\\sum_{j=1}^n x_iz_i) \\\\= \\sum_{i=1}^n \\sum_{j=1}^n x_ix_j z_i z_j \\\\= \\sum_{i,j=1}^n (x_ix_j) (z_iz_j)$$å±•å¼€åæˆ‘ä»¬å‘ç°ï¼ŒK(x, z)è¿˜å¯ä»¥å†™æˆ$K(x, z) = Ï•(x)^T Ï•(z)$ï¼Œå…¶ä¸­ï¼š$$Ï•(x) = \\begin{bmatrix} x _1x_1 \\ x _1x_2 \\\\x _1x_3 \\ x _2x_1 \\ x _2x_2 \\ x _2x_3 \\ x _3x_1 \\ x _3x_2 \\ x _3x_3 \\ \\end{bmatrix}$$åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ˜ å°„åç‰¹å¾çš„å†…ç§¯å’ŒåŸå§‹ç‰¹å¾çš„å†…ç§¯çš„å¹³æ–¹æ˜¯ç­‰ä»·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬åªéœ€è¦è®¡ç®—åŸå§‹ç‰¹å¾çš„å†…ç§¯å†è¿›è¡Œå¹³æ–¹å°±å¯ä»¥äº†ï¼Œå¹¶ä¸éœ€è¦å…ˆå¾—åˆ°æ˜ å°„åå†è®¡ç®—æ˜ å°„åç‰¹å¾çš„å†…ç§¯ã€‚è®¡ç®—åŸå§‹ç‰¹å¾å†…ç§¯çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n)ï¼Œè€Œè®¡ç®—æ˜ å°„ç‰¹å¾çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n^2)ã€‚ å†çœ‹å¦å¤–ä¸€ä¸ªkernels$$K(x, z) = (x^T z + c)^2 \\\\= \\sum_{i,j=1}^n (x_ix_j) (z_iz_j) + \\sum_{i=1}^n (\\sqrt {scx_i})( \\sqrt {scx_j} ) + c^2$$åŒæ ·æˆ‘ä»¬ä¹Ÿå¯ä»¥æ”¹å†™ä¸Šé¢çš„å¼å­ æ‰€ä»¥å¹¿æ³›çš„æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰$$K(x, z) = (x^T z + c)^d$$è¿™ä¸ªkernelå°†nç»´çš„ç‰¹å¾æ˜ å°„ä¸º(d, n+d)ç»´ï¼Œå³è¿™é‡Œé¢å¯¹åº”çš„å¤šé¡¹å¼$x_{i1}$, $x_{i2}$, â€¦, $x_{ik}$æœ€å¤šåˆ°dç»´ã€‚å°½ç®¡ç©ºé—´ç»´åº¦ä¸ºO(n^d)ï¼Œä½†è®¡ç®—æ—¶é—´ä»ç„¶åªæ˜¯O(n)ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸éœ€è¦å°†æ˜ å°„åçš„ç‰¹å¾å…¨éƒ¨è®¡ç®—å‡ºæ¥å†è®¡ç®—å†…ç§¯ã€‚ ä½†å› ä¸ºè®¡ç®—çš„æ˜¯å†…ç§¯ï¼Œæœ‰IRä¸­çš„ä½™å¼¦ç›¸ä¼¼åº¦å¯å­©å­ï¼Œå¦‚æœxå’Œzçš„å‘é‡å¤¹è§’è¶Šå°ï¼Œé‚£ä¹ˆæ ¸å‡½æ•°çš„å€¼å°±è¶Šå¤§ã€‚åä¹‹å°±è¶Šå°ã€‚å› æ­¤æ ¸å‡½æ•°å€¼æ˜¯Ï•(x)å’ŒÏ•(z)ç›¸ä¼¼åº¦ã€‚ å†çœ‹å¦å¤–ä¸€ä¸ªå¾ˆå‡½æ•°$$K(x, z) = exp(\\frac {||x - z||^2} {2\\sigma^2})$$åœ¨è¿™ä¸ªæ ¸å‡½æ•°ä¸­ï¼Œå¦‚æœxå’Œzå¾ˆç›¸è¿‘ï¼Œåˆ™(||x-z||~= 0)ï¼Œé‚£ä¹ˆæ ¸å‡½æ•°å€¼ä¸º1. å¦‚æœç›¸å·®å¾ˆå¤§ï¼Œåˆ™(||x-z||&gt;&gt; 0), é‚£ä¹ˆå¾ˆå‡½æ•°çš„å€¼çº¦ç­‰äº0. ç”±äºè¿™ä¸ªæ ¸å‡½æ•°ç±»ä¼¼äºé«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤æˆä¸ºé«˜æ–¯æ ¸å‡½æ•°ï¼Œä¹Ÿå«åšå¾„å‘åŸºå‡½æ•°(Radial Basis Function ç®€ç§°RBF)ï¼Œå®ƒèƒ½å¤ŸæŠŠåŸå§‹ç‰¹å¾æ˜ å°„åˆ°æ— ç©·ç»´ã€‚ ç±»ä¼¼äºé«˜æ–¯å¾ˆå‡½æ•°ï¼Œæ¯”è¾ƒxå’Œzçš„ç›¸ä¼¼åº¦ï¼Œå¹¶æ˜ å°„åˆ°0~1ä¹‹é—´ã€‚logistic Regressionä¸­ï¼Œ sigmoidå‡½æ•°ä¹Ÿå¯ä»¥ï¼Œæ‰€ä»¥è¿˜æœ‰sigmoidæ ¸å‡½æ•°ã€‚$$K(x, z) = tanh(\\beta Â· xz + b)$$ä¸‹é¢æœ‰å¼ å›¾è¯´æ˜åœ¨ä½ç»´çº¿æ€§ä¸å¯åˆ†æ—¶ï¼Œæ˜ å°„åˆ°é«˜ç»´åå°±å¯åˆ†äº†ï¼Œä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°ã€‚ åœ¨SVMä¸­ï¼Œå¯¹äºè®­ç»ƒæ ·æœ¬å­¦ä¹ å‡ºwå’Œbå‚æ•°åï¼Œå¯¹äºæ–°æ¥çš„æ ·æœ¬æˆ‘ä»¬åªéœ€è¦è®¡ç®—$w^Tx + b$æ¥åˆ¤æ–­ã€‚é‚£ä¹ˆåœ¨ä½¿ç”¨äº†æ ¸å‡½æ•°ä¹‹åï¼Œåˆ™éœ€è¦ç›¸åº”çš„æ”¹ä¸º$w^TÏ•(x) + b$ é‚£ä¹ˆæ˜¯éœ€è¦å…ˆè®¡ç®—å¥½Ï•(x)å†è¿›è¡Œé¢„æµ‹å‘¢ï¼Ÿå®é™…ä¸Šä¸éœ€è¦çš„, ä¹‹å‰è®¡ç®—è¿‡$$w^Tx + b = (\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)})^T x + b \\\\= \\sum_{i=1}^m \\alpha_i y^{(i)} âŸ¨x^{(i)}, xâŸ© + b$$æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦å°†$$âŸ¨x^{(i)}, xâŸ© \\\\æ›¿æ¢ä¸º \\\\K(x^{(i)}, x)$$ 6. æ ¸å‡½æ•°çš„æœ‰æ•ˆæ€§åˆ¤æ–­æ ¸å‡½æ•°çš„æœ‰æ•ˆæ€§ï¼Œå³åˆ¤æ–­æ˜¯å¦å­˜åœ¨Ï•, ä½¿å¾—ä¸‹é¢å¼å­æˆç«‹$$K(x,z)=âŸ¨Ï•(x)Ï•(z)âŸ©$$å‡è®¾æˆ‘ä»¬æœ‰æ ¸Kå’Œmä¸ªè®­ç»ƒæ ·æœ¬{x(1),x(2),â€¦,x(m)}, å®šä¹‰ä¸€ä¸ª (mÃ—m) çš„çŸ©é˜µK$$K_{ij} = K(x^{(i)}, x^{(j)})$$å¦‚æœæ­¤æ—¶Kæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„kerneï¼Œé‚£ä¹ˆåˆ™æœ‰ï¼š$$K_{ij} = k(x^{(i)}, x^{(j)}) = Ï•(x^{(i)})Ï•(x^{(i)}) \\\\= Ï•(x^{(j)})^T Ï•(x^{(i)}) = K(x^{(j)}, x^{(i)}) = K_{ji}$$å³Kæ˜¯å¯¹ç§°çŸ©é˜µã€‚ç°åœ¨æˆ‘ä»¬ç”¨Ï•k(x)ä¸æ¾³æ˜¯å‘é‡Ï•(x)çš„ç¬¬kä¸ªå…ƒç´ ï¼Œå¯¹ä»»æ„çš„å‘é‡zéƒ½æœ‰ï¼š ä»ä¸Šé¢çš„è¯æ˜æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼Œå¦‚æœKæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„kernelï¼Œé‚£ä¹ˆå¯¹äºåœ¨è®­ç»ƒé›†ä¸Šçš„æ ¸çŸ©é˜µKä¸€ç‚¹æ˜¯åŠæ­£å®šçš„ã€‚äº‹å®ä¸Šï¼Œè¿™ä¸ä»…ä»…æ˜¯ä¸ªå¿…è¦æ¡ä»¶ï¼Œä¹Ÿæ˜¯å……åˆ†æ¡ä»¶ã€‚æœ‰æ•ˆæ ¸ä¹Ÿå«åšMercer Kernel 7. Reference http://zhihaozhang.github.io/2014/05/11/svm3/ http://www.cnblogs.com/bourneli/p/4199990.html http://www.cnblogs.com/90zeng/p/Lagrange_duality.html","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Machine Learning - Ensemble Learning","date":"2017-06-13T04:27:45.000Z","path":"2017/06/13/Machine-Learning-Ensemble-Learning/","text":"1. é›†æˆå­¦ä¹  (Ensemble Learning)åœ¨å¯¹æ–°çš„æ•°æ®å®ä¾‹è¿›è¡Œåˆ†ç±»çš„æ—¶å€™ï¼Œé›†æˆå­¦ä¹ é€šè¿‡è®­ç»ƒå¥½å¤šä¸ªå­¦ä¹ å™¨ï¼ŒæŠŠè¿™äº›åˆ†ç±»å™¨çš„çš„åˆ†ç±»ç»“æœè¿›è¡ŒæŸç§ç»„åˆ (æ¯”å¦‚æŠ•ç¥¨) å†³å®šåˆ†ç±»ç»“æœï¼Œä»¥å–å¾—æ›´å¥½çš„ç»“æœï¼Œå°±æ˜¯æˆ‘ä»¬ç”Ÿæ´»ä¸­é‚£å¥è¯â€œä¸‰ä¸ªè‡­çš®åŒ é¡¶ä¸ªè¯¸è‘›äº®â€ï¼Œé€šè¿‡ä½¿ç”¨å¤šä¸ªå†³ç­–è€…å…±åŒå†³ç­–ä¸€ä¸ªå®ä¾‹çš„åˆ†ç±»ä»è€Œæé«˜åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ åŒè´¨é›†æˆ (Homogeneous) éœ€è¦æ˜¯åŒç§ç±»å‹ï¼Œæ¯”å¦‚å…¨éƒ¨æ˜¯å†³ç­–æ ‘æˆ–ç¥ç»ç½‘ç»œç­‰ï¼Œæ¯ä¸ªä¸ªä½“å­¦ä¹ å™¨ç§°ä¹‹ä¸ºåŸºå­¦ä¹ å™¨ (base learner)ï¼Œç›¸åº”çš„å­¦ä¹ ç®—æ³•ç§°ä¸ºåŸºå­¦ä¹ ç®—æ³• (base learning algorithm) å¼‚è´¨é›†æˆ (Heterogenous) å¯ä»¥ç”±ä¸åŒçš„å­¦æ ¡ç®—æ³•ç”Ÿæˆï¼Œè¿™æ—¶å€™å°±ä¸å†æœ‰åŸºå­¦ä¹ ç®—æ³•ï¼Œæ¯ä¸ªä¸ªä½“å­¦ä¹ å™¨ç§°ä¸ºç»„ä»¶å­¦ä¹ å™¨ (component learner) ä»¥ä¸‹æˆ‘ä»¬é‡ç‚¹è®¨è®ºåŒè´¨é›†æˆã€‚ 2. åˆ†ç±»å™¨çš„é€‰æ‹© å·®å¼‚æ€§ é—®é¢˜ï¼šå¦‚ä½•é€‰æ‹©/æ„å»ºå·®å¼‚æ€§çš„åŸºåˆ†ç±»å™¨ï¼Ÿ(ans: section 3) ç²¾åº¦ &gt; 0.5 ç²¾åº¦ç•¥é«˜äº50%çš„åˆ†ç±»å™¨ç§°ä¹‹ä¸ºå¼±å­¦ä¹ å™¨ (weak learner) é—®é¢˜ï¼šå¦‚ä½•æŠ•ç¥¨é€‰æ‹©å‡ºæœ€ä½³çš„é¢„æµ‹ï¼Ÿ(ans: section 4) å¦‚ä½•ç»„åˆ å‡è®¾åœ¨äºŒåˆ†ç±»å™¨ä¸­ï¼Œè¿™é‡Œæœ‰ä¸‰ä¸ªåˆ†ç±»å™¨åœ¨æµ‹è¯•ä¸‰ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªåˆ†ç±»å™¨çš„æ­£ç¡®ç‡éƒ½æ˜¯66%ï¼Œé‚£ä¹ˆç»„åˆå‡ºçš„ç»“æœå¤§è‡´å¯ä»¥åˆ†æˆä»¥ä¸‹å‡ ç§ é›†æˆæ€§èƒ½æå‡ | | test1 | test2 | test3 || :â€“: | :â€”: | :â€”: | :â€”: || h1 | right | right | wrong || h2 | wrong | right | right || h3 | right | wrong | right | å¦‚æœåªæ˜¯ç®€å•æŠ•ç¥¨æ³•çš„è¯ï¼Œåœ¨æ¯ä¸ªtestä¸­ï¼Œæœ‰ä¸¤ä¸ªåˆ†ç±»å™¨æ˜¯å¯¹çš„ï¼Œä¸€ä¸ªæ˜¯é”™çš„ï¼Œé‚£ä¹ˆæŠ•ç¥¨å‡ºæ¥çš„ç»“æœæ˜¯å¯¹çš„ï¼Œæœ€ç»ˆç²¾åº¦å¯ä»¥è¾¾åˆ°100%ã€‚ é›†æˆä¸èµ·ä½œç”¨ | | test1 | test2 | test3 || :â€“: | :â€”: | :â€”: | :â€”: || h1 | right | right | wrong || h2 | right | right | wrong || h3 | right | right | wrong | ä¸‰ä¸ªåˆ†ç±»å™¨å¯¹ä¸‰ä¸ªtestè¿›è¡Œé¢„æµ‹ï¼Œæ°å¥½ä¸‰ä¸ªåˆ†ç±»å™¨å¯¹test3çš„æƒ…å†µåˆ†ç±»éƒ½æ˜¯é”™çš„ï¼Œè€Œå¯¹test1å’Œtest2çš„ç»“æœéƒ½é¢„æµ‹æ­£ç¡®ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„æµ‹è¯•ç»“æœæ²¡æœ‰å½±å“ï¼Œéƒ½æ˜¯66%ã€‚ é›†æˆèµ·è´Ÿä½œç”¨ | | test1 | test2 | test3 || â€”- | â€”â€“ | â€”â€“ | â€”â€“ || h1 | right | wrong | wrong || h2 | wrong | right | wrong || h3 | wrong | wrong | right | å’Œç¬¬ä¸€ç§æƒ…å†µç›¸åï¼Œæœ€ç»ˆçš„æµ‹è¯•ç»“æœæ˜¯33% é€šè¿‡ä¸Šé¢çš„ä¾‹å­ï¼Œå¯ä»¥åæ˜ å‡ºä¸€ä¸ªé—®é¢˜ï¼Œåœ¨åŒè´¨é›†æˆä¸­ï¼Œå¦‚ä½•æ„å»ºå¤šä¸ªè¯¯å·®æ˜¯ç›¸äº’ç‹¬ç«‹çš„åŸºå­¦ä¹ å™¨ã€‚å› ä¸ºè¿™äº›åŸºå­¦ä¹ å™¨æ˜¯ç”¨çš„åŒä¸€ç§ç®—æ³•ï¼ŒåŸºæœ¬ä¹Ÿæ˜¯ä½¿ç”¨åŒä¸€ç»„æ•°æ®ï¼Œè§£å†³çš„ä¹Ÿæ˜¯åŒä¸€ä¸ªé—®é¢˜ã€‚ æ ¹æ®ä¸ªä½“å­¦æœŸçš„ç”Ÿæˆæ–¹å¼ï¼Œé›†æˆå­¦ä¹ æ–¹æ³•å¤§è‡´å¯ä»¥åˆ†æˆä¸¤å¤§ç±»ï¼š Boosting ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œå¿…é¡»ä¸²è¡Œç”Ÿæˆçš„åºåˆ—åŒ–æ–¹æ³• Bagging å’Œ Random Forest ä¸ªä½“å­¦ä¹ å™¨ä¹‹å‰ä¸å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œå¯åŒæ—¶ç”Ÿæˆçš„å¹¶è¡ŒåŒ–æ–¹æ³• 3. æ„å»ºå·®å¼‚æ€§åŸºåˆ†ç±»å™¨åœ¨åŒä¸€ä¸ªæ•°æ®é›†ä¸Šï¼Œæ„å»ºä¸åŒçš„ï¼Œå…·æœ‰å·®å¼‚æ€§çš„åˆ†ç±»å™¨ (å¥½è€Œä¸åŒ)ï¼Œå°±æ˜¯é€šè¿‡æŠ½æ ·æŠ€æœ¯è·å–å¤šä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œç”Ÿæˆå¤šä¸ªå·®å¼‚æ€§åˆ†ç±»å™¨ã€‚ç›®å‰ä¸»è¦çš„æ–¹æ³•æœ‰ï¼šBagging å’Œ Boostingã€‚ 3.1 Boostingæå‡æ–¹æ³•æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œé€šè¿‡æ”¹å˜æ ·æœ¬åˆ†å¸ƒï¼Œä½¿å¾—åˆ†ç±»å™¨èšé›†åœ¨é‚£äº›å¾ˆéš¾åˆ†çš„æ ·æœ¬ä¸Šï¼Œå¯¹é‚£äº›å®¹æ˜“é”™åˆ†çš„æ•°æ®åŠ å¼ºå­¦ä¹ ï¼Œå¢åŠ é”™åˆ†æ•°æ®çš„æƒé‡ï¼Œè¿™æ ·é”™åˆ†çš„æ•°æ®å†ä¸‹ä¸€è½®çš„è¿­ä»£å°±æœ‰æ›´å¤§çš„ä½œç”¨ (å¯¹é”™åˆ†æ•°æ®è¿›è¡Œæƒ©ç½š)ã€‚ å…·ä½“æ¥è¯´ï¼Œå…ˆä»åˆå§‹è®­ç»ƒé›†è®­ç»ƒå‡ºä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼Œå†æ ¹æ®è¿™ä¸ªåŸºå­¦ä¹ å™¨çš„è¡¨ç°å¯¹è®­ç»ƒæ ·æœ¬åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—å…ˆå‰åŸºå­¦ä¹ å™¨åšé”™çš„è®­ç»ƒæ ·æœ¬åœ¨åç»­å—åˆ°æ›´å¤šçš„å…³æ³¨ï¼Œç„¶ååŸºäºè°ƒæ•´åçš„æ ·æœ¬åˆ†å¸ƒæ¥è®­ç»ƒä¸‹ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›å¦‚æ­¤é‡å¤è¿›è¡Œï¼Œä¸€ç›´åˆ°åŸºå­¦ä¹ å™¨çš„æ•°ç›®åˆ°è¾¾äº‹å…ˆæŒ‡å®šçš„Tå€¼ï¼Œç„¶åå°†æ‰€æœ‰çš„åŸºå­¦ä¹ å™¨ç›¸ç»“åˆã€‚(é—®é¢˜ï¼šå¦‚ä½•è°ƒæ•´åˆ†å¸ƒï¼Ÿ) æ•°æ®çš„æƒé‡æœ‰ä¸¤ä¸ªä½œç”¨ï¼Œä¸€æ–¹é¢æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æƒå€¼ä½œä¸ºæŠ½æ ·åˆ†å¸ƒï¼Œè¿›è¡Œå¯¹æ•°æ®çš„æŠ½æ ·ï¼Œå¦ä¸€æ–¹é¢åˆ†ç±»å™¨å¯ä»¥ä½¿ç”¨æƒå€¼å­¦ä¹ æœ‰åˆ©äºé«˜æƒé‡æ ·æœ¬çš„åˆ†ç±»å™¨ã€‚æŠŠä¸€ä¸ªå¼±åˆ†ç±»å™¨æå‡ä¸ºä¸€ä¸ªå¼ºåˆ†ç±»å™¨ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒAdaBoostç®—æ³• (è¥¿ç“œä¹¦P173) Example å‡è®¾ç°åœ¨æœ‰ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨å¦‚ä¸‹è¡¨ | | é¢„æµ‹ + | é¢„æµ‹ - | count || :â€”â€”-: | :â€“: | :â€“: | :â€”: || å®é™… + | 24 | 16 | 40 || å®é™… - | 9 | 51 | 60 || count | 33 | 67 | 100 | é”™è¯¯ç‡è®¡ç®—$$\\epsilon = \\frac {(9 + 16)} {100} = 0.25$$ é”™åˆ†æ ·æœ¬çš„æƒå€¼æ›´æ–°$$w_e = \\frac 1 {2*\\epsilon} = 2$$ æ­£ç¡®æ ·æœ¬çš„æƒå€¼æ›´æ–°$$w_r = \\frac 1 {2*(1 - \\epsilon)} = \\frac 2 3$$ æ‰€ä»¥è®²æƒé‡ä¹˜ä»¥ç›¸åº”çš„æ•°æ®é‡ï¼Œå¾—åˆ°æ–°çš„æ•°æ®åˆ†å¸ƒï¼Œå¦‚ä¸‹ é¢„æµ‹ + é¢„æµ‹ - count å®é™… + 24 * 2/3 = 16 16 * 2 = 32 16+32=48 å®é™… - 9 * 2 = 18 51 * 2/3 = 34 18 + 34 = 52 count 16 + 18 = 34 32 + 34 = 66 100 3.2 Baggingé€šè¿‡å¯¹åŸæ•°æ®é›†è¿›è¡Œæœ‰æ”¾å›çš„é‡‡æ · (bootstrap sampling) æ„å»ºå‡ºå¤§å°å’ŒåŸæ•°æ®é›†å¤§å°ä¸€æ ·çš„æ–°æ•°æ®é›†D1ï¼ŒD2ï¼ŒD3â€¦..ï¼Œç„¶åç”¨è¿™äº›æ–°çš„æ•°æ®é›†è®­ç»ƒå¤šä¸ªåˆ†ç±»å™¨H1ï¼ŒH2ï¼ŒH3â€¦.ã€‚å› ä¸ºæ˜¯æœ‰æ”¾å›çš„é‡‡æ ·æ‰€ä»¥ä¸€äº›æ ·æœ¬å¯èƒ½ä¼šå‡ºç°å¤šæ¬¡ï¼Œè€Œå…¶ä»–æ ·æœ¬ä¼šè¢«å¿½ç•¥ï¼Œç†è®ºä¸Šåˆå§‹è®­ç»ƒé›†ä¸­çº¦æœ‰63.2%çš„æ ·æœ¬ä¼šå‡ºç°åœ¨é‡‡æ ·é›†ä¸­ã€‚(è¥¿ç“œä¹¦P27) è‡ªåŠ©é‡‡æ ·æ³• (Bootstrap sampling) å‡è®¾ç»™å®šä¸€ä¸ªåŒ…å«äº†mä¸ªæ ·æœ¬çš„æ•°æ®é›†Dï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæ–°çš„æ•°æ®D1ï¼Œå¤§å°å’ŒDä¸€æ ·çš„ã€‚é‚£ä¹ˆæ¯æ¬¡æˆ‘ä»¬ä»Dä¸­å–å‡ºä¸€ä¸ªæ ·æœ¬å…¥åˆ°D1ä¸­ï¼Œä¹‹åæŠŠæ ·æœ¬æ”¾å›Dä¸­ï¼Œé‡æ–°é‡‡æ ·ï¼Œæ€»å…±é‡‡æ ·mæ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸ºmçš„D1ã€‚å› ä¸ºæˆ‘ä»¬æŠŠæ ·æœ¬åˆé‡æ–°æ”¾å…¥Dä¸­ï¼Œä¸‹æ¬¡è¿˜æœ‰å¯èƒ½æŠ½å–åˆ°ï¼Œå¯¼è‡´åœ¨D1ä¸­ï¼Œæœ‰äº›æ ·æœ¬ä¼šå‡ºç°å¤šæ¬¡ï¼Œè€Œæœ‰äº›æ ·æœ¬åˆ™ä¸€æ¬¡ä¹Ÿä¸ä¼šå‡ºç°ã€‚æ‰€ä»¥æ ¹æ®å¦‚ä¸‹å…¬å¼ï¼š$$lim_{m -&gt; \\infty} (1 - \\frac 1 m)^m -&gt; \\frac 1 e \\approx 0.368$$æœ€ç»ˆåœ¨åˆå§‹æ ·æœ¬æ•°æ®é›†Dä¸­ï¼Œæœ‰36.8%çš„æ ·æœ¬æ˜¯ä¸€æ¬¡ä¹Ÿæ²¡æœ‰å‡ºç°çš„ï¼Œè€Œ63.2%é‡å¤å‡ºç°äº†ï¼Œè¿™æ ·æˆ‘ä»¬å°±æ”¹å˜äº†åˆå§‹æ ·æœ¬æ•°æ®é›†çš„åˆ†å¸ƒã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†D1ä½œä¸ºè®­ç»ƒé›†ï¼ŒD\\D1ï¼Œå³å‰©ä¸‹çš„36.8%æœªå‡ºç°åœ¨D1ä¸­çš„ä½œä¸ºæµ‹è¯•é›†ã€‚è¿™æ ·çš„æµ‹è¯•ç»“æœç§°ä¹‹ä¸ºåŒ…å¤–ä¼°è®¡ (out-of-bag-estimate) Baggingé€šè¿‡é™ä½åŸºåˆ†ç±»å™¨æ–¹å·®æ”¹å–„äº†æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤Baggingçš„æ€§èƒ½ä¾èµ–äºåŸºåˆ†ç±»å™¨çš„ç¨³å®šæ€§ï¼Œå¦‚æœåŸºåˆ†ç±»å™¨æ˜¯ä¸ç¨³å®šçš„ï¼ŒBaggingæœ‰åŠ©äºå‡ä½è®­ç»ƒæ•°æ®çš„éšæœºæ‰°åŠ¨å¯¼è‡´çš„è¯¯å·®ï¼Œä½†æ˜¯å¦‚æœåŸºåˆ†ç±»å™¨æ˜¯ç¨³å®šçš„ï¼Œå³å¯¹æ•°æ®å˜åŒ–ä¸æ•æ„Ÿï¼Œé‚£ä¹ˆBaggingæ–¹æ³•å°±å¾—ä¸åˆ°æ€§èƒ½çš„æå‡ï¼Œç”šè‡³ä¼šå‡ä½ï¼Œå› ä¸ºæ–°æ•°æ®é›†åªæœ‰63%ã€‚ 3.3 Random Forestéšæœºæ£®æ—æ˜¯Baggingçš„ä¸€ä¸ªæ‹“å±•å˜ä½“ï¼ŒåŸºäºBaggingæ¡†æ¶è¿›ä¸€æ­¥é™ä½äº†äº†æ¨¡å‹çš„æ–¹å·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨çš„å†³ç­–æ ‘ä¸ä¼ ç»Ÿçš„å†³ç­–æ ‘ç•¥æœ‰ä¸åŒã€‚ ä¼ ç»Ÿçš„å†³ç­–æ ‘ï¼Œæ¯”å¦‚C45æˆ–è€…CARTï¼Œæ¯å½“éœ€è¦åˆ’åˆ†ä¸€ä¸ªå±æ€§çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šå°†æ‰€æœ‰çš„å±æ€§çš„éƒ½è¿›è¡Œåˆ’åˆ†ä¸€éï¼Œç„¶åè®¡ç®—å…¶åˆ’åˆ†ä¹‹åä¸åˆ’åˆ†ä¹‹å‰ç†µçš„åå·®ï¼Œæˆ–è€…è®¡ç®—GiniæŒ‡æ•°ç­‰ï¼Œç„¶åä»ä¸­é€‰å–å‡ºæœ€ä½³å±æ€§è¿›è¡Œåˆ’åˆ†ã€‚ è€Œéšæœºæ£®æ—ï¼Œå¯¹åŸºå†³ç­–æ ‘çš„æ¯ä¸ªç»“ç‚¹ï¼Œå…ˆä»è¯¥èŠ‚ç‚¹çš„å±æ€§é›†åˆä¸­éšæœºé€‰æ‹©ä¸€ä¸ªåŒ…å«kä¸ªå±æ€§çš„å­é›†ï¼Œç„¶åå†ä»è¿™ä¸ªå­é›†ä¸­é€‰å–ä¸€ä¸ªæœ€ä¼˜å±æ€§ç”¨äºåˆ’åˆ† (ä»ç‰¹å¾çš„ä¸åŒå­é›†æ¥æ„å»ºæ ‘)ã€‚è¿™é‡Œçš„å‚æ•°kæ§åˆ¶äº†éšæœºæ€§çš„å¼•å…¥ç¨‹åº¦ï¼š è‹¥k=dï¼Œåˆ™åŸºå†³ç­–æ ‘å’Œä¼ ç»Ÿå†³ç­–æ ‘ç›¸åŒã€‚ è‹¥k=1ï¼Œåˆ™æ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªå±æ€§ç”¨äºåˆ’åˆ† æ¨èå€¼k=log_2 d éšæœºæ£®æ—é™¤äº†åŸºäºBaggingä¸­å¯¹äºæ ·æœ¬çš„æ‰°åŠ¨ï¼Œè¿˜åŒæ—¶å¯¹æ•°æ®é›†ä¸­çš„å±æ€§åˆ’åˆ†è¿›è¡Œäº†æ‰°åŠ¨ï¼Œè¿™æ ·å¯ä»¥å¢åŠ åŸºå­¦ä¹ å™¨çš„å¤šæ ·æ€§å’Œå·®å¼‚æ€§ï¼Œä½¿å¾—æœ€ç»ˆé›†æˆçš„æ³›åŒ–æ€§å¾—åˆ°æå‡ã€‚ 3.4 å¢å¼ºå¤šæ ·æ€§ æ•°æ®æ ·æœ¬æ‰°åŠ¨ è¾“å…¥å±æ€§æ‰°åŠ¨ è¾“å‡ºè¡¨ç¤ºæ‰°åŠ¨ ç®—æ³•å‚æ•°æ‰°åŠ¨ 4. ç»„åˆç­–ç•¥4.1 å½’å›é¢„æµ‹ (æ•°å€¼é¢„æµ‹) ç®€å•å¹³å‡å€¼æ³• å°±æ˜¯å–å„ä¸ªåˆ†ç±»å™¨ç»“æœçš„å¹³å‡å€¼ï¼ˆä¼šä¸ä¼šé™ä½è¿™é‡Œé¢æœ€å¥½çš„é‚£ä¸ªåˆ†ç±»å™¨çš„ç²¾åº¦ï¼‰$$H(x) = \\frac 1 T \\sum_{i=1}^T h_i(x)$$ åŠ æƒå¹³å‡æ³• ç»™ä¸åŒçš„åˆ†ç¦»å™¨èµ‹äºˆä¸ä¸€æ ·çš„æƒé‡å€¼ï¼Œç„¶åæ±‚å’Œ$$H(x) = \\sum_{i=1}^Tw_i Â· h_i(x)$$ 4.2 åˆ†ç±»é¢„æµ‹ (ç±»åˆ«é¢„æµ‹) ç®€å•æŠ•ç¥¨æ³• æ¯ä¸ªåˆ†ç±»å™¨çš„æƒé‡å¤§å°éƒ½ä¸€æ ·ï¼Œå°‘æ•°æœä»å¤šæ•° ç»å¯¹å¤šæ•°æŠ•ç¥¨æ³•ç¥¨æ•°è¿‡åŠï¼Œå¦åˆ™æ‹’ç»é¢„æµ‹ ç›¸å¯¹å¤šæ•°æŠ•ç¥¨æ³•ç¥¨æ•°æœ€å¤šçš„é‚£ä¸ª åŠ æƒæŠ•ç¥¨æ³• ç»™æ¯ä¸ªåˆ†ç±»å™¨èµ‹äºˆä¸€ä¸ªæƒé‡ï¼Œç„¶åæ ¹æ®æƒé‡æ¥æŠ•ç¥¨ï¼Œå¾—åˆ°ç¥¨æ•°é«˜çš„æœ€ä¸ºè¾“å‡ºç»“æœ æ¦‚ç‡æŠ•ç¥¨æ³•ï¼ˆå’Œç®€å•åˆ†ç±»æœ‰ä»€ä¹ˆä¸åŒï¼Ÿï¼‰ æœ‰çš„åˆ†ç±»å™¨çš„è¾“å‡ºæ˜¯æœ‰æ¦‚ç‡ä¿¡æ¯çš„ï¼Œæ¯”å¦‚ åˆ†ç±»å™¨Aè¾“å‡ºç»“æœ1çš„æ¦‚ç‡ä¸º75% åˆ†ç±»å™¨Bè¾“å‡ºç»“æœ0çš„æ¦‚ç‡ä¸º80% åˆ†ç±»å™¨Cè¾“å‡ºç»“æœ1çš„æ¦‚ç‡ä¸º52% æœ€ç»ˆè¾“å‡ºçš„ç»“æœä¸ºï¼Ÿï¼Ÿï¼Ÿï¼ˆæ¦‚ç‡ç›¸åŠ è¿˜æ˜¯æ¦‚ç‡çš„å¹³å‡å€¼ï¼Œç›¸åŠ å¯èƒ½æ€§é«˜ï¼‰ 4.3 å­¦ä¹ æ³•5. Reference è¥¿ç“œä¹¦ç¬¬å…«ç«  - å‘¨å¿—å éå‚è€ƒï¼Œæ¨èé˜…è¯» å°èœé¸Ÿå¯¹å‘¨å¿—åå¤§ç¥gcForestçš„ç†è§£ gcForestç®—æ³•ç†è§£","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"Ensemble","slug":"Ensemble","permalink":"http://chenson.com/tags/Ensemble/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸‰ï¼‰","date":"2017-06-09T06:08:52.000Z","path":"2017/06/09/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸‰ï¼‰/","text":"1. ä¸ºä»€ä¹ˆè¦ä½¿ç”¨Secondary Sortåœ¨Hadoopä¸­ï¼Œä»Mapåˆ°Reduceçš„è¿‡ç¨‹ä¸­ï¼Œkeyæ˜¯ä¸æ–­è¢«sortçš„ã€‚æ‰€ä»¥ä»mapå‡ºæ¥çš„æ—¶å€™å†™å…¥åˆ°ä¸€ä¸ªintermediate output fileçš„æ—¶å€™ï¼Œkeyæ˜¯æœ‰åºçš„ã€‚ç„¶åreduceä¸æ–­çš„ä»ä¸åŒçš„clusteré‡Œé¢fetché‡Œé¢çš„key-value pairsçš„æ—¶å€™ï¼Œä»ç„¶å¤šæ¬¡sortè¿™äº›pairsã€‚æ‰€ä»¥æœ€ç»ˆè¿›å…¥åˆ°reducerçš„keyæ˜¯æœ‰åºçš„ï¼Œä½†æ˜¯valueæ˜¯æ— åºçš„ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦å¯¹valueä¹Ÿè¿›è¡Œæ’åºå‘¢ï¼ŸGoogleçš„MRå†…ç½®äº†å‡½æ•°å¯¹valueä¹Ÿå¯ä»¥æ’åºï¼Œä½†æ˜¯Hadoopä¸è¡Œï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±å»å®šåˆ¶partitionerç­‰å»å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚ ä¸¾ä¸ªæ —å­ï¼š è¾“å…¥æ–‡ä»¶æ ¼å¼å¦‚ä¸‹ 12345678910112015,1,242015,3,542015,1,32015,2,-432015,4,52015,3,462014,2,642015,1,42015,1,212015,2,352015,2,20 â€‹ æœŸæœ›çš„è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼ˆvalueæ˜¯æœ‰åºçš„ï¼‰ 123452014-2 642015-1 3ï¼Œ4ï¼Œ21ï¼Œ242015-2 -43ï¼Œ0ï¼Œ352015-3 46ï¼Œ562015-4 5 â€‹ Hadoopé»˜è®¤çš„è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼ˆvalueæ˜¯æ— åºçš„ï¼‰ 123452014-2 642015-1 21ï¼Œ4ï¼Œ3ï¼Œ242015-2 0ï¼Œ35ï¼Œ-432015-3 56ï¼Œ462015-4 5 2. è§£å†³æ–¹æ¡ˆ ä¼ ç»Ÿçš„è§£å†³æ–¹æ³• å°±æ˜¯è¿›å…¥åˆ°åŒä¸€ä¸ªreducerçš„æ—¶å€™ï¼Œè¿™äº›åŒä¸€ä¸ªå¯ä»¥çš„valuesæ˜¯åœ¨ä¸€ä¸ªlisté‡Œé¢çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å…ˆæŠŠè¿™ä¸ªlisté‡Œé¢çš„valueå­˜åˆ°å†…å­˜ä¸­å»ï¼Œç„¶ååœ¨å†…å­˜ä¸­å°†è¿™äº›valueæ’åºã€‚è¿™ä¸ªæ–¹æ³•åªé€‚ç”¨äºæ•°æ®é‡è¾ƒå°çš„æ—¶å€™ï¼Œå½“æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™ï¼Œå†…å­˜å¹¶ä¸èƒ½åŒæ—¶å­˜å…¥è¿™äº›valuesï¼Œç¨‹åºå°±æŠ¥é”™æ— æ³•æ­£å¸¸è¿è¡Œã€‚ åˆ©ç”¨Hadoopç‰¹ç‚¹çš„æ–¹æ³• æ—¢ç„¶Hadoopå¯ä»¥å¯¹Keyè¿›è¡Œæ’åºï¼Œé‚£ä¹ˆå¯ä»¥åˆ©ç”¨è¿™ç‚¹ï¼Œå°†ä¹‹å‰çš„key-value pairs ç»„æˆä¸€ä¸ªæ–°çš„keyï¼Œè®©Hadoopå¯¹è¿™äº›Keyè¿›è¡Œæ’åºï¼Œæ —å­å¦‚ä¸‹ 12345// åŸå…ˆçš„key-value pair(2015,1) 21// æ–°çš„composite-key-value pair((2015,1),21) 21 è¿™é‡Œçš„(2015,1)æ˜¯æˆ‘ä»¬åŸå…ˆå°±æœ‰çš„keyï¼Œä¸ºäº†åŒºåˆ†æ–°çš„keyï¼Œå°†è¿™ä¸ªåŸæœ‰çš„key (k, v1) ,ç§°ä¸ºnatural keyï¼Œå°†æ–°çš„key ((k, v1), v2)ç§°ä¸ºcomposite keyã€‚å°†v2ç§°ä¸ºnatural valueã€‚å…·ä½“å¦‚ä¸‹å›¾ â€‹ ä¸ºäº†å®ç°Hadoopå¯¹æ–°çš„composite keyè¿›è¡Œæ’åºï¼Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰partitionerå’Œgroupingæ¥ç¡®ä¿è¿™äº›composite keyä¸­natural keyç›¸åŒçš„ä¼šè¢«åˆ†é…åˆ°åŒä¸€ä¸ªreducerã€‚å› ä¸ºåœ¨composite keyä¸­ï¼Œå³ä½¿(k, v1)ç›¸åŒï¼Œåªè¦v2ä¸åŒï¼Œé»˜è®¤çš„partitionerå°±ä¼šè®¤ä¸ºè¿™æ˜¯ä¿©ä¸ªä¸åŒçš„keyï¼Œå°±å¾ˆæœ‰å¯èƒ½è®²è¿™ä¸ªcomposite keyåˆ†é…åˆ°ä¸åŒçš„reduceré‡Œå»ã€‚ â€‹ 3.å®ç°è¿‡ç¨‹ partitioner: å°†natural keyç›¸åŒçš„å‘é€åˆ°åŒä¸€ä¸ªreduceré‡Œå» åœ¨åŒä¸€ä¸ª[]é‡Œé¢è¯´æ˜æ˜¯ä¸€ä¸ªreducerï¼Œvalueä»»ç„¶æ˜¯æ— åºçš„ 123456789[((2014-2,64),64)][((2015-1,24),24), ((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,56),56), ((2015-3,46),46)][((2015-4,5),5)] â€‹ grouping comparator: å°†natural keyç›¸åŒçš„ä½œä¸ºä¸€ä¸ªgroupæ’åº 123456789[((2014-2,64),64)][((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21), ((2015-1,24),24)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,46),46), ((2015-3,56),56)][((2015-4,5),5)] â€‹ è¿›å…¥åˆ°Reducerçš„æ ¼å¼ (æœ‰ç‚¹é‡Œä¸ç†è§£æ­¤æ—¶composite keyé‡Œé¢çš„natural valueæ˜¯å¦‚ä½•ç¡®å®šçš„) 123456789((2014-2,64), (64))((2015-1,24), (2ï¼Œ4ï¼Œ21ï¼Œ24))((2015-2,35), (-43,0,35))((2015-3,46), (46,56))((2015-4,5), (5)) â€‹ æœ€ç»ˆçš„è¾“å‡º 12342014-2 642015-1 3,4,21,242015-2 -43,0,352015-4 5 æ•´ä¸ªæµç¨‹å›¾å¦‚ä¸‹ï¼ˆå›¾å†…æ•°æ®ä¸ä¸Šé¢æ•°æ®ä¸ç¬¦åˆï¼‰ 4. ä»£ç  composite key å°†æ—§çš„Keyï¼ˆnatural keyï¼‰å’ŒValueç»„åˆæˆæ–°çš„Keyï¼ˆcomposite keyï¼‰çš„ä»£ç  12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.secondarySort;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class Entry implements WritableComparable&lt;Entry&gt; &#123; private String yearMonth; private int count; public Entry() &#123; &#125; @Override public int compareTo(Entry entry) &#123; int result = this.yearMonth.compareTo(entry.getYearMonth()); if (result == 0) &#123; result = compare(count, entry.getCount()); &#125; return result; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(yearMonth); dataOutput.writeInt(count); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.yearMonth = dataInput.readUTF(); this.count = dataInput.readInt(); &#125; public String getYearMonth() &#123; return yearMonth; &#125; public void setYearMonth(String yearMonth) &#123; this.yearMonth = yearMonth; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public static int compare(int a, int b) &#123; return a &lt; b ? -1 : (a &gt; b ? 1 : 0); &#125; @Override public String toString() &#123; return yearMonth; &#125;&#125; â€‹ Partitioner 1234567891011package com.secondarySort; import org.apache.hadoop.mapreduce.Partitioner; public class EntryPartitioner extends Partitioner&lt;Entry, Integer&gt; &#123; @Override public int getPartition(Entry entry, Integer integer, int numberPartitions) &#123; return Math.abs((entry.getYearMonth().hashCode() % numberPartitions)); &#125;&#125; â€‹ Grouping Compartor 1234567891011121314151617package com.secondarySort; import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class EntryGroupingComparator extends WritableComparator &#123; public EntryGroupingComparator() &#123; super(Entry.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; Entry a1 = (Entry) a; Entry b1 = (Entry) b; return a1.getYearMonth().compareTo(b1.getYearMonth()); &#125;&#125; â€‹ Mapper 1234567891011121314151617181920212223public class SecondarySortMapper extends Mapper&lt;LongWritable, Text, Entry, Text&gt; &#123; private Entry entry = new Entry(); private Text value = new Text(); @Override protected void map(LongWritable key, Text lines, Context context) throws IOException, InterruptedException &#123; String line = lines.toString(); String[] tokens = line.split(\",\"); // YYYY = tokens[0] // MM = tokens[1] // count = tokens[2] String yearMonth = tokens[0] + \"-\" + tokens[1]; int count = Integer.parseInt(tokens[2]); entry.setYearMonth(yearMonth); entry.setCount(count); value.set(tokens[2]); context.write(entry, value); &#125;&#125; â€‹ Reducer 123456789101112public class SecondarySortReducer extends Reducer&lt;Entry, Text, Entry, Text&gt; &#123; @Override protected void reduce(Entry key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder builder = new StringBuilder(); for (Text value : values) &#123; builder.append(value.toString()); builder.append(\",\"); &#125; context.write(key, new Text(builder.toString())); &#125;&#125; â€‹ Deriver 123456789101112131415Configuration conf = new Configuration();Job job = Job.getInstance(conf);job.setJarByClass(Iteblog.class);job.setJobName(\"SecondarySort\"); FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setOutputKeyClass(Entry.class);job.setOutputValueClass(Text.class); job.setMapperClass(SecondarySortMapper.class);job.setReducerClass(SecondarySortReducer.class);job.setPartitionerClass(EntryPartitioner.class);job.setGroupingComparatorClass(EntryGroupingComparator.class); 5. å¸¸ç”¨çš„Secondary Sortä»£ç  IntPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; int first, second; // public IntPair() &#123;&#125; // // public IntPair(int left, int right) &#123; // set(left, right); // &#125; public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; public String toString()&#123; return \"(\" + first + \",\" + second + \")\"; &#125; @Override public void readFields(DataInput arg0) throws IOException &#123; // TODO Auto-generated method stub first = arg0.readInt(); second = arg0.readInt(); &#125; @Override public void write(DataOutput arg0) throws IOException &#123; // TODO Auto-generated method stub arg0.writeInt(first); arg0.writeInt(second); &#125; // å…³é”®ï¼šè‡ªå®šä¹‰ç±»å‹çš„æ¯”è¾ƒæ–¹æ³• @Override public int compareTo(IntPair arg0) &#123; // TODO Auto-generated method stub if (first != arg0.first) &#123; return first &lt; arg0.first ? -1 : 1; &#125; else if (second != arg0.second) &#123; return second &lt; arg0.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; public int hashCode() &#123; return first * 157 + second; &#125; public boolean equals(Object right) &#123; if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125;&#125;public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; &#123; // ç±»å‹è¦å’ŒMapperè¾“å‡ºçš„ä¸€æ · @Override public int getPartition(IntPair arg0, IntWritable arg1, int arg2) &#123; // TODO Auto-generated method stub return Math.abs((arg0.getFirst() * 127) % arg2); &#125;&#125;/* * ç¬¬ä¸€ç§æ–¹æ³•ï¼Œå®ç°æ¥å£RawComparator æ•°æ®ç±»å‹çš„æ¯”è¾ƒåœ¨MapReduceä¸­å¼åŠå…¶é‡è¦çš„, * Mapreduceä¸­æœ‰ä¸€ä¸ªæ’åºé˜¶æ®µï¼Œkeyå’Œå…¶ä»–çš„keyç›¸æ¯”è¾ƒã€‚ é’ˆå¯¹æ­¤ï¼ŒHadoop æä¾›çš„ä¸€ä¸ªä¼˜åŒ–æ˜¯ RawComparator * * public static class GroupingComparator implements RawComparator&lt;IntPair&gt;&#123; * * @Override public int compare(IntPair arg0, IntPair arg1) &#123; // TODO * Auto-generated method stub int l = arg0.getFirst(); int r = * arg0.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; * * @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int * s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, * Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; &#125; */// æ–¹æ³•äºŒpublic static class GroupingComparator extends WritableComparator &#123; protected GroupingComparator() &#123; super(IntPair.class, true);// è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•° &#125; public int compare(WritableComparable w1, WritableComparable w2) &#123; IntPair i1 = (IntPair) w1; IntPair i2 = (IntPair) w2; int l = i1.getFirst(); int r = i2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125;&#125; â€‹ TextPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package TextPair;// cc TextPair A Writable implementation that stores a pair of Text objects// cc TextPairComparator A RawComparator for comparing TextPair byte representations// cc TextPairFirstComparator A custom RawComparator for comparing the first field of TextPair byte representations// vv TextPairimport java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.io.WritableUtils;public class TextPair implements WritableComparable&lt;TextPair&gt; &#123; private Text first; private Text second; public TextPair() &#123; set(new Text(), new Text()); &#125; public TextPair(String first, String second) &#123; set(new Text(first), new Text(second)); &#125; public TextPair(Text first, Text second) &#123; set(first, second); &#125; public void set(Text first, Text second) &#123; this.first = first; this.second = second; &#125; public Text getFirst() &#123; return first; &#125; public Text getSecond() &#123; return second; &#125; @Override public void write(DataOutput out) throws IOException &#123; first.write(out); second.write(out); &#125; @Override public void readFields(DataInput in) throws IOException &#123; first.readFields(in); second.readFields(in); &#125; @Override public int hashCode() &#123; return first.hashCode() * 163 + second.hashCode(); &#125; @Override public boolean equals(Object o) &#123; if (o instanceof TextPair) &#123; TextPair tp = (TextPair) o; return first.equals(tp.first) &amp;&amp; second.equals(tp.second); &#125; return false; &#125; @Override public String toString() &#123; return first + \"\\t\" + second; &#125; @Override public int compareTo(TextPair tp) &#123; int cmp = first.compareTo(tp.first); if (cmp != 0) &#123; return cmp; &#125; return second.compareTo(tp.second); &#125; // ^^ TextPair // vv TextPairComparator public static class Comparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public Comparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); int cmp = TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); if (cmp != 0) &#123; return cmp; &#125; return TEXT_COMPARATOR.compare(b1, s1 + firstL1, l1 - firstL1, b2, s2 + firstL2, l2 - firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; &#125; static &#123; WritableComparator.define(TextPair.class, new Comparator()); &#125; // ^^ TextPairComparator // vv TextPairFirstComparator public static class FirstComparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public FirstComparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); return TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; if (a instanceof TextPair &amp;&amp; b instanceof TextPair) &#123; return ((TextPair) a).first.compareTo(((TextPair) b).first); &#125; return super.compare(a, b); &#125; &#125; // ^^ TextPairFirstComparator // vv TextPair&#125;// ^^ TextPair","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems(2)","date":"2017-05-15T08:43:49.000Z","path":"2017/05/15/Machine-Learning-Recommender-Systems-2/","text":"1. ååŒè¿‡æ»¤ï¼Œç»™ç”¨æˆ·æ¨èç‰©å“1.1 åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤ç®—æ³•UserCF ä¸»è¦é€šè¿‡åˆ†æç”¨æˆ·çš„è¡Œä¸ºè®°å½•ï¼Œè®¡ç®—ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦ è¯¥ç®—æ³•è®¤ä¸ºç‰©å“Aå’Œç‰©å“Bå…·æœ‰å¾ˆå¤§çš„ç›¸ä¼¼åº¦æ˜¯å› ä¸ºå–œæ¬¢ç‰©å“Açš„ç”¨æˆ·å¤§éƒ¨åˆ†ä¹Ÿå–œæ¬¢ç‰©å“B 1.1.1 æ­¥éª¤ æ‰¾åˆ°å’Œç›®æ ‡ç”¨æˆ·ç›¸ä¼¼çš„ç”¨æˆ·é›†åˆ æ‰¾åˆ°è¿™ä¸ªé›†åˆä¸­çš„ç”¨æˆ·å–œæ¬¢ï¼Œä¸”å’Œç›®æ ‡ç”¨æˆ·æ²¡æœ‰å¬è¯´è¿‡çš„çš„ç‰©å“æ¨èç»™ç›®æ ‡ç”¨æˆ· 1.1.2 è®¡ç®—ç›¸ä¼¼åº¦ å¾—åˆ°ç”¨æˆ·ä¹‹é—´çš„å…´è¶£ç›¸ä¼¼åº¦ä¹‹åï¼Œå¯»æ‰¾æœ€ç›¸è¿‘çš„Kä¸ªç”¨æˆ· 1.2åŸºäºç‰©å“çš„ååŒè¿‡æ»¤ç®—æ³•1.2.1 è®¡ç®—æ­¥éª¤ è®¡ç®—ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦ æ ¹æ®ç‰©å“çš„ç›¸ä¼¼åº¦å’Œç”¨æˆ·çš„å†å²è¡Œä¸ºç»™ç”¨æˆ·ç”Ÿæˆæ¨èåˆ—è¡¨ 1.2.2 è®¡ç®—ç›¸ä¼¼åº¦ 1.3 éšè¯­ä¹‰æ¨¡å‹ UserCF æ‰¾åˆ°å’Œä»–ä»¬çœ‹äº†åŒæ ·ä¹¦çš„å…¶ä»–ç”¨æˆ·ï¼Œå³æ˜¯å…´è¶£ç›¸ä¼¼ç”¨æˆ·ã€‚ç„¶åç»™ç”¨æˆ·æ¨èè¿™äº›ç”¨æˆ·ç›¸ä¼¼ç”¨æˆ·æ‰€å–œæ¬¢çš„å…¶ä»–ä¹¦ç± ItemCF åœ¨å·²ç»çœ‹è¿‡çš„ä¹¦ä¸­ï¼Œå¯»æ‰¾ç›¸ä¼¼çš„ä¹¦ï¼Œå³å’Œè¿™äº›çœ‹è¿‡çš„ä¹¦ï¼ŒåŒæ—¶å‡ºç°åœ¨å…¶ä»–ç”¨æˆ·çš„çœ‹è¿‡çš„ä¹¦ä¸­ï¼Œç„¶åæ¨èè¿™äº›å…¶ä»–çš„ä¹¦ å…¶ä»–æ–¹æ³• å¯ä»¥å¯¹ç‰©å“çš„å…´è¶£è¿›è¡Œåˆ†ç±»ï¼Œå¯¹äºæŸä¸ªç”¨æˆ·ï¼Œé¦–å…ˆå¾—åˆ°ä»–çš„å…´è¶£åˆ†ç±»ï¼Œç„¶åä»è¿™äº›åˆ†ç±»ä¸­æŒ‘å‡ºä»–å¯èƒ½å–œæ¬¢çš„ç‰©å“ å¦‚ä½•ç»™ç‰©å“è¿›è¡Œåˆ†ç±» éšå«è¯­ä¹‰åˆ†ææŠ€æœ¯(latent variable analysis) å¦‚ä½•ç¡®å®šç”¨æˆ·çš„å…´è¶£ï¼Œå³å¯¹å“ªäº›ç±»çš„ç‰©å“æ„Ÿå…´è¶£ï¼Œä»¥åŠæ„Ÿå…´è¶£çš„ç¨‹åº¦ å¯¹äºä¸€ä¸ªç»™å®šçš„ç±»ï¼Œé€‰æ‹©å“ªäº›å±äºè¿™ä¸ªç±»çš„ç‰©å“æ¨èç»™ç”¨æˆ·ï¼Ÿä»¥åŠå¦‚ä½•ç¡®å®šè¿™äº›ç‰©å“åœ¨è¿™ä¸ªä¸­çš„æƒé‡ã€‚å³å¦‚ä½•åœ¨è¿™ä¸ªç±»ä¸­ï¼ŒæŒ‘é€‰å‡ºåˆé€‚çš„ç‰©å“æ¨èç»™ç”¨æˆ· 1.3.1 LFM (latent factor model)1.4 ç”¨æˆ·æ ‡ç­¾æ•°æ® User Generated Content2. éœ€è¦è§£å†³çš„é—®é¢˜ - è¯„åˆ†é¢„æµ‹ï¼ˆç”¨æˆ·Aå¯¹ç”µå½±xçš„è¯„åˆ†é¢„æµ‹ï¼‰ 2.1 å®éªŒæ–¹æ³•2.1.1 åˆ’åˆ†è®­ç»ƒé›† ä¸æ—¶é—´æ— å…³ï¼Œå¯ä»¥å‡åŒ€åˆ†å¸ƒéšæœºæ¢åˆ†æ•°æ®é›†ã€‚å³å¯¹æ¯ä¸€ä¸ªç”¨æˆ·ï¼Œéšæœºé€‰å–ä¸€äº›è¯„åˆ†è®°å½•ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä¸‹çš„ä½œä¸ºæµ‹è¯•é›† ä¸æ—¶é—´ç›¸å…³ï¼Œé‚£ä¹ˆéœ€è¦å°†ç”¨æˆ·çš„æ—§è¡Œä¸ºä½œä¸ºè®­ç»ƒé›†ï¼Œè®²ç”¨æˆ·çš„æ–°è¡Œä¸ºä½œä¸ºæµ‹è¯•é›† ä¾‹å­ Netflixçš„è¯„åˆ†é¢„æµ‹ç³»ç»Ÿä¸­ï¼Œå°†æ¯ä¸ªç”¨æˆ·çš„è¯„åˆ†è®°å½•æŒ‰ç…§ä»æ—©åˆ°æ™šè¿›è¡Œæ’åºï¼Œç„¶åå°†ç”¨æˆ·æœ€åçš„10%çš„è¯„åˆ†è®°å½•ä½œä¸ºæµ‹è¯•é›†ï¼Œ90%çš„è¯„åˆ†è®°å½•ä½œä¸ºè®­ç»ƒé›†ã€‚ 2.1.2 è¯„åˆ†æ ‡å‡†$$RMSE = \\frac {\\sqrt {\\sum_{(u, i) \\in T} (r_{ui} - \\hat r_{ui})^2 }} {|Test|}$$ 2.1.3 è¯„åˆ†é¢„æµ‹ç®—æ³• å¹³å‡å€¼ æœ€ç®€å•çš„æ–¹æ³•ï¼šåˆ©ç”¨å¹³å‡å€¼é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ† å…¨å±€å¹³å‡å€¼ è®¡ç®—åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸­ï¼Œæ‰€æœ‰è¯„åˆ†è®°å½•çš„è¯„åˆ†å¹³å‡å€¼ ç”¨æˆ·è¯„åˆ†å¹³å‡å€¼ è®¡ç®—ç”¨æˆ·uåœ¨è®­ç»ƒé›†ä¸­æ‰€ç»™å‡ºçš„è¯„åˆ†çš„å¹³å‡å€¼ ç‰©å“è¯„åˆ†å¹³å‡å€¼è®¡ç®—è¯¥ç‰©å“wåœ¨è®­ç»ƒé›†ä¸­è¢«è¯„ä»·äº†çš„è¯„åˆ†çš„å¹³å‡å€¼ ç”¨æˆ·å¯¹ç‰©å“åˆ†ç±»çš„å¹³å‡å€¼ å‡è®¾è¿™é‡Œæœ‰ä¸¤ä¸ªåˆ†ç±»ï¼Œä¸€ä¸ªæ˜¯ç”¨æˆ·åˆ†ç±»å‡½æ•°Uï¼Œä¸€ä¸ªæ˜¯ç‰©å“åˆ†ç±»Wï¼ŒU(u)å®šä¹‰äº†ç”¨æˆ·uæ‰€å±çš„åˆ†ç±»ï¼ŒW(w)å®šä¹‰äº†ç‰©å“wæ‰€å±çš„åˆ†ç±»ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è®­ç»ƒé›†ä¸­åŒç±»ç”¨æˆ·å¯¹åŒç±»ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ† ä¹‹å‰æ˜¯ä¸‰ç§å¹³å‡å€¼å…¶å®æ˜¯ç”¨æˆ·å¯¹ç‰©å“åˆ†ç±»çš„å¹³å‡å€¼çš„ä¸€ç§ç‰¹ä¾‹ U(u) = 0ï¼ŒW(w) = 0ï¼Œé‚£ä¹ˆå°±æ˜¯å…¨å±€å¹³å‡å€¼ U(u) = uï¼ŒW(w) = 0ï¼Œé‚£ä¹ˆå°±æ˜¯ç”¨æˆ·è¯„åˆ†å¹³å‡å€¼ U(u) = 0ï¼ŒW(w) = wï¼Œé‚£ä¹ˆå°±æ˜¯ç‰©å“è¯„åˆ†å¹³å‡å€¼ åœ¨ä»¥ä¸Šçš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è€ƒè™‘åˆ°ç”¨æˆ·çš„æ´»è·ƒåº¦å’Œç‰©å“çš„æµè¡Œç¨‹åº¦ã€‚å®é™…ä¸Šå¯ä»¥å°†è¿™ä¸¤ç‚¹è€ƒè™‘è¿›å»ï¼Œå¯¹æ´»è·ƒç”¨æˆ·å’Œæµè¡Œçš„ç‰©å“ç»™å®šä¸€ç‚¹penalty 2.1.4 åŸºäºé¢†åŸŸçš„æ–¹æ³•ï¼ˆåŸºäºç”¨æˆ·çš„é¢†åŸŸå’ŒåŸºäºç‰©å“çš„é¢†åŸŸç®—æ³•ï¼‰ åŸºäºç”¨æˆ·çš„é¢†åŸŸç®—æ³•$$\\hat r_{ui} = \\overline r_u +\\frac {\\sum_{v \\in S(u, K) \\bigcap N(i)} w_{uv}(r_{vi} - \\overline r_v)}{\\sum_{v \\in S(u, K) \\bigcap N(i)} |w_{uv}|}$$*E(r_u) æ˜¯ç”¨æˆ·uå¯¹ä»–ç‚¹è¯„è¿‡çš„æ‰€æœ‰ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼ S(u, K) æ˜¯å’Œç”¨æˆ·uå…´è¶£æœ€ç›¸ä¼¼çš„Kä¸ªç”¨æˆ·çš„é›†åˆ N(i) æ˜¯å¯¹ç‰©å“iç‚¹è¯„è¿‡åˆ†æ•°çš„ç”¨æˆ·é›†åˆ r_vi æ˜¯ç”¨æˆ·vå¯¹ç‰©å“içš„è¯„åˆ† E(r_v) æ˜¯ç”¨æˆ·vå¯¹ä»–è¯„åˆ†è¿‡çš„æ‰€æœ‰ç‰©å“è¯„åˆ†çš„å¹³å‡å€¼ w_uv æ˜¯ç”¨æˆ·uå’Œvä¹‹é—´çš„ç›¸ä¼¼åº¦ â€‹ åŸºäºç‰©å“çš„é¢†åŸŸç®—æ³•$$\\hat r_{ui} = \\overline r_i +\\frac {\\sum_{j \\in S(i, K) \\bigcap N(u)} w_{ij}(r_{uj} - \\overline r_i)}{\\sum_{j \\in S(i, K) \\bigcap N(u)} |w_{ij}|}$$*E(r_i) æ˜¯ç‰©å“içš„å¹³å‡åˆ†ï¼Œæ˜¯æ‰€æœ‰ç”¨æˆ·å¯¹ç‰©å“iç‚¹è¯„è¿‡çš„åˆ†æ•°çš„å¹³å‡å€¼ S(i, K) æ˜¯å’Œç‰©å“iæœ€ç›¸ä¼¼çš„Kä¸ªç‰©å“çš„é›†åˆ N(u) æ˜¯ç”¨æˆ·uç‚¹è¯„å¤šåˆ†æ•°çš„ç‰©å“é›†åˆ r_uj æ˜¯ç”¨æˆ·uå¯¹ç‰©å“jçš„è¯„åˆ† w_ij æ˜¯ç‰©å“iå’Œjä¹‹é—´çš„ç›¸ä¼¼åº¦ 2.1.5 è®¡ç®—ç›¸ä¼¼åº¦ ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆcosine similarity)$$w_{ij} = \\frac {\\sum_{u \\in U} r_{ui} r_{uj}} {\\sqrt{\\sum_{u \\in U}r_{ui}^2 \\sum_{u \\in U}r_{uj}^2}}$$ çš®å°”é€Šç³»æ•°ï¼ˆpearson correlationï¼‰$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_i) (r_{uj} - \\overline r_j)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_i)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_j)^2}}$$ ä¿®æ­£ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆadjust cosine similarityï¼‰ï¼ˆåœ¨MovieLensæ•°æ®é›†ä¸Šæ•ˆæœæœ€å¥½ï¼‰$$w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_u) (r_{uj} - \\overline r_u)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_u)^2 \\sum_{u \\in U}(r_{uj} - \\overline r_u)^2}}$$ 2.1.6 éšè¯­ä¹‰æ¨¡å‹çš„çŸ©é˜µåˆ†è§£æ¨¡å‹ ï¼ˆLatent Factor Model)éšå«ç±»åˆ«æ¨¡å‹ã€éšè¯­ä¹‰æ¨¡å‹ç­‰ï¼Œåœ¨æœ¬è´¨ä¸Šéƒ½æ˜¯ä¸ºäº†æ‰¾å‡ºæŸä¸€ä¸œè¥¿çš„æ½œåœ¨çš„ä¸»é¢˜æˆ–è€…åˆ†ç±»ã€‚åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå¯ä»¥åŸºäºç”¨æˆ·çš„è¡Œä¸ºåˆ©ç”¨éšè¯­ä¹‰æ¨¡å‹ï¼Œå¯¹itemè¿›è¡Œè‡ªåŠ¨èšç±»ï¼Œè¿™æ ·å¯ä»¥é¿å…äº†äººä¸ºåˆ†ç±»çš„åå·®ã€‚ ä¸¾ä¾‹è¯´æ˜ï¼š â€‹ ç”¨æˆ·Aå–œæ¬¢çœ‹æ•°å­¦ï¼Œå†å²å’Œè®¡ç®—æœºçš„ä¹¦ç± â€‹ ç”¨æˆ·Bå–œæ¬¢çœ‹æœºå™¨å­¦ä¹ ï¼Œç¼–ç¨‹è¯­è¨€å’Œç¦»æ•£æ•°å­¦æ–¹é¢çš„ä¹¦ç± â€‹ ç”¨æˆ·Cå–œæ¬¢çœ‹å¤§å¸ˆçš„ä½œå“ï¼Œæ¯”å¦‚ä¸“é—¨çœ‹Knuthæˆ–è€…Jiawei Hançš„ä¹¦ç± é‚£ä¹ˆç³»ç»Ÿåœ¨å¯¹ç”¨æˆ·çš„å–œå¥½è¿›è¡Œæ¨èçš„æ—¶å€™ï¼Œéœ€è¦æ‰¾å‡ºåŒå±äºç”¨æˆ·å…´è¶£åœˆå­çš„ä¹¦ç±ã€‚å¯¹äºä¹‹å‰æåˆ°è¿‡çš„ä¸‰ä¸ªç”¨æˆ·æ¥è¯´ â€‹ ç”¨æˆ·Açš„åœˆå­ï¼šæ•°å­¦ã€è®¡ç®—æœºã€å†å² â€‹ ç”¨æˆ·Bçš„åœˆå­ï¼šè¿™ä¸‰æœ¬ä¹¦å¯ä»¥åŒæ—¶åˆ†åˆ°è®¡ç®—æœºçš„åœˆå­ï¼Œä½†æ˜¯ç¦»æ•£æ•°å­¦å´åˆå¯ä»¥åˆ†åˆ°æ•°å­¦åœˆå­å» â€‹ ç”¨æˆ·Cçš„åœˆå­ï¼šæ ¹æ®ä½œè€…ä¸åŒæ¥åˆ’åˆ†åœˆå­ï¼Œé‚£ä¹ˆè¿™ä¸ªåœˆå­å°±å’Œä¹‹å‰ç”¨æˆ·Aã€Bçš„è§’åº¦æ˜¯ä¸åŒçš„ã€‚ å‡è®¾è®©äººå·¥æ¥å®Œæˆä¹‹å‰ä¹¦ç±çš„åˆ†ç±»ï¼Œé‚£ä¹ˆç»å¸¸ä¼šç¢°åˆ°åˆ’åˆ†çš„ç²’åº¦ä¸åŒï¼Œè§’åº¦ä¸åŒç­‰æƒ…å†µã€‚ åŒæ—¶ï¼Œéœ€è¦æ³¨æ„ä¸€ä¸‹ä¸¤ç‚¹ï¼š ç”¨æˆ·Aå¯¹è¿™ä¸‰ä¸ªç±»åˆ«ä¹¦ç±æ„Ÿå…´è¶£ï¼Œä¸ä»£è¡¨ä¸å¯¹å…¶ä»–ç±»åˆ«çš„æ•°æ®æ„Ÿå…´è¶£ åŒä¸€æœ¬ä¹¦å¯ä»¥å±äºå¤šä¸ªç±»åˆ«ï¼Œæ‰€ä»¥æ¯æœ¬ä¹¦åœ¨æ¯ä¸ªç±»åˆ«é‡Œé¢éƒ½æœ‰ä¸€ä¸ªæƒé‡ï¼Œæƒé‡å€¼è¶Šå¤§ï¼Œè¯´æ˜å±äºè¿™ä¸ªç±»åˆ«çš„å¯èƒ½æ€§è¶Šé«˜ é‚£ä¹ˆï¼ŒLFMæ˜¯å¦‚ä½•è§£å†³ä¸Šé¢çš„å‡ ä¸ªé—®é¢˜çš„å‘¢ï¼Ÿ å›ç­”ä¸Šé¢çš„é—®é¢˜å‰ï¼Œæˆ‘ä»¬å¯ä»¥æ€è€ƒä¸€ä¸‹æˆ‘ä»¬éœ€è¦åšçš„å“ªäº›å·¥ä½œã€‚ å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰çš„Userçœ‹åšåˆ—ï¼ŒæŠŠæ‰€æœ‰çš„Itemçœ‹åšè¡Œï¼Œæ„å»ºä¸€ä¸ªmxnçš„äºŒç»´çŸ©é˜µï¼Œå¦‚ä¸‹å›¾ æœ€å·¦è¾¹çš„RçŸ©é˜µæ˜¯ä¸€ä¸ªuser-itemçŸ©é˜µï¼ŒçŸ©é˜µå€¼Rijè¡¨ç¤ºçš„æ˜¯ç”¨æˆ·iå¯¹item jçš„å…´è¶£åº¦ï¼Œæˆ–è€…æ˜¯è¯„åˆ†ã€‚é‚£ä¹ˆæˆ‘ä»¬çš„è¯„åˆ†é¢„æµ‹ï¼Œå°±å¯ä»¥è½¬æ¢æˆå¯¹è¿™ä¸ªçŸ©é˜µä¸­çš„æŸäº›å€¼ï¼ˆç¼ºå¤±å€¼ï¼‰çš„é¢„æµ‹ï¼ŒåŒæ—¶éœ€è¦ä¿è¯æˆ‘ä»¬çš„é¢„æµ‹å€¼å¯¹äºè¿™ä¸ªçŸ©é˜µçš„æ‰°åŠ¨çš„æœ€å°çš„ã€‚ï¼ˆå³è¡¥å…¨ä¹‹åçŸ©é˜µçš„ç‰¹å¾å€¼å’Œè¡¥å…¨ä¹‹å‰çš„ç‰¹å¾å€¼ç›¸å·®ä¸å¤§ï¼Œå…·ä½“è§SVDåˆ†è§£ï¼‰ è€Œå³è¾¹çš„ä¸¤ä¸ªPå’ŒQçŸ©é˜µå°±æ˜¯LFMæ‰€åšçš„ï¼ŒLFMç®—æ³•ä»æ•°æ®é›†æ±‡æ€»æŠ½å‡ºè‹¥å¹²ä¸ªclassï¼Œè®¡ç®—å‡ºæ‰€æœ‰userå¯¹è¿™äº›classçš„æ„Ÿå…´è¶£é•¿åº¦ï¼Œå³PçŸ©é˜µã€‚åŒæ—¶è®¡ç®—å‡ºæ‰€æœ‰itemåœ¨è¿™äº›classä¸­çš„æƒé‡å€¼ï¼Œå³QçŸ©é˜µã€‚PçŸ©é˜µä½œä¸ºuserå’Œitemä¹‹é—´è¿æ¥çš„æ¡¥æ¢ï¼Œæ‰€ä»¥Rå¯ä»¥è¡¨ç¤ºä¸ºPçŸ©é˜µå’ŒQçŸ©é˜µç›¸ä¹˜ã€‚$$R_{UI} = P_U Q_I = \\sum_{k=1}^K = P_{U,k}Q_{k,I}$$ä»¥ä¸‹æ˜¯LFMçš„ä¼˜ç‚¹ï¼š ä¸éœ€è¦å…³å¿ƒçŸ©é˜µPæ˜¯æ€ä¹ˆæ„å»ºçš„ï¼Œå³ä¸éœ€è¦å¦‚ä½•ç»™ç‰©å“è¿›è¡Œèšç±»ã€åˆ’åˆ†ç­‰ï¼ˆè§’åº¦ï¼Œç²’åº¦ç­‰ï¼‰ QçŸ©é˜µä¸­ï¼Œå¯¹äºä¸€ä¸ªitemå¹¶ä¸æ˜¯æ˜ç¡®ç»™åˆ’åˆ†åˆ°æŸä¸€ä¸ªåˆ†ç±»ï¼Œè€Œæ˜¯è®¡ç®—è¿™ä¸ªitemå±äºè¿™äº›æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡ï¼Œå€¼è¶Šå¤§å¯èƒ½æ€§è¶Šé«˜ åŒç†ï¼ŒPçŸ©é˜µä¸­ï¼Œå¯¹äºä¸€ä¸ªuserå¹¶æ²¡æœ‰é™å®šåœ¨æŸäº›classä¸­ï¼Œè€Œæ˜¯è®¡ç®—è¿™ä¸ªuserå¯¹äºè¿™äº›classesçš„æ„Ÿå…´è¶£ç¨‹åº¦ è™½ç„¶æˆ‘ä»¬çŸ¥é“äº†LFMä¸ºæˆ‘ä»¬åšäº†å“ªäº›å·¥ä½œï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯ä¸çŸ¥é“è¯¥å¦‚ä½•æ±‚è§£å‡ºçŸ©é˜µPå’ŒçŸ©é˜µQä¸­çš„å‚æ•°å€¼ï¼Œä¸€èˆ¬çš„åšæ³•æ˜¯æœ€ä¼˜æŸå¤±å‡½æ•°æ¥æ±‚å‚æ•°ã€‚ ä¼ ç»Ÿçš„SVDåˆ†è§£ ä¼ ç»Ÿæ–¹æ³•ä¸­ï¼Œç»™å®šä¸€ä¸ªuser-itemçš„çŸ©é˜µRã€‚ é¦–å…ˆéœ€è¦å¯¹è¯„åˆ†çŸ©é˜µRä¸­çš„ç¼ºå¤±å€¼è¿›è¡Œç®€å•çš„è¡¥å…¨ï¼Œæ¯”å¦‚ç”¨å…¨å±€å¹³å‡å€¼ï¼Œæˆ–è€…ç”¨æˆ·/ç‰©å“çš„å¹³å‡å€¼è¡¥å…¨ï¼Œå¾—åˆ°è¡¥å…¨åçš„çŸ©é˜µRâ€™ å¾—åˆ°è¡¥å…¨åçš„çŸ©é˜µRâ€™ï¼Œæ¥ç€å¯ä»¥åˆ©ç”¨SVDåˆ†è§£ï¼Œå°†Râ€™åˆ†è§£æˆå¦‚ä¸‹å½¢å¼$$Râ€™ = U^TSV \\\\R \\in R^{m n} \\\\U \\in R^{k m} \\\\V \\in R^{k n} \\\\S \\in R^{k k}$$Uå’ŒVæ˜¯ä¸¤ä¸ªæ­£äº¤çŸ©é˜µï¼ŒSæ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šçš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯çŸ©é˜µçš„å¥‡å¼‚å€¼ã€‚ ä¸ºäº†å¯¹Râ€™è¿›è¡Œé™ç»´ï¼Œå¯ä»¥å–æœ€å¤§çš„fä¸ªå¥‡å¼‚å€¼ç»„æˆå¯¹ç„¦çŸ©é˜µSfï¼Œå¹¶ä¸”æ‰¾åˆ°è¿™ä¸ªfä¸ªå¥‡å¼‚å€¼æ±‡ä¸­æ¯ä¸ªå€¼åœ¨Uã€VçŸ©é˜µä¸­å¯¹åº”çš„è¡Œå’Œåˆ—ï¼Œå¾—åˆ°Ufã€Vfï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªé™ç»´åçš„è¯„åˆ†çŸ©é˜µï¼š$$R_fâ€™=U_f^TS_fV_f$$è¯¥æ–¹æ³•çš„ä¸€äº›ç¼ºç‚¹ï¼š åœ¨ç°å®ä¸­ï¼ŒRçŸ©é˜µåŸºæœ¬ä¸Šä¼šæ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œå³95%çš„æ•°æ®æ˜¯ç¼ºå¤±çš„ï¼ŒåŒæ—¶è¯¥çŸ©é˜µéå¸¸çš„å¤§ã€‚ä¸€ç»è¡¥å…¨ï¼Œè¯¥çŸ©é˜µå°±æ˜¯ä¸€ä¸ªç¨ å¯†çŸ©é˜µï¼Œå‚¨å­˜å¼€é”€éå¸¸çš„å¤§ è®¡ç®—å¤æ‚åº¦éå¸¸çš„é«˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¡¥å…¨ä¹‹åçš„ç¨ å¯†çŸ©é˜µ â€‹ Funk-SVDåˆ†è§£ï¼Œå³ Latent Factor Modelï¼ˆLFMï¼‰ http://sifter.org/~simon/journal/20061211.html ä»çŸ©é˜µçš„è§’åº¦ï¼Œå°†è¯„åˆ†çŸ©é˜µRåˆ†è§£æˆä¸¤ä¸ªä½çº¬åº¦ç›¸ä¹˜ï¼š$$\\hat R = P^TQ \\\\R \\in R^{mn} \\\\P \\in R^{fm} \\\\Q \\in R^{fn}$$Pã€Qæ˜¯ä¸¤ä¸ªé™ç»´åçš„çŸ©é˜µï¼Œé‚£ä¹ˆå¯¹äºç”¨æˆ·uå¯¹äºç‰©å“içš„è¯„åˆ†çš„é¢„æµ‹å€¼å¯ä»¥^R(u, i) = ^r_uiï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—ï¼š$$\\hat r_{ui} =b_{ui} + \\sum_fp_{uf}q_{if} \\\\p_{uf} = P(u, f) \\\\p_{if} = Q(i, f)$$Simon Funk-SVDçš„æ€æƒ³æ˜¯ç›´æ¥é€šè¿‡è®­ç»ƒé›†ä¸­çš„è§‚å¯Ÿå€¼ï¼Œåˆ©ç”¨*æœ€å°åŒ–RMSEå­¦ä¹ Pã€QçŸ©é˜µã€‚ æŸå¤±å‡½æ•°çš„è®¡ç®—ï¼š$$C(p, q) = \\sum_{(u, i) \\in Train} (r_{ui} - \\hat r_{ui})^2= \\sum_{(u, i) \\in Train}(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})^2 + \\lambda(||p_u||^2 + ||q_i||^2) \\\\\\hat r_{ui} = \\mu + b_u + b_i + p_u^Tq_i$$è¦æœ€å°åŒ–ä¸Šé¢çš„æŸå¤±å‡½æ•°ï¼Œå¯ä»¥åˆ©ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ã€‚ä»¥ä¸‹æ˜¯ç®€å•çš„æ¨å¯¼è¯¥å…¬å¼ ä¸Šé¢çš„cost functionä¸­æ¬§ä¸¤ä¸ªå‚æ•°på’Œqï¼Œé¦–å…ˆ å¯¹ä»–ä»¬åˆ†åˆ«æ±‚åå¯¼ï¼Œæ±‚å‡ºæœ€å¿«ä¸‹é™çš„æ–¹å‘$$\\frac {âˆ‚C} {âˆ‚p_{uf}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})q_{ik} + 2\\lambda p_{uk} \\\\\\frac {âˆ‚C} {âˆ‚q_{if}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})p_{uk} + 2\\lambda q_{ik}$$ ç„¶åæ ¹æ®éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œéœ€è¦å°†å‚æ•°æ²¿æœ€å¿«çš„ä¸‹é™æ–¹å‘å‰è¿›ï¼Œå³å¯å¾—åˆ°å¦‚ä¸‹çš„é€’æ¨å…¬å¼ï¼š$$p_{uf} = p_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) q_{ik} - \\lambda p_{uk}) \\\\q_{if} = q_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) p_{uk} - \\lambda q_{ik})$$ æ‰€ä»¥ï¼Œæ‰§è¡ŒLFMéœ€è¦ï¼š æ ¹æ®æ•°æ®é›†åˆå§‹åŒ–På’ŒQçŸ©é˜µï¼ˆå¦‚ä½•åˆå§‹åŒ–ï¼‰ ç¡®å®šå››ä¸ªå‚æ•°ï¼šåˆ†ç±»ä¹¦Fï¼Œè¿­ä»£æ¬¡æ•°Nï¼Œå­¦ä¹ é€Ÿç‡Î±ï¼ˆÎ± = 0.9ï¼‰ å’Œæ­£åˆ™åŒ–å‚æ•°*Î» â€‹ ä¼ªä»£ç  1234567891011121314151617181920def LFM(user_items, F, N, alpha, lambda): #åˆå§‹åŒ–P,QçŸ©é˜µ [P, Q] = InitModel(user_items, F) #å¼€å§‹è¿­ä»£ For step in range(0, N): #ä»æ•°æ®é›†ä¸­ä¾æ¬¡å–å‡ºuserä»¥åŠè¯¥userå–œæ¬¢çš„itermsé›† for user, items in user_item.iterms(): #éšæœºæŠ½æ ·ï¼Œä¸ºuseræŠ½å–ä¸itemsæ•°é‡ç›¸å½“çš„è´Ÿæ ·æœ¬ï¼Œå¹¶å°†æ­£è´Ÿæ ·æœ¬åˆå¹¶ï¼Œç”¨äºä¼˜åŒ–è®¡ç®— samples = RandSelectNegativeSamples(items) #ä¾æ¬¡è·å–itemå’Œuserå¯¹è¯¥itemçš„å…´è¶£åº¦ for item, rui in samples.items(): #æ ¹æ®å½“å‰å‚æ•°è®¡ç®—è¯¯å·® eui = eui - Predict(user, item) #ä¼˜åŒ–å‚æ•° for f in range(0, F): P[user][f] += alpha * (eui * Q[f][item] - lambda * P[user][f]) Q[f][item] += alpha * (eui * P[user][f] - lambda * Q[f][item]) #æ¯æ¬¡è¿­ä»£å®Œåï¼Œéƒ½è¦é™ä½å­¦ä¹ é€Ÿç‡ã€‚ä¸€å¼€å§‹çš„æ—¶å€™ç”±äºç¦»æœ€ä¼˜å€¼ç›¸å·®ç”šè¿œï¼Œå› æ­¤å¿«é€Ÿä¸‹é™ï¼› #å½“ä¼˜åŒ–åˆ°ä¸€å®šç¨‹åº¦åï¼Œå°±éœ€è¦æ”¾æ…¢å­¦ä¹ é€Ÿç‡ï¼Œæ…¢æ…¢çš„æ¥è¿‘æœ€ä¼˜å€¼ã€‚ alpha *= 0.9 Baseline Estimats å¯¹æ¯”åŸºçº¿è€ƒè™‘åˆ°é‡å£éš¾è°ƒï¼Œæœ‰äº›userä¼šç»™å‡ºæ¯”è¾ƒé«˜çš„åˆ†æ•°ï¼Œæœ‰äº›è¦å»ä¸¥æ ¼çš„usersä¼šç»™å‡ºæ¯”è¾ƒä½çš„åˆ†æ•°ï¼Œè€Œæœ‰äº›è´¨é‡å¥½çš„å•†å“ä¼šå¾—åˆ°æ¯”è¾ƒé«˜çš„åˆ†æ•°ï¼Œè´¨é‡å·®çš„åˆ†æ•°è¾ƒä½ã€‚æ‰€ä»¥ä¸ºäº†è°ƒæ•´è¿™äº›ï¼Œå¼•å…¥äº†baseline estimateã€‚æ¯”å¦‚ä¸ºäº†ä¼°è®¡æŸä¸ªç”¨æˆ·uä¼šç»™ç”µå½±iæ‰“çš„è¯„åˆ†ï¼š$$b_{ui} = u + b_u + b_i$$ $\\mu$ æ˜¯è¯¥ç‰©å“çš„æ•´ä½“å¹³å‡å€¼ $b_u$ æ˜¯ç”¨æˆ·æ‰“åˆ†ç›¸å¯¹æ•´ä½“ç”¨æˆ·æ‰“åˆ†å¹³å‡å€¼çš„åå·® $b_i$ æ˜¯è¯¥ç‰©å“ç›¸å¯¹æ•´ä½“å¹³å‡å€¼çš„åå·® ä¸¾ä¸ªä¾‹å­ï¼šé¢„æµ‹è±†ç“£ç”¨æˆ·å°æ˜ç»™ç”µå½±æ³°å¦å°¼å…‹å·çš„è¯„åˆ† æ³°å¦å°¼å…‹å·åœ¨è±†ç“£ä¸Šçš„å¹³å‡åˆ†æ•°æ˜¯3.7åˆ†(u) æ³°å¦å°¼å…‹å·çš„å¹³å‡åˆ†æ•°åˆæ¯”æ‰€æœ‰ç”µå½±åœ¨è±†ç“£ä¸Šçš„å¹³å‡åˆ†æ•°é«˜0.5åˆ†($b_i$) ä½†æ˜¯å°æ˜æ˜¯ä¸ªç”µå½±çˆ±å¥½è€…ï¼Œæ¯”å¹³å‡ç”¨æˆ·æ‰“åˆ†åä½0.3åˆ†($b_u$) æ‰€ä»¥ä¸è€ƒè™‘regularizedçš„è¯ï¼Œé¢„æµ‹å°æ˜ç»™æ³°å¦å°¼å…‹å·çš„è¯„åˆ†åº”è¯¥æ˜¯ 3.7 - 0.3 + 0.5 = 3.9 æ‰€ä»¥è¿™é‡Œï¼Œå¾—åˆ° $b_u$ å’Œ $b_i$ çš„å€¼å¾ˆé‡è¦ è¿™é‡Œå¯¹$b_u$å’Œ$b_i$åŠ å…¥äº†penalityï¼Œä¸ºäº†é˜²æ­¢overfittingã€‚ è¿™é‡Œ$r_ui$æ˜¯æˆ‘ä»¬è®­ç»ƒæ•°æ®ä¸­çš„å·²çŸ¥ratingï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ $\\mu$æ˜¯æ•´ä½“å¹³å‡å€¼ï¼Œä¹Ÿå¯ä»¥æ ¹æ®è®­ç»ƒæ•°æ®è®¡ç®—å‡ºæ¥ $\\lambda_2$å’Œ$\\lambda_3$æ˜¯æˆ‘ä»¬æ‰‹åŠ¨è®¾ç½®çš„å‚æ•°ï¼ŒMovieLensæ•°æ®ä¸Šï¼Œ20æ¯”è¾ƒåˆé€‚ R(u)å’ŒR(i)ä¸ºç”¨æˆ·uratingè¿‡çš„ç‰©å“çš„é›†åˆï¼Œå’Œç‰©å“iè¢«ratingè¿‡ç”¨æˆ·çš„é›†åˆ æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™äº›æ•°æ®ï¼Œè®¡ç®—å‡º$b_u$å’Œ$b_i$b_i$ï¼ˆæ³¨æ„ç»´åº¦ï¼‰$$b_i = \\frac {\\sum_{u \\in R(u)}(r_{ui} - \\mu)} {\\lambda_2 + |R(i)|} \\\\b_u = \\frac {\\sum_{i \\in R(u)}(r_{ui} - \\mu - b_i)} {\\lambda_3 + |R(u)|}$$é™¤äº†ä¸Šé¢çš„æ–¹æ³•ï¼Œè¿˜æœ‰ä¸€ç§æ›´ä¸ºç®€ä¾¿çš„æ–¹æ³•è®¡ç®—$b_u$å’Œ$b_i$ï¼Œå°±æ˜¯ç›´æ¥ä½¿ç”¨userï¼Œitemçš„ratingçš„å¹³å‡å€¼ä¼°è®¡$$b_u = \\frac {\\sum R(u)} {len(R(u))} \\\\b_i = \\frac {\\sum R(i)} {len(R(i))}$$ Neighborhood Models item-oriented algorithm: a rating is estimated using known rating made by the same user on similarity items. user-oriented algorithm: estimate unknown ratings based on recorded ratings of like minded users. Similarity measure between items Pearson correlation è®¡ç®—ç‰©å“iå’Œjçš„ç›¸ä¼¼åº¦$$s_{ij} = \\frac {n_{ij}} {n_{ij} + \\lambda_2}Ï_{ij}$$ n_ij è¡¨ç¤ºéƒ½å¯¹ç‰©å“iå’Œjè¯„åˆ†è¿‡çš„ç”¨æˆ·çš„æ•°é‡ Ïijæ˜¯çš®å°”é€Šç³»æ•°ï¼Œé€šå¸¸å–ï¼Ÿï¼Ÿï¼Ÿ Î»2 é€šå¸¸å–100 12345678910111213é¢„æµ‹è¯„åˆ†$$\\hat r_&#123;ui&#125; = b_&#123;ui&#125; +\\frac&#123;\\sum_&#123;j \\in S^k(i; u)&#125; s_&#123;ij&#125;(r_&#123;ui&#125; - b_&#123;ui&#125;)&#125;&#123;\\sum_&#123;j \\in S^k(i; u)&#125;s_&#123;ij&#125;&#125;$$åœ¨ç”¨æˆ·uæ‰€æœ‰è¯„åˆ†è¿‡çš„ç‰©å“ä¸­ï¼Œæ‰¾åˆ°ç›¸ä¼¼åº¦å’Œiæœ€é«˜çš„kä¸ªç‰©å“ï¼ˆk-neighborsï¼‰ï¼Œç”¨$S^k(i; u)$è¡¨ç¤ºä½†æ˜¯è¿™ç§ç®—æ³•è¿˜æ˜¯æœ‰ä¸€äº›å±€é™æ€§ï¼Œæ¯”å¦‚å¯¹äºä¸¤ä¸ªå®Œå…¨æ²¡æœ‰å…³ç³»çš„ç‰©å“ä¹‹é—´çš„é¢„æµ‹ã€‚æˆ–è€…æ˜¯å¯¹äºæŸäº›ç‰©å“ï¼Œæœ€ä¸ºç›¸ä¼¼çš„kä¸ªç‰©å“ç¼ºå¤±ï¼Œæ‰€ä»¥å¯ä»¥ä¿®æ­£ä»¥ä¸Šçš„å…¬å¼ï¼ˆä¸æ˜¯å¾ˆæ˜ç™½è¿™é‡Œï¼‰$$\\hat r_&#123;ui&#125; = b_&#123;ui&#125; + \\sum_&#123;j \\in S^k(i; u)&#125; \\theta^u_&#123;ij&#125;(r_&#123;ui&#125; - b_&#123;ui&#125;) \\\\\\&#123;\\theta^u_&#123;ij&#125; | j \\in S^k(i; u)\\&#125;$$","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆäºŒï¼‰","date":"2017-05-04T14:13:45.000Z","path":"2017/05/04/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆäºŒï¼‰/","text":"1. Mapper 2. Reducer 3. MapReduceæ•°æ®éƒ½æ˜¯ä»¥ key-values pairs çš„å½¢å¼åœ¨Mapperå’ŒReduceä¹‹é—´ä¼ é€’çš„ Mapperè¾“å‡ºçš„ key-value pairs åº”è¯¥å’Œ Reducerè¾“å…¥çš„ key-value pairs ç±»å‹æ˜¯ä¸€æ ·çš„ åœ¨Reducerä¸­ï¼Œæ˜¯ key-valueLists pairs çš„å½¢å¼ 4. Deriverä¹Ÿå°±æ˜¯åˆå§‹åŒ–é…ç½®MRç„¶åè°ƒç”¨æ‰§è¡Œï¼Œä¸€èˆ¬å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ï¼š 123456789101112131415161718192021222324252627282930313233// è€APIpublic void run(String inputPath, String outputPath) throws Exception &#123; JobConf conf = new JobConf(WordCount.class); conf.setJobName(\"wordcount\"); // the keys are words (strings) conf.setOutputKeyClass(Text.class); // the values are counts (ints) conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(MapClass.class); conf.setReducerClass(Reduce.class); FileInputFormat.addInputPath(conf, new Path(inputPath)); FileOutputFormat.setOutputPath(conf, new Path(outputPath)); JobClient.runJob(conf);&#125;// æ–°APIpublic void run(String IN, String OUT) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1);&#125; 5. Data Flowåœ¨hadoopä¸­æ‰€æœ‰çš„Mapperå’ŒReduceréƒ½æ˜¯ç‹¬ç«‹å·¥ä½œçš„ï¼Œè¿™ä¹Ÿæ˜¯hadoopåˆ†å¸ƒå¼èƒ½å¤Ÿç¨³å®šè¿è¡Œçš„åŸå› ä¹‹ä¸€(æœ‰åˆ©äºå®¹é”™å¤„ç†)ã€‚åœ¨MapReduceæ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œåªæœ‰ä¸€æ¬¡æ•°æ®ç›¸äº’äº¤äº’ï¼Œå°±æ˜¯Mapperåˆ°Reducerè¿™ä¸ªè¿‡ç¨‹ã€‚ä»Mapperè¾“å‡ºçš„æ‰€æœ‰çš„intermediate dataä¼šè¢«ç»Ÿä¸€shuffle(å¿…é¡»è¦ç­‰æ‰€æœ‰MRæ‰§è¡Œå®Œæ¯•å—ï¼Ÿ)ï¼Œç„¶ååŒä¸€ä¸ªkeyçš„key-value pairs ä¼šè¢«åˆ†é…åˆ°åŒä¸€ä¸ªreducerä¸­å»ã€‚ 6. A Closer Lookç¬¬äº”éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„æ˜¯MRçš„å®è§‚æµç¨‹ï¼Œå…·ä½“çš„æµç¨‹å…·ä½“å¯ä»¥åˆ†ä¸º Map -&gt; Combiner -&gt; Partitioner -&gt; Sort -&gt; Shuffle -&gt; Sort -&gt; Reduce(è¿™é‡Œå¯¹shuffleçš„å®šä¹‰æœ‰ç‚¹ä¸åŒï¼Œä¸ªäººè®¤ä¸ºä»mapçš„è¾“å‡ºåˆ°reduceçš„è¾“å…¥è¿™æ®µè¿‡ç¨‹å¯ä»¥ç§°ä¹‹ä¸ºshuffleã€‚åŒæ—¶æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œcombineræ˜¯åœ¨mapperæœ€ç»ˆè¾“å‡ºå‰å¤šæ¬¡è°ƒç”¨çš„ï¼Œä»¥åŠåœ¨reduceré‡Œä¹Ÿæœ‰è°ƒç”¨) ä¸‹å›¾ä¸­ï¼Œinput filesåœ¨è¿›å…¥åˆ°Mapperä¹‹å‰æ—¶ï¼Œä¼šå¯¹è¿™äº›æ–‡ä»¶splitï¼Œå› ä¸ºä¸€ä¸ªmapperä¸€èˆ¬æ˜¯64MBæˆ–è€…128MBï¼Œå½“å¤§äºçš„æ—¶å€™éœ€è¦å¯¹è¾“å…¥æ–‡ä»¶å¤„ç†ï¼Œç„¶åä¼ ç»™RecordReadersï¼Œä»¥key-value pairsçš„å½¢å¼ä¼ ç»™mapperã€‚ å¯¹äºæ‰€æœ‰Mapperçš„outputï¼Œå…ˆä¼šå¯¹å…¶è¿›è¡Œparitionæ“ä½œï¼Œä¹Ÿå°±æ˜¯å†³å®šå»å“ªä¸€ä¸ªReducerã€‚å½“ç¡®å®šå¥½å“ªäº›Reducerï¼Œè¿™äº›key-value paris å°±ä¼šä¼ å…¥åˆ°è¯¥Reducerç›¸åº”çš„åˆ†åŒºï¼Œç„¶åå¯¹é½è¿›è¡Œæ’åºã€‚æœ€åå°†sortå¥½çš„key-valueLists ä¼ å…¥åˆ°Reducerè¿›è¡Œå¤„ç†ã€‚ 7. ShuffleHadoop The Definitive Guide P197 Mapper æ ¹æ®å®˜æ–¹å›¾ï¼ŒMapperçš„outputå‡ºæ¥çš„key-values pairsä¼šå…ˆè¿›å…¥åˆ°buffer memory(é»˜è®¤100MBå¤§å°)ï¼Œä½†bufferåˆ°80%å®¹é‡çš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹ä¼šspillåˆ°diskå»ï¼Œå¦‚æœæ­¤æ—¶bufferè¿˜æœªæ…¢çš„æƒ…å†µä¸‹ï¼Œmapperç»§ç»­è¾“å‡ºåˆ°bufferï¼Œå¦‚æœæ»¡äº†çš„è¯mapperä¼šè¢«blockï¼Œç›´åˆ°å¯ä»¥å†™å…¥ã€‚ åœ¨bufferä¸­çš„å†…å®¹spillåˆ°diskä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªpartitionerçš„æ­¥éª¤ã€‚å¯¹è¿™äº›å³å°†å†™å…¥åˆ°diskçš„å†…å®¹åˆ†ç»„ï¼ŒåŒä¸€ä¸ªkeyå’ŒåŒä¸€ä¸ªreducerçš„ker-value pairsä¼šåœ¨ä¸€èµ·ã€‚ç„¶åè¿™äº›key-value pairsæ’åº(sort)ã€‚å¦‚æœæ­¤æ—¶æˆ‘ä»¬å®šä¹‰äº†combiner functionï¼Œåœ¨è¾“å‡ºå‰ï¼Œè¿™äº›pairsä¼šå…ˆcombineï¼Œç„¶åè¾“å‡ºã€‚ä¹Ÿå°±æ˜¯è¯´combiner functionæ˜¯åœ¨partitionerä¹‹å? (Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer) æ¯å½“bufferè¾¾åˆ°é‚£ä¸ªthresholdçš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹å†™å…¥åˆ°diskä¸­(æ³¨æ„æ˜¯æ¯ä¸ªclusterè‡ªå·±çš„local diskï¼Œè€Œä¸æ˜¯HDFS)ï¼Œæ­¤æ—¶ä¼šæ–°å»ºä¸€ä¸ªä¸´æ—¶çš„spillæ–‡ä»¶ï¼Œæ¯æ¬¡å†™å…¥éƒ½ä¼šæ–°å»ºä¸€ä¸ªï¼Œç„¶åè¿™ä¸ªmapç»“æŸå‰è¿™äº›spill filesä¼šè¢«mergeåˆ°ä¸€ä¸ªpartitionedå’Œsortedçš„æ–‡ä»¶é‡Œå»ã€‚é™¤äº†ä¹‹å‰partitionçš„è¾“å‡ºåè°ƒç”¨äº†ä¸€æ¬¡combinerä¸€æ¬¡ä¹‹å¤–ï¼Œä½†åˆå¹¶è¿™äº›ä¸ªspill files( &gt;=3 )çš„æ—¶å€™ï¼Œä¼šç»§ç»­è°ƒç”¨combinerå»åˆå¹¶åŒä¸€ä¸ªreduceré‡Œçš„åŒä¸€ä¸ªkeyçš„valueï¼Œæ‰€ä»¥combineråœ¨output fileè¢«ç”Ÿæˆä¹‹å‰ï¼Œä¼šè¢«è°ƒç”¨è®¸å¤šæ¬¡ï¼Œä»¥å‡å°‘ä¹‹åioçš„æ¬¡æ•°ã€‚ä½†å½“spill filesåªæ˜¯1ä¸ªæˆ–åˆ™ä¸¤ä¸ªçš„æ—¶å€™ï¼Œå¹¶ä¸ä¼šè°ƒç”¨combinerã€‚æ³¨æ„æœ‰äº›æƒ…å†µä¸‹combinerå¹¶ä¸é€‚åˆä½¿ç”¨ï¼Œæ¯”å¦‚æ±‚å¹³å‡å€¼ã€‚ è€Œä¸åŒmapperç”Ÿæˆçš„spillæ–‡ä»¶æœ€ç»ˆä¼šè¢«mergeæˆ {key:[v1, v2, v3â€¦], â€¦}è¿™ç§å½¢å¼ã€‚(è¿™é‡Œæœ‰ç–‘é—®ï¼Œåšå®¢å›¾å’Œå®˜æ–¹å›¾æœ‰ç‚¹ä¸ä¸€æ ·ã€‚æŒ‰ç…§å®˜æ–¹å›¾çš„ç†è§£ï¼Œä¸€ä¸ªmapperå¯¹åº”çš„æ˜¯ä¸€ä¸ªspill fileï¼Œæ‰€ä»¥æœ€ç»ˆæ˜¯å¤šä¸ªspill filesï¼Ÿè¿˜æ˜¯è¿™äº›spill filesåœ¨ä¼ ç»™Reducerä¹‹å‰ä¼šè¢«mergeæˆä¸Šè¿°çš„listå½¢å¼ï¼Ÿä¸€ä¸ªclusterä¸€ä¸ªæœ€ç»ˆçš„output file)â€‹ Reducer ä¹‹å‰mapperç«¯çš„æ‰€æœ‰å·¥ä½œå·²ç»å®Œæˆäº†ï¼Œæ‰€æœ‰çš„mapperçš„outputéƒ½å·²ç»è¢«å†™å…¥åˆ°äº†ä¸€ä¸ªoutputæ–‡ä»¶é‡Œé¢å»äº†ã€‚é‚£ä¹ˆReducerå°±æ˜¯è¦æŠŠè¿™ä¸ªæ–‡ä»¶é‡Œé¢çš„key-valueLists pairs åˆ†ç»™ä¸åŒçš„reducersï¼Œè€Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¹‹ä¸ºFetchï¼Œå°±æ˜¯å°†ç›¸åº”çš„key-valueList pairs æ‹‰å–åˆ°ç›¸åº”çš„reducersé‡Œé¢å»ã€‚ åœ¨Reduceré˜¶æ®µï¼Œæ¯ä¸ªreducerä¼šè°ƒç”¨çº¿ç¨‹ä»å¤šä¸ªä¸åŒçš„clusterçš„output fileé‡Œé¢fetchæ•°æ®ï¼Œç„¶åå¯¹è¿™äº›æ•°æ®mergeã€‚è¿™é‡Œçš„è¿‡ç¨‹å’Œä¹‹å‰çš„mapperæœ‰ç‚¹åƒã€‚reducerä¹Ÿæœ‰ä¸€ä¸ªbuffer memory(é€šè¿‡JVMçš„heap sizeæ¥è®¾ç½®)ï¼Œfetchçš„æ•°æ®ä¼šè¢«ä¼ åˆ°é‡Œé¢(å¦‚æœæ”¾å¾—ä¸‹)ï¼Œå½“bufferè¾¾åˆ°thresholdçš„å€¼çš„æ—¶å€™ï¼Œbufferé‡Œé¢çš„å†…å®¹ä¼šè¢«mergeç„¶åspillåˆ°diské‡Œé¢å»(å®é™…ä¸Šæœ‰å¤šç§å½¢å¼å­˜æ”¾è¿™ä¸ªæ–‡ä»¶ï¼Œè¿™é‡Œä¸è®¨è®º)ï¼Œå¦‚æœä¹‹å‰æˆ‘ä»¬å·²ç»å®šä¹‰äº†combinerï¼Œè¿™é‡Œcombinerä¹Ÿä¼šè¢«è°ƒç”¨ã€‚ç›´åˆ°æ‰€æœ‰çš„mapperçš„output fileéƒ½è¢«fetchåˆ°ä¸€ä¸ªæ–‡ä»¶é‡Œå»ï¼Œreducerä¼šåœ¨è¾“å…¥å‰sorté‡Œé¢çš„å†…å®¹(å®é™…ä¸Šmergeçš„æ—¶å€™å·²ç»sortäº†)ï¼Œç„¶åè¿™ä¸ªå·²ç»æ’å¥½åºçš„fileå°±ä¼šè¢«ä¼ åˆ°reduceré‡Œé¢æ‰§è¡Œï¼Œæœ€ç»ˆè¾“å‡ºåˆ°HDFSä¸Šã€‚","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Sparkç¬”è®°","date":"2017-04-10T06:55:40.000Z","path":"2017/04/10/Sparkç¬”è®°/","text":"1. Resilient Distributed Dataset (RDD)RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators. RDDæ˜¯ä¸€ä¸ªå®¹é”™çš„ã€å¹¶è¡Œçš„æ•°æ®ç»“æ„ï¼Œå¯ä»¥è®©ç”¨æˆ·æ˜¾ç¤ºåœ°å°†æ•°æ®å­˜å‚¨åˆ°ç£ç›˜å’Œå†…å­˜ä¸­ï¼Œå¹¶èƒ½æ§åˆ¶æ•°æ®çš„åˆ†åŒºã€‚ Resilient RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Formally, an RDD is a read-only, partitioned collection of records. Rdds can be created throught deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel. There two ways to create RDDs. Parallelizing parallelizing an existing collection in your driver program Referencing reference a dataset in an external storage system, such as a shared file system, HDFS, HBase, or ant data source offering a Hadoop Input Format Sparkæä¾›äº†RDDä¸Šçš„ä¸¤ç±»æ“ä½œï¼štransformation å’Œ action Transformation: return a new RDD å½“å¯¹RDDè¿›è¡Œtransformationæ“ä½œçš„æ—¶å€™ï¼Œè¿™äº›æ“ä½œä¸ä¼šç«‹åˆ»å°±æ‰§è¡Œï¼Œè€Œæ˜¯å°†è¿™äº›æ“ä½œè®°å½•ä¸‹æ¥ã€‚å¦‚æœæœ‰å¤šä¸ªtransformationæ“ä½œæ—¶ï¼Œæ¯ä¸€æ¬¡å˜æ¢éƒ½æ˜¯ä¸€ä¸ªæ¥ç€ä¸€ä¸ªï¼Œæ­¤æ—¶å°±å½¢æˆäº†ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ã€‚è¿™ä¸ªå›¾å°±æ˜¯æ•°æ®å®¹é”™çš„å…³é”®æ‰€åœ¨ã€‚å¦‚æœå‡ºç°æ•°æ®ä¸¢å¤±çš„æ—¶å€™ï¼Œåªéœ€è¦æŸ¥æ‰¾è¿™ä¸ªå›¾å°±èƒ½æ ¹æ®ä¸¢å¤±çš„RDDè¿›è¡Œæ•°æ®æ¢å¤ Action: evaluates and returns a new value å½“å¯¹RDDåšactionæ“ä½œçš„æ—¶å€™ï¼Œè¿™ç±»actionä¸€èˆ¬ä½œç”¨æ˜¯è¿”å›ä¸€ä¸ªå€¼æˆ–è€…æ•°ç»„ç­‰ï¼Œæˆ–è€…æ˜¯å°†æ•°æ®æŒä¹…åŒ–åˆ°ç£ç›˜å½“ä¸­ã€‚æ­¤æ—¶æ‰§è¡Œactionï¼Œä¼šçœŸæ­£çš„æäº¤jobï¼Œæ‰§è¡Œä¹‹å‰çš„transformationè®°å½•çš„DAGã€‚æ‰§è¡ŒDAGç­–ç•¥ä¸­ï¼Œä¼šæœ‰å¤šä¸ªstageï¼Œæ¯ä¸€ä¸ªstageé’Ÿæœ‰å¤šä¸ªtaskï¼Œè¿™äº›taskå°±ä¼šè¢«åˆ†é…åˆ°å„ä¸ªnodesè¿›è¡Œæ‰§è¡Œ The difference between flatMap and Map flatMap Example map: å¯¹RDDæ¯ä¸ªå…ƒç´ è½¬æ¢ï¼Œå°†å‡½æ•°ç”¨äºRDDä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œå°†è¿”å›å€¼æ„æˆæ–°çš„RDDã€‚flatMap: å¯¹RDDæ¯ä¸ªå…ƒç´ è½¬æ¢, ç„¶åå†æ‰å¹³åŒ–ï¼ˆå³å°†æ‰€æœ‰å¯¹è±¡åˆå¹¶ä¸ºä¸€ä¸ªå¯¹è±¡ï¼‰ã€‚å°†å‡½æ•°åº”ç”¨äºrddä¹‹ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå°†è¿”å›çš„è¿­ä»£å™¨çš„æ‰€æœ‰å†…å®¹æ„æˆæ–°çš„rdd Example1ï¼š 123456// data æœ‰ä¸¤è¡Œæ•°æ®ï¼Œç¬¬ä¸€è¡Œ a b cï¼Œç¬¬äºŒè¡Œ 1 2 3scala&gt; data.map(line1 =&gt; line1.split(\",\")).collect()res11: Array[Array[String]] = Array(Array(a, b, c),Array(1, 2, 3))scala&gt; data.flatMap(line1 =&gt; line1.split(\",\")).collect()res13: Array[String] = Array(a, b, c, 1, 2, 3) Example2ï¼š 1234567891011scala&gt; val rdd = sc.parallelize(List(\"coffee panda\",\"happy panda\",\"happiest panda party\"))scala&gt; rdd.map(x=&gt;x).collectres0: Array[String] = Array(coffee panda, happy panda, happiest panda party) scala&gt; rdd.map(x=&gt;x.split(\" \")).collectres1: Array[Array[String]] = Array(Array(coffee, panda), Array(happy, panda), Array(happiest, panda, party))// ç›¸æ¯”ä¹‹å‰ï¼ŒflatMapå°†å‡ ä¸ªarrayåˆå¹¶æˆäº†ä¸€ä¸ªarrayscala&gt; rdd.flatMap(x=&gt;x.split(\" \")).collectres2: Array[String] = Array(coffee, panda, happy, panda, happiest, panda, party) 2. Reference","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.com/tags/Spark/"}]},{"title":"å­—ç¬¦ä¸²æœç´¢ç®—æ³• - BM","date":"2017-04-02T04:28:44.000Z","path":"2017/04/02/å­—ç¬¦ä¸²æœç´¢ç®—æ³•-BM/","text":"å½“æˆ‘ä»¬è¦åœ¨æŸä¸€ä¸ªæ–‡æœ¬ä¸­è¦åŒ¹é…æŸä¸€ä¸ªå­—ç¬¦ä¸²çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ä¸€ä¸ªä¸€ä¸ªåŒ¹é…ï¼Œä¹Ÿå°±æ˜¯ä»å¤´å¼€å§‹åŒ¹é…æ–‡æœ¬å’Œå­—ç¬¦ä¸²ï¼Œä½†å‘ç°ä¸åŒçš„æ—¶å€™ï¼Œå°±æŠŠæ•´ä¸ªå­—ç¬¦ä¸²å³ç§»ä¸€ä½ï¼Œç„¶åä»å¤´é‡æ–°å’Œæ–‡æœ¬æ¯”è¾ƒã€‚å½“å…¨éƒ¨ç›¸åŒçš„æ—¶å€™ï¼Œåˆ™å³ç§»å­—ç¬¦ä¸²çš„é•¿åº¦ã€‚åœ¨ä¹‹å‰çš„ KMP ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬æ˜¯æ‰¾åˆ°å°½å¯èƒ½å³ç§»çš„æœ€å¤§ä½æ•°ï¼Œè€Œä¸æ˜¯ä¸€ä½ä¸€ä½çš„ç§»åŠ¨ï¼Œè¿™æ ·ç›¸æ¯”åŸå…ˆæ•ˆç‡å·²ç»æé«˜äº†å¾ˆå¤šäº†ã€‚ä½†åœ¨å¤§éƒ¨åˆ†çš„ç¼–è¾‘å™¨ä¸­ï¼Œâ€œæŸ¥æ‰¾â€åŠŸèƒ½ä½¿ç”¨çš„ä¸æ˜¯ KMP ï¼Œ è€Œæ˜¯ BM ç®—æ³•ã€‚ 1. Boyer-Moore Algorithmå’Œ KMP ä¸åŒçš„æ˜¯ï¼ŒBM æ˜¯ä» Pattern P å€’ç€åŒ¹é…ä¸Šæ¥çš„ é¦–å…ˆå®šä¹‰ä¸¤ä¸ªé¢„å¤„ç†çš„æ–¹æ³• åå­—ç¬¦ (Bad Character Heuristic) å½“æ–‡æœ¬ T ä¸­çš„æŸä¸ªå­—ç¬¦è·Ÿ Pattern P çš„æŸä¸ªå­—ç¬¦ä¸åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬ç§°æ–‡æœ¬ T ä¸­çš„è¿™ä¸ªä¸åŒ¹é…çš„å­—ç¬¦ä¸ºåå­—ç¬¦ã€‚ å¥½åç¼€ (Good Suffix Heuristic) å½“æ–‡æœ¬ T ä¸­çš„æŸä¸ªå­—ç¬¦è·Ÿ Pattern P çš„æŸä¸ªå­—ç¬¦ä¸åŒ¹é…æ—¶ï¼Œæˆ‘ä»¬ç§°æ–‡æœ¬ T ä¸­çš„å·²ç»åŒ¹é…çš„å­—ç¬¦ä¸²ä¸ºå¥½åç¼€ã€‚(å› ä¸ºç®—æ³•ä»å°¾å·´åˆ°å¤´éƒ¨æ¯”è¾ƒ) å¥½åç¼€çš„ä½ç½®ä»¥æœ€åä¸€ä¸ªå­—ç¬¦ä¸ºå‡†ã€‚å‡å®šâ€ABCDEFâ€çš„â€EFâ€æ˜¯å¥½åç¼€ï¼Œåˆ™å®ƒçš„ä½ç½®ä»¥â€Fâ€ä¸ºå‡†ï¼Œå³5ï¼ˆä»0å¼€å§‹è®¡ç®—ï¼‰ã€‚ å¦‚æœâ€å¥½åç¼€â€åœ¨æœç´¢è¯ä¸­åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®ä¸º -1ã€‚æ¯”å¦‚ï¼Œâ€EFâ€åœ¨â€ABCDEFâ€ä¹‹ä¸­åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®ä¸º-1ï¼ˆå³æœªå‡ºç°ï¼‰ã€‚ å¦‚æœâ€å¥½åç¼€â€æœ‰å¤šä¸ªï¼Œåˆ™é™¤äº†æœ€é•¿çš„é‚£ä¸ªâ€å¥½åç¼€â€ï¼Œå…¶ä»–â€å¥½åç¼€â€çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®å¿…é¡»åœ¨å¤´éƒ¨ã€‚æ¯”å¦‚ï¼Œå‡å®šâ€BABCDABâ€çš„â€å¥½åç¼€â€æ˜¯â€DABâ€ã€â€ABâ€ã€â€Bâ€ï¼Œè¯·é—®è¿™æ—¶â€å¥½åç¼€â€çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®æ˜¯ä»€ä¹ˆï¼Ÿå›ç­”æ˜¯ï¼Œæ­¤æ—¶é‡‡ç”¨çš„å¥½åç¼€æ˜¯â€Bâ€ï¼Œå®ƒçš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½®æ˜¯å¤´éƒ¨ï¼Œå³ç¬¬0ä½ã€‚è¿™ä¸ªè§„åˆ™ä¹Ÿå¯ä»¥è¿™æ ·è¡¨è¾¾ï¼šå¦‚æœæœ€é•¿çš„é‚£ä¸ªâ€å¥½åç¼€â€åªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™å¯ä»¥æŠŠæœç´¢è¯æ”¹å†™æˆå¦‚ä¸‹å½¢å¼è¿›è¡Œä½ç½®è®¡ç®—â€(DA)BABCDABâ€ï¼Œå³è™šæ‹ŸåŠ å…¥æœ€å‰é¢çš„â€DAâ€ã€‚ æŒ‰ç…§æ­£å¸¸æ¯”è¾ƒæ–¹æ³•ï¼Œå½“å‘ç°ä¸€ä¸ªåå­—ç¬¦çš„æ—¶å€™ï¼Œä¼šå‡ºç°ä¸€ä¸‹ä¸‰ç§æƒ…å†µ åå­—ç¬¦å®Œå…¨ä¸å‡ºç°åœ¨ Pattern P ä¸­ï¼Œé‚£ä¹ˆå°±å¯ä»¥å®Œå…¨è·³è¿‡è¿™ä¸ªå­—ç¬¦ï¼Œå› ä¸ºæˆ‘ä»¬æ ¹æœ¬ä¸å¯èƒ½åŒ¹é…åˆ°è¿™ä¸ªå­—ç¬¦ã€‚ åå­—ç¬¦å‡ºç°åœ¨è¿˜æœªæ¯”è¾ƒçš„å‰ç¼€å½“ä¸­ (åªå‡ºç°åœ¨å‰ç¼€ï¼Œå‡ºç°åœ¨å‰ç¼€å’Œåç¼€) æ­¤æ—¶å°† Pattern P å³ç§»å¯¹é½è¿™ä¸¤ä¸ªå­—ç¬¦ï¼Œç„¶åä»å°¾éƒ¨ç»§ç»­æ¯”è¾ƒ åå­—ç¬¦åªå‡ºç°åœ¨ä¹‹å‰çš„å¥½åç¼€å½“ä¸­ï¼ˆä¹Ÿå°±æ˜¯å·²ç»æ¯”è¾ƒè¿‡äº†ï¼‰ å¯¹è¿™ç§æƒ…å†µä¸åšå¤„ç† æ ¹æ®ä¸Šé¢å‡ ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥æ€»ç»“å‡ºä»¥ä¸‹åå­—ç¬¦è§„åˆ™ï¼š åç§»ä½æ•° = åå­—ç¬¦çš„ä½ç½® - æœç´¢è¯ä¸­çš„ä¸Šä¸€æ¬¡å‡ºç°ä½ç½® è¿™é‡Œæœ‰ä¸¤ç§ç‰¹æ®Šæƒ…å†µ å¦‚æœåå­—ç¬¦ä¸å­˜åœ¨äº Pattern P ä¸­ï¼Œåˆ™æœ€åä¸€æ¬¡å‡ºç°çš„ä½ç½®ä¸º -1ã€‚ å¦‚æœåå­—ç¬¦åœ¨ Pattern P ä¸­çš„ä½ç½®ä½äºå¤±é…ä½ç½®çš„å³ä¾§ï¼Œåˆ™æ­¤å¯å‘æ³•ä¸æä¾›ä»»ä½•å»ºè®®ã€‚ é™¤äº†åå­—ç¬¦ï¼Œæˆ‘ä»¬åŒæ ·å¯ä»¥åˆ©ç”¨å¥½åç¼€æ¥æé«˜æŸ¥æ‰¾æ•ˆç‡ æ¨¡å¼åç§»ä½æ•° = å¥½åç¼€åœ¨æ¨¡å¼ä¸­çš„å½“å‰ä½ç½® - å¥½åç¼€åœ¨æ¨¡å¼ä¸­æœ€å³å‡ºç°ä¸”å‰ç¼€å­—ç¬¦ä¸åŒçš„ä½ç½® ä¸¾ä¸ªæ —å­è®¡ç®—ï¼š 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 åå­—ç¬¦çš„ç§»åŠ¨ æ­¤æ—¶åå­—ç¬¦çš„ä½ç½®æ˜¯4ï¼Œåå­—ç¬¦ I ä¸åœ¨å­—ç¬¦ä¸²å½“ä¸­ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸ºï¼š 4 - (-1) = 5 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 å¥½åç¼€çš„ç§»åŠ¨ æ­¤æ—¶å¥½åç¼€æ˜¯ MPLEï¼Œå…¶ä¸­ PLE å‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³ä½ç½®ä¸º (01) 2ã€‚ æ­¤æ—¶å¥½åç¼€çš„å­—ç¬¦æ˜¯8(ä»¥æœ€åä¸€ä¸ªä¸ºå‡†)ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸ºï¼š8 - 2 = 6 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å¥½åç¼€çš„æŸ¥æ‰¾æ•ˆç‡æ›´é«˜ã€‚ å¦‚ä½•è®¡ç®—å¥½åç¼€æˆ‘ä»¬ä¹‹å‰å·²ç»æè¿‡äº†ï¼Œç»§ç»­ä»¥ä¸Šé¢æœ€åä¸€ä¸ªä¾‹å­è®²è§£ã€‚ MPLE : æœªå‡ºç°ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› PLE : æœªå‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› LE : æœªå‡ºç°åœ¨å¤´éƒ¨ï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º -1ï¼› E : å‡ºç°åœ¨å¤´éƒ¨ï¼Œè¡¥å……è™šæ‹Ÿå­—ç¬¦ â€˜MPLâ€™Eï¼Œå‰ç¼€å­—ç¬¦ä¸ºç©ºï¼Œæœ€å³å‡ºç°çš„ä½ç½®ä¸º 0ï¼› æ­¤æ—¶ï¼Œæ‰€æœ‰çš„â€å¥½åç¼€â€ï¼ˆMPLEã€PLEã€LEã€Eï¼‰ä¹‹ä¸­ï¼Œåªæœ‰â€Eâ€åœ¨â€EXAMPLEâ€è¿˜å‡ºç°åœ¨å¤´éƒ¨ï¼Œæ‰€ä»¥åç§» 6 - 0 = 6ä½ã€‚ å¦‚æœä¹‹å‰æˆ‘ä»¬åªåˆ©ç”¨åå­—ç¬¦çš„è¯ï¼Œç§»åŠ¨çš„ä½æ•°åªæœ‰ 2 - (-1) = 3ä½ æ‰€ä»¥æˆ‘ä»¬æœ€ç»ˆçš„ç§»åŠ¨ä½ç½®ï¼Œæ˜¯åœ¨è¿™ä¸¤è€…ä¹‹é—´å–æœ€å¤§å€¼æ¥ç§»åŠ¨ã€‚ æ‰€ä»¥åœ¨æŸ¥æ‰¾ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é¢„è®¡ç®—ç”Ÿæˆåå­—ç¬¦è§„åˆ™è¡¨ å’Œ å¥½åç¼€è§„åˆ™è¡¨ï¼Œåˆ°æ—¶å€™åªéœ€è¦æŸ¥è¡¨å°±å¯ä»¥äº†ã€‚ç»§ç»­ä¸Šé¢çš„ä¾‹å­ã€‚ æ ¹æ®ä¸Šé¢è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬å°† Pattern P å³ç§»6ä½ã€‚ç„¶åé‡æ–°ä»å°¾éƒ¨å¼€å§‹æ¯”è¾ƒ æ–‡æœ¬ä¸­Pä¸åŒ¹é…ï¼Œæ­¤æ—¶åªèƒ½ä½¿ç”¨åå­—ç¬¦è§„åˆ™ï¼ŒPä¸Šä¸€æ¬¡å‡ºç°çš„ä½ç½®ä¸º4ï¼Œåˆ™ç§»åŠ¨6-4=2ä½ ç§»åŠ¨2ä½åï¼Œå‘ç°æ–‡æœ¬ä¸­çš„E ä¸ Pattern Pä¸­çš„E åŒ¹é…ï¼Œåˆ™ç»§ç»­å€’åºæ¯”è¾ƒï¼Œç›´åˆ°å‘ç°å…¨éƒ¨åŒ¹é…ï¼Œåˆ™åŒ¹é…åˆ°çš„ç¬¬ä¸€ä¸ªå®Œæ•´çš„æ¨¡å¼ P è¢«å‘ç°ã€‚ ç»§ç»­ä¸‹å»åˆ™æ˜¯ä¾æ®å¥½åç¼€è§„åˆ™è®¡ç®—å¥½åç¼€ â€œEâ€ çš„åç§»ä½ç½®ä¸º 6 - 0 = 6 ä½ï¼Œç„¶åç»§ç»­å€’åºæ¯”è¾ƒæ—¶å‘ç°å·²è¶…å‡ºæ–‡æœ¬ T çš„èŒƒå›´ï¼Œæœç´¢ç»“æŸã€‚ 2. Reference å­—ç¬¦ä¸²åŒ¹é…çš„Boyer-Mooreç®—æ³• Boyer-Moore å­—ç¬¦ä¸²åŒ¹é…ç®—æ³•","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"PostgreSQLå¤ä¹ ç¬”è®°","date":"2017-03-23T23:38:55.000Z","path":"2017/03/24/PostgreSQLå¤ä¹ ç¬”è®°/","text":"1 å¸¸è§é—®é¢˜1.1 åŒ¹é…ä¸€è‡´çš„ç¼–ç è§„åˆ™123SET character_set_client=&apos;utf8&apos;; -- gbkSET character_set_connection=&apos;utf8&apos;; -- gbkSET character_set_results=&apos;utf8&apos;; -- gbk 1234-- åˆ›å»ºæ•°æ®åº“æ—¶å€™è®¾ç½®CREATE DATABASE `test`CHARACTER SET &apos;utf8&apos;COLLATE &apos;utf8_general_ci&apos;; 1.2 å¦‚ä½•ç†è§£ç´¢å¼•ç´¢å¼•å­—é¢ä¸Šç†è§£å°±æ˜¯å¯¹æ•°æ®æ‰€å»ºç«‹ç›®å½•ï¼Œå®ƒå¯ä»¥åŠ å¿«æˆ‘ä»¬çš„æŸ¥è¯¢é€Ÿåº¦ï¼Œä½†æ˜¯åŒæ—¶ä¹Ÿé™ä½äº†å¢åˆ æ”¹çš„é€Ÿåº¦ã€‚ åˆ›å»ºåŸåˆ™ ä¸è¦è¿‡åº¦ä½¿ç”¨ç´¢å¼• æœ€å¥½åœ¨æŸ¥è¯¢é¢‘ç¹çš„åˆ—ä¸Šä½¿ç”¨ç´¢å¼• å¦‚æœæ„å»ºç´¢å¼•ï¼Œè¿™ä¸€åˆ—å°½é‡æ˜¯ç¦»æ•£å€¼ï¼Œè€Œä¸è¦è¿‡äºè¿ç»­çš„åŒºé—´ ç´¢å¼•çš„ç±»å‹ æ™®é€šçš„ç´¢å¼• index å”¯ä¸€çš„ç´¢å¼• unique index ä¸€å¼ è¡¨ä¸Šï¼Œåªèƒ½æœ‰ä¸€ä¸ªä¸»é”®ï¼Œä½†æ˜¯å¯ä»¥æœ‰ä¸€ä¸ªæˆ–æ˜¯å¤šä¸ªå”¯ä¸€ç´¢å¼• ä¸»é”®ç´¢å¼• primary key ä¸èƒ½é‡å¤ 12-- æŸ¥çœ‹ä¸€å¼ è¡¨ä¸Šçš„æ‰€æœ‰ç´¢å¼•show index from TABLE_NAMES; 1.3 æ¨¡ç³ŠæŸ¥è¯¢ % åŒ¹é…ä»»æ„å­—ç¬¦ _ åŒ¹é…å•ä¸ªå­—ç¬¦ ä¸¾ä¸ªæ —å­ 1234567891011SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC%&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC_&apos;; 1.4 ç†è§£ COUNT è§3.1 èšé›†å‡½æ•° 1.5 ç†è§£ UNION å’Œ UNION ALL UNION ç”¨äºåˆå¹¶ä¸¤ä¸ªæˆ–æ˜¯å¤šä¸ªSELECTè¯­å¥çš„ç»“æœé›† æ³¨æ„ï¼š SELECTè¯­å¥å¿…é¡»æ‹¥æœ‰ç›¸åŒçš„æ•°é‡çš„åˆ— åˆ—çš„éœ€è¦æ‹¥æœ‰ç›¸ä¼¼çš„æ•°æ®ç±»å‹ æ¯æ¡SELECTè¯­å¥ä¸­çš„åˆ—çš„é¡ºåºå¿…é¡»æ˜¯ä¸€è‡´çš„ ç»“æœä¸å…è®¸æœ‰é‡å¤ï¼Œå¦åˆ™ä½¿ç”¨ UNION ALL 123456SELECT mid, sex, ageFROM TABLE_1UNION -- UNIAON ALL -- å…è®¸æœ‰é‡å¤çš„å€¼å‡ºç°åœ¨ç»“æœé›†ä¸­SELECT mid, sex, ageFROM TABLE_2;-- ORDER BY mid; -- å¯ä»¥å¯¹å…¶ç»“æœè¿›è¡Œæ’åºï¼Œæ³¨æ„çš„æ˜¯æ’åºåªæ˜¯é’ˆå¯¹åˆå¹¶åçš„ç»“æœé›†æ’åº 1.6 ç†è§£ JOINï¼ˆå·¦é“¾æ¥ï¼Œå†…é“¾æ¥å’Œå¤–é“¾æ¥ï¼‰ ä¸åŒçš„å‡ ç§JOINç±»å‹ï¼Œä»¥åŠä¹‹é—´çš„å·®å¼‚ JOINï¼šå¦‚æœè¡¨ä¸­è‡³å°‘æœ‰ä¸€ä¸ªåŒ¹é…ï¼Œåˆ™è¿”å›è¡Œ LEFT JOINï¼šå³ä½¿å³è¡¨ä¸­æ²¡æœ‰åŒ¹é…ï¼Œä¹Ÿä»å·¦è¾¹è¿”å›æ‰€æœ‰çš„è¡Œ RIGHT JOINï¼šå³ä½¿å·¦è¡¨ä¸­æ²¡æœ‰åŒ¹é…ï¼Œä¹Ÿä»å³è¡¨ä¸­è¿”å›æ‰€æœ‰çš„è¡Œ FULL JOINï¼šåªè¦å…¶ä¸­ä¸€ä¸ªè¡¨å­˜åœ¨åŒ¹é…ï¼Œå°±è¿”å›è¡Œ INNER JOIN å¹³å¸¸æˆ‘ä»¬éœ€è¦é“¾æ¥ä¸¤ä¸ªè¡¨çš„æ—¶å€™ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹æ³• 123SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨JOINæ¥å®ç°ä¸Šé¢çš„è¯­å¥ 12345SELECT Person.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName LEFT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName æ³¨æ„åˆ°ä¸Šé¢çš„ï¼Œå·¦è¾¹æ‰€æœ‰çš„è¡Œéƒ½è¿”å›äº†ï¼Œå³ä½¿æ²¡æœ‰å‡ºç°åœ¨å³è¡¨å½“ä¸­ï¼Œæ²¡æœ‰çš„å€¼ä¸ºNULL RIGHT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName æ³¨æ„ï¼Œå³ä½¿å·¦è¾¹æ²¡æœ‰å…¨éƒ¨åŒ¹é…åˆ°å³è¾¹ï¼Œä¾ç„¶åœ¨æœ€åçš„OrderNoä¸­ï¼Œè¿”å›äº†å³è¡¨æ‰€æœ‰çš„è¡Œæ•°ï¼Œæ²¡æœ‰çš„å€¼ä¸ºNULL 1.7 ç†è§£ HAVING è§3.2 åœ¨SQLä¸­å¢åŠ HAVINGå­å¥çš„åŸå› æ˜¯ç”±äºWHEREä¸­æ— æ³•ä½¿ç”¨èšåˆå‡½æ•° 2 æ•°æ®åº“çš„åŸºæœ¬æ“ä½œ2.1 è¡¨çš„ä½¿ç”¨ ä½¿ç”¨åˆ—çº¦æŸå»ºè¡¨ 12345678CREATE [TEMPORARY] TABLE è¡¨å ( -- TEMPORARY å»ºç«‹ä¸€å¼ ä¸´æ—¶çš„è¡¨col_not_null INT NOT NULL, -- åˆ—å ç±»å‹ &#123;çº¦æŸ1 çº¦æŸ2 ...&#125;col_unique INT UNIQUE,col_prikey INT PRIMARY KEY, -- NOT NULL + UNIQUE ä¸»é”®col_default INT DEFAULT 42,col_check INT CHECK(col_check &lt; 42)col_ref INT REFERENCES -- çº¦æŸè¿™ä¸ªå€¼å¿…é¡»æ˜¯å¦ä¸€ç‹¬ç«‹çš„è¡¨çš„æŸä¸ªåˆ—ä¸­çš„æŸä¸ªå€¼); ä½¿ç”¨è¡¨çº§çº¦æŸå»ºè¡¨ 1234567CREATE TABLE è¡¨å ( myKey_1 INT, myKey_2 INT, myString varchar(15), CONSTRAINT cs1 CHECK (myString &lt;&gt; &apos;&apos;), -- ä¸èƒ½ä½ç©ºå­—ç¬¦ä¸² CONSTRAINT CS2 PRIMARY KEY (myKey_1, myKey_2)); -- ä¿®æ”¹è¡¨ç»“æ„ æ·»åŠ æ–°åˆ— 1ALTER TABLE è¡¨å ADD COLUMN åˆ—å ç±»å‹; é‡å‘½åæ–°æ·»åŠ çš„åˆ— 1ALTER TABLE è¡¨å RENAME COLUMN åˆ—å TO æ–°åˆ—å; æ”¹å˜ä¸€äº›çº¦æŸå’Œå…¶ä»–è§„åˆ™ 12ALTER TABLE è¡¨å DROP CONSTRAINT cs1; -- DROPçº¦æŸALTER TABLE è¡¨å ADD CONSTRAINT cs3 UNIQUE(åˆ—å); --æ·»åŠ æ–°çš„çº¦æŸ ä¿®æ”¹åˆ—çš„ç±»å‹ 1ALTER TABLE è¡¨å ALTER åˆ—å TYPE æ–°ç±»å‹; é‡å‘½åè¡¨å 1ALTER TABLE è¡¨å RENAME TO æ–°è¡¨å; ä½¿ç”¨ä¸´æ—¶è¡¨ ä¸´æ—¶è¡¨çš„åŠŸèƒ½åŸºæœ¬å’Œè¡¨æ˜¯å·®ä¸å¤šçš„ï¼ŒåŒºåˆ«åœ¨äºå½“ä½ çš„ä¼šè¯ç»“æŸæ—¶ï¼Œä½ ä¸æ•°æ®åº“è¿æ¥æ–­å¼€åï¼Œä¸´æ—¶è¡¨ä¼šè‡ªåŠ¨è¢«åˆ é™¤ã€‚ é”®çš„çº¦æŸ ä½œä¸ºä¸€ä¸ªåˆ—çš„çº¦æŸçš„å¤–é”®ï¼ˆåˆ—çº¦æŸï¼‰ 1234567CREATE TABLE è¡¨å ( ... ... customer_id INTEGER NOT NUll REFERENCES customer(customer_id), -- å…³è”åˆ°customerè¡¨ ... ...);-- REFERENCES å¤–è¡¨å(å¤–è¡¨åä¸­çš„åˆ—) è¡¨çº§çº¦æŸ 123456CREATE TABLE è¡¨å( ... ... customer_id INTEGER NOT NULL, ... ... CONSTRAINT è¡¨å_åˆ—å_fk FOREIGN KEY(åˆ—å) REFERENCES å¤–è¡¨å(å¤–é¢ä¸­çš„åˆ—)) æ³¨æ„ï¼šæ¯”è¾ƒæ¨èçš„æ˜¯ä½¿ç”¨è¡¨çº§çº¦æŸï¼Œè€Œä¸æ˜¯æ··å’Œç§ç”¨è¡¨çº§å’Œåˆ—çº§çº¦æŸ â€‹ çº¦æŸåè¡¨å_åˆ—å_fkå…è®¸å¤–é¢æ›´å®¹æ˜“å®šä½é”™è¯¯èµ„æº 2.2 è§†å›¾ å»ºç«‹è§†å›¾ 123CREATE VIEW è§†å›¾çš„åå­— AS selectç³»åˆ—è¯­å¥;-- ä¾‹å­CREATE VIEW item_price AS SELECT item_id, description, sell_price FROM item; å½“è§†å›¾å»ºç«‹å¥½çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨è¡¨ä¸€æ ·æ¥æŸ¥è¯¢è¿™ä¸ªè§†å›¾ï¼Œå¯ä»¥ä½¿ç”¨SELECTæˆ–WHEREè¯­å¥ç­‰ã€‚ æ¯æ¬¡åœ¨è§†å›¾ä¸­æ‰§è¡ŒSELECTæ—¶ï¼Œæ•°æ®éƒ½ä¼šè¢«é‡å»ºï¼Œæ‰€ä»¥æ•°æ®æ€»æ˜¯æœ€æ–°çš„ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåœ¨è§†å›¾è¢«å»ºç«‹çš„æ—¶å€™å†»ç»“çš„æ‹·è´ã€‚ä¹Ÿå°±æ˜¯å½“ä¸ä¹‹ç›¸å…³çš„è¡¨çš„æ•°æ®å‘ç”Ÿè¯¥è¡¨çš„æ—¶å€™ï¼ŒVIEWé‡Œé¢çš„æ•°æ®ä¹Ÿéšä¹‹æ”¹å˜ï¼Œè€Œä¸æ˜¯å‚¨å­˜äº†å»ºç«‹VIEWçš„æ—¶å€™çš„æ‹·è´å¯¹è±¡ã€‚æˆ–è€…ä¹Ÿå¯ä»¥ç†è§£ç±»ä¼¼æŒ‡é’ˆæŒ‡å‘åŸå…ˆçš„è¡¨ï¼Œå½“åŸå…ˆçš„è¡¨å‘ç”Ÿå˜åŒ–ï¼Œè¿™è¾¹çš„æ•°æ®è‡ªç„¶è€Œç„¶çš„å°±èƒ½å¤Ÿè¯»å–å‡ºæ¥ã€‚ å½“ç„¶ï¼ŒSELECTè¯­å¥æ˜¯å¯ä»¥åœ¨å¤šä¸ªä¸åŒçš„è¡¨ä¸­æå–æ•°æ®çš„ã€‚ åˆ é™¤å’Œæ›¿æ¢VIEW 12DROP VIEW åå­—; -- ä¸å½±å“æˆ‘ä»¬å·²æœ‰çš„æ•°æ®CREATE OR REPLACE VIEW åå­— AS æ–°çš„selectç³»åˆ—è¯­å¥; ä¸€äº›ä¸VIEWå¸¸ç”¨çš„æŒ‡ä»¤ 12\\dv -- æŸ¥çœ‹å½“å‰æ•°æ®åº“ä¸­çš„æ‰€æœ‰çš„VIEW\\d VIEWçš„åå­— -- æŸ¥çœ‹å…·ä½“çš„æŸä¸€ä¸ªVIEWçš„ç»“æ„ 2.3 INSERTè¯­å¥ åŸºæœ¬æ’å…¥è¯­å¥ 123INSERT INTO è¡¨å VALUES (æ¯åˆ—çš„å€¼çš„åˆ—è¡¨);INSERT INTO customer VALUES(18, &apos;Mr&apos;, &apos;Jeff&apos;, &apos;Baggott&apos;, &apos;Midtown Street A\\\\33&apos;, &apos;Milltown&apos;, &apos;MT9 8NQ&apos;, &apos;746 3956&apos;); PSï¼šè¿™ç§æ“ä½œå¾ˆå±é™©ï¼ŒSQLæ³¨å…¥æ”»å‡» æ¨èçš„å®‰å…¨æ–¹æ³• 12345INSERT INTO è¡¨å(åˆ—åçš„åˆ—è¡¨) VALUES(è·Ÿä¹‹å‰çš„åˆ—çš„åˆ—è¡¨å¯¹åº”åˆ—çš„æ•°å€¼);INSERT INTO customer(customer_id, title, fname, lname, addressline, ...)VALUES(19, &apos;Mrs&apos;, &apos;Sarah&apos;, &apos;Harvey&apos;, &apos;84 Willow Way&apos;, ...); PSï¼šé¿å…åœ¨æ’å…¥æ•°æ®çš„æ—¶å€™ä¸ºserialç±»å‹çš„æ•°æ®æä¾›æ•°å€¼ï¼Œå› ä¸ºè¿™ä¸ªæ˜¯ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ çš„ è®¿é—®åºåˆ—ç”Ÿæˆå™¨ åºåˆ—ç”Ÿæˆå™¨æ€» æ˜¯è¢«å‘½åä¸º&lt;è¡¨å&gt;_&lt;åˆ—å&gt;_seq 123currval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;);nextval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;);setval(&apos;åºåˆ—ç”Ÿæˆå™¨å&apos;, æ–°çš„å€¼); æ’å…¥ç©ºå€¼ 123INSERT INTO customer VALUES(16, &apos;Mr&apos;, NULL, &apos;Smith&apos;,&apos;23 Harlestone&apos;, &apos;Milltown&apos;, &apos;MT7 7HI&apos;, &apos;746 3725&apos;); ä½¿ç”¨ \\copy å‘½ä»¤ æ­¥éª¤ å…ˆç”Ÿæˆå¦‚ä¸‹æ ¼å¼çš„æ•°æ® â€‹ å†ç”Ÿæˆå¦‚ä¸‹æ ¼å¼çš„æ•°æ®ï¼Œä¿å­˜æˆ.sqlæ‹“å±•åçš„æ–‡æœ¬æ–‡ä»¶ â€‹ ä½¿ç”¨\\copyå‘½ä»¤å¯¼å…¥æ•°æ® â€‹ PSï¼šSQL é‡Œå¤´çš„ COPY å‘½ä»¤æœ‰ä¸€ä¸ªä¼˜ç‚¹ï¼šå®ƒæ˜æ˜¾æ¯”\\copy å‘½ä»¤å¿«ï¼Œå› ä¸ºå®ƒç›´æ¥é€šè¿‡æœåŠ¡å™¨è¿›ç¨‹æ‰§è¡Œã€‚\\copy å‘½ä»¤æ˜¯åœ¨å®¢æˆ·è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œæœ‰å¯èƒ½éœ€è¦é€šè¿‡ç½‘ç»œä¼ è¾“æ‰€æœ‰æ•°æ®ã€‚è€Œä¸” COPY åœ¨å‘ç”Ÿé”™è¯¯çš„æ—¶å€™ä¼šæ›´å¯é ã€‚é™¤éä½ æœ‰å¤§é‡çš„æ•°æ®ï¼Œå¦åˆ™åŒºåˆ«ä¸ä¼šå¤ªæ˜æ˜¾ã€‚ 2.5 ä»æ•°æ®åº“ä¸­åˆ é™¤æ•°æ® DELETEè¯­å¥ è¯­æ³•ç±»ä¼¼äºUPDATEè¯­å¥ 1DELETE FROM è¡¨å WHERE æ¡ä»¶; TRUNCATEè¯­å¥ï¼ˆä¸æ¨èï¼Œå› ä¸ºä¸å®‰å…¨ï¼‰ TRUNCATEè¯­å¥æ˜¯æŠŠè¡¨ä¸­æ‰€æœ‰çš„æ•°æ®éƒ½åˆ é™¤ï¼Œä½†æ˜¯ä¿ç•™è¿™å¼ è¡¨çš„ç»“æ„ï¼Œä¹Ÿå°±æ˜¯è¯´æœ€åå‰©ä¸‹äº†ä¸€å¼ ç©ºè¡¨ï¼Œæ‰€æœ‰çš„è¡Œéƒ½è¢«åˆ é™¤äº†ã€‚ 1TRUNCATE TABLE è¡¨å; DROPè¯­å¥ DROPè¯­å¥å°±æ˜¯åˆ é™¤äº†æ•´å¼ è¡¨çš„å†…å®¹ï¼ŒåŒ…æ‹¬è¡¨çš„ç»“æ„ã€‚DROPå®Œä¹‹åï¼Œè¿™å¼ è¡¨å°±æ˜¯ä¸å­˜åœ¨çš„äº† 1DROP TABLE è¡¨å; 2.6 ä¿®æ”¹æ•°æ®åº“ä¸­çš„æ•°æ® UPDATEè¯­å¥ 1UPDATE è¡¨å SET åˆ—å = å€¼ WHERE æ¡ä»¶; 1UPDATE customer SET town = &apos;Leicester&apos;, zipcode = &apos;LE4 2WQ&apos; WHERE ä¸€äº›æ¡ä»¶; å¦‚æœæ²¡æœ‰WHEREå­å¥çš„è¯ï¼Œä¼šå¯¼è‡´è¡¨ä¸­çš„å¾ˆå¤šç”šè‡³æ˜¯æ‰€æœ‰çš„è¡Œéƒ½è¢«åŒæ—¶æ›´æ–°äº† é€šè¿‡å¦ä¸€ä¸ªè¡¨æ›´æ–° 1UPDATE è¡¨å FROM è¡¨å WHERE æ¡ä»¶; 3 é«˜çº§æ•°æ®é€‰æ‹©3.1 èšé›†å‡½æ•° Group By and count(*) é”™è¯¯ä½¿ç”¨ 1SELECT count(*), town FROM customer; æ­£ç¡®ä½¿ç”¨ 1SELECT count(*), town FROM customer GROUP BY town; ç»“æœæ˜¯è·å¾—ä¸€ä¸ªåŸé•‡çš„åˆ—è¡¨ä»¥åŠæ¯ä¸ªåŸé•‡çš„å®¢æˆ·æ•°é‡ï¼ˆcount(*)) åŒæ—¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨ä¸¤ä¸ªcolumns nameåœ¨GROUP BYä¸­ï¼Œç„¶åç”¨ORDER BYæŒ‡å®šæ’åˆ—é¡ºåºã€‚æ²¡æœ‰GROUP BYçš„è¯æŒ‰ç…§GROUP BYä¸­çš„townï¼Œlnameæ’åº Having Havingæ˜¯ä¸€ç§ç”¨äºèšé›†å‡½æ•°çš„WHEREä»å¥ï¼Œæˆ‘ä»¬ä½¿ç”¨HAVINGæ¥çº¦æŸè¿”å›çš„ç»“æœä¸ºé’ˆå¯¹ç‰¹å®šçš„èšé›†çš„æ¡ä»¶ä¸ºçœŸçš„è¡Œï¼Œæ¯”å¦‚count(*) &gt; 1 PSï¼šèšé›†å‡½æ•°æ— æ³•åœ¨WHEREä»å¥ä¸­ä½¿ç”¨ï¼Œåªèƒ½ç”¨åœ¨HAVINGä»å¥ä¸­ ä¸¾ä¸ªæ —å­ï¼š é€‰å‡ºæœ‰è¶…è¿‡ä¸€ä¸ªå®¢æˆ·çš„åŸé•‡ï¼Œåœ¨é‡Œä½¿ç”¨ä¸€ä¸ªHAVINGä»å¥æ¥çº¦æŸå¤§ä¸€çš„è¡Œ SELECTä¸­ä»å¥çš„ä¼˜å…ˆåº¦ 1234567SELECTFROM WHEREGROUP BYHAVINGORDER BY -- DESC ä¸‹é™LIMIT -- ç”¨äºé™åˆ¶rowsæ˜¯è¡Œæ•° mysql count(Column_Name) count(*) ç»Ÿè®¡æ‰€æœ‰çš„è¡Œ count(column_name) ç»Ÿè®¡æ‰€è¿™ä¸ªåˆ—ä¸­å€¼ä¸æ˜¯NULLçš„è¡Œ count(Distinct column) åªç»Ÿè®¡è¿™ä¸ªåˆ—ä¸­å”¯ä¸€çš„æƒ…å†µï¼Œä¸é‡å¤ç»Ÿè®¡ min min å‡½æ•°ä½¿ç”¨ä¸€ä¸ªåˆ—ååšå‚æ•°ä¸”è¿”å›è¿™ä¸ªåˆ—ä¸­æœ€å°çš„å€¼ã€‚å¯¹äº numeric ç±»å‹çš„åˆ—ï¼Œç»“æœåº”è¯¥å’Œé¢„æœŸä¸€ æ ·ã€‚å¯¹äºæ—¶æ€ç±»å‹ï¼Œä¾‹å¦‚ date çš„å€¼ï¼Œå®ƒè¿”å›æœ€å°çš„æ—¥æœŸï¼Œæ—¥æœŸæ—¢å¯ä»¥æ˜¯è¿‡å»ä¹Ÿå¯ä»¥æ˜¯æœªæ¥ã€‚å¯¹äºå˜é•¿çš„å­—ç¬¦ä¸²ï¼ˆvarchar ç±» å‹ï¼‰ï¼Œç»“æœå¯èƒ½å’Œé¢„æœŸæœ‰ç‚¹ä¸åŒï¼šå®ƒåœ¨å­—ç¬¦ä¸²å³è¾¹æ·»åŠ ç©ºç™½åå†è¿›è¡Œæ¯”è¾ƒã€‚ min å‡½æ•°å¿½ç•¥ NULL å€¼ã€‚å¿½ç•¥ NULL å€¼ æ˜¯æ‰€æœ‰çš„èšé›†å‡½æ•°çš„ä¸€ä¸ªç‰¹ç‚¹ï¼Œé™¤äº† count(*)ï¼ˆå½“ç„¶ï¼Œæ˜¯å¦ä¸€ä¸ªç”µè¯å·ç æ˜¯æœ€å°å€¼åˆæ˜¯å¦ä¸€ä¸ªé—®é¢˜äº† PSï¼šå°å¿ƒåœ¨ varchar ç±»å‹çš„åˆ—ä¸­ä½¿ç”¨ min æˆ–è€… maxï¼Œå› ä¸ºç»“æœå¯èƒ½ä¸æ˜¯ä½ é¢„æœŸçš„ã€‚ max sum Sum å‡½æ•°ä½¿ç”¨ä¸€ä¸ªåˆ—åä½œä¸ºå‚æ•°å¹¶æä¾›åˆ—çš„å†…å®¹çš„åˆè®¡ã€‚å’Œ min å’Œ max ä¸€æ ·ï¼ŒNULL å€¼è¢«å¿½ç•¥ã€‚ å’Œ count ä¸€æ ·ï¼Œsum å‡½æ•°æ”¯æŒ DISTINCT å˜ä½“ã€‚ä½ å¯ä»¥è®©å®ƒåªç»Ÿè®¡ä¸é‡å¤å€¼çš„å’Œï¼Œæ‰€ä»¥å¤šæ¡å€¼ç›¸åŒçš„è¡Œåªä¼šè¢«åŠ ä¸€ æ¬¡ avg æˆ‘ä»¬è¦çœ‹çš„æœ€åä¸€ä¸ªèšé›†å‡½æ•°æ˜¯ avgï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªåˆ—ååšå‚æ•°å¹¶è¿”å›è¿™ä¸ªåˆ—æ•°å€¼çš„å¹³å‡å€¼ã€‚å’Œ sum ä¸€æ ·ï¼Œå®ƒå¿½ç•¥ NULL å€¼ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹ â€‹ 3.2 å­æŸ¥è¯¢ é—®é¢˜ä¸€ æ‰¾åˆ°ä»·æ ¼æ¯”å¹³å‡ä»·æ ¼é«˜çš„å•†å“é¡¹ç›® æ–¹æ³•ä¸€ï¼ˆåœŸæ–¹æ³•ï¼‰ æ–¹æ³•äºŒï¼ˆç”¨åµŒå¥—WHEREä»å¥ï¼‰ é—®é¢˜äºŒ æ‰¾åˆ°é‚£äº›æˆæœ¬é«˜äºå¹³å‡æˆæœ¬ä½†å”®ä»·ä½äºå¹³å”®ä»·çš„äº§å“ æ–¹æ³•ä¸€ï¼ˆåœŸæ–¹æ³•ï¼‰ æ–¹æ³•äºŒï¼ˆç”¨åµŒå¥—WHEREä»å¥ï¼‰ é—®é¢˜ä¸‰ - è¿”å›å¤šè¡Œè®°å½•çš„å­æŸ¥è¯¢ ä¹‹å‰çš„ä¸¤ä¸ªé—®é¢˜ä¸­ï¼ŒWHEREä¸­çš„å­æŸ¥è¯¢ä¸­çš„SELECTå­—å¥è¿”å›çš„æœ€ååªæœ‰ä¸€ä¸ªå€¼â€”â€”å› ä¸ºç”¨äº†count()èšé›†å‡½æ•°ã€‚å¦‚æœWHEREä¸­çš„SELECTå­å¥è¿”å›å¤šä¸ªç»“æœå€¼å‘¢ï¼Ÿ ç­”æ¡ˆæ˜¯ç”¨ WHERE column_name IN (RESULTS) å½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨NOT IN æ¥æ’å‡ºé€‰é¡¹ 3.3 ç›¸å…³å­æŸ¥è¯¢â€‹ åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œè¿™é‡Œçš„ä¸¤ä¸ªSELECTå®é™…ä¸Šæ˜¯ä¸ç›¸å…³çš„ï¼Œä¹Ÿå°±æ˜¯åœ¨å†…éƒ¨çš„SELECTçš„ç»“æœåŸºç¡€ä¸Šï¼Œå¤–éƒ¨SELECTå†åšç»§ç»­æŸ¥è¯¢ â€‹ ä½†æ˜¯ç›¸å…³å­æŸ¥è¯¢åˆ™æ˜¯å†…å¤–çš„SELECTä¸­ï¼Œè¡¨ä¸è¡¨ä¹‹é—´æ˜¯æœ‰å…³ç³»çš„ æ ¼å¼ PSï¼šå»ºè®®åœ¨ç›¸å…³å­æŸ¥è¯¢ä¸­ä½¿ç”¨è¡¨çš„åˆ«å 3.4 UNIONé“¾æ¥ æ ¼å¼ 12345SELECT town FROM tcustUNIONSELECT town FROM customer;SELECT town, zipcode FROM tcust UNION SELECT town, zipcode FROM customer; PSï¼šUNION è¿æ¥çš„ä½¿ç”¨æœ‰ä¸€äº›é™åˆ¶ã€‚ä½ è¦è¿æ¥çš„ä¸¤ä¸ªä»ä¸¤ä¸ªè¡¨ä¸­æŸ¥æ‰¾åˆ—è¡¨çš„åˆ—å¿…é¡»æœ‰ç›¸åŒåˆ—æ•°ï¼Œè€Œä¸”é€‰æ‹©çš„æ¯ä¸ªåˆ—å¿…é¡»éƒ½æœ‰ç›¸å…¼å®¹çš„ç±»å‹ã€‚ è¿™ä¸ªæŸ¥è¯¢ï¼Œè™½ç„¶éå¸¸æ— æ„ä¹‰ï¼Œä½†æ˜¯æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸º PostgreSQL å¯ä»¥è¿æ¥è¿™ä¸¤ä¸ªåˆ—ï¼Œå³ä½¿ title æ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„åˆ—è€Œ town æ˜¯ä¸€ä¸ªå˜é•¿çš„åˆ—ï¼Œå› ä¸ºä»–ä»¬éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ã€‚ä¾‹å¦‚å¦‚æœæˆ‘ä»¬å°è¯•è¿æ¥ customer_id å’Œ townï¼ŒPostgreSQL ä¼šå‘Šè¯‰æˆ‘ä»¬ æ— æ³•åšåˆ°ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªåˆ—çš„ç±»å‹ä¸åŒã€‚ 3.5 è‡ªè¿æ¥ 3.6 å¤–é“¾æ¥4. è¡¨çš„ç®¡ç†â€‹ 5. äº‹åŠ¡å’Œé”","tags":[{"name":"SQL","slug":"SQL","permalink":"http://chenson.com/tags/SQL/"}]},{"title":"Machine Learning - Linear Regression and Logistic Regression","date":"2017-03-17T01:06:30.000Z","path":"2017/03/17/Machine-Learning-Linear-Regression-and-Logistic-Regression/","text":"1. ä¸‰è¦ç´ å½“ä¸€å¼€å§‹æ¥è§¦Andrewåœ¨Courseraä¸Šçš„MLå…¬å¼€è¯¾çš„æ—¶å€™ï¼Œå¯¹çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’è¿™ä¸¤ç§æ¨¡å‹æœ‰ä¸ªå¤§ä½“çš„è®¤è¯†ã€‚ä½†æ˜¯åœ¨ä¸Šå®Œcs229çš„å‰ä¸‰èŠ‚è¯¾ï¼Œåˆæ­¥äº†è§£äº†è¿™ä¸¤ç§æ¨¡å‹èƒŒåçš„æ•°å­¦æ¨¡å‹ï¼ŒLinear Regressionå’ŒLogistic RegressionèƒŒåçš„æ¦‚ç‡åˆ†å¸ƒï¼Œäº†è§£åˆ°äº†è¿™ä¸¤ç§æ¦‚ç‡åˆ†å¸ƒå…¶å®åªæ˜¯exponential familyä¸­çš„ç‰¹ä¾‹ã€‚ä½†åŒæ—¶ä¹Ÿå¼€å§‹å¯¹ä¸€äº›æ¦‚å¿µæ€§çš„ä¸œè¥¿æ„Ÿè§‰å¾ˆæ¨¡ç³Šï¼Œæ‰€ä»¥è§‰å¾—æœ‰å¿…è¦å¥½å¥½æ•´ç†ä¸€ä¸‹è¿™éƒ¨åˆ†çš„å†…å®¹ã€‚ 1. 1 Hypothesisé¦–å…ˆå¯¹äºæ ·æœ¬æ•°æ®ï¼Œè¾“å…¥xå’Œè¾“å‡ºyä¹‹é—´æ˜¯é€šè¿‡Target functionåœ¨è½¬æ¢çš„ï¼Œä¹Ÿå°±æ˜¯ Target function f(x) = yã€‚ä½†æ˜¯æˆ‘ä»¬å¹¶ä¸çŸ¥é“è¿™ä¸ªf(x)éƒ½æ˜¯æ€æ ·çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å‡è®¾äº†è¿™ä¹ˆä¸€ä¸ªHypothesis functionå»æ¨¡æ‹Ÿè¿™ä¸ªTarget functionï¼Œä½¿å¾—æˆ‘ä»¬ç”¨åŒæ ·çš„è¾“å…¥xä¼šçš„ä¸€ä¸ªé¢„æµ‹å€¼yâ€™ï¼Œä½¿å¾—è¿™ä¸ªyâ€™ä¸æ–­é€¼è¿‘çœŸå®å€¼yã€‚ Linear Regression$$H(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx$$ Logistic Regression$$H(x) = g(\\theta^Tx) = \\frac 1 {1 + e^{-\\theta^Tx}}$$ç­‰ä»·äºï¼ˆå³log oddsï¼Œlogitï¼‰$$ln \\frac y {1 - y} = \\theta^T x = ln \\frac {p(y=1| x; \\theta)} {p(y=0| x; \\theta)}$$ 1. 2 Cost functionCost functionå‘¢ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥å«åšError functionï¼Œå°±æ˜¯ç”¨æˆ‘ä»¬ä¸Šé¢å‡è®¾çš„Hypothe functionæ‰€é¢„æµ‹å‡ºæ¥çš„å€¼yâ€™å’ŒçœŸå®å€¼yä¹‹é—´çš„è¯¯å·®ã€‚è€Œæˆ‘ä»¬éœ€è¦åšçš„æ˜¯æ ¹æ®å‡è®¾å‡ºçš„Hypothesis functionï¼Œå–ä¸€ä¸ªåˆé€‚çš„æƒé‡å€¼ï¼Œå³thetaçš„å€¼ï¼Œä½¿å…¶å–çš„ä¸€ä¸ªè¾ƒä½çš„costï¼Œä¹Ÿå°±æ˜¯è¿™é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®æœ€å°ã€‚ Ordinary Least Squares (Square Loss Function) å¸¸ç”¨çš„æ–¹æ³•æ˜¯æœ€å°äºŒä¹˜æ³• $$J(\\theta) = \\frac 1 2 \\sum_{i=i}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$ â€‹ å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»æ¦‚ç‡çš„è§’åº¦æ¥ç†è§£è¿™ä¸ªé—®é¢˜$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$â€‹ è¿™é‡Œçš„Ïµæ˜¯æˆ‘ä»¬é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œè¿™ä¸ªé—®é¢˜æˆ‘ä»¬ä¼šç•™åˆ°åé¢é‡ç‚¹è®²è§£ã€‚ 0-1 Loss Function Absolute Loss Function Log Loss Function 1.3 Algorithmè‡³äºæ€ä¹ˆä½¿å¾—ä¸Šé¢çš„cost functionæœ€å°å‘¢ï¼Œå› ä¸ºå¯¹äºæŸäº›æ•°æ®ï¼Œå…¶featuresæœ‰æˆåƒä¸Šç™¾ä¸ªï¼Œæˆ‘ä»¬å¾ˆéš¾å»æ‰¾åˆ°è¿™ä¸ªæœ€å°çš„æå€¼ç‚¹ï¼Œä½¿å¾—cost functionæœ€ä¸‹ï¼Œæ‰€ä»¥è¿™ä¸ªAlgorithmå°±æ˜¯ç”¨æ¥æ‰¾cost functionçš„æœ€å°å€¼çš„ã€‚å¸¸ç”¨çš„æ–¹æ³•æœ‰å¦‚ä¸‹ Gradient Descent åœ¨æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯ LMS update rules(Least Mean Squares)$$\\theta_j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}= \\alpha e^{(i)} x^{(i)}$$ å½“æˆ‘ä»¬çš„é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹å‰çš„è¯¯å·®Ïµå¾ˆå°æ—¶ï¼Œæˆ‘ä»¬å°±åªéœ€è¦å¯¹Î¸åšå‡ºå¾ˆå°çš„è°ƒæ•´ï¼Œåä¹‹ï¼Œè¯´æ˜å½“å‰çš„Î¸ä¸å¯¹ï¼Œéœ€è¦è°ƒæ•´çš„å¹…åº¦æ¯”è¾ƒå¤§ã€‚ç›´åˆ°æœ€åæ”¶æ•›ä¸ºæ­¢ã€‚ ä¸Šé¢Repeatä¸­çš„æ­¥éª¤å®é™…æ˜¯ç­‰åŒäºcost functionå¯¹Î¸æ±‚å¯¼çš„è¿‡ç¨‹ï¼Œæ‰€ä»¥ä¸ºäº†ä¿è¯æ”¶æ•›çš„æ•ˆæœï¼Œcost functionåº”è¯¥æ˜¯è¦ convex fuctionï¼Œå°±ä¸ä¼šå¯¼è‡´åœç•™åœ¨äº†local optimç‚¹ã€‚ æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼š â€‹ å¯è§†åŒ–åå¤§æ¦‚çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š æ ¹æ®å¯¹äºå“ªäº›Î¸æ±‚å¯¼ï¼ŒGradient Descentè¿˜å¯ä»¥ç»§ç»­åˆ†æˆä¸åŒçš„å‡ ç§æ–¹æ³• Batch Gradient Descent Stochastic Gradient Descent Mini Batch Gradient Descent SGD with mini-batch é—®é¢˜ï¼šä¸ºä»€ä¹ˆä¸‹é™æ˜¯ - ï¼ŒÎ¸å¤§Jä¸€å®šå¤§å—ï¼Ÿ Normal Equation (linear regression) æ¨å¯¼è¿‡ç¨‹æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦çš„æ•°å­¦çŸ¥è¯†æ¯”è¾ƒå¤šï¼Œè¿™é‡Œåªç»™å‡ºç»“è®ºã€‚æƒ³è¦çœ‹å…·ä½“æ¨å¯¼è¿‡ç¨‹çš„è¿˜è¯´çœ‹cs229çš„ç¬¬äºŒèŠ‚è¯¾å§ ï¼šï¼‰(cs229-notes-1, p11)$$\\theta = (X^TX)^{-1}X^T\\vec y$$ â€‹ Newton BGFS Simulated Annealing 2. Generalized Linear Model (GLM) å¹¿ä¹‰çº¿æ€§æ¨¡å‹ä¹‹å‰æˆ‘ä»¬åœ¨cost functionä¸­æåˆ°è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¦‚ç‡çš„è§’åº¦æ¥ç†è§£è¯¯å·®è¿™ä¸ªé—®é¢˜ã€‚å¯¹äºLinear Regressionå’ŒLogistic Regressionï¼Œæˆ‘ä»¬éƒ½å¯ä»¥å‡è®¾ï¼š$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$è¿™é‡Œè¯¯å·®Ïµå‡è®¾ä¸ºIID (independently and identically distributed) 2.1 Linear Regressionåœ¨Linear Regressionä¸­ï¼Œyæ˜¯è¿ç»­çš„å€¼ï¼Œæ‰€ä»¥è¯¯å·®Ïµä¹Ÿæ˜¯ä¸€ä¸ªè¿ç»­çš„å€¼ã€‚å‡è®¾è¯¯å·®Ïµæ˜¯ç¬¦åˆGaussian Distribution (Normal Distribution)ï¼Œæ‰€ä»¥æœ‰ Gaussian Distribution $$y | x; \\theta âˆ¼ N (Î¼, Ïƒ^2)$$ Probability of error â€‹ Ïƒå®é™…ä¸Šæ˜¯ä¸å½±å“æ¦‚ç‡çš„åˆ†å¸ƒçš„ï¼Œæ‰€ä»¥å‡è®¾Ïƒ = 1ï¼Œæ‰€ä»¥è¿™é‡Œå¯ä»¥å¿½ç•¥äº†ã€‚å› æ­¤ä¹Ÿå°±æ˜¯ç­‰åŒäºå¦‚ä¸‹ Likelihood$$L(\\theta) = L(\\theta; X, \\vec y) = p(\\vec y | X; \\theta)$$ â€‹ ä»¥ä¸Šæ˜¯åœ¨ç»™å®šè¾“å…¥xå’Œæƒé‡Î¸ä¸‹ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å€¼æ˜¯çœŸå®å€¼yçš„æ¦‚ç‡ï¼Œæ‰€ä»¥è¿™ä¸ªæ¦‚ç‡å‘¢ï¼Œå½“ç„¶æ˜¯è¶Šé«˜è¶Šå¥½å•¦ã€‚æˆ‘ä»¬å°±æ˜¯è¦æƒ³åŠæ³•å» maximum likelihoodã€‚ â€‹ å¯¹è¿™ä¸ªæ¦‚ç‡å–ä¸ªlogï¼ˆä¸å½±å“ç»“æœï¼‰ï¼Œæœ‰ â€‹ å¯ä»¥çœ‹åˆ°æœ€ç»ˆçš„å¼å­é‡Œé¢ï¼Œæˆ‘ä»¬å°±æ˜¯è¦æ±‚cost functionçš„æœ€å°å€¼ã€‚ 2.2 Logistic Regressionåœ¨Logistic Regressionä¸­ï¼Œ yæ˜¯ç¦»æ•£çš„å€¼ {0, 1}ï¼Œæ‰€ä»¥è¯¯å·®Ïµä¹Ÿæ˜¯ä¸€ä¸ªç¦»æ•£çš„å€¼ {0, 1}ã€‚å‡è®¾è¯¯å·®Ïµæ˜¯ç¬¦åˆBernoulli Distributionï¼Œæ‰€ä»¥æœ‰ Bernoulli distribution Probability of error æŠŠè¯¯å·®Ïµä»£å…¥åˆ°ä¸Šé¢çš„bernoulli functionï¼Œå¯å¾— Likelihood â€‹ åŒæ ·ï¼Œå¯¹ä¸Šé¢å»logä¹‹åæœ‰ â€‹ åŒæ ·ï¼Œæˆ‘ä»¬å°½é‡è¦maximize the likelihoodï¼Œå°±ç›¸å½“äºè¦æœ€å°åŒ–åé¢çš„é‚£éƒ¨åˆ†ï¼ˆcost functionï¼‰ã€‚è¿™é‡Œå¯ä»¥ç”¨Gradient Ascentç®—æ³•æ¥æ±‚æœ€å¤§å€¼ï¼Œä½†æ˜¯å’ŒGradient Descentä¸ä¸€æ ·ï¼ˆä¸ºä»€ä¹ˆï¼‰$$\\theta := \\theta + \\alphaâˆ‡_\\theta l(\\theta)$$â€‹ æ³¨æ„è¿™é‡Œæ˜¯åŠ å·ï¼Œåœ¨æ±‚cost functionçš„æœ€å°å€¼æ—¶ï¼Œç”¨çš„æ˜¯å‡å·ã€‚ â€‹ å¯¹å…¶æ±‚å¯¼å¯å¾— â€‹ æ‰€ä»¥æœ‰ â€‹ Digression Perception ï¼Ÿ ï¼Ÿ ï¼Ÿ 2.3 GLMä¼¼ä¹åˆ°ç°åœ¨ï¼Œè®²äº†åŠå¤©éƒ½ä¹Ÿæ²¡è®²ä»€ä¹ˆæ˜¯å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œå®é™…ä¸Šå‘¢ï¼Œä¸Šé¢æˆ‘ä»¬å·²ç»ä»è¯¯å·®æ¦‚ç‡çš„è§’åº¦ä¸Šæ¥åˆ†æäº†çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’ä¸¤ç§ç‰¹ä¾‹ï¼Œå› ä¸ºä»–ä»¬è¯¯å·®æœä»çš„æ¦‚ç‡åˆ†å¸ƒéƒ½æ˜¯å±äºExponential Familyä¸­çš„ä¸€ç§ã€‚ The Exponential Family Î· è¢«ç§°ä½œnatural parameterï¼Œå®ƒæ˜¯æŒ‡æ•°åˆ†å¸ƒæ—å”¯ä¸€çš„å‚æ•°T(y) è¢«ç§°ä½œsufficient statisticï¼Œå¾ˆå¤šæƒ…å†µä¸‹T(y)=y a(Î·) è¢«ç§°ä½œ log partition functionTå‡½æ•°ã€aå‡½æ•°ã€bå‡½æ•°å…±åŒç¡®å®šä¸€ç§åˆ†å¸ƒ é‚£ä¹ˆè¿™ä¸ªæ¨¡å‹å’Œä¸Šé¢æˆ‘ä»¬æåˆ°è¿‡çš„Gaussian Distribution å’ŒBernoulli Distributionæœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿå…¶å®ä¸Šé¢çš„è¿™å‡ ä¸ªå‚æ•°å–ä¸åŒçš„å€¼çš„æ—¶å€™ï¼Œå³å¯å¾—åˆ°ä¸åŒçš„åˆ†å¸ƒæ¨¡å‹ Gaussian Distribution Bernolli Distribution 2.4 å¦‚ä½•æ„å»ºä¸€ä¸ªGLMæ¨¡å‹åœ¨ä¸Šé¢æˆ‘ä»¬åªæ˜¯çœ‹åˆ°äº†ä¸€ä¸ªé€šç”¨çš„GLMæ¦‚ç‡æ¨¡å‹ å®é™…ä¸Šå¯¹äºæ„å»ºè¿™ä¹ˆä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œéœ€è¦ä½œå‡ºä¸‰ä¸ªå‡è®¾ä½œä¸ºå‰ææ¡ä»¶ï¼š p(y | x; Î¸) âˆ¼ ExponentialFamily(Î·). å¯¹äºç»™å®šçš„è¾“å…¥xï¼ŒÎ¸å’Œè¾“å‡ºyéœ€è¦æœä»æŸä¸€ç§æŒ‡æ•°åˆ†å¸ƒï¼Œè¿™ä¸ªæŒ‡æ•°åˆ†å¸ƒç”±Î· å†³å®šçš„ å¯¹äºç»™å®šçš„è¾“å…¥xï¼Œé¢„æµ‹T(y)çš„å€¼ï¼Œä¸”ç»å¸¸T(y) = yã€‚è€Œæˆ‘ä»¬æ˜¯é¢„æµ‹æ˜¯H(x) éœ€è¦æ»¡è¶³ H(x) = E[y|x] å¯¹äºè‡ªç„¶å‚æ•°Î·å’Œè¾“å…¥xä¹‹é—´ï¼Œéœ€è¦å­˜åœ¨ç›¸å…³æ€§å…³ç³»çš„ï¼Œå³ï¼šÎ· = Î¸T x","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"å­—ç¬¦ä¸²æœç´¢ç®—æ³• - KMP","date":"2017-03-15T06:48:47.000Z","path":"2017/03/15/å­—ç¬¦ä¸²æœç´¢ç®—æ³•-KMP/","text":"1. ç†è§£ä¸€ï¼šéƒ¨åˆ†åŒ¹é…è¡¨+å·²åŒ¹é…æ•°å­—1.1 Partial Match Table ä¸Šæ¥å…ˆä¸Šä¸ªç»“è®ºï¼Œè¿™ä¸ªå…ˆæš‚æ—¶ä¸ç®¡æ€ä¹ˆç”Ÿæˆï¼Œç”¨äºKMPè¡¨çš„ç§»åŠ¨ã€‚ ç§»åŠ¨ä½æ•° = å·²åŒ¹é…çš„å­—ç¬¦æ•° - å¯¹åº”çš„éƒ¨åˆ†åŒ¹é…å€¼ åŒ¹é…åˆ°äº†ç¬¬å…­ä¸ªå­—ç¬¦Bï¼ŒBåœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯2ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯6 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 6 - 2 = 4ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨4ä½ã€‚ åŒ¹é…åˆ°äº†ç¬¬ä¸‰ä¸ªå­—ç¬¦Cï¼ŒCçš„å‰ä¸€ä¸ªå­—ç¬¦Båœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯0ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯2 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 2 - 0 = 2ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨2ä½ å› ä¸ºç¬¬ä¸€ä¸ªå­—ç¬¦ä¸åŒ¹é…ï¼Œå°±å°†æ•´ä¸ªå­—ç¬¦ä¸²å‘åç§»ä¸€ä½ åŒ¹é…åˆ°äº†ç¬¬å…­ä¸ªå­—ç¬¦Bï¼ŒBåœ¨ä¸Šè¡¨ä¸­çš„å€¼æ˜¯2ï¼Œå·²ç»åŒ¹é…çš„å­—ç¬¦æ•°æ˜¯6 æ‰€ä»¥ç§»åŠ¨çš„ä½æ•°æ˜¯ 6 - 2 = 4ï¼Œå°†æœç´¢è¯å‘åç§»åŠ¨4ä½ é€ä¸ªæ¯”è¾ƒï¼Œç›´åˆ°å®Œå…¨åŒ¹é… å¦‚æœè¿˜éœ€è¦ç»§ç»­æœç´¢çš„è¯ï¼ŒDåœ¨ä¸Šè¡¨ä¸­çš„å€¼ä¸º0ï¼ŒåŒ¹é…åˆ°çš„ä¸ªæ•°ä¸º7ï¼Œç§»åŠ¨çš„ä½æ•°= 7 - 0 = 7ï¼Œå°†æ•´ä¸ªå­—ç¬¦ä¸²å¾€åç§»åŠ¨7ä½ã€‚æ¥ç€å°±æ˜¯é‡å¤ä¹‹å‰çš„æ¯”è¾ƒæ­¥éª¤äº†ã€‚ 1.2 è®¡ç®— Partial Match Table â€‹ è¿™é‡Œéœ€è¦ç†è§£ä¸¤ä¸ªæ¦‚å¿µï¼šå‰ç¼€å’Œåç¼€ â€‹ â€œå‰ç¼€â€ æŒ‡é™¤äº†æœ€åä¸€ä¸ªå­—ç¬¦ä»¥å¤–ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²çš„å…¨éƒ¨å¤´éƒ¨ç»„åˆï¼› â€‹ â€œåç¼€â€ æŒ‡é™¤äº†ç¬¬ä¸€ä¸ªå­—ç¬¦ä»¥å¤–ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²çš„å…¨éƒ¨å°¾éƒ¨ç»„åˆã€‚ â€‹ è€Œæˆ‘ä»¬éœ€è¦çš„Partial Match Tableå°±æ˜¯å‰ç¼€å’Œåç¼€çš„æœ€é•¿å…±æœ‰å…ƒç´ çš„é•¿åº¦ â€‹ ç»§ç»­ä»¥ä¸Šé¢çš„ä¾‹å­è®²è§£ â€œAâ€çš„å‰ç¼€å’Œåç¼€éƒ½ä¸ºç©ºé›†ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABâ€çš„å‰ç¼€ä¸º[A]ï¼Œåç¼€ä¸º[B]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABCâ€çš„å‰ç¼€ä¸º[A, AB]ï¼Œåç¼€ä¸º[BC, C]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦0ï¼›â€œABCDâ€çš„å‰ç¼€ä¸º[A, AB, ABC]ï¼Œåç¼€ä¸º[BCD, CD, D]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ï¼›â€œABCDAâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD]ï¼Œåç¼€ä¸º[BCDA, CDA, DA, A]ï¼Œå…±æœ‰å…ƒç´ ä¸ºâ€Aâ€ï¼Œé•¿åº¦ä¸º1ï¼›â€œABCDABâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD, ABCDA]ï¼Œåç¼€ä¸º[BCDAB, CDAB, DAB, AB, B]ï¼Œå…±æœ‰å…ƒç´ ä¸ºâ€ABâ€ï¼Œé•¿åº¦ä¸º2ï¼›â€œABCDABDâ€çš„å‰ç¼€ä¸º[A, AB, ABC, ABCD, ABCDA, ABCDAB]ï¼Œåç¼€ä¸º[BCDABD, CDABD, DABD, ABD, BD, D]ï¼Œå…±æœ‰å…ƒç´ çš„é•¿åº¦ä¸º0ã€‚ äº†è§£äº†KMPçš„åŸç†ä¹‹åï¼Œæ¥çœ‹ä¸€ä¸‹ä»£ç è¯¥æ€ä¹ˆå†™ã€‚ ä¸¾ä¸ªæ —å­ï¼š Text = a b a c a a b a c c a b a c a b a a b b Pattern = a b a c a b æ ¹æ®å‰é¢çš„Partial Match Table, æˆ‘ä»¬å¯ä»¥ç®—å‡ºPatternçš„è¿™ä¸ªè¡¨ P a b a c a b steps 0 0 1 0 1 2 æ­¤æ—¶æˆ‘ä»¬ç”¨ä¸¤ä¸ªæŒ‡é’ˆ i å’Œ j æ¥è¡¨ç¤º Text å’Œ Pattern ä¸­çš„å­—ç¬¦ã€‚ å½“ T[ i : i + j ] == P[ 1 : j ] çš„æ—¶å€™ï¼Œå°±æ˜¯ Text ä¸­åŒ…å«äº†æˆ‘ä»¬éœ€è¦æŸ¥æ‰¾çš„ Pattern å…ˆè®© i å’Œ j éƒ½ä» 1 å¼€å§‹ï¼ˆpythonä»£ç ä¸­ä»0å¼€å§‹ï¼‰ å½“T[ i ] = P[ j ]çš„æ—¶å€™ï¼Œæ­¤æ—¶æŒ‡é’ˆåœ¨ Text å’Œ Pattern ä¸Šéƒ½å¾€å‰å„èµ°ä¸€æ­¥ï¼Œå³j+1ï¼Œi+1 å½“ i = 6ï¼Œj = 6 çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºT[ i ] != P[ j ]ï¼Œæ­¤æ—¶ j å°±ä¸èƒ½å†ç»§ç»­å¾€å‰èµ°äº†ï¼Œéœ€è¦é€€å›å»å‡ æ­¥ã€‚ é‚£ä¹ˆåˆ°åº•æ˜¯å‡ æ­¥å‘¢ï¼Œç»è¿‡ä¸Šé¢æŸ¥è¡¨ï¼Œæ­¤æ—¶åŒ¹é…åˆ°5ï¼Œé‡å¤çš„å­—ç¬¦ä¸²ä¸ªæ•°ä¸º1ï¼Œæ„æ€æ˜¯å¯¹äºè¿™ä¸ªå­—ç¬¦ä¸² abacaï¼Œabaca å’Œ abaca ä¸­æœ‰ä¸€ä¸ªé‡å¤äº†ï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦å†æ¯”è¾ƒè¿™ä¸ªï¼Œè·³è¿‡è¿™ä¸ªå­—ç¬¦ï¼Œç§»åŠ¨çš„ä¸ªæ•°ä¸º 6 - 1 = 5ï¼Œå°†å­—ç¬¦ä¸² Pattern å‘å‰æŒª5ä½ï¼Œæ–°çš„ j å°±ç­‰äº1äº†ï¼Œç„¶åé‡å¤ä¹‹å‰çš„æ­¥éª¤ã€‚ 1.3 Pythonä»£ç 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091DEBUG = TrueDEBUG = False# ç”Ÿæˆ partial match tabledef PMT(sent): length = len(sent) result = [0] for i in range(2, length + 1): sub_sent = sent[0 : i] sub_len = len(sub_sent) surfix = [] prefix = [] for j in range(sub_len): prefix.append(sub_sent[0 : j]) surfix.append(sub_sent[j + 1: sub_len ]) prefix.remove('') surfix.remove('') all_fix = list(set(prefix + surfix)) max_len = 0 com_str = '' for fix in all_fix: if fix in prefix and fix in surfix: if len(fix) &gt; max_len: max_len = len(fix) com_str = fix result.append(max_len) return result# KMPç®—æ³•def KMP(text, pattern): if (len(text) &lt; len(pattern)): return 'Not Match' pmt = PMT(pattern) if DEBUG: print('Current Pattern is = ', pattern) print('Partial Match Table = ', pmt) i = 0 j = 0 while(i &lt; len(text)): if (text[i] == pattern[j]): if (j == (len(pattern) - 1)): return 'Match, the position in text is: &#123;&#125; - &#123;&#125; = &#123;&#125;'.format(i - j, i, text[i-j: i]) i += 1 j += 1 continue else: if j &gt;= 1: j = pmt[j - 1] else: i += 1 return 'Not Match'# æµ‹è¯•å‡½æ•°def test(text, pattern): result = KMP(text, pattern) print('Resutl = ', result) print('Pattern =', pattern) if DEBUG: for i in range(len(text)): print('&#123;:3s&#125;'.format(str(i)), end='-') print('') for i in range(len(text)): print('&#123;:3s&#125;'.format(text[i]), end='-') print('') print('\\n')# æµ‹è¯•éƒ¨åˆ†text = 'BBC ABCDAB ABCDABCDABDE'pattern = 'ABCDABD'test(text, pattern)text = 'a b a c a a b a c c a b a c a b a a b b'pattern = 'a b a c a b'test(text, pattern)text = 'BBC ABCDAB ABCDABCDABE'pattern = 'ABCDABD'test(text, pattern)text = 'BBC AB'pattern = 'ABCDABD'test(text, pattern)text = 'ABCDABD'pattern = 'ABCDABD'test(text, pattern) 1.4 æµ‹è¯•ç»“æœ12345678910111213141516171819202122232425Text = BBC ABCDAB ABCDABCDABDEPattern = ABCDABDResutl = Match, the position in text is: 15 - 21 = ABCDABText = a b a c a a b a c c a b a c a b a a b bPattern = a b a c a bResutl = Match, the position in text is: 20 - 30 = a b a c a Text = BBC ABCDAB ABCDABCDABEPattern = ABCDABDResutl = Not MatchText = BBC ABPattern = ABCDABDResutl = Not MatchText = ABCDABDPattern = ABCDABDResutl = Match, the position in text is: 0 - 6 = ABCDAB[Finished in 0.1s] 1.5 å‚è€ƒå†…å®¹é˜®ä¸€å³° - å­—ç¬¦ä¸²åŒ¹é…çš„KMPç®—æ³• 2. ç†è§£äºŒï¼šéƒ¨åˆ†åŒ¹é…è¡¨+idx2.1 Partial Match Tableé¦–å…ˆç”Ÿæˆå’Œä¸Šé¢ä¸€æ ·çš„å‰ç¼€åç¼€è¡¨(0-base å’Œ 1-base) 2.2 Example 0-base æ‰¾åˆ°åŸæ–‡å’Œæœç´¢è¯ä¸åŒçš„é‚£ä¸ªå­—ç¬¦é‡Œé¢çš„åŒ¹é…å€¼ï¼Œç„¶åæŠŠæœç´¢å­—ç¬¦ä¸²å³ç§»åˆ°idx=åŒ¹é…å€¼çš„ä½ç½®ï¼Œè¿‡ç¨‹å¦‚ä¸‹å›¾ 1-base ä¸0-baseä¸åŒçš„æ˜¯ï¼Œæ‰¾çš„ä¸æ˜¯æœ€åä¸€ä¸ªä¸åŒçš„å­—ç¬¦ è€Œæ˜¯æœ€åä¸€ä¸ªç›¸åŒçš„å­—ç¬¦é‡Œé¢çš„åŒ¹é…å€¼ï¼Œç„¶åæŠŠæœç´¢å­—ç¬¦ä¸²å³ç§»åˆ°idx=åŒ¹é…å€¼çš„ä½ç½®ï¼Œè¿‡ç¨‹å¦‚ä¸‹å›¾","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸€ï¼‰","date":"2017-03-06T13:44:33.000Z","path":"2017/03/06/Hadoopæƒå¨æŒ‡å—ç¬”è®°ï¼ˆä¸€ï¼‰/","text":"1. Hadoop1.1 åˆè¯†Hadoopéå¸¸å¥½çš„Tutorial åœ¨å­¦ä¹ hadoopä¹‹å‰ï¼Œæˆ‘è§‰å¾—æœ‰å¿…è¦äº†è§£ä¸€ä¸‹hadoopçš„åŸºæœ¬æ„æˆä»¥åŠä¸€äº›æœ¯è¯­ã€‚ Block HDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks. By making a block large enough, the time to transfer the data from the diskcan be made to be significantly larger than the time to seek to the start of the block.Thus the time to transfer a large file made of multiple blocks operates at the disk transferrate.A quick calculation shows that if the seek time is around 10 ms, and the transfer rateis 100 MB/s, then to make the seek time 1% of the transfer time, we need to make theblock size around 100 MB. The default is actually 64 MB, although many HDFS installationsuse 128 MB blocks. This figure will continue to be revised upward as transferspeeds grow with new generations of disk drives.This argument shouldnâ€™t be taken too far, however. Map tasks in MapReduce normallyoperate on one block at a time, so if you have too few tasks (fewer than nodes in thecluster), your jobs will run slower than they could otherwise. Node ç®€å•çš„è¯´å°±æ˜¯ä¸€å°ä¸»æœºï¼Œä¸€å°ç”µè„‘ã€‚åœ¨hadoopä¸­ï¼Œæœ‰NameNode, DataNode, Secondary NameNode, JobTracker Node, TaskTracker Node, CheckpointNode å’Œ BackupNodeã€‚å¯¹ä¸€ä¸ªclusterï¼ŒNameNodeåªèƒ½æœ‰ä¸€ä¸ªï¼ŒDataNodeå¯ä»¥æœ‰å¤šä¸ª Rack ä¸­æ–‡æœºæŸœ/æœºæ¶ï¼Œå°±æ˜¯ç”¨æ¥å­˜æ”¾nodeçš„storageï¼Œé€šå¸¸ä¸€ä¸ªrackæœ‰å‡ åä¸ªnodesç»„æˆï¼Œè¿™äº›nodeså­˜æ”¾åœ¨åŒä¸€ä¸ªæœºæŸœï¼Œè¿æ¥ä¸€ä¸ªäº¤æ¢æœº A Node is simply a computer. This is typically non-enterprise, commodity hardware for nodes that contain data. Storage of Nodes is called as rack. A rack is a collection of 30 or 40 nodes that are physically stored close together and are all connected to the same network switch. Network bandwidth between any two nodes in rack is greater than bandwidth between two nodes on different racks.A Hadoop Cluster is a collection of racks. 1.2 Major Components Distributed Filesystem â€” hadoopä¸­æ˜¯HDFS MapReduce 1.5 Install configuration123456# å¾…æ•´ç†hadoop fshadoop dfs# fs refers to any file system, it oucld be local or HDFS# but dfs refers to only HDFS file system 2. MapReduce2.1 åˆè¯†MapReduce æ•´ä¸ªè¿‡ç¨‹å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼ŒInputï¼Œ MapReduce and Output åœ¨inputå’Œoutputé˜¶æ®µï¼Œæ•°æ®æ˜¯å­˜åœ¨HDFSæ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œå…¶ç³»ç»Ÿçš„block sizeå¤§å°é»˜è®¤æ˜¯64/128MBã€‚ åœ¨MapReduceä¸­ï¼Œåˆå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼ŒMap and Reduceï¼Œæ•°æ®ä»map functionåˆ°reduce functionæ˜¯å­˜åœ¨local diskä¸­ï¼Œ(soreing in HDFS with replication would be overkill)ï¼Œç„¶åé€šè¿‡networkä¼ è¾“æ•°æ®. åœ¨æ¯ä¸ªé˜¶æ®µä¸­ï¼Œinputå’Œoutputçš„æ•°æ®éƒ½æ˜¯ä»¥ (key, values) æ ¼å¼è¿›è¡Œå¤„ç†çš„ï¼Œç„¶åé€šè¿‡ map function å’Œ reduce function è¿›è¡Œå¤„ç†ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œinput dataçš„keyæ˜¯ä»æ•°æ®æ–‡ä»¶å¼€å§‹å¤„çš„è¡Œæ•°çš„åç§»é‡ï¼Œä½†æ˜¯map functionè¾“å‡ºçš„keyæ˜¯å¹´ä»½æ•°æ®ï¼Œä»¥åŠreduce functionè¾“å‡ºçš„keyæ˜¯ä¹Ÿä¸åŒçš„ã€‚æ‰€ä»¥è¿™ä¸‰ä¸ªkey-value pairsæ˜¯ä¸åŒçš„ã€‚ åŸå§‹æ•°æ® Key-Values ä»¥ä¸Šä¸ºåŸå§‹æ•°æ®ä¸­inputè¿›æ¥åçš„key-valuesçš„æ•°æ®ã€‚ç„¶åmap functioné˜¶æ®µï¼Œæå–å‡ºä¸Šé¢æ–‡ä»¶ä¸­çš„ 1950 å’Œ 0001 ä¹‹ç±»çš„æ•°æ®ï¼Œç»„æˆæ–°çš„key-valuesä½œä¸ºè¾“å‡ºç»™ä¸‹ä¸€é˜¶æ®µã€‚ Key-Values in Map Function åœ¨å°†Map Functionçš„è¾“å‡ºä¼ ç»™Reduce Functionä¹‹å‰ï¼Œå®é™…ä¸ŠMapReduce Frameworkè¿˜æ˜¯æœ‰å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªå¤„ç†æ­¥éª¤ã€‚ä»æœ€ä¸Šçš„å›¾ä¸€ä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥çœ‹åˆ°Mapå’ŒReduceä¹‹é—´æœ‰ä¸€ä¸ª Shuffle çš„è¿‡ç¨‹ã€‚å› ä¸ºä¹‹å‰æˆ‘ä»¬æåˆ°äº†ï¼ŒMapçš„è¿‡ç¨‹ä¸­ï¼Œåªæ˜¯å®ç°äº†ä¸€ä¸ªkey-valueåŒ¹é…çš„è¿‡ç¨‹ï¼Œæ‰€æœ‰å‡ºæ¥çš„æ•°æ®ä¹Ÿæ˜¯æ— åºçš„ï¼Œè€Œ Shuffle å°±æ˜¯å¯¹è¿™ä¸ªè¾“å‡º sort &amp; group çš„è¿‡ç¨‹ï¼Œç„¶åå°†è¾“å‡ºä¼ ç»™ Reduce Function è¿›è¡Œå¤„ç† å½“æ•°æ®ä»Reduce Functionä¸­å¤„ç†å®Œåå‡ºæ¥çš„å¤§æ¦‚å¦‚ä¸‹ï¼Œæ³¨æ„è¿™ä¸ªreduceåªæ˜¯é€‰æ‹©æœ€å¤§å€¼ï¼Œå…¶ä»–reduce functionå¯èƒ½åšçš„æ˜¯ç»Ÿè®¡æˆ–è€…å®ç°å…¶ä»–åŠŸèƒ½ã€‚ ç°åœ¨å†çœ‹å¦å¤–ä¸€ä¸ªç»å…¸çš„WordCountçš„ä¾‹å­ åœ¨Hadoopç³»ç»Ÿä¸­ï¼Œå¤„ç†ä¸€ä¸ªwordcountçš„ä»»åŠ¡å¯ä»¥å¤§è‡´åˆ†æˆå››ä¸ªä¸»è¦é˜¶æ®µï¼Œinputï¼Œmapï¼Œreduceï¼Œoutputã€‚å…¶ä¸­ Map å’Œ Reduce å¯ä»¥ç»§ç»­ç»†åˆ†ï¼Œå³åˆ†æˆå¤šä¸ª map tasks å’Œ reduce tasksã€‚ è¿™äº›tasksç„¶åè¢« YARN ç»™åˆ†é…é›†ç¾¤ä¸­å¤šå°ä¸åŒçš„æœºå™¨å¤„ç†ã€‚è¿™å…¶ä¸­çš„ç»†èŠ‚ç­‰åˆ°å¾€åå†è®¨è®ºã€‚ ä¸Šé¢æåˆ°çš„åˆ†æˆå¤šä¸ªtasksæ—¶ï¼Œåº”è¯¥æ˜¯input dataåˆ‡ç‰‡åˆ†ç»™å¤šä¸ªmapsï¼ˆè€Œä¸æ˜¯ä¸€ä¸ªå¤§çš„mapåˆ†æˆå¤šä¸ªå°çš„tasksï¼‰ï¼Œ æ¯ä¸ªMapReduceåˆ†åˆ°ä¸€ä¸ªfixed-sized çš„æ•°æ®ï¼Œé€šå¸¸æ˜¯64/128MBï¼Œè¿™ä¸ªè¿‡ç¨‹å«åš input splitsã€‚ç„¶åæ¯ä¸ªsplitåˆ†é…ä¸€ä¸ªmap taskï¼ŒåŒæ—¶è¿è¡Œåœ¨ä¸åŒçš„æœºå™¨ä¸Šå¤„ç†ã€‚è¿™æ ·åˆ’åˆ†çš„å¥½å¤„æ˜¯æœ‰åˆ©äºload-balancingï¼Œå¯¹äºæ€§èƒ½è¾ƒå¥½çš„æœºå™¨å¯ä»¥å¤„ç†æ›´è¿‡æ˜¯splitsã€‚ 2.2 Data Flow ä¸Šå›¾å¯ä»¥çœ‹å‡ºhadoopçš„æ•´ä¸ªæ•°æ®æµå‘ï¼Œå…¶ä¸­è™šçº¿ä»£è¡¨æ˜¯åœ¨ä¸€ä¸ªnodeï¼Œå®çº¿ä»£è¡¨çš„æ˜¯ä¸åŒnodeä¹‹é—´ã€‚åœ¨åŒä¸€ä¸ªnodeä¹‹é—´ï¼Œæ•°æ®çš„è¯»å–å­˜å‚¨å°±æœ‰é€Ÿåº¦ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸åŒnodeä¹‹é—´ï¼Œä¹Ÿå°±æ˜¯ä¸åŒä¸»æœºä¹‹é—´ï¼Œå°±å¿…é¡»é€šè¿‡networkè¿›è¡Œä¼ è¾“ï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚ Partition å½“åªæœ‰ä¸€ä¸ªreduceçš„æ—¶å€™ï¼Œmap functionçš„outputå½“ç„¶å°±ç›´æ¥ä¼ ç»™è¿™ä¸ªreduceäº†ã€‚ä½†æ˜¯å½“æœ‰å¤šä¸ªreduceçš„æ—¶å€™ï¼Œæ€ä¹ˆåŠå‘¢ï¼Ÿæ­¤æ—¶mapä¼šå°†å…¶è¾“å‡ºè¿›è¡Œpartition(åˆ†åŒº)ï¼Œæ¯ä¸€ä¸ªreduceçš„ä»»åŠ¡éƒ½ä¼šåˆ›å»ºä¸€ä¸ªåˆ†åŒºï¼Œä¸”æ¯ä¸€ä¸ªreduce taskéƒ½ä¼šæœ‰ä¸€ä¸ªpartition (There can be many keys (and their associated values)in each partition, but the records for any given key are all in a single partition)ï¼Œä¹Ÿå°±æ˜¯è¯´åŒä¸€ä¸ªkeyä¼šåœ¨åŒä¸€ä¸ªpartitionä¸­ã€‚ Shuffle and Sort åœ¨mapå’Œreduceä¹‹é—´çš„data flowæ˜¯Shuffleï¼Œä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªreduceå¯ä»¥æ¥å—æ¥è‡ªå¤šä¸ªä¸åŒçš„mapçš„outputï¼Œå…¶ä¸­åŒ…å«äº†sortï¼Œpartitionç­‰è¿‡ç¨‹ã€‚ Combiner Functions ä¹‹å‰æˆ‘ä»¬è®¨è®ºè¿‡ï¼Œdata flowåœ¨mapå’Œreduceä¹‹é—´æ˜¯é€šè¿‡networkè¿›è¡Œä¼ è¾“çš„ï¼Œä½†æˆ‘ä»¬çŸ¥é“map functionçš„outputæ˜¯ä¸€ä¸ªä¸ªkey-valueçš„é”®å€¼å¯¹çš„ï¼Œè¿™äº›key-value parisä¸­ï¼Œæœ‰äº›æ˜¯å¯ä»¥é€šè¿‡combiner functionè¿›è¡Œcombineçš„ï¼Œè¿™æ ·åšçš„ç›®çš„æ˜¯å‡å°mapå’Œreduceä¹‹é—´ä¼ è¾“çš„æ•°æ®å¤§å°ï¼ŒåŠ å¿«ä¼ è¾“æ•°æ®ã€‚ Combiner Functionåœ¨è®¸å¤šæƒ…å†µå’Œ Reduce Functionæ˜¯å¾ˆåƒçš„ï¼Œå› ä¸ºåšçš„å·¥ä½œå’Œreduceæ˜¯æ¯”è¾ƒç±»ä¼¼çš„ï¼Œåªæ˜¯å¤„ç†çš„æ˜¯å±€éƒ¨mapçš„output(å› æ­¤Combineræ˜¯è¿è¡Œåœ¨map outputç«¯)ï¼Œå‡å°‘data flowçš„sizeã€‚ä½†æ˜¯å¯¹äºæ˜¯å¦è°ƒç”¨combiner functionï¼Œè¿™ä¸ªæ˜¯ä¸ç¡®å®šçš„ã€‚å› ä¸ºæœ‰äº›æƒ…å†µä¸‹çš„outputæ˜¯ä¸é€‚åˆè¿›è¡Œcombineï¼Œæœ‰äº›åˆ™åˆæ˜¯è¦å¤šæ¬¡è°ƒç”¨è¿›è¡Œåˆå¹¶ã€‚å› ä¸ºè¿™ä¸ªï¼ŒCombineræ˜¯å¯é€‰çš„ï¼Œå³å¯ä»¥è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥ä¸è°ƒç”¨ï¼Œå½“ä¸è°ƒç”¨çš„æ—¶å€™ï¼Œå°±å¿…éœ€ä¸èƒ½å½±å“ç¨‹åºçš„æ­£å¸¸è¿è¡Œã€‚æ‰€ä»¥Combinerçš„inputå’Œoutputæ˜¯ä¸€æ ·çš„ï¼Œå’ŒMapperçš„outputã€Reducerçš„inputä¸€æ ·ã€‚ å¯¹äºæœ‰äº›ç‰¹æ®Šæƒ…å†µï¼Œç”šè‡³è¿reduce functionéƒ½ä¸éœ€è¦ã€‚ ä¸¾ä¸ªæ —å­ï¼š é€‚ç”¨æƒ…å†µï¼ˆCommutative &amp; Associativeï¼‰ â€‹ Reduce Function Combiner Function â€‹ Commutative: max(a, b) = max(b, a) â€‹ Associative: max(max(a, b), c) = max(a, max(b, c)) ä¸é€‚ç”¨æƒ…å†µï¼š ä¼ªä»£ç  In-Combiner Function Advantage ç›¸æ¯”Combinerï¼ŒIn-Combinerçš„æ•ˆç‡æ›´é«˜ã€‚ å¯ä»¥å‡å°‘ä¸€äº›Mapperå’ŒReducerä¹‹é—´çš„key-value pairsï¼Œå¯ä»¥å‡å°‘å¤„ç†è¿™éƒ¨åˆ†çš„å¼€é”€ã€‚å› ä¸ºCombineråªæ˜¯å‡å°‘äº†ä¸€äº›Mapperå’ŒReducerä¹‹é—´çš„intermediate dataï¼Œä½†æ˜¯å¹¶æ²¡å‡å°‘ä»Mapperçš„outputå‡ºæ¥çš„key-value pairsçš„æ•°é‡ã€‚ä½†æ˜¯In-Combineræ˜¯æ˜¯Mapper çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯è¯´key-value pairsåœ¨Mapper è¾“å‡ºå‰å°±å·²ç»å‡å°‘äº†ã€‚ å‡å°‘äº†key-value pairså¯ä»¥å‡å°‘ç³»ç»Ÿçš„object serialization and deserialization çš„å¼€é”€ï¼Œå³åƒåœ¾å›æ”¶æœºåˆ¶ Disadvantage å†…å­˜ä½¿ç”¨ï¼Œå› ä¸ºè¦ä¿å­˜ä¸€ä¸ªarrayåœ¨å†…å­˜ä¸­ï¼Œå½“æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™æœ‰å¯èƒ½ä¼šçˆ†äº†ã€‚è§£å†³æ–¹æ¡ˆ æœ‰ä¸¤ä¸ªï¼Œç¬¬ä¸€æ˜¯é™åˆ¶arrayçš„ä¸ªæ•°ï¼Œç¬¬äºŒæ˜¯é™åˆ¶å†…å­˜çš„ä½¿ç”¨ã€‚å½“è¿™ä¿©åˆ°è¾¾æŸä¸€ä¸ªé˜ˆå€¼çš„æ—¶å€™ï¼Œå°±å‘é€ç»™Reducerã€‚ ç¬¬äºŒæ˜¯è®²ä¸€ä¸ªMapçš„è¿‡ç¨‹åˆ†æˆå‡ ä¸ªéƒ¨åˆ†ï¼Œå¯¼è‡´debugä¸­å¯èƒ½å‡ºç°oedering-dependent bugsï¼Œè°ƒè¯•å¯èƒ½æ¯”è¾ƒå›°éš¾ã€‚ â€‹ â€‹ â€‹","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Recommender-Systems/","text":"1. What is Recommender Systemså¯¹äºæ¨èç³»ç»Ÿçš„å®šä¹‰ï¼Œæˆ‘ä»¬å…ˆä¸¾å‡ ä¸ªä¾‹å­æ¥ç†è§£ä¸€ä¸‹ã€‚ ç”µå½±ç½‘ç«™ç»™ç”¨æˆ·æ¨èç”µå½±ï¼Œå¯ä»¥æ ¹æ®è¯¥ç”¨æˆ·ä»¥å¾€çš„è¯„åˆ†ï¼Œæ¯”å¦‚ç»™æµªæ¼«çˆ±æƒ…ç”µå½±è¯„åˆ†é«˜ï¼Œç»™åŠ¨ä½œç‰‡è¯„åˆ†è¾ƒä½ï¼Œé‚£ä¹ˆç³»ç»Ÿå¯ä»¥æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œç»™ç”¨æˆ·æ¨èåå‘æµªæ¼«çˆ±æƒ…çš„ç”µå½± å¦‚æœæ˜¯æ–°ç”¨æˆ·å‘¢ï¼Ÿæˆ‘ä»¬æ²¡æœ‰è¯¥ç”¨æˆ·çš„è¯„åˆ†ä¿¡æ¯ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æ ¹æ®æ•´ä¸ªç³»ç»Ÿä¸­ï¼ŒæŸäº›ç”µå½±è¯„åˆ†è¾ƒé«˜è¿›è¡Œæ¨è é‚£ä¹ˆå¦‚æœæ˜¯æ–°ç½‘ç«™ï¼Œæ–°ç”¨æˆ·å‘¢ï¼Ÿ ä»¥ä¸Šä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ¨èç³»ç»Ÿåˆ†æˆä¸¤ç±»ã€‚ Content-based systems Content-basedï¼Œå°±æ˜¯åŸºäºå·²æœ‰çš„ä¿¡æ¯è¿›è¡Œæ¨èã€‚å…·ä½“å“ªäº›ä¿¡æ¯å‘¢ï¼Ÿåœ¨ä¸Šé¢çš„ç”µå½±æ¨èç³»ç»Ÿä¸­ï¼Œæœ‰ä¸¤ç±»ä¿¡æ¯éœ€è¦åˆ†æã€‚ ç¬¬ä¸€ï¼Œæ˜¯Userçš„è¯„åˆ†ä¿¡æ¯ï¼Œæ¯”å¦‚ç»™çˆ±æƒ…ç‰‡è¯„åˆ†é«˜ï¼Œç»™åŠ¨ä½œç‰‡è¯„åˆ†ä½ã€‚ ç¬¬äºŒï¼Œæ˜¯Movieçš„ç‰¹å¾ä¿¡æ¯ï¼Œæ¯”å¦‚è¿™éƒ¨ç”µå½±åå‘çˆ±æƒ…ç‰‡å¤šä¸€äº›ï¼Œä½†ä¹Ÿæœ‰ä¸€éƒ¨åˆ†æç¬‘ã€‚æ‰€ä»¥åœ¨Aï¼ˆçˆ±æƒ…ç‰‡ï¼‰å’ŒBï¼ˆæç¬‘ç‰‡ï¼‰ä¸­ï¼Œ Açš„æƒé‡æ›´é«˜ï¼ŒBçš„è¾ƒä½ åŸºäºä»¥ä¸Šä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥ç»™ç”¨æˆ·æ¨èä»–æ‰€å–œæ¬¢çš„ç”µå½±ã€‚ Collaborative filterring systems ååŒè¿‡æ»¤å™¨ï¼Œåˆ™æ˜¯åŸºäºç”¨æˆ·/ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦è¿›è¡Œæ¨èçš„ã€‚å³ç”¨æˆ·Aå’Œç”¨æˆ·Béƒ½å–œæ¬¢çˆ±æƒ…ã€æµªæ¼«ç”µå½±ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠç”¨æˆ·Aè¯„åˆ†è¿‡çš„çˆ±æƒ…æµªæ¼«ç”µå½±ï¼Œæ¨èç»™ç”¨æˆ·Bã€‚ 2. Content-based systems2.1 Problem Analysisä»¥ç”µå½±æ¨èç³»ç»Ÿä¸ºä¾‹ï¼Œå‡è®¾æˆ‘ä»¬å·²ç»å¯¹ç³»ç»Ÿä¸­çš„ç”µå½±ç‰¹å¾æœ‰äº†è¾ƒä¸ºå®Œå–„ï¼Œå³æˆ‘ä»¬çŸ¥é“æŸéƒ¨ç”µå½±å±äºçˆ±æƒ…ç‰‡å¤šå°‘åˆ†ï¼Œå±äºåŠ¨ä½œç‰‡å¤šå°‘åˆ†ã€‚ é‚£ä¹ˆæˆ‘ä»¬ç°åœ¨ä»¥Aliceä¸ºä¾‹ï¼Œå¥¹å¯¹ä¸¤éƒ¨çˆ±æƒ…ç‰‡è¯„åˆ†æ¯”è¾ƒé«˜ï¼Œå¯¹äºä¸¤éƒ¨åŠ¨ä½œç‰‡è¯„åˆ†ä¸º0ã€‚é‚£ä¹ˆç³»ç»Ÿå°±å¯ä»¥ç»™Aliceæ¨èåå‘çˆ±æƒ…æµªæ¼«çš„ï¼Œä¸”ä¸æ€ä¹ˆå±äºåŠ¨ä½œç‰‡çš„ç”µå½±ã€‚ Movies Alice - Î¸(1) Bob - Î¸(2) Carol - Î¸(3) Dave - Î¸(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love ï¼Ÿ 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 2.2 Optimization Objectiveå®é™…ä¸Šæˆ‘ä»¬å·²ç»å‡è®¾ä¹‹å‰å¯¹æ‰€æœ‰ç”µå½±çš„ç‰¹å¾è¿›è¡Œäº†ç»Ÿè®¡ï¼Œæ‰€ä»¥æ­¤æ—¶æœ‰ç”µå½±ç‰¹å¾å‘é‡Xï¼Œä»¥åŠç”¨æˆ·å¯¹äºç”µå½±çš„è¯„åˆ†Yå‘é‡ã€‚æ ¹æ®æ­¤æ—¶å·²æœ‰çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡ºthetaçš„å€¼ã€‚æ‰€ä»¥èƒ½å¤Ÿå¯¹äºé‚£ä¹ˆæ²¡æœ‰è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œæ ¹æ®thetaå’Œxæ±‚å‡ºåˆ†æ•°yã€‚ å› ä¸ºä¸€å¼€å§‹theatçš„å€¼æ˜¯éšæœºçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨Linear Regressionçš„æ–¹æ³•ï¼Œä¸æ–­å‡å°‘cost functionçš„å€¼æ±‚å‡ºthetaã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºè¿™é‡Œæ˜¯å¤šä¸ªç”¨æˆ·ï¼Œæ¯ä¸€ä¸ªç”¨æˆ·æˆ‘ä»¬æ±‚å‡ºä¸€ä¸ªthetaå€¼ã€‚æœ€åå¯¹äºå¤šä¸ªç”¨æˆ·ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å‡ºå¤šä¸ªthetaå€¼ã€‚ Actually, we can assume that we have known all features about the all movies, that is x1, x2, â€¦, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 2.2 Gradient descent update 2.3 Gradient descent in Logistic Regression Question: Why we donâ€™t need $\\frac 1 m$? Anwser: As there are only one user However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to learn theta and features at the same time 3. Collaborative filtering3.1 Proble motivation Movies Alice - Î¸(1) Bob - Î¸(2) Carol - Î¸(3) Dave - Î¸(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love ? 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 3.2 How to do åœ¨ä¹‹å‰éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬äº†è§£åˆ°äº†content-basedï¼Œæ˜¯å·²çŸ¥ x å’Œ yï¼Œæ±‚ thetaã€‚ Assume:$$\\theta^{(1)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(2)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(3)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\space\\theta^{(4)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\spacex^{(1)} = \\begin{bmatrix} 1 \\ 1.0 \\ 0.0 \\end{bmatrix}$$For Movie 1, we can calculate the result of Movie1 rating by all users.$$\\theta^{(1)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(2)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(3)} * x^{(1)} \\approx 0 \\\\\\\\theta^{(4)} * x^{(1)} \\approx 0$$ ä½†æ˜¯å¯¹äºæœ‰äº›æƒ…å†µï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“xçš„ç‰¹å¾å€¼ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ é€†å‘æ€è€ƒï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ theat å’Œ yï¼Œæ¥æ±‚ x çš„å€¼ã€‚ é‚£ä¹ˆå¯¹äº thetaå’Œxçš„å€¼éƒ½ä¸çŸ¥é“çš„æƒ…å†µä¸‹å‘¢ï¼Ÿ å¯¹æ¯”ç‰¹å¾ Linear Regression Collaborative filtering ç‰¹æ€§å‘é‡X å·²çŸ¥æ•°æ® å¾…æ±‚è§£æ•°æ® æƒé‡ Î¸ å¾…æ±‚è§£æ•°æ® å¾…æ±‚è§£æ•°æ® yå€¼ å·²çŸ¥æ•°æ® å·²çŸ¥æ•°æ® 3.3 Optimization Algorithm For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. $$Î¸ -&gt; x -&gt; Î¸ -&gt; x -&gt; Î¸ -&gt; x -&gt; Î¸ -&gt; x -&gt; â€¦$$ Actually, these two steps are Linear Regression, we shoud do that simultaneously to update theta and x. 3.4 Collaborative filtering Optimization Algorithm å®é™…ä¸Šï¼Œä¸Šé¢æ˜¯ä¸¤ä¸ª LRçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢ä¸¤æ­¥åˆå¹¶åˆ°ä¸€èµ·ï¼Œè¿™ä¸ªå°±æ˜¯collaborative filterringï¼Œ æ­¤æ—¶çš„optimizatino object å°±ä» J(theta) å’Œ J(X) å˜ä¸ºäº† J(theta, X)ã€‚ å…·ä½“æ­¥éª¤å¦‚ä¸‹ 3.5 Vectorization: Low rank matrix factorizationé¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæŠŠè¯„åˆ†Yç”¨å‘é‡è¡¨ç¤ºå‡ºæ¥ï¼ŒåŒæ—¶è¡¨ç¤ºä¸ºThetaå’ŒXä¸¤ä¸ªçŸ©é˜µçš„ä¹˜ç§¯$$Y= \\begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \\ 5 &amp; ? &amp; ?&amp; 0 \\ ? &amp; 4 &amp; 0 &amp; ? \\ 0 &amp; 0 &amp; 5 &amp; 4 \\ 0 &amp; 0 &amp; 5 &amp; 0\\end{bmatrix} =\\begin{bmatrix}(\\theta^{(1)})^T(x^{(1)}) &amp; (\\theta^{(2)})^T(x^{(1)}) &amp; â€¦ &amp; (\\theta^{(n_u)})^T(x^{(1)}) \\\\(\\theta^{(1)})^T(x^{(2)}) &amp; (\\theta^{(2)})^T(x^{(2)}) &amp; â€¦ &amp; (\\theta^{(n_u)})^T(x^{(2)}) \\\\â€¦ &amp; â€¦ &amp; â€¦ &amp; â€¦ \\\\(\\theta^{(1)})^T(x^{(n_m)}) &amp; (\\theta^{(2)})^T(x^{(n_m)}) &amp; â€¦ &amp; (\\theta^{(n_u)})^T(x^{(n_m)})\\end{bmatrix} = X * \\Thetaâ€™, R \\in (n_m Ã— n_u)$$ $$X = \\begin{bmatrix}â€”(x^{(1)})^Tâ€” \\\\â€”(x^{(2)})^Tâ€” \\\\â€¦ \\\\â€”(x^{(n_m)})^Tâ€”\\end{bmatrix},x^{(n_m)} = \\begin{bmatrix}x^{(n_m)}_1 \\ x^{(n_m)}_2 \\ â€¦ \\ x^{(n_m)}_n\\end{bmatrix}, R \\in (n_m Ã— n)$$ $$\\Theta = \\begin{bmatrix}â€”(\\theta^{(1)})^Tâ€” \\\\â€”(\\theta^{(2)})^Tâ€” \\\\â€¦ \\\\â€”(\\theta^{(n_u)})^Tâ€”\\end{bmatrix},\\theta^{(n_u)} = \\begin{bmatrix}\\theta^{(n_u)}_1 \\ \\theta^{(n_u)}_2 \\ â€¦ \\ \\theta^{(n_u)}_n\\end{bmatrix}, R \\in (n_u Ã— n)$$ 3.6 Mean Normalizationå¯¹äºé‚£äº›æ–°æ³¨å†Œç”¨æˆ·ï¼Œç³»ç»Ÿä¸­æ²¡æœ‰è®°å½•ä»–ä»¬çš„åå¥½ï¼Œåˆ™é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚ å…ˆè®¡ç®—å‡ºæ¯éƒ¨ç”µå½±è¯„åˆ†çš„å¹³å‡å€¼muï¼Œç„¶åæŠŠæ‰€æœ‰çš„è¯„åˆ†éƒ½å‡å»å¹³å‡å€¼ï¼ˆæ­¤åå¤„ç†è¿‡çš„è¯„åˆ†å¹³å‡å€¼ä¸º0ï¼‰ã€‚è™½ç„¶è¿™æ ·åšå¯¹æœ‰è¯„åˆ†è®°å½•ç”¨æˆ·æ˜¯å¤šä½™çš„ï¼Œä½†å´å¯ä»¥å§æ²¡æœ‰è¯„åˆ†è®°å½•çš„ç”¨æˆ·ç»™ç»Ÿä¸€è¿›æ¥ï¼Œé¿å…å…¨æ˜¯0çš„æƒ…å†µã€‚ 4. Implement Algorithm4.1 Cost Function without Regularization Tipsï¼šè¿™é‡Œéœ€è¦è®¡ç®—çš„åªæ˜¯é’ˆå¯¹é‚£äº›å·²ç»è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œå¯¹äºç”¨æˆ·æ²¡æœ‰è¯„åˆ†è¿‡çš„ä¸éœ€è¦è®¡ç®—ã€‚ 4.2 Collaborative filtering gradient$$\\frac {\\partial J} {\\partial x_k^{(1)}} , \\frac {\\partial J} {\\partial x_k^{(2)}} , â€¦, \\frac {\\partial J} {\\partial x_k^{(n_m)}} \\space\\space for \\space each \\space movie\\\\\\frac {\\partial J} {\\partial \\theta_k^{(1)}} , \\frac {\\partial J} {\\partial \\theta_k^{(2)}} , â€¦, \\frac {\\partial J} {\\partial \\theta_k^{(n_u)}} \\space\\space for \\space each \\space user$$Tipsï¼š å¯¹äºä½¿ç”¨vectorizationæ–¹æ³•ï¼Œæœ€ç»ˆåªæœ‰ä¸¤ä¸ªfor-loopï¼Œä¸€ä¸ªè®¡ç®—$X_{grad}$ï¼Œä¸€ä¸ªè®¡ç®—$Theta_{grad}$ å¦‚ä½•å¯¹Xå’ŒThetaæ±‚åå¯¼æ•°ï¼Ÿ $$(Theta_{grad}(i, :))^T = \\begin{bmatrix}\\frac {\\partial J} {\\partial \\theta^{(i)}_1} \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_2} \\\\â€¦ \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_n}\\end{bmatrix}$$ åŒæ ·ï¼Œæˆ‘ä»¬åªéœ€è€ƒè™‘ç”¨æˆ·å·²ç»è¯„åˆ†è¿‡çš„ç”µå½±ï¼Œç”¨å…¶ä½œä¸ºè®­ç»ƒæ ·æœ¬ å› ä¸ºVectorizationéå¸¸å®¹æ˜“æä¹±å„ä¸ªmatrixï¼Œæ‰€ä»¥å»ºè®®å…ˆæ•´ç†ä¸€ä¸‹å„ä¸ªmatrixçš„sizeï¼Œè®¡ç®—æ—¶å¯ä»¥æ ¹æ®matrixçš„sizeè¿›è¡Œè®¡ç®—ã€‚ 4.3 Implementationæ³¨æ„è¿™é‡Œå¹¶æ²¡æœ‰ç»™å‡ºå®Œæ•´çš„ä»£ç  (Octave/Matlab)ï¼Œéƒ½åªæ˜¯ä¸»è¦çš„éƒ¨åˆ†ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950% Theta : nu x n% X : nm x n% Y : nm x nu% R : nm x nupred = X * Theta'; %nm x nu 'diff = pred - Y;% Cost Function with regularizationJ = 0.5 * sum(sum((diff.^2) .* R));J = J + (lambda * 0.5) * sum(sum(Theta.^2)); % regularized term of theta.J = J + (lambda * 0.5) * sum(sum(X.^2)); % regularized term of x.% calculate Xfor i = 1 : num_movies, % the row vector of all users that have rated movie i idx = find(R(i, :) == 1); % (1 * r) % the list of users who have rated on movie i Theta_temp = Theta(idx, :); % (r * n) Y_temp = Y(i, idx); % (1 * r) X_temp = X(i, :); % (1 * n) % ((1 * n) * (n * r) -(1 * r)) * (r * n) = (1 * n) X_grad(i, :) = (X_temp * Theta_temp' - Y_temp) * Theta_temp; %' % regularization X_grad(i, :) = X_grad(i, :) + lambda * X_temp;end% calculate Thetafor i = 1 : num_users, % the row vector of all movies that user i has rated idx = find(R(:, i) == 1)'; % (1 * r) ' Theta_temp = Theta(i, :); % (1 * n) Y_temp = Y(idx, i); % (r * 1) X_temp = X(idx, :); % (r * n) % ((r * n) * (n * 1) - (r * 1)) * (r * n) = (1 * n) Theta_grad(i, :) = (X_temp * Theta_temp' - Y_temp)' * X_temp; % regularization Theta_grad(i, :) = Theta_grad(i, :) + lambda * Theta_temp;endgrad = [X_grad(:); Theta_grad(:)]; â€‹â€‹â€‹","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Machine Learning - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-SVM/","text":"1. Optimisation Objective$$h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\\\z = -\\theta^Tx$$ Why we need do that? 2. Hypothesis Function2.1 Logistic Regression$$\\frac 1 m \\sum_{i=1}^m [ y^{(i)} (-log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) (-log(1 - h_\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum_{j=1}^n \\theta_j^2$$ 2.2 Support Vector Machine$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ Analysisï¼š ä¸ºäº†ä½¿å¾—cost functionå–å¾—æœ€å°å€¼ï¼Œæˆ‘ä»¬ä»¤C*W + Péƒ¨åˆ†ä¸­ï¼ŒC*Wä¸ºé›¶ã€‚å³ï¼š å½“ y = 1æ—¶ï¼Œ cost1 = 0ï¼Œæ‰€ä»¥ z &gt;= 1 å½“ y = 0æ—¶ï¼Œ cost0 = 0ï¼Œæ‰€ä»¥ z &lt;= -1 Noteï¼š1. cost0 and cost1 å¯¹åº”çš„æ˜¯ä¸Šå›¾ä¸­å·¦å³ä¸¤è¾¹çš„cost functionï¼Œå› ä¸ºy=0å’Œy=1çš„ç›®æ ‡å‡½æ•°ã€‚ å¸¸æ•°Cå–ä¸€ä¸ªå¾ˆå¤§çš„å€¼æ—¶æ¯”è¾ƒå¥½ã€‚å› ä¸ºC*W + Pï¼Œ æ‰€ä»¥Cå¤§åˆ™Wä¼šå˜å°ï¼Œå³ç›¸å¯¹penalityå°±ä¼šå˜å¤§ï¼ŒWä¼šå˜å° ä¸ºä»€ä¹ˆè¦é‡æ–°é€‰å®šä¸€ä¸ªcost function ï¼Ÿï¼ˆé€»è¾‘å›å½’çš„ä¸´ç•Œç‚¹ä¸º0ï¼Œä½†æ˜¯SVMçš„ä¸´ç•Œç‚¹æ˜¯1ï¼Œæ‰€ä»¥SVMæ›´åŠ ç²¾ç¡®ã€‚ ï¼‰ å¯¹åº”çš„çº¿æ€§é€»è¾‘å›å½’ï¼Ÿå³æ¬¡æ•°ä¸å¤§äº1çš„ï¼Ÿ Decision Boundary ä¸æ˜¯ä¸€æ¡ç›´çº¿çš„æƒ…å†µ 3. Large Margin Classifier12ç»“è®ºï¼šå¸¸æ•°Cå–ä¸€ä¸ªæ¯”è¾ƒå¤§çš„å€¼æ¯”è¾ƒå®¹æ˜“è·å¾—Large Margin ClassifierCå¤§ï¼Œåˆ™æ¯”è¾ƒå®¹æ˜“è·å¾— ä»¥ä¸Šä¸ºä¸¤ç±»åˆ†å¸ƒæ¯”è¾ƒå‡åŒ€çš„æ—¶å€™ï¼ŒDecision Boundaryä¸ºå›¾ä¸­é»‘è‰²çš„çº¿ï¼Œæ‰€æœ‰ç‚¹ç¦»é»‘è‰²çš„è·ç¦»éƒ½ç›¸å¯¹æ¯”è¾ƒå¤§æ¯”è¾ƒå‡åŒ€ï¼Œä½†æ˜¯å½“å­˜åœ¨å¹²æ‰°ç‚¹çš„æ—¶å€™å¦‚ä¸‹å›¾ï¼ŒDecision Boundaryä¼šç”±é»‘è‰²å˜ä¸ºç²‰çº¢è‰²ã€‚æ‰€ä»¥Cçš„å–å€¼ä¸èƒ½å¤ªå¤§ï¼Œä¹Ÿä¸èƒ½å¤ªå°ã€‚éœ€è¦æ±‚å‡ºæœ€ä¼˜è§£ 4. Mathmatics Behind Large Margin Classification4.1 Vector Inner Product Noteï¼š å¦‚ä½•æ±‚æŠ•å½±pçš„å€¼ï¼Ÿ å½“è§’åº¦ &lt; 90Â°ï¼Œpä¸ºæ­£æ•°ã€‚å½“è§’åº¦ &gt; 90Â°æ—¶ï¼Œpä¸ºè´Ÿæ•°ã€‚ å‘é‡å†…ç§¯$$u^Tv = ||u|| Â· ||v|| Â· cosÎ¸ = ||u|| Â· p_{v,u} = ||v|| Â· p_{u,v} = u_1v_1+u_2v_2$$ 4.2 SVM Cost Function$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ å½“Cå–ä¸€ä¸ªä¸€ä¸ªå¾ˆå¤§çš„å€¼æ—¶ï¼Œcost functionåªå‰©ä¸‹åé¢Pçš„éƒ¨åˆ†ã€‚ å‡è®¾Î¸0 = 0$$\\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + â€¦ + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + â€¦ + \\theta^2_n)= \\frac 1 2 ||\\theta||^2$$ æ‰€ä»¥ï¼š$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\p^{(i)}æ˜¯ç‚¹åˆ°å‘é‡\\thetaçš„projectionï¼Œå³ç‚¹åˆ°Decision Boundaryçš„è·ç¦»$$ä¸Šé¢æˆ‘ä»¬è®¨è®ºäº†ï¼Œå½“Cå–åˆ°ä¸€ä¸ªåˆé€‚çš„ã€è¾ƒå¤§çš„æ•°å€¼æ—¶ï¼ŒSVMçš„cost functionå°±åªå‰©ä¸‹åé¢Pçš„éƒ¨åˆ†ï¼Œå³$$\\frac 1 2 ||\\theta||^2$$æˆ‘ä»¬è¦å‡å°cost functionï¼Œæ‰€ä»¥éœ€è¦å‡å°Î¸çš„å€¼ã€‚ å½“Î¸å–åˆ°ä¸€ä¸ªæ¯”è¾ƒå°çš„å€¼çš„æ—¶å€™ï¼Œè¿˜éœ€è¦æ»¡è¶³ä¸Šé¢è®¨è®ºçš„ï¼š$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\$$æ‰€ä»¥Î¸æ¯”è¾ƒå°æ—¶ï¼Œåªèƒ½å¢åŠ pçš„å€¼å»æ»¡è¶³p*||Î¸|| &gt;= 1 æˆ–è€… p*||Î¸||&lt;= -1ã€‚ è¿™æ ·å°±ä¿è¯äº†pçš„å€¼æ¯”è¾ƒå¤§ï¼Œå³ç‚¹åˆ°Decision Boundaryçš„å¤§é—´è·ã€‚ 5. Kernels5.1 Kernels &amp; Similarityé¦–å…ˆï¼Œæˆ‘ä»¬å›æƒ³ä¸€ä¸‹ä¹‹å‰çš„logistic regressionä¸­å¯¹äºnon-linear æƒ…å†µçš„æ‹Ÿåˆã€‚ Predict y = 1, if$$\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3x_2 + \\theta_4x_1^2 + \\theta_5x_2^2 + â€¦ &gt;= 0 \\\\\\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + â€¦ &gt;= 0$$å³å°†fnå®šä¹‰ä¸ºxçš„å¹‚æ¬¡é¡¹ç»„åˆï¼Œå¦‚ä¸‹ï¼š$$f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, â€¦$$ ä½†æ˜¯åœ¨SVMä¸­ï¼Œæˆ‘ä»¬è¦é‡æ–°å®šä¹‰fnï¼Œå¼•å…¥Kernelçš„æ¦‚å¿µï¼Œå³ç”¨ kernel functionæ¥è¡¨ç¤ºfnã€‚ Note: l æ˜¯landmarkï¼Œä¸”å¦‚æœtraining setsé‡Œé¢çš„æ•°é‡ä¸ºnçš„è¯ï¼Œåˆ™landmarkçš„æ•°é‡ä¹Ÿä¸ºnã€‚ å‡è®¾training setsæ•°é‡ä¸ºnï¼Œåˆ™å¯¹äºä¸€ä¸ªæ–°çš„exampleæ¥è¯´ï¼Œå¯è®¡ç®—å‡ºnä¸ªæ–°çš„ç‰¹å¾f1â€¦fnã€‚ç„¶åç”¨æ–°çš„ç‰¹å¾ï¼Œå¯¹è¯¥exampleè¿›è¡Œåˆ¤æ–­ï¼ˆä½ç»´è½¬ä¸ºé«˜ç»´çš„è¿‡ç¨‹ï¼‰ kernel functionä¸ºguassian functionã€‚å½“xä¸landmark lè¶Šæ¥è¿‘æ—¶ï¼Œä¸¤ç‚¹çš„è·ç¦»è¶Šå°ï¼Œå€¼æ¥è¿‘1 5.2 SVM with Kernels å¯¹æ¯”ä¹‹å‰çš„cost functionï¼Œå¯ä»¥å‘ç°è¿™é‡ŒÎ¸å’Œf(x)è·Ÿä¹‹å‰çš„ä¸åŒã€‚ åœ¨logistic regression ä¸­ï¼ŒÎ¸çš„ç»´åº¦ä¸º(n+1) x 1, åŒ…å«Î¸0ï¼Œ ä¸”nä¸ºå•ä¸ªexampleçš„ç‰¹å¾ä¸ªæ•° åœ¨SVM with kernelä¸­ï¼Œf(x)çš„ä¸ªæ•°ä¸ºmï¼Œå…¶ä¸­mæ˜¯training setsä¸­çš„ä¸ªæ•°ï¼Œæ‰€ä»¥Î¸çš„ç»´åº¦åº”è¯¥æ˜¯(m+1)x1 Steps ç»™å®šä¸€ç»„training setsï¼Œæ ¹æ®æ¯ä¸ªexampleï¼Œé€‰å–mä¸ªlandmarkç‚¹ è®¡ç®—æ¯ä¸€ä¸ªexampleä¸æ‰€æœ‰landmarkçš„ç›¸è¯†åº¦ï¼Œç›¸åŒä¸º1ï¼Œéå¸¸ä¸åŒæ¥è¿‘ä¸º0ã€‚è®¡ç®—ç›¸è¯†åº¦çš„kernel functionä¸ºGaussian Function æœ€ç»ˆï¼Œå¯¹äºæ¯ä¸€ä¸ªexampleé‡Œé¢éƒ½å¯ä»¥è®¡ç®—å‡ºmä¸ªæ–°çš„featureï¼Œæ‰€ä»¥å¯¹äºè¿™ä¸ªtraining setsè€Œè¨€ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªm*mçš„çŸ©é˜µï¼Ÿ å°†å¾—åˆ°çš„m*mçš„çŸ©é˜µï¼Œä»£å…¥åˆ°Hypothesisä¸­ï¼Œè®¡ç®—å‡ºÎ¸çš„å€¼ã€‚ 5.4 SVM parameters C = 1/Î» Large C Small Î» Large Î¸ Lower Bias High Variance Over Fitting Small C Large Î» Small Î¸ Higher Bias Low Variance Under Fitting Ïƒ Large Ïƒ more smoothly Higher Bias Lower Variance Under Fitting Small Ïƒ less smoothly Lower Bias Higher Variance Over Fitting","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Pythonæ•°æ®åˆ†æç¬”è®°ï¼ˆä¸€ï¼‰","date":"2016-12-15T11:37:30.000Z","path":"2016/12/15/Pythonæ•°æ®åˆ†æç¬”è®°ï¼ˆä¸€ï¼‰/","text":"1. å¸¸è§é—®é¢˜ Pandas.dataframeé‡Œé¢ .values, .iloc, .ix, .loc çš„åŒºåˆ« Different Choices for Indexing loc: only work on index / labeliloc: work on position, from 0, 1, 2, â€¦ â€¦ix: You can get data from dataframe without it being in the indexat: get scalar values. Itâ€™s a very fast lociat: Get scalar values. Itâ€™s a very fast iloc 12345678910111213141516171819202122232425262728&gt;&gt;&gt; df = pd.DataFrame(&#123;'A':['a', 'b', 'c'], 'B':[54, 67, 89]&#125;, index=[100, 200, 300])&gt;&gt;&gt; df A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df.iloc[0] # ç”¨ä½ç½®æ¥ç´¢å¼•A aB 54&gt;&gt;&gt; df.loc[100] # ç”¨åˆå§‹åŒ–æ—¶è®¾ç½®çš„indexæ¥ç´¢å¼•ï¼Œä¹Ÿå°±æ˜¯è‡ªå·±ç»™rowè®¾ç½®çš„labelA aB 54&gt;&gt;&gt; df.loc[100:300] A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df['A'] # ç´¢å¼• columns100 a200 b300 cName: A, dtype: object&gt;&gt;&gt; df.A # ç´¢å¼• columns100 a200 b300 cName: A, dtype: object Pandas å’Œ Numpyä¹‹é—´çš„è½¬æ¢ np.ndarray è½¬åŒ–ä¸º pd.dataframe 1pd.DataFrame(example) pd.dataframe è½¬åŒ–ä¸º np.ndarray 1example.values[:, :] è¯»å†™æ•ˆç‡çš„å¯¹æ¯” npyè¯»å†™æ•ˆç‡æœ€é«˜ï¼Œä½†æœ€è´¹ç¡¬ç›˜ç©ºé—´ï¼Œæ¯”å¦‚np.load(), np.save() csvå…¶æ¬¡ï¼Œæ¯”å¦‚pd.Dataframe.to_csv()ï¼Œpd.load_csv() txtè¯»å†™ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å¾ˆå¿«ï¼Œä½†æ˜¯éœ€è¦é¢‘ç¹çš„splitï¼Œå¯¹æ ¼å¼è§„èŒƒçš„æ•°æ®æ¯”è¾ƒéº»çƒ¦ è‡³äºç®€å•çš„excelå’Œwordï¼Œå¯ä»¥ç”¨xlrd,xlwtæ¥æ“ä½œ 2. Numpy Nç»´æ•°ç»„å¯¹è±¡ï¼Œå¯ä»¥åˆ©ç”¨è¿™ç§æ•°ç»„å¯¹è±¡å¯¹æ•´å—æ•°æ®è¿›è¡Œä¸€äº›ç§‘å­¦è¿ç®—ï¼Œå°±æ˜¯æŠŠarrayå½“åšä¸€ç§å¯¹è±¡é‡Œæ“ä½œã€‚è¿™å’ŒPythonä¸­çš„arrayæ˜¯ä¸åŒçš„ã€‚ ä¸¾ä¸ªæ —å­ï¼š åœ¨Pythonä¸­ 123&gt;&gt;&gt; data = [1, 2, 3]&gt;&gt;&gt; data * 10[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3] Numpyçš„ndarray 12345&gt;&gt;&gt; data2 = np.array([1, 2, 3])&gt;&gt;&gt; data2array([1, 2, 3])&gt;&gt;&gt; data2 * 10array([10, 20, 30]) è½´(axes) å’Œ ç§©(rank) è½´è¡¨ç¤ºçš„æ˜¯ä¸€ç§ç»´åº¦ï¼Œå¦‚ä¸€ç»´çš„æ•°æ®ï¼ŒäºŒç»´çš„æ•°æ®ï¼Œä¸‰ç»´çš„æ•°æ®ç­‰ 12345678910111213141516171819202122&gt;&gt;&gt; data3 = np.array([1, 2, 3]) # æ³¨æ„è¿™é‡Œçš„**æ–¹æ‹¬å·**&gt;&gt;&gt; data3.ndim # æŸ¥çœ‹ç»´åº¦1&gt;&gt;&gt; data3 = np.array([[1,2,3], [1,2,3], [1,2,3]])&gt;&gt;&gt; data3.ndim2&gt;&gt;&gt; data3 = np.array([[[1], [1], [1]], [[1], [1], [1]],[[1], [1], [1]]])&gt;&gt;&gt; data3.ndim3&gt;&gt;&gt; data3.shape(3, 3, 1) # ç»´åº¦ä»æœ€å¤–å±‚åˆ°é‡Œå±‚&gt;&gt;&gt; data3.size9 # 3 * 3 * 1 = 9#ä¸€ä¸ªç”¨æ¥æè¿°æ•°ç»„ä¸­å…ƒç´ ç±»å‹çš„å¯¹è±¡ï¼Œå¯ä»¥é€šè¿‡åˆ›é€ æˆ–æŒ‡å®šdtypeä½¿ç”¨æ ‡å‡†Pythonç±»å‹ã€‚å¦å¤–NumPyæä¾›å®ƒè‡ªå·±çš„æ•°æ®ç±»å‹ã€‚&gt;&gt;&gt; data3.dtype# æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚å¤§å°ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå…ƒç´ ç±»å‹ä¸ºfloat64çš„æ•°ç»„itemsizå±æ€§å€¼ä¸º8(=64/8),åˆå¦‚ï¼Œä¸€ä¸ªå…ƒç´ ç±»å‹ä¸ºcomplex32çš„æ•°ç»„itemå±æ€§ä¸º4(=32/8).&gt;&gt;&gt; data3.itermsize# åŒ…å«å®é™…æ•°ç»„å…ƒç´ çš„ç¼“å†²åŒºï¼Œé€šå¸¸æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨è¿™ä¸ªå±æ€§ï¼Œå› ä¸ºæˆ‘ä»¬æ€»æ˜¯é€šè¿‡ç´¢å¼•æ¥ä½¿ç”¨æ•°ç»„ä¸­çš„å…ƒç´ ã€‚&gt;&gt;&gt; data3.data å¸¸ç”¨çš„æ•°ç»„åˆ›å»ºå‡½æ•° æ‰“å°æ•°ç»„ 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(6) # 1d array&gt;&gt;&gt; print(a)[0 1 2 3 4 5]&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(12).reshape(4,3) # 2d array&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]&gt;&gt;&gt;&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4) # 3d array&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] å¦‚æœä¸€ä¸ªæ•°ç»„ç”¨æ¥æ‰“å°å¤ªå¤§äº†ï¼ŒNumPyè‡ªåŠ¨çœç•¥ä¸­é—´éƒ¨åˆ†è€Œåªæ‰“å°è§’è½ 1234567891011&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt;&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] ç¦ç”¨è¿™ç§reshapeæ¥æ‰“å°æ•´ä¸ªæ•°ç»„ï¼Œéœ€è¦å¯¹printoptionå‚æ•°è¿›è¡Œè®¾ç½® 1&gt;&gt;&gt; np.set_printoptions(threshold='nan') åŸºæœ¬çš„æ•°æ®è¿ç®— 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; b = np.array(list(range(5, 10)))&gt;&gt;&gt; a + barray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; a - barray([-5, -5, -5, -5, -5])&gt;&gt;&gt; a * barray([ 0, 6, 14, 24, 36])&gt;&gt;&gt; a += b&gt;&gt;&gt; aarray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; c = np.ones(5)&gt;&gt;&gt; carray([ 1., 1., 1., 1., 1.])&gt;&gt;&gt; c + aarray([ 6., 8., 10., 12., 14.])&gt;&gt;&gt; c = np.ones(5, dtype=int)&gt;&gt;&gt; c + aarray([ 6, 8, 10, 12, 14]) æ•°ç»„çš„è¿ç®— è¿™å°±ç±»ä¼¼åœ¨Matlab/Octaveä¸­ï¼Œå¯¹matrix/arrayä¸­çš„æ•°æ®æ‰§è¡Œæ‰¹é‡è¿ç®—ï¼Œå³Vectorizationï¼Œå‰ææ˜¯matrix/arrayçš„å¤§å°å¿…é¡»æ»¡è¶³å¯¹åº”çš„è¦æ±‚ã€‚ ç”¨æ•°ç»„è¡¨è¾¾å¼å¯ä»¥ä»£æ›¿å¾ªç¯æ“ä½œï¼ŒçŸ¢é‡åŒ–çš„è¿ç®—æ˜¯Numpyçš„ä¼˜åŠ¿ã€‚ æ•°ç»„è½¬ç½®å’Œè½´å¯¹æ¢ 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.T # æ•°ç»„è½¬ç½®ï¼Œè½´å¯¹æ¢array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]])&gt;&gt;&gt; np.dot(a.T, a) # å†…ç§¯array([[125, 140, 155, 170, 185], [140, 158, 176, 194, 212], [155, 176, 197, 218, 239], [170, 194, 218, 242, 266], [185, 212, 239, 266, 293]])&gt;&gt;&gt; b = np.arange(16).reshape((2, 2, 4))&gt;&gt;&gt; barray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt;&gt;&gt; b.transpose((1, 0 , 2)) # å¯¹é«˜ç»´æ•°ç»„ï¼Œtransposeéœ€è¦å¾—è¦ä¸€ä¸ªç”±è½´ç¼–å·ç»„æˆçš„å…ƒç»„æ‰èƒ½å¯¹è¿™äº›è½´è¿›è¡Œè½¬ç½®array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) ç´¢å¼•å’Œåˆ‡ç‰‡ 1234567891011121314151617181920212223242526272829303132333435# ä¸€ç»´æ•°æ®&gt;&gt;&gt; a = np.arange(5)array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:]array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[1 : 4]array([1, 2, 3])&gt;&gt;&gt; a[1]1# äºŒç»´æ•°æ®&gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6]])array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:, 1:3]array([[2, 3], [5, 6]])&gt;&gt;&gt; b[1][2]6&gt;&gt;&gt; b[1, 2]6# ä¸‰ç»´æ•°ç»„&gt;&gt;&gt; c = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; c[1]array([[ 7, 8, 9], [10, 11, 12]]) é€šç”¨å‡½æ•° P111 ç»™ array æ·»åŠ  columns å’Œ rows 123456789101112# æ–¹æ³•ä¸€np.c_[array1, array2] # æ·»åŠ  columnsnp.r_[array1, array2] # æ·»åŠ  row# æ–¹æ³•äºŒ è¢«æ’å…¥çš„è¡Œnp.insert(a, 2, values=b, axis=1) # æ·»åŠ  columns# æ–¹æ³•ä¸‰a = np.concatenate((a, -np.ones((a.shape[0], 1))), axis=1) # æ·»åŠ  columns# æ–¹æ³•å››np.column_stack((a,b)) 3. Pandasåœ¨Pandasä¸­ï¼ŒSerieså’ŒDataFrameæ˜¯ä¸¤ä¸ªä¸»è¦çš„æ•°æ®ç»“æ„ Series ç±»ä¼¼ä¸€ç»´æ•°ç»„ï¼Œç”±ä¸€ç»„æ•°æ®ï¼ˆå„ç§Numptæ•°æ®ç±»å‹ ( list, dictç­‰ )ï¼‰å’Œä¸€ç»„å¯¹äºçš„æ•°æ®æ ‡ç­¾ç»„æˆ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&gt;&gt;&gt; obj = pd.Series(list(range(3, 8)))&gt;&gt;&gt; obj0 31 42 53 64 7dtype: int64&gt;&gt;&gt; obj.valuesarray([3, 4, 5, 6, 7])&gt;&gt;&gt; obj.index # è¿™é‡Œå°±æ˜¯ index ç´¢å¼•ï¼Œæ²¡æœ‰è®¾ç½®çš„æ˜¯æ—¶å€™ç³»ç»Ÿä¼šè‡ªåŠ¨è®¾ç½®ä¸ºinteger indexRangeIndex(start=0, stop=5, step=1)&gt;&gt;&gt; obj[1] # å¯ä»¥ç”¨è¿™ç´¢å¼•ï¼Œè·Ÿlistç”¨æ³•ç±»ä¼¼ï¼Œä½†æ˜¯listä¸­åªèƒ½æ˜¯æ•°å­—ï¼Œä½†pandasä¸­å¯ä»¥è‡ªå®šä¹‰indexçš„ç´¢å¼•4# è‡ªå®šä¹‰ç´¢å¼•ï¼Œæ™®é€šçš„listå’ŒNumpyçš„æ•°ç»„å°±ä¸è¡Œ&gt;&gt;&gt; obj2 = pd.Series([1,3,4], index=['a', 'b', 'c'])&gt;&gt;&gt; obj2['a']1# ç´¢å¼•å¯ä»¥ç›´æ¥ç”¨æ¥æ•°ç»„è¿ç®—ï¼Œè¿™äº›åœ¨æ•°æ®æ¸…æ´—çš„æ—¶å€™æ¯”è¾ƒå¸¸ç”¨&gt;&gt;&gt; index = obj2 &gt; 2&gt;&gt;&gt; indexa Falseb Truec Truedtype: bool &gt;&gt;&gt; obj2[index]b 3c 4dtype: int64# å…¶ä»–æ“ä½œ&gt;&gt;&gt; np.log(obj2)a 0.000000b 1.098612c 1.386294dtype: float64 # å½“åšå­—å…¸æ¥index&gt;&gt;&gt; 2 in obj2False &gt;&gt;&gt; 3 in obj2False&gt;&gt;&gt; 'a' in obj2 # åªèƒ½æ˜¯index (key)ï¼Œä¸èƒ½æ˜¯valuesTrue# ç”¨å­—å…¸æ¥åˆå§‹åŒ– Series&gt;&gt;&gt; obj3 = &#123;'one': 1, 'two': 2, 'three' : 3&#125;&gt;&gt;&gt; obj3&#123;'one': 1, 'three': 3, 'two': 2&#125;&gt;&gt;&gt; index = ['One', 'Two', 'Three']&gt;&gt;&gt; value = [1, 2, 3]&gt;&gt;&gt; obj4 = pd.Series(value, index)&gt;&gt;&gt; objOne 1Two 2Three 3dtype: int64 # æ‰‹åŠ¨ä¿®æ”¹ç´¢å¼•ï¼Œä¸”ç´¢å¼•çš„å€¼æ˜¯ä¸èƒ½é‡å¤çš„&gt;&gt;&gt; obj4.index = ['one', 'two', 'Three']&gt;&gt;&gt; obj4one 1two 2Three 3dtype: int64 # é‡å»ºç´¢å¼•ï¼Œå¦‚æœç´¢å¼•ä¸å­˜åœ¨çš„å€¼ï¼Œåˆ™å¼•å…¥NaN&gt;&gt;&gt; obj4.reindex(['one', 'two', 'Threeee']) # fill_value=0one 1.0two 2.0Threeee NaNdtype: float64# å¯é€‰ ffill/pad å‘å‰å¡«å……æˆ–è€…bfill/backfill å‘åå¡«å……&gt;&gt;&gt; obj4.ffill()one 1.0two 2.0Threeee 2.0dtype: float64# å½“ç„¶ä¹Ÿå¯ä»¥drop columnsçš„å†…å®¹&gt;&gt;&gt; obj4.drop('Threeee')one 1.0two 2.0dtype: float64 # ä½†æ˜¯ä¸èƒ½è¿™ä¹ˆä¿®æ”¹æ•°å€¼&gt;&gt;&gt; obj4.values = [4, 5, 6]---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2702 else:-&gt; 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):AttributeError: can't set attributeDuring handling of the above exception, another exception occurred:AttributeError Traceback (most recent call last)&lt;ipython-input-41-b4392e8960b0&gt; in &lt;module&gt;()----&gt; 1 obj4.values = [4, 5, 6]/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):-&gt; 2705 object.__setattr__(self, name, value) 2706 2707 # ----------------------------------------------------------------------AttributeError: can't set attribute â€‹ DataFrame DataFrameæ˜¯ä¸€ä¸ªè¡¨æ ¼å‹çš„æ•°æ®ç»“æ„ï¼ŒåŒ…å«äº†ä¸€ç»„æœ‰åºçš„åˆ—ï¼Œæ¯åˆ—å¯ä»¥æ˜¯ä¸åŒçš„å€¼ç±»å‹ã€‚æ‰€æœ‰å¯ä»¥çœ‹åšè¿™æ˜¯ä¸€ä¸ªäºŒç»´çš„æ•°ç»„ï¼Œæœ‰è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼• åˆ›å»ºDataFrameå’ŒåŸºç¡€æ“ä½œ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; dates = pd.date_range('20170305', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6, 4), index=dates, &gt;&gt;&gt; columns=list('ABCD'))&gt;&gt;&gt; df A B C D2017-03-05 -1.616361 0.027641 0.328074 1.0386272017-03-06 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 -0.350594 0.231137 -0.004755 0.6014602017-03-08 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 -0.227631 -0.186816 -0.370522 0.343896# å½“ç„¶ä¹Ÿå¯ä»¥æ‰‹åŠ¨ä¼ è¿›æ¥åˆ›å»ºdf&gt;&gt;&gt; df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)&gt;&gt;&gt; df2Ones A B C D2017-03-05 1 -1.616361 0.027641 0.328074 1.0386272017-03-06 1 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 1 -0.350594 0.231137 -0.004755 0.6014602017-03-08 1 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 1 -0.227631 -0.186816 -0.370522 0.343896# çœ‹DataFrameçš„attributes&gt;&gt;&gt; df.&lt;TAB&gt;df.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D æŸ¥çœ‹DataFrameé‡Œé¢çš„æ•°æ® 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt;&gt; df.head() # é»˜è®¤5è¡Œ A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401&gt;&gt;&gt; df.tail(3) # æ‰‹åŠ¨è®¾ç½®æ‰“å°çš„è¡Œæ•° A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988&gt;&gt;&gt; df.index # index æ˜¯ row çš„ ç´¢å¼•DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df.columnsIndex([u'A', u'B', u'C', u'D'], dtype='object')&gt;&gt;&gt; df.valuesarray([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804&gt;&gt;&gt; df.sort_index(axis=1, ascending=False) # æ˜¯å¦æŒ‰ç…§ columnsçš„å€¼ä¸‹é™æ¥æ’åº D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690&gt;&gt;&gt; df.sort_values(by='B') # æŒ‰ç…§columns B å‡åºæ¥ A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 ç´¢å¼•columnså’Œrows 4. Reference pythonï¼Œnumpyï¼Œpandasæ•°æ®å¤„ç†ä¹‹å°æŠ€å·§ ####","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.com/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.com/tags/Pandas/"}]}]