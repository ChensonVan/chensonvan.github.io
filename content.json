[{"title":"PostgreSQL复习笔记","date":"2017-03-23T23:38:55.000Z","path":"2017/03/24/PostgreSQL复习笔记/","text":"1 常见问题1.1 匹配一致的编码规则123SET character_set_client=&apos;utf8&apos;; -- gbkSET character_set_connection=&apos;utf8&apos;; -- gbkSET character_set_results=&apos;utf8&apos;; -- gbk 1234-- 创建数据库时候设置CREATE DATABASE `test`CHARACTER SET &apos;utf8&apos;COLLATE &apos;utf8_general_ci&apos;; 1.2 如何理解索引索引字面上理解就是对数据所建立目录，它可以加快我们的查询速度，但是同时也降低了增删改的速度。 创建原则 不要过度使用索引 最好在查询频繁的列上使用索引 如果构建索引，这一列尽量是离散值，而不要过于连续的区间 索引的类型 普通的索引 index 唯一的索引 unique index 一张表上，只能有一个主键，但是可以有一个或是多个唯一索引 主键索引 primary key 不能重复 12-- 查看一张表上的所有索引show index from TABLE_NAMES; 1.3 模糊查询 % 匹配任意字符 _ 匹配单个字符 举个栗子 1234567891011SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC%&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC_&apos;; 1.4 理解 COUNT 见3.1 聚集函数 1.5 理解 UNION 和 UNION ALL UNION 用于合并两个或是多个SELECT语句的结果集 注意： SELECT语句必须拥有相同的数量的列 列的需要拥有相似的数据类型 每条SELECT语句中的列的顺序必须是一致的 结果不允许有重复，否则使用 UNION ALL 123456SELECT mid, sex, ageFROM TABLE_1UNION -- UNIAON ALL -- 允许有重复的值出现在结果集中SELECT mid, sex, ageFROM TABLE_2;-- ORDER BY mid; -- 可以对其结果进行排序，注意的是排序只是针对合并后的结果集排序 1.6 理解 JOIN（左链接，内链接和外链接） 不同的几种JOIN类型，以及之间的差异 JOIN：如果表中至少有一个匹配，则返回行 LEFT JOIN：即使右表中没有匹配，也从左边返回所有的行 RIGHT JOIN：即使左表中没有匹配，也从右表中返回所有的行 FULL JOIN：只要其中一个表存在匹配，就返回行 INNER JOIN 平常我们需要链接两个表的时候，可以用以下方法 123SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P 同时，我们也可以使用JOIN来实现上面的语句 12345SELECT Person.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName LEFT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意到上面的，左边所有的行都返回了，即使没有出现在右表当中，没有的值为NULL RIGHT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意，即使左边没有全部匹配到右边，依然在最后的OrderNo中，返回了右表所有的行数，没有的值为NULL 1.7 理解 HAVING 见3.2 在SQL中增加HAVING子句的原因是由于WHERE中无法使用聚合函数 2 数据库的基本操作2.1 表的使用 使用列约束建表 12345678CREATE [TEMPORARY] TABLE 表名 ( -- TEMPORARY 建立一张临时的表col_not_null INT NOT NULL, -- 列名 类型 &#123;约束1 约束2 ...&#125;col_unique INT UNIQUE,col_prikey INT PRIMARY KEY, -- NOT NULL + UNIQUE 主键col_default INT DEFAULT 42,col_check INT CHECK(col_check &lt; 42)col_ref INT REFERENCES -- 约束这个值必须是另一独立的表的某个列中的某个值); 使用表级约束建表 1234567CREATE TABLE 表名 ( myKey_1 INT, myKey_2 INT, myString varchar(15), CONSTRAINT cs1 CHECK (myString &lt;&gt; &apos;&apos;), -- 不能位空字符串 CONSTRAINT CS2 PRIMARY KEY (myKey_1, myKey_2)); -- 修改表结构 添加新列 1ALTER TABLE 表名 ADD COLUMN 列名 类型; 重命名新添加的列 1ALTER TABLE 表名 RENAME COLUMN 列名 TO 新列名; 改变一些约束和其他规则 12ALTER TABLE 表名 DROP CONSTRAINT cs1; -- DROP约束ALTER TABLE 表名 ADD CONSTRAINT cs3 UNIQUE(列名); --添加新的约束 修改列的类型 1ALTER TABLE 表名 ALTER 列名 TYPE 新类型; 重命名表名 1ALTER TABLE 表名 RENAME TO 新表名; 使用临时表 临时表的功能基本和表是差不多的，区别在于当你的会话结束时，你与数据库连接断开后，临时表会自动被删除。 键的约束 作为一个列的约束的外键（列约束） 1234567CREATE TABLE 表名 ( ... ... customer_id INTEGER NOT NUll REFERENCES customer(customer_id), -- 关联到customer表 ... ...);-- REFERENCES 外表名(外表名中的列) 表级约束 123456CREATE TABLE 表名( ... ... customer_id INTEGER NOT NULL, ... ... CONSTRAINT 表名_列名_fk FOREIGN KEY(列名) REFERENCES 外表名(外面中的列)) 注意：比较推荐的是使用表级约束，而不是混和私用表级和列级约束 ​ 约束名表名_列名_fk允许外面更容易定位错误资源 2.2 视图 建立视图 123CREATE VIEW 视图的名字 AS select系列语句;-- 例子CREATE VIEW item_price AS SELECT item_id, description, sell_price FROM item; 当视图建立好的时候，我们可以像使用表一样来查询这个视图，可以使用SELECT或WHERE语句等。 每次在视图中执行SELECT时，数据都会被重建，所以数据总是最新的，而不是一个在视图被建立的时候冻结的拷贝。也就是当与之相关的表的数据发生该表的时候，VIEW里面的数据也随之改变，而不是储存了建立VIEW的时候的拷贝对象。或者也可以理解类似指针指向原先的表，当原先的表发生变化，这边的数据自然而然的就能够读取出来。 当然，SELECT语句是可以在多个不同的表中提取数据的。 删除和替换VIEW 12DROP VIEW 名字; -- 不影响我们已有的数据CREATE OR REPLACE VIEW 名字 AS 新的select系列语句; 一些与VIEW常用的指令 12\\dv -- 查看当前数据库中的所有的VIEW\\d VIEW的名字 -- 查看具体的某一个VIEW的结构 2.3 INSERT语句 基本插入语句 123INSERT INTO 表名 VALUES (每列的值的列表);INSERT INTO customer VALUES(18, &apos;Mr&apos;, &apos;Jeff&apos;, &apos;Baggott&apos;, &apos;Midtown Street A\\\\33&apos;, &apos;Milltown&apos;, &apos;MT9 8NQ&apos;, &apos;746 3956&apos;); PS：这种操作很危险，SQL注入攻击 推荐的安全方法 12345INSERT INTO 表名(列名的列表) VALUES(跟之前的列的列表对应列的数值);INSERT INTO customer(customer_id, title, fname, lname, addressline, ...)VALUES(19, &apos;Mrs&apos;, &apos;Sarah&apos;, &apos;Harvey&apos;, &apos;84 Willow Way&apos;, ...); PS：避免在插入数据的时候为serial类型的数据提供数值，因为这个是系统自动添加的 访问序列生成器 序列生成器总 是被命名为&lt;表名&gt;_&lt;列名&gt;_seq 123currval(&apos;序列生成器名&apos;);nextval(&apos;序列生成器名&apos;);setval(&apos;序列生成器名&apos;, 新的值); 插入空值 123INSERT INTO customer VALUES(16, &apos;Mr&apos;, NULL, &apos;Smith&apos;,&apos;23 Harlestone&apos;, &apos;Milltown&apos;, &apos;MT7 7HI&apos;, &apos;746 3725&apos;); 使用 \\copy 命令 步骤 先生成如下格式的数据 ​ 再生成如下格式的数据，保存成.sql拓展名的文本文件 ​ 使用\\copy命令导入数据 ​ PS：SQL 里头的 COPY 命令有一个优点：它明显比\\copy 命令快，因为它直接通过服务器进程执行。\\copy 命令是在客户进程中执行，有可能需要通过网络传输所有数据。而且 COPY 在发生错误的时候会更可靠。除非你有大量的数据，否则区别不会太明显。 2.5 从数据库中删除数据 DELETE语句 语法类似于UPDATE语句 1DELETE FROM 表名 WHERE 条件; TRUNCATE语句（不推荐，因为不安全） TRUNCATE语句是把表中所有的数据都删除，但是保留这张表的结构，也就是说最后剩下了一张空表，所有的行都被删除了。 1TRUNCATE TABLE 表名; DROP语句 DROP语句就是删除了整张表的内容，包括表的结构。DROP完之后，这张表就是不存在的了 1DROP TABLE 表名; 2.6 修改数据库中的数据 UPDATE语句 1UPDATE 表名 SET 列名 = 值 WHERE 条件; 1UPDATE customer SET town = &apos;Leicester&apos;, zipcode = &apos;LE4 2WQ&apos; WHERE 一些条件; 如果没有WHERE子句的话，会导致表中的很多甚至是所有的行都被同时更新了 通过另一个表更新 1UPDATE 表名 FROM 表名 WHERE 条件; 3 高级数据选择3.1 聚集函数 Group By and count(*) 错误使用 1SELECT count(*), town FROM customer; 正确使用 1SELECT count(*), town FROM customer GROUP BY town; 结果是获得一个城镇的列表以及每个城镇的客户数量（count(*)) 同时我们也可以用两个columns name在GROUP BY中，然后用ORDER BY指定排列顺序。没有GROUP BY的话按照GROUP BY中的town，lname排序 Having Having是一种用于聚集函数的WHERE从句，我们使用HAVING来约束返回的结果为针对特定的聚集的条件为真的行，比如count(*) &gt; 1 PS：聚集函数无法在WHERE从句中使用，只能用在HAVING从句中 举个栗子： 选出有超过一个客户的城镇，在里使用一个HAVING从句来约束大一的行 SELECT中从句的优先度 1234567SELECTFROM WHEREGROUP BYHAVINGORDER BY -- DESC 下降LIMIT -- 用于限制rows是行数 mysql count(Column_Name) count(*) 统计所有的行 count(column_name) 统计所这个列中值不是NULL的行 count(Distinct column) 只统计这个列中唯一的情况，不重复统计 min min 函数使用一个列名做参数且返回这个列中最小的值。对于 numeric 类型的列，结果应该和预期一 样。对于时态类型，例如 date 的值，它返回最小的日期，日期既可以是过去也可以是未来。对于变长的字符串（varchar 类 型），结果可能和预期有点不同：它在字符串右边添加空白后再进行比较。 min 函数忽略 NULL 值。忽略 NULL 值 是所有的聚集函数的一个特点，除了 count(*)（当然，是否一个电话号码是最小值又是另一个问题了 PS：小心在 varchar 类型的列中使用 min 或者 max，因为结果可能不是你预期的。 max sum Sum 函数使用一个列名作为参数并提供列的内容的合计。和 min 和 max 一样，NULL 值被忽略。 和 count 一样，sum 函数支持 DISTINCT 变体。你可以让它只统计不重复值的和，所以多条值相同的行只会被加一 次 avg 我们要看的最后一个聚集函数是 avg，它使用一个列名做参数并返回这个列数值的平均值。和 sum 一样，它忽略 NULL 值。这里是一个示例 ​ 3.2 子查询 问题一 找到价格比平均价格高的商品项目 方法一（土方法） 方法二（用嵌套WHERE从句） 问题二 找到那些成本高于平均成本但售价低于平售价的产品 方法一（土方法） 方法二（用嵌套WHERE从句） 问题三 - 返回多行记录的子查询 之前的两个问题中，WHERE中的子查询中的SELECT字句返回的最后只有一个值——因为用了count()聚集函数。如果WHERE中的SELECT子句返回多个结果值呢？ 答案是用 WHERE column_name IN (RESULTS) 当然也可以使用NOT IN 来排出选项 3.3 相关子查询​ 在之前的例子中，这里的两个SELECT实际上是不相关的，也就是在内部的SELECT的结果基础上，外部SELECT再做继续查询 ​ 但是相关子查询则是内外的SELECT中，表与表之间是有关系的 格式 PS：建议在相关子查询中使用表的别名 3.4 UNION链接 格式 12345SELECT town FROM tcustUNIONSELECT town FROM customer;SELECT town, zipcode FROM tcust UNION SELECT town, zipcode FROM customer; PS：UNION 连接的使用有一些限制。你要连接的两个从两个表中查找列表的列必须有相同列数，而且选择的每个列必须都有相兼容的类型。 这个查询，虽然非常无意义，但是是有效的，因为 PostgreSQL 可以连接这两个列，即使 title 是一个固定长度的列而 town 是一个变长的列，因为他们都是字符串类型。例如如果我们尝试连接 customer_id 和 town，PostgreSQL 会告诉我们 无法做到，因为这两个列的类型不同。 3.5 自连接 3.6 外链接4. 表的管理​ 5. 事务和锁","tags":[{"name":"SQL","slug":"SQL","permalink":"http://chenson.com/tags/SQL/"}]},{"title":"Machine Learning - Linear Regression and Logistic Regression","date":"2017-03-17T01:06:30.000Z","path":"2017/03/17/Machine-Learning-Linear-Regression-and-Logistic-Regression/","text":"1. 三要素当一开始接触Andrew在Coursera上的ML公开课的时候，对线性回归和逻辑回归这两种模型有个大体的认识。但是在上完cs229的前三节课，初步了解了这两种模型背后的数学模型，Linear Regression和Logistic Regression背后的概率分布，了解到了这两种概率分布其实只是exponential family中的特例。但同时也开始对一些概念性的东西感觉很模糊，所以觉得有必要好好整理一下这部分的内容。 1. 1 Hypothesis首先对于样本数据，输入x和输出y之间是通过Target function在转换的，也就是 Target function f(x) = y。但是我们并不知道这个f(x)都是怎样的，所以我们假设了这么一个Hypothesis function去模拟这个Target function，使得我们用同样的输入x会的一个预测值y’，使得这个y’不断逼近真实值y。 Linear Regression$$H(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx$$ Logistic Regression$$H(x) = g(\\theta^Tx) = \\frac 1 {1 + e^{-\\theta^Tx}}$$等价于（即log odds，logit）$$ln \\frac y {1 - y} = \\theta^T x = ln \\frac {p(y=1| x; \\theta)} {p(y=0| x; \\theta)}$$ 1. 2 Cost functionCost function呢，实际上也可以叫做Error function，就是用我们上面假设的Hypothe function所预测出来的值y’和真实值y之间的误差。而我们需要做的是根据假设出的Hypothesis function，取一个合适的权重值，即theta的值，使其取的一个较低的cost，也就是这预测值与真实值之间的误差最小。 Ordinary Least Squares (Square Loss Function) 常用的方法是最小二乘法 $$J(\\theta) = \\frac 1 2 \\sum_{i=i}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$ ​ 当然我们也可以从概率的角度来理解这个问题$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$​ 这里的ϵ是我们预测值与实际值之间的误差，这个问题我们会留到后面重点讲解。 0-1 Loss Function Absolute Loss Function Log Loss Function 1.3 Algorithm至于怎么使得上面的cost function最小呢，因为对于某些数据，其features有成千上百个，我们很难去找到这个最小的极值点，使得cost function最下，所以这个Algorithm就是用来找cost function的最小值的。常用的方法有如下 Gradient Descent 在梯度下降中，我们采用的是 LMS update rules(Least Mean Squares)$$\\theta_j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}= \\alpha e^{(i)} x^{(i)}$$ 当我们的预测值与实际值之前的误差ϵ很小时，我们就只需要对θ做出很小的调整，反之，说明当前的θ不对，需要调整的幅度比较大。直到最后收敛为止。 上面Repeat中的步骤实际是等同于cost function对θ求导的过程，所以为了保证收敛的效果，cost function应该是要 convex fuction，就不会导致停留在了local optim点。 推导过程如下： ​ 可视化后大概的过程如下： 根据对于哪些θ求导，Gradient Descent还可以继续分成不同的几种方法 Batch Gradient Descent Stochastic Gradient Descent Mini Batch Gradient Descent SGD with mini-batch 问题：为什么下降是 - ，θ大J一定大吗？ Normal Equation (linear regression) 推导过程比较复杂，需要的数学知识比较多，这里只给出结论。想要看具体推导过程的还说看cs229的第二节课吧 ：）(cs229-notes-1, p11)$$\\theta = (X^TX)^{-1}X^T\\vec y$$ ​ Newton BGFS Simulated Annealing 2. Generalized Linear Model (GLM) 广义线性模型之前我们在cost function中提到过，我们可以从概率的角度来理解误差这个问题。对于Linear Regression和Logistic Regression，我们都可以假设：$$y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}$$这里误差ϵ假设为IID (independently and identically distributed) 2.1 Linear Regression在Linear Regression中，y是连续的值，所以误差ϵ也是一个连续的值。假设误差ϵ是符合Gaussian Distribution (Normal Distribution)，所以有 Gaussian Distribution $$y | x; \\theta ∼ N (μ, σ^2)$$ Probability of error ​ σ实际上是不影响概率的分布的，所以假设σ = 1，所以这里可以忽略了。因此也就是等同于如下 Likelihood$$L(\\theta) = L(\\theta; X, \\vec y) = p(\\vec y | X; \\theta)$$ ​ 以上是在给定输入x和权重θ下，我们的预测值是真实值y的概率，所以这个概率呢，当然是越高越好啦。我们就是要想办法去 maximum likelihood。 ​ 对这个概率取个log（不影响结果），有 ​ 可以看到最终的式子里面，我们就是要求cost function的最小值。 2.2 Logistic Regression在Logistic Regression中， y是离散的值 {0, 1}，所以误差ϵ也是一个离散的值 {0, 1}。假设误差ϵ是符合Bernoulli Distribution，所以有 Bernoulli distribution Probability of error 把误差ϵ代入到上面的bernoulli function，可得 Likelihood ​ 同样，对上面去log之后有 ​ 同样，我们尽量要maximize the likelihood，就相当于要最小化后面的那部分（cost function）。这里可以用Gradient Ascent算法来求最大值，但是和Gradient Descent不一样（为什么）$$\\theta := \\theta + \\alpha∇_\\theta l(\\theta)$$​ 注意这里是加号，在求cost function的最小值时，用的是减号。 ​ 对其求导可得 ​ 所以有 ​ Digression Perception ？ ？ ？ 2.3 GLM似乎到现在，讲了半天都也没讲什么是广义线性模型，实际上呢，上面我们已经从误差概率的角度上来分析了线性回归和逻辑回归两种特例，因为他们误差服从的概率分布都是属于Exponential Family中的一种。 The Exponential Family η 被称作natural parameter，它是指数分布族唯一的参数T(y) 被称作sufficient statistic，很多情况下T(y)=y a(η) 被称作 log partition functionT函数、a函数、b函数共同确定一种分布 那么这个模型和上面我们提到过的Gaussian Distribution 和Bernoulli Distribution有什么关系呢？其实上面的这几个参数取不同的值的时候，即可得到不同的分布模型 Gaussian Distribution Bernolli Distribution 2.4 如何构建一个GLM模型在上面我们只是看到了一个通用的GLM概率模型 实际上对于构建这么一个概率模型，需要作出三个假设作为前提条件： p(y | x; θ) ∼ ExponentialFamily(η). 对于给定的输入x，θ和输出y需要服从某一种指数分布，这个指数分布由η 决定的 对于给定的输入x，预测T(y)的值，且经常T(y) = y。而我们是预测是H(x) 需要满足 H(x) = E[y|x] 对于自然参数η和输入x之间，需要存在相关性关系的，即：η = θT x","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"KMP算法","date":"2017-03-15T06:48:47.000Z","path":"2017/03/15/KMP算法/","text":"参考内容 阮一峰 - 字符串匹配的KMP算法 Partial Match Table 上来先上个结论，这个先暂时不管怎么生成，用于KMP表的移动。 移动位数 = 已匹配的字符数 - 对应的部分匹配值 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是6 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位。 匹配到了第三个字符C，C在上表中的值是0，已经匹配的字符数是2 所以移动的位数是 2 - 0 = 2，将搜索词向后移动2位 因为第一个字符不匹配，就将整个字符串向后移一位 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是2 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位 逐个比较，直到完全匹配 如果还需要继续搜索的话，D在上表中的值为0，匹配到的个数为7，移动的位数= 7 - 0 = 7，将整个字符串往后移动7位。接着就是重复之前的比较步骤了。 计算Partial Match Table ​ 这里需要理解两个概念：前缀和后缀 ​ “前缀” 指除了最后一个字符以外，一个字符串的全部头部组合； ​ “后缀” 指除了第一个字符以外，一个字符串的全部尾部组合。 ​ 而我们需要的Partial Match Table就是前缀和后缀的最长共有元素的长度 ​ 继续以上面的例子讲解 “A”的前缀和后缀都为空集，共有元素的长度为0；“AB”的前缀为[A]，后缀为[B]，共有元素的长度为0；“ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；“ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；“ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1；“ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2；“ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。 了解了KMP的原理之后，来看一下代码该怎么写。 举个栗子： Text = a b a c a a b a c c a b a c a b a a b b Pattern = a b a c a b 根据前面的Partial Match Table, 我们可以算出Pattern的这个表 P a b a c a b steps 0 0 1 0 1 2 此时我们用两个指针 i 和 j 来表示 Text 和 Pattern 中的字符。 当 T[ i : i + j ] == P[ 1 : j ] 的时候，就是 Text 中包含了我们需要查找的 Pattern 先让 i 和 j 都从 1 开始（python代码中从0开始） 当T[ i ] = P[ j ]的时候，此时指针在 Text 和 Pattern 上都往前各走一步，即j+1，i+1 当 i = 6，j = 6 的时候，我们可以看出T[ i ] != P[ j ]，此时 j 就不能再继续往前走了，需要退回去几步。 那么到底是几步呢，经过上面查表，此时匹配到5，重复的字符串个数为1，意思是对于这个字符串 abaca，abaca 和 abaca 中有一个重复了，我们就不需要再比较这个，跳过这个字符，移动的个数为 6 - 1 = 5，将字符串 Pattern 向前挪5位，新的 j 就等于1了，然后重复之前的步骤。 Python代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091DEBUG = TrueDEBUG = False# 生成 partial match tabledef PMT(sent): length = len(sent) result = [0] for i in range(2, length + 1): sub_sent = sent[0 : i] sub_len = len(sub_sent) surfix = [] prefix = [] for j in range(sub_len): prefix.append(sub_sent[0 : j]) surfix.append(sub_sent[j + 1: sub_len ]) prefix.remove('') surfix.remove('') all_fix = list(set(prefix + surfix)) max_len = 0 com_str = '' for fix in all_fix: if fix in prefix and fix in surfix: if len(fix) &gt; max_len: max_len = len(fix) com_str = fix result.append(max_len) return result# KMP算法def KMP(text, pattern): if (len(text) &lt; len(pattern)): return 'Not Match' pmt = PMT(pattern) if DEBUG: print('Current Pattern is = ', pattern) print('Partial Match Table = ', pmt) i = 0 j = 0 while(i &lt; len(text)): if (text[i] == pattern[j]): if (j == (len(pattern) - 1)): return 'Match, the position in text is: &#123;&#125; - &#123;&#125; = &#123;&#125;'.format(i - j, i, text[i-j: i]) i += 1 j += 1 continue else: if j &gt;= 1: j = pmt[j - 1] else: i += 1 return 'Not Match'# 测试函数def test(text, pattern): result = KMP(text, pattern) print('Resutl = ', result) print('Pattern =', pattern) if DEBUG: for i in range(len(text)): print('&#123;:3s&#125;'.format(str(i)), end='-') print('') for i in range(len(text)): print('&#123;:3s&#125;'.format(text[i]), end='-') print('') print('\\n')# 测试部分text = 'BBC ABCDAB ABCDABCDABDE'pattern = 'ABCDABD'test(text, pattern)text = 'a b a c a a b a c c a b a c a b a a b b'pattern = 'a b a c a b'test(text, pattern)text = 'BBC ABCDAB ABCDABCDABE'pattern = 'ABCDABD'test(text, pattern)text = 'BBC AB'pattern = 'ABCDABD'test(text, pattern)text = 'ABCDABD'pattern = 'ABCDABD'test(text, pattern) 测试结果 12345678910111213141516171819202122232425Text = BBC ABCDAB ABCDABCDABDEPattern = ABCDABDResutl = Match, the position in text is: 15 - 21 = ABCDABText = a b a c a a b a c c a b a c a b a a b bPattern = a b a c a bResutl = Match, the position in text is: 20 - 30 = a b a c a Text = BBC ABCDAB ABCDABCDABEPattern = ABCDABDResutl = Not MatchText = BBC ABPattern = ABCDABDResutl = Not MatchText = ABCDABDPattern = ABCDABDResutl = Match, the position in text is: 0 - 6 = ABCDAB[Finished in 0.1s]","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.com/tags/Algorithm/"}]},{"title":"Hadoop权威指南笔记（一）","date":"2017-03-06T13:44:33.000Z","path":"2017/03/07/Hadoop权威指南笔记（一）/","text":"1. Hadoop1.1 初识Hadoop非常好的Tutorial 在学习hadoop之前，我觉得有必要了解一下hadoop的基本构成以及一些术语。 Block HDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks. By making a block large enough, the time to transfer the data from the diskcan be made to be significantly larger than the time to seek to the start of the block.Thus the time to transfer a large file made of multiple blocks operates at the disk transferrate.A quick calculation shows that if the seek time is around 10 ms, and the transfer rateis 100 MB/s, then to make the seek time 1% of the transfer time, we need to make theblock size around 100 MB. The default is actually 64 MB, although many HDFS installationsuse 128 MB blocks. This figure will continue to be revised upward as transferspeeds grow with new generations of disk drives.This argument shouldn’t be taken too far, however. Map tasks in MapReduce normallyoperate on one block at a time, so if you have too few tasks (fewer than nodes in thecluster), your jobs will run slower than they could otherwise. Node 简单的说就是一台主机，一台电脑。在hadoop中，有NameNode, DataNode, Secondary NameNode, JobTracker Node, TaskTracker Node, CheckpointNode 和 BackupNode。对一个cluster，NameNode只能有一个，DataNode可以有多个 Rack 中文机柜/机架，就是用来存放node的storage，通常一个rack有几十个nodes组成，这些nodes存放在同一个机柜，连接一个交换机 A Node is simply a computer. This is typically non-enterprise, commodity hardware for nodes that contain data. Storage of Nodes is called as rack. A rack is a collection of 30 or 40 nodes that are physically stored close together and are all connected to the same network switch. Network bandwidth between any two nodes in rack is greater than bandwidth between two nodes on different racks.A Hadoop Cluster is a collection of racks. 1.2 Major Components Distributed Filesystem — hadoop中是HDFS MapReduce 1.5 Install configuration123456# 待整理hadoop fshadoop dfs# fs refers to any file system, it oucld be local or HDFS# but dfs refers to only HDFS file system 2. MapReduce2.1 初识MapReduce 整个过程可以分为三个阶段，Input， MapReduce and Output 在input和output阶段，数据是存在HDFS文件系统中，其系统的block size大小默认是64/128MB。 在MapReduce中，又可以分为两个阶段，Map and Reduce，数据从map function到reduce function是存在local disk中，(soreing in HDFS with replication would be overkill)，然后通过network传输数据. 在每个阶段中，input和output的数据都是以 (key, values) 格式进行处理的，然后通过 map function 和 reduce function 进行处理。在本例中，input data的key是从数据文件开始处的行数的偏移量，但是map function输出的key是年份数据，以及reduce function输出的key是也不同的。所以这三个key-value pairs是不同的。 原始数据 Key-Values 以上为原始数据中input进来后的key-values的数据。然后map function阶段，提取出上面文件中的 1950 和 0001 之类的数据，组成新的key-values作为输出给下一阶段。 Key-Values in Map Function 在将Map Function的输出传给Reduce Function之前，实际上MapReduce Framework还是有对数据进行一个处理步骤。从最上的图一中，我们仍然可以看到Map和Reduce之间有一个 Shuffle 的过程。因为之前我们提到了，Map的过程中，只是实现了一个key-value匹配的过程，所有出来的数据也是无序的，而 Shuffle 就是对这个输出 sort &amp; group 的过程，然后将输出传给 Reduce Function 进行处理 当数据从Reduce Function中处理完后出来的大概如下，注意这个reduce只是选择最大值，其他reduce function可能做的是统计或者实现其他功能。 现在再看另外一个经典的WordCount的例子 在Hadoop系统中，处理一个wordcount的任务可以大致分成四个主要阶段，input，map，reduce，output。其中 Map 和 Reduce 可以继续细分，即分成多个 map tasks 和 reduce tasks。 这些tasks然后被 YARN 给分配集群中多台不同的机器处理。这其中的细节等到往后再讨论。 上面提到的分成多个tasks时，应该是input data切片分给多个maps（而不是一个大的map分成多个小的tasks）， 每个MapReduce分到一个fixed-sized 的数据，通常是64/128MB，这个过程叫做 input splits。然后每个split分配一个map task，同时运行在不同的机器上处理。这样划分的好处是有利于load-balancing，对于性能较好的机器可以处理更过是splits。 2.2 Data Flow 上图可以看出hadoop的整个数据流向，其中虚线代表是在一个node，实线代表的是不同node之间。在同一个node之间，数据的读取存储就有速度上的优势，不同node之间，也就是不同主机之间，就必须通过network进行传输，速度较慢。 Partition 当只有一个reduce的时候，map function的output当然就直接传给这个reduce了。但是当有多个reduce的时候，怎么办呢？此时map会将其输出进行partition(分区)，每一个reduce的任务都会创建一个分区，且每一个reduce task都会有一个partition (There can be many keys (and their associated values)in each partition, but the records for any given key are all in a single partition)，也就是说同一个key会在同一个partition中。 Shuffle and Sort 在map和reduce之间的data flow是Shuffle，从上图可以看出，一个reduce可以接受来自多个不同的map的output，其中包含了sort，partition等过程。 Combiner Functions 之前我们讨论过，data flow在map和reduce之间是通过network进行传输的，但我们知道map function的output是一个个key-value的键值对的，这些key-value paris中，有些是可以通过combiner function进行combine的，这样做的目的是减小map和reduce之间传输的数据大小，加快传输数据。 Combiner Function在许多情况和 Reduce Function是很像的，因为做的工作和reduce是比较类似的，只是处理的是局部map的output(因此Combiner是运行在map output端)，减少data flow的size。但是对于是否调用combiner function，这个是不确定的。因为有些情况下的output是不适合进行combine，有些则又是要多次调用进行合并。因为这个，Combiner是可选的，即可以调用，也可以不调用，当不调用的时候，就必需不能影响程序的正常运行。所以Combiner的input和output是一样的，和Mapper的output、Reducer的input一样。 对于有些特殊情况，甚至连reduce function都不需要。 举个栗子： 适用情况（Commutative &amp; Associative） ​ Reduce Function Combiner Function ​ Commutative: max(a, b) = max(b, a) ​ Associative: max(max(a, b), c) = max(a, max(b, c)) 不适用情况： 伪代码 In-Combiner Function Advantage 相比Combiner，In-Combiner的效率更高。 可以减少一些Mapper和Reducer之间的key-value pairs，可以减少处理这部分的开销。因为Combiner只是减少了一些Mapper和Reducer之间的intermediate data，但是并没减少从Mapper的output出来的key-value pairs的数量。但是In-Combiner是是Mapper 的一部分，也就是说key-value pairs在Mapper 输出前就已经减少了。 减少了key-value pairs可以减少系统的object serialization and deserialization 的开销，即垃圾回收机制 Disadvantage 内存使用，因为要保存一个array在内存中，当数据量很大的时候有可能会爆了。解决方案 有两个，第一是限制array的个数，第二是限制内存的使用。当这俩到达某一个阈值的时候，就发送给Reducer。 第二是讲一个Map的过程分成几个部分，导致debug中可能出现oedering-dependent bugs，调试可能比较困难。 ​ ​ ​","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.com/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.com/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Recommender-Systems/","text":"1. What is Recommender Systems对于推荐系统的定义，我们先举几个例子来理解一下。 电影网站给用户推荐电影，可以根据该用户以往的评分，比如给浪漫爱情电影评分高，给动作片评分较低，那么系统可以根据这些信息，给用户推荐偏向浪漫爱情的电影 如果是新用户呢？我们没有该用户的评分信息。那么我们可以根据整个系统中，某些电影评分较高进行推荐 那么如果是新网站，新用户呢？ 以上例子，我们可以把推荐系统分成两类。 Content-based systems Content-based，就是基于已有的信息进行推荐。具体哪些信息呢？在上面的电影推荐系统中，有两类信息需要分析。 第一，是User的评分信息，比如给爱情片评分高，给动作片评分低。 第二，是Movie的特征信息，比如这部电影偏向爱情片多一些，但也有一部分搞笑。所以在A（爱情片）和B（搞笑片）中， A的权重更高，B的较低 基于以上两部分信息，我们可以给用户推荐他所喜欢的电影。 Collaborative filterring systems 协同过滤器，则是基于用户/物品之间的相似度进行推荐的。即用户A和用户B都喜欢爱情、浪漫电影，我们就可以把用户A评分过的爱情浪漫电影，推荐给用户B。 2. Content-based systems2.1 Problem Analysis以电影推荐系统为例，假设我们已经对系统中的电影特征有了较为完善，即我们知道某部电影属于爱情片多少分，属于动作片多少分。 那么我们现在以Alice为例，她对两部爱情片评分比较高，对于两部动作片评分为0。那么系统就可以给Alice推荐偏向爱情浪漫的，且不怎么属于动作片的电影。 Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love ？ 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 2.2 Optimization Objective实际上我们已经假设之前对所有电影的特征进行了统计，所以此时有电影特征向量X，以及用户对于电影的评分Y向量。根据此时已有的信息，我们需要求出theta的值。所以能够对于那么没有评分过的电影，根据theta和x求出分数y。 因为一开始theat的值是随机的，所以我们用Linear Regression的方法，不断减少cost function的值求出theta。 值得注意的是，因为这里是多个用户，每一个用户我们求出一个theta值。最后对于多个用户，我们需要求出多个theta值。 Actually, we can assume that we have known all features about the all movies, that is x1, x2, …, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 2.2 Gradient descent update 2.3 Gradient descent in Logistic Regression Question: Why we don’t need $\\frac 1 m$? Anwser: As there are only one user However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to learn theta and features at the same time 3. Collaborative filtering3.1 Proble motivation Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love ? 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 3.2 How to do 在之前部分中，我们了解到了content-based，是已知 x 和 y，求 theta。 Assume:$$\\theta^{(1)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(2)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(3)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\space\\theta^{(4)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\spacex^{(1)} = \\begin{bmatrix} 1 \\ 1.0 \\ 0.0 \\end{bmatrix}$$For Movie 1, we can calculate the result of Movie1 rating by all users.$$\\theta^{(1)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(2)} * x^{(1)} \\approx 5 \\\\\\\\theta^{(3)} * x^{(1)} \\approx 0 \\\\\\\\theta^{(4)} * x^{(1)} \\approx 0$$ 但是对于有些情况，我们并不知道x的特征值，该怎么办呢？ 逆向思考，我们也可以通过 theat 和 y，来求 x 的值。 那么对于 theta和x的值都不知道的情况下呢？ 对比特征 Linear Regression Collaborative filtering 特性向量X 已知数据 待求解数据 权重 θ 待求解数据 待求解数据 y值 已知数据 已知数据 3.3 Optimization Algorithm For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. $$θ -&gt; x -&gt; θ -&gt; x -&gt; θ -&gt; x -&gt; θ -&gt; x -&gt; …$$ Actually, these two steps are Linear Regression, we shoud do that simultaneously to update theta and x. 3.4 Collaborative filtering Optimization Algorithm 实际上，上面是两个 LR的问题，我们可以将上面两步合并到一起，这个就是collaborative filterring， 此时的optimizatino object 就从 J(theta) 和 J(X) 变为了 J(theta, X)。 具体步骤如下 3.5 Vectorization: Low rank matrix factorization首先，我们先把评分Y用向量表示出来，同时表示为Theta和X两个矩阵的乘积$$Y= \\begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \\ 5 &amp; ? &amp; ?&amp; 0 \\ ? &amp; 4 &amp; 0 &amp; ? \\ 0 &amp; 0 &amp; 5 &amp; 4 \\ 0 &amp; 0 &amp; 5 &amp; 0\\end{bmatrix} =\\begin{bmatrix}(\\theta^{(1)})^T(x^{(1)}) &amp; (\\theta^{(2)})^T(x^{(1)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(1)}) \\\\(\\theta^{(1)})^T(x^{(2)}) &amp; (\\theta^{(2)})^T(x^{(2)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(2)}) \\\\… &amp; … &amp; … &amp; … \\\\(\\theta^{(1)})^T(x^{(n_m)}) &amp; (\\theta^{(2)})^T(x^{(n_m)}) &amp; … &amp; (\\theta^{(n_u)})^T(x^{(n_m)})\\end{bmatrix} = X * \\Theta’, R \\in (n_m × n_u)$$ $$X = \\begin{bmatrix}—(x^{(1)})^T— \\\\—(x^{(2)})^T— \\\\… \\\\—(x^{(n_m)})^T—\\end{bmatrix},x^{(n_m)} = \\begin{bmatrix}x^{(n_m)}_1 \\ x^{(n_m)}_2 \\ … \\ x^{(n_m)}_n\\end{bmatrix}, R \\in (n_m × n)$$ $$\\Theta = \\begin{bmatrix}—(\\theta^{(1)})^T— \\\\—(\\theta^{(2)})^T— \\\\… \\\\—(\\theta^{(n_u)})^T—\\end{bmatrix},\\theta^{(n_u)} = \\begin{bmatrix}\\theta^{(n_u)}_1 \\ \\theta^{(n_u)}_2 \\ … \\ \\theta^{(n_u)}_n\\end{bmatrix}, R \\in (n_u × n)$$ 3.6 Mean Normalization对于那些新注册用户，系统中没有记录他们的偏好，则采用以下方法。 先计算出每部电影评分的平均值mu，然后把所有的评分都减去平均值（此后处理过的评分平均值为0）。虽然这样做对有评分记录用户是多余的，但却可以吧没有评分记录的用户给统一进来，避免全是0的情况。 4. Implement Algorithm4.1 Cost Function without Regularization Tips：这里需要计算的只是针对那些已经评分过的电影，对于用户没有评分过的不需要计算。 4.2 Collaborative filtering gradient$$\\frac {\\partial J} {\\partial x_k^{(1)}} , \\frac {\\partial J} {\\partial x_k^{(2)}} , …, \\frac {\\partial J} {\\partial x_k^{(n_m)}} \\space\\space for \\space each \\space movie\\\\\\frac {\\partial J} {\\partial \\theta_k^{(1)}} , \\frac {\\partial J} {\\partial \\theta_k^{(2)}} , …, \\frac {\\partial J} {\\partial \\theta_k^{(n_u)}} \\space\\space for \\space each \\space user$$Tips： 对于使用vectorization方法，最终只有两个for-loop，一个计算$X_{grad}$，一个计算$Theta_{grad}$ 如何对X和Theta求偏导数？ $$(Theta_{grad}(i, :))^T = \\begin{bmatrix}\\frac {\\partial J} {\\partial \\theta^{(i)}_1} \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_2} \\\\… \\\\\\frac {\\partial J} {\\partial \\theta^{(i)}_n}\\end{bmatrix}$$ 同样，我们只需考虑用户已经评分过的电影，用其作为训练样本 因为Vectorization非常容易搞乱各个matrix，所以建议先整理一下各个matrix的size，计算时可以根据matrix的size进行计算。 4.3 Implementation注意这里并没有给出完整的代码 (Octave/Matlab)，都只是主要的部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950% Theta : nu x n% X : nm x n% Y : nm x nu% R : nm x nupred = X * Theta'; %nm x nu 'diff = pred - Y;% Cost Function with regularizationJ = 0.5 * sum(sum((diff.^2) .* R));J = J + (lambda * 0.5) * sum(sum(Theta.^2)); % regularized term of theta.J = J + (lambda * 0.5) * sum(sum(X.^2)); % regularized term of x.% calculate Xfor i = 1 : num_movies, % the row vector of all users that have rated movie i idx = find(R(i, :) == 1); % (1 * r) % the list of users who have rated on movie i Theta_temp = Theta(idx, :); % (r * n) Y_temp = Y(i, idx); % (1 * r) X_temp = X(i, :); % (1 * n) % ((1 * n) * (n * r) -(1 * r)) * (r * n) = (1 * n) X_grad(i, :) = (X_temp * Theta_temp' - Y_temp) * Theta_temp; %' % regularization X_grad(i, :) = X_grad(i, :) + lambda * X_temp;end% calculate Thetafor i = 1 : num_users, % the row vector of all movies that user i has rated idx = find(R(:, i) == 1)'; % (1 * r) ' Theta_temp = Theta(i, :); % (1 * n) Y_temp = Y(idx, i); % (r * 1) X_temp = X(idx, :); % (r * n) % ((r * n) * (n * 1) - (r * 1)) * (r * n) = (1 * n) Theta_grad(i, :) = (X_temp * Theta_temp' - Y_temp)' * X_temp; % regularization Theta_grad(i, :) = Theta_grad(i, :) + lambda * Theta_temp;endgrad = [X_grad(:); Theta_grad(:)]; ​​​","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Machine Learning - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-SVM/","text":"1. Optimisation Objective$$h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\\\z = -\\theta^Tx$$ Why we need do that? 2. Hypothesis Function2.1 Logistic Regression$$\\frac 1 m \\sum_{i=1}^m [ y^{(i)} (-log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) (-log(1 - h_\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum_{j=1}^n \\theta_j^2$$ 2.2 Support Vector Machine$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ Analysis： 为了使得cost function取得最小值，我们令C*W + P部分中，C*W为零。即： 当 y = 1时， cost1 = 0，所以 z &gt;= 1 当 y = 0时， cost0 = 0，所以 z &lt;= -1 Note：1. cost0 and cost1 对应的是上图中左右两边的cost function，因为y=0和y=1的目标函数。 常数C取一个很大的值时比较好。因为C*W + P， 所以C大则W会变小，即相对penality就会变大，W会变小 为什么要重新选定一个cost function ？（逻辑回归的临界点为0，但是SVM的临界点是1，所以SVM更加精确。 ） 对应的线性逻辑回归？即次数不大于1的？ Decision Boundary 不是一条直线的情况 3. Large Margin Classifier12结论：常数C取一个比较大的值比较容易获得Large Margin ClassifierC大，则比较容易获得 以上为两类分布比较均匀的时候，Decision Boundary为图中黑色的线，所有点离黑色的距离都相对比较大比较均匀，但是当存在干扰点的时候如下图，Decision Boundary会由黑色变为粉红色。所以C的取值不能太大，也不能太小。需要求出最优解 4. Mathmatics Behind Large Margin Classification4.1 Vector Inner Product Note： 如何求投影p的值？ 当角度 &lt; 90°，p为正数。当角度 &gt; 90°时，p为负数。 向量内积$$u^Tv = ||u|| · ||v|| · cosθ = ||u|| · p_{v,u} = ||v|| · p_{u,v} = u_1v_1+u_2v_2$$ 4.2 SVM Cost Function$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum_{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ 当C取一个一个很大的值时，cost function只剩下后面P的部分。 假设θ0 = 0$$\\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + … + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + … + \\theta^2_n)= \\frac 1 2 ||\\theta||^2$$ 所以：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\p^{(i)}是点到向量\\theta的projection，即点到Decision Boundary的距离$$上面我们讨论了，当C取到一个合适的、较大的数值时，SVM的cost function就只剩下后面P的部分，即$$\\frac 1 2 ||\\theta||^2$$我们要减小cost function，所以需要减小θ的值。 当θ取到一个比较小的值的时候，还需要满足上面讨论的：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\\\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\\\$$所以θ比较小时，只能增加p的值去满足p*||θ|| &gt;= 1 或者 p*||θ||&lt;= -1。 这样就保证了p的值比较大，即点到Decision Boundary的大间距。 5. Kernels5.1 Kernels &amp; Similarity首先，我们回想一下之前的logistic regression中对于non-linear 情况的拟合。 Predict y = 1, if$$\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3x_2 + \\theta_4x_1^2 + \\theta_5x_2^2 + … &gt;= 0 \\\\\\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + … &gt;= 0$$即将fn定义为x的幂次项组合，如下：$$f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, …$$ 但是在SVM中，我们要重新定义fn，引入Kernel的概念，即用 kernel function来表示fn。 Note: l 是landmark，且如果training sets里面的数量为n的话，则landmark的数量也为n。 假设training sets数量为n，则对于一个新的example来说，可计算出n个新的特征f1…fn。然后用新的特征，对该example进行判断（低维转为高维的过程） kernel function为guassian function。当x与landmark l越接近时，两点的距离越小，值接近1 5.2 SVM with Kernels 对比之前的cost function，可以发现这里θ和f(x)跟之前的不同。 在logistic regression 中，θ的维度为(n+1) x 1, 包含θ0， 且n为单个example的特征个数 在SVM with kernel中，f(x)的个数为m，其中m是training sets中的个数，所以θ的维度应该是(m+1)x1 Steps 给定一组training sets，根据每个example，选取m个landmark点 计算每一个example与所有landmark的相识度，相同为1，非常不同接近为0。计算相识度的kernel function为Gaussian Function 最终，对于每一个example里面都可以计算出m个新的feature，所以对于这个training sets而言，会得到一个m*m的矩阵？ 将得到的m*m的矩阵，代入到Hypothesis中，计算出θ的值。 5.4 SVM parameters C = 1/λ Large C Small λ Large θ Lower Bias High Variance Over Fitting Small C Large λ Small θ Higher Bias Low Variance Under Fitting σ Large σ more smoothly Higher Bias Lower Variance Under Fitting Small σ less smoothly Lower Bias Higher Variance Over Fitting","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]},{"title":"Python数据分析笔记（一）","date":"2016-12-15T11:37:30.000Z","path":"2016/12/15/Python数据分析笔记（一）/","text":"1. 常见问题 Pandas.dataframe里面 .values, .iloc, .ix, .loc 的区别 Different Choices for Indexing loc: only work on index / labeliloc: work on position, from 0, 1, 2, … …ix: You can get data from dataframe without it being in the indexat: get scalar values. It’s a very fast lociat: Get scalar values. It’s a very fast iloc 12345678910111213141516171819202122232425262728&gt;&gt;&gt; df = pd.DataFrame(&#123;'A':['a', 'b', 'c'], 'B':[54, 67, 89]&#125;, index=[100, 200, 300])&gt;&gt;&gt; df A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df.iloc[0] # 用位置来索引A aB 54&gt;&gt;&gt; df.loc[100] # 用初始化时设置的index来索引，也就是自己给row设置的labelA aB 54&gt;&gt;&gt; df.loc[100:300] A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df['A'] # 索引 columns100 a200 b300 cName: A, dtype: object&gt;&gt;&gt; df.A # 索引 columns100 a200 b300 cName: A, dtype: object Pandas 和 Numpy之间的转换 np.ndarray 转化为 pd.dataframe 1pd.DataFrame(example) pd.dataframe 转化为 np.ndarray 1example.values[:, :] 读写效率的对比 npy读写效率最高，但最费硬盘空间，比如np.load(), np.save() csv其次，比如pd.Dataframe.to_csv()，pd.load_csv() txt读写，当然也可以很快，但是需要频繁的split，对格式规范的数据比较麻烦 至于简单的excel和word，可以用xlrd,xlwt来操作 2. Numpy N维数组对象，可以利用这种数组对象对整块数据进行一些科学运算，就是把array当做一种对象里操作。这和Python中的array是不同的。 举个栗子： 在Python中 123&gt;&gt;&gt; data = [1, 2, 3]&gt;&gt;&gt; data * 10[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3] Numpy的ndarray 12345&gt;&gt;&gt; data2 = np.array([1, 2, 3])&gt;&gt;&gt; data2array([1, 2, 3])&gt;&gt;&gt; data2 * 10array([10, 20, 30]) 轴(axes) 和 秩(rank) 轴表示的是一种维度，如一维的数据，二维的数据，三维的数据等 12345678910111213141516171819202122&gt;&gt;&gt; data3 = np.array([1, 2, 3]) # 注意这里的**方括号**&gt;&gt;&gt; data3.ndim # 查看维度1&gt;&gt;&gt; data3 = np.array([[1,2,3], [1,2,3], [1,2,3]])&gt;&gt;&gt; data3.ndim2&gt;&gt;&gt; data3 = np.array([[[1], [1], [1]], [[1], [1], [1]],[[1], [1], [1]]])&gt;&gt;&gt; data3.ndim3&gt;&gt;&gt; data3.shape(3, 3, 1) # 维度从最外层到里层&gt;&gt;&gt; data3.size9 # 3 * 3 * 1 = 9#一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。&gt;&gt;&gt; data3.dtype# 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8).&gt;&gt;&gt; data3.itermsize# 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。&gt;&gt;&gt; data3.data 常用的数组创建函数 打印数组 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(6) # 1d array&gt;&gt;&gt; print(a)[0 1 2 3 4 5]&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(12).reshape(4,3) # 2d array&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]&gt;&gt;&gt;&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4) # 3d array&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 如果一个数组用来打印太大了，NumPy自动省略中间部分而只打印角落 1234567891011&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt;&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] 禁用这种reshape来打印整个数组，需要对printoption参数进行设置 1&gt;&gt;&gt; np.set_printoptions(threshold='nan') 基本的数据运算 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; b = np.array(list(range(5, 10)))&gt;&gt;&gt; a + barray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; a - barray([-5, -5, -5, -5, -5])&gt;&gt;&gt; a * barray([ 0, 6, 14, 24, 36])&gt;&gt;&gt; a += b&gt;&gt;&gt; aarray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; c = np.ones(5)&gt;&gt;&gt; carray([ 1., 1., 1., 1., 1.])&gt;&gt;&gt; c + aarray([ 6., 8., 10., 12., 14.])&gt;&gt;&gt; c = np.ones(5, dtype=int)&gt;&gt;&gt; c + aarray([ 6, 8, 10, 12, 14]) 数组的运算 这就类似在Matlab/Octave中，对matrix/array中的数据执行批量运算，即Vectorization，前提是matrix/array的大小必须满足对应的要求。 用数组表达式可以代替循环操作，矢量化的运算是Numpy的优势。 数组转置和轴对换 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.T # 数组转置，轴对换array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]])&gt;&gt;&gt; np.dot(a.T, a) # 内积array([[125, 140, 155, 170, 185], [140, 158, 176, 194, 212], [155, 176, 197, 218, 239], [170, 194, 218, 242, 266], [185, 212, 239, 266, 293]])&gt;&gt;&gt; b = np.arange(16).reshape((2, 2, 4))&gt;&gt;&gt; barray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt;&gt;&gt; b.transpose((1, 0 , 2)) # 对高维数组，transpose需要得要一个由轴编号组成的元组才能对这些轴进行转置array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) 索引和切片 1234567891011121314151617181920212223242526272829303132333435# 一维数据&gt;&gt;&gt; a = np.arange(5)array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:]array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[1 : 4]array([1, 2, 3])&gt;&gt;&gt; a[1]1# 二维数据&gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6]])array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:, 1:3]array([[2, 3], [5, 6]])&gt;&gt;&gt; b[1][2]6&gt;&gt;&gt; b[1, 2]6# 三维数组&gt;&gt;&gt; c = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; c[1]array([[ 7, 8, 9], [10, 11, 12]]) 通用函数 P111 给 array 添加 columns 和 rows 123456789101112# 方法一np.c_[array1, array2] # 添加 columnsnp.r_[array1, array2] # 添加 row# 方法二 被插入的行np.insert(a, 2, values=b, axis=1) # 添加 columns# 方法三a = np.concatenate((a, -np.ones((a.shape[0], 1))), axis=1) # 添加 columns# 方法四np.column_stack((a,b)) 3. Pandas在Pandas中，Series和DataFrame是两个主要的数据结构 Series 类似一维数组，由一组数据（各种Numpt数据类型 ( list, dict等 )）和一组对于的数据标签组成 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&gt;&gt;&gt; obj = pd.Series(list(range(3, 8)))&gt;&gt;&gt; obj0 31 42 53 64 7dtype: int64&gt;&gt;&gt; obj.valuesarray([3, 4, 5, 6, 7])&gt;&gt;&gt; obj.index # 这里就是 index 索引，没有设置的是时候系统会自动设置为integer indexRangeIndex(start=0, stop=5, step=1)&gt;&gt;&gt; obj[1] # 可以用这索引，跟list用法类似，但是list中只能是数字，但pandas中可以自定义index的索引4# 自定义索引，普通的list和Numpy的数组就不行&gt;&gt;&gt; obj2 = pd.Series([1,3,4], index=['a', 'b', 'c'])&gt;&gt;&gt; obj2['a']1# 索引可以直接用来数组运算，这些在数据清洗的时候比较常用&gt;&gt;&gt; index = obj2 &gt; 2&gt;&gt;&gt; indexa Falseb Truec Truedtype: bool &gt;&gt;&gt; obj2[index]b 3c 4dtype: int64# 其他操作&gt;&gt;&gt; np.log(obj2)a 0.000000b 1.098612c 1.386294dtype: float64 # 当做字典来index&gt;&gt;&gt; 2 in obj2False &gt;&gt;&gt; 3 in obj2False&gt;&gt;&gt; 'a' in obj2 # 只能是index (key)，不能是valuesTrue# 用字典来初始化 Series&gt;&gt;&gt; obj3 = &#123;'one': 1, 'two': 2, 'three' : 3&#125;&gt;&gt;&gt; obj3&#123;'one': 1, 'three': 3, 'two': 2&#125;&gt;&gt;&gt; index = ['One', 'Two', 'Three']&gt;&gt;&gt; value = [1, 2, 3]&gt;&gt;&gt; obj4 = pd.Series(value, index)&gt;&gt;&gt; objOne 1Two 2Three 3dtype: int64 # 手动修改索引，且索引的值是不能重复的&gt;&gt;&gt; obj4.index = ['one', 'two', 'Three']&gt;&gt;&gt; obj4one 1two 2Three 3dtype: int64 # 重建索引，如果索引不存在的值，则引入NaN&gt;&gt;&gt; obj4.reindex(['one', 'two', 'Threeee']) # fill_value=0one 1.0two 2.0Threeee NaNdtype: float64# 可选 ffill/pad 向前填充或者bfill/backfill 向后填充&gt;&gt;&gt; obj4.ffill()one 1.0two 2.0Threeee 2.0dtype: float64# 当然也可以drop columns的内容&gt;&gt;&gt; obj4.drop('Threeee')one 1.0two 2.0dtype: float64 # 但是不能这么修改数值&gt;&gt;&gt; obj4.values = [4, 5, 6]---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2702 else:-&gt; 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):AttributeError: can't set attributeDuring handling of the above exception, another exception occurred:AttributeError Traceback (most recent call last)&lt;ipython-input-41-b4392e8960b0&gt; in &lt;module&gt;()----&gt; 1 obj4.values = [4, 5, 6]/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):-&gt; 2705 object.__setattr__(self, name, value) 2706 2707 # ----------------------------------------------------------------------AttributeError: can't set attribute ​ DataFrame DataFrame是一个表格型的数据结构，包含了一组有序的列，每列可以是不同的值类型。所有可以看做这是一个二维的数组，有行索引和列索引 创建DataFrame和基础操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; dates = pd.date_range('20170305', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6, 4), index=dates, &gt;&gt;&gt; columns=list('ABCD'))&gt;&gt;&gt; df A B C D2017-03-05 -1.616361 0.027641 0.328074 1.0386272017-03-06 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 -0.350594 0.231137 -0.004755 0.6014602017-03-08 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 -0.227631 -0.186816 -0.370522 0.343896# 当然也可以手动传进来创建df&gt;&gt;&gt; df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)&gt;&gt;&gt; df2Ones A B C D2017-03-05 1 -1.616361 0.027641 0.328074 1.0386272017-03-06 1 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 1 -0.350594 0.231137 -0.004755 0.6014602017-03-08 1 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 1 -0.227631 -0.186816 -0.370522 0.343896# 看DataFrame的attributes&gt;&gt;&gt; df.&lt;TAB&gt;df.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D 查看DataFrame里面的数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt;&gt; df.head() # 默认5行 A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401&gt;&gt;&gt; df.tail(3) # 手动设置打印的行数 A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988&gt;&gt;&gt; df.index # index 是 row 的 索引DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df.columnsIndex([u'A', u'B', u'C', u'D'], dtype='object')&gt;&gt;&gt; df.valuesarray([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804&gt;&gt;&gt; df.sort_index(axis=1, ascending=False) # 是否按照 columns的值下降来排序 D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690&gt;&gt;&gt; df.sort_values(by='B') # 按照columns B 升序来 A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 索引columns和rows 4. Reference python，numpy，pandas数据处理之小技巧 ####","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.com/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.com/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.com/tags/Pandas/"}]}]