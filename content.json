[{"title":"MLR算法初探","date":"2020-01-06T04:51:22.000Z","path":"2020/01/06/MLR算法初探/","text":"1. Introduction目前面临的问题 样本多 维度高 特征之间存在内在关系 常见做法 低维度强特征+GBDT 数据丧失了分辨率 数据是仍然是对历史行为的记忆，缺乏推广性 细粒度高维度特征+LR【模型能力有限，依赖人工特征工程】 自动学习高阶特征-FM/FFM FM只能你和特定的非线性模型，如2阶FM只能你和特征之间的线性关系和二次关系 深度神经网络 大规模工业级稀疏数据，难度较大 所以就以上几个问题，主要需要解决的是【大规模】和【非线性】 2. MLR(Mixed Logistic Regression)2.1 模型结构MLR混合逻辑斯特回归，全称是Large Scale Piecewise Linear Model，即LS-PLM大规模分片线性模型，可以看做是LR的一个推广，采用分治的思想，将数据分成不同的多个区域（通过领域知识先验将整个空间划分成多个区域，如何的？），每个区域里面训练一个线性模型，不同的区域之间做了平滑的连接（这里不是很理解如何做的平滑链接），最终拟合了高维空间的非线性分类面，目标函数的数学表达式如下 \\begin {align} p(y=1|x) &= \\sum_{i=1}^m \\pi_i(x)·\\eta_i(x) \\\\ &= \\sum_{i=1}^m \\frac {\\exp(u_i^T x)} {\\sum_{j=1}^m \\exp(u_j^T x)}· \\frac 1 {1 + exp(-w_i^X x)} \\\\ &= \\sum_{i=1}^m p(z=i|x)p(y|z=i,x) \\tag 1 \\\\ arg \\min_{\\Theta} f(\\Theta) &= loss(\\Theta) + \\lambda||\\Theta||_{2, 1} + \\beta||\\Theta||_1 \\tag 2 \\\\ loss(\\Theta) &=-\\sum_{t=1}^n[y_t\\log(p(y_t=1|x_t, \\Theta)) + (1-y_t)\\log(p(y_t=0|x_t, \\Theta))] \\tag 3 \\end {align}以上数学公式用神经网络的架构表示如下 这里$\\Theta = \\{u_1, u_2, …, u_m, w_1, w_2, …, w_m\\} \\in \\Bbb R^{d\\times 2m}$表示的是模型的参数 超参数$m$表示的是区块的分片数量，当$m=1$时候MLR退化为普通的LR，且模型的非线性拟合能力随着$m$的增大变强 $\\pi_i(x)$代表了领域先验，决定了空间划分，表示的是当前样本属于第$i$个区域的隶属度，总共$m$个隶属度的值，可以组成一个向量，不难看出来这里用了Softmax的思想做了归一化 $\\eta(x)$表示的是第$i$个区域里面的线性分类器，输出的是一个预测值，同样也可以组成一个向量 总共$m$个区域，输出了两个长度为$m$的向量，两个向量做内积，即可算出相应的预测值 MLR的算法优势有如下两点 端到端的非线性模型 非线性比较好理解，端到端是如何体现出来的（仅仅是不需要人工设计特征，但是高阶特征是如何体现的？） 稀疏性 从公式(2)在建模的时候引入了范数正则，训练出来的模型具有较高的稀疏性，具体分别为 $L_{2,1} = ||\\Theta||_{2, 1} = \\sum_{i=1}^d \\sqrt {\\sum_{j=1}^{2m}} \\theta_{ij}^2$，用于特征选择，这里特征维度为$d$，每一维特征都和$2m$个模型参数相关。同一维度对应多个权重 — 分组稀疏正则 问题：这里和$||\\Theta||_2$一样的吧 $L_1 = ||\\Theta||_1 = \\sum_{ij}|\\theta_{ij}|$，用于保证模型的稀疏性，可以提高训练&amp;预测速度，同时增加可解释性 这里$L_{2,1}$和$L_1$是非光滑的，会导致公式(2)的损失函数为非凸非光滑的，在数学上表现为不是处处可导，如何进行梯度下降是个问题 sol：虽然不是处处可导，但是函数处处方向可导，用方向导数找到最快下降方向，并用拟牛顿法进行加速【LBFGS算法】 稀疏性是如何保证的，如FTRL里面，需要修改优化算法，才能保证较好的稀疏性，即使是用L1范数，有时候因为计算机的问题，难以保证较高的稀疏性 同时，该算法面临的问题有如下几点 目标函数非凸、非光滑（不可导，不存在次梯度） 超大规模数据，高维度 2.1 相关特性2.1.1 结构先验2.1.2 线性偏置用于解决物品展示的页面、位置等影响点击概率的问题 物品的特征$x$，偏移向量$y$（场景，页数，位置等），学习联合概率$p(x, y)$ 将上述的偏移向量$y$加入到损失按数中，实践中对位置bias信息建模 2.1.3 模型级联MLR支持与LR模型的级联式联合训练，类似Wide&amp;Deep模型 3. 优化算法3.1 基础知识这理解MLR优化算法之前，先补一下相关的基础知识（一下内容基本引用自知乎专栏为什么梯度反方向是函数值局部下降最快的方向？） 3.1.1 导数 物理意义：函数数在该点上的变化率，即自变量的变化趋于零的时候，函数值的变化与自变量变化的比值 f'(x_0) = \\lim_{\\Delta x \\to 0} \\frac {\\Delta y} {\\Delta x} = \\lim_{\\Delta x \\to 0} \\frac {f(x_0 + \\Delta x) - f(x_0)} {\\Delta x} 几何意义：表示的函数曲线上的某一点$P_0$的切线， 一元函数中只有一个自变量，也就是只存在一个方向上的变化，没有偏导数 如果函数在$P_0$处可导，那么在该点处一定是连续的，且在该点的左右极限存在且相等（即左导数和右导数相等） 3.1.2 偏导数偏导数至少有两个变量，如$z = f(x, y)$，二元函数与一元函数相比，是从曲线到曲面 曲线上一个点，对应的切线只有一条 曲面是一个点，对应的切线有无数条 所以偏导数指的是多元函数沿坐标轴的变化率（这里坐标轴很重要，偏导和方向导数的一个重要区别，另外一个肯可能是对偏导的要求更高，偏导数要左右导数存在且相等才可以） $f_x(x, y)$指的是函数在在$y$方向不变，函数值沿着$x$轴方向的变化率 $f_y(x, y)$指的是函数在在$x$方向不变，函数值沿着$y$轴方向的变化率 具体如下图 偏导数$f_x(x, y_0)$表示的几何意义为：曲线被平面$y=y_0$所截得的曲面在点$M_0$处的切线$M_0T_x$对$x$轴的斜率 问题：如果从二元升级到三元、四元等高维数据呢？ 偏导数的存在，只能保证与坐标轴平行的方向上函数的极限值等于函数值 3.1.3 方向导数之前提到的偏导数，之的是多元函数沿着坐标轴的变化率，这里重点是坐标轴，已经把变化的方向限制在了坐标轴这几个方向上。如果是任意方向的话，则需要引入方向导数的概念，以下图为例 按照偏微分的定义，此时只有$x$和$y$两个方向可以变化，如果此时需要向方向$u$变化的话，需要根据$f_x(x, y)$和$f_y(x, y)$两个偏导数合成第三个偏导数$f_u(x, y)$，感觉很向量很接近，可以用两个单位向量，合成任意方向的向量一样。同时对于方向$u$的导数，记为 D_uf = \\lim_{t \\to 0} \\frac {f(x_0 + t\\cos \\theta, y_0 + t \\sin \\theta) - f(x_0, y_0)} {t}这样就可以计算曲面沿着任意方向的变化率了，用偏微分表达的话，则是 \\begin {align} D_uf(x,y) &= f_x(x, y)\\cos \\theta + f_y(x,y)\\sin \\theta \\\\ &= \\bf A · \\bf I \\\\ &= |A|·|I|\\cos \\alpha \\\\ \\bf A &= (f_x(x, y), f_y(x, y)) \\\\ \\bf I &= (\\cos \\theta, \\sin \\theta) \\end {align}这里$\\alpha$为向量$\\bf A$和向量$\\bf I$之间的夹角，要使得$D_uf(x, y)$取得最大值，只需要令$\\cos \\alpha = 1$，即$\\alpha$为0度的时候 这里是二元的情况，同理在多元的情况下，也是找对应变化率最大的方向 问题：偏导数和方向导数的区别【以下两个Sol存疑，不同文章有不同说法，待确认】 Sol1：偏导数并不是在某几个轴的方向导数，方向导数是单向的，只要从一个方向趋近极限就可以，但偏导数要左右导数存在且相等才可以，比如圆锥的顶点出，各方向的方向导数都是存在的，但是偏导数不存在 Sol2：偏导数为坐标轴方向上的方向导数，其他方向的方向导数为偏导数和合成 3.1.4 梯度梯度是一个向量，既有大小，也有方向。以上面提到的二元函数$z=f(x, y)$为例，在点$P_0(x_0, y_0)$点存在偏导数$f_x(x_0, y_0)$和$f_y(x_0, y_0)$，此时向量$[f_x(x_0, y_0), f_y(x_0, y_0)]$为$f(x, y)$在点$P_0(x_0, y_0)$的梯度，记做 \\nabla f = [\\frac {\\partial z} {\\partial x}, \\frac {\\partial z} {\\partial y}]具体几何意义为 梯度的方向就是函数$f(x, y)$在$z$点处，方向导数最大的方向，也就是增长最快的方向 梯度的模的方向导数的最大值 所以可以理解为，梯度是沿着各个轴的偏导数的向量，偏导数只是该向量里面的一个值，偏导数构成的向量为梯度 3.2 优化原理3.2.1 计算方向导数这一部分内容不是很理解，主要参考了知乎专栏阿里妈妈大规模分片线性模型里面的内容 之前提到了，由于$L_{2,1}$和$L_1$，目标函数是非凸非光滑的，此时是无法使用传统的梯度下降，因为有些地方是不可导的，解决方案是用方向导数。字面意思，就是某个方向上的导数，且在每一个方向上都是有导数的，梯度和方向导数的关系就是在梯度方向上的方向导数最大。以下面的图为例，紫色的表示某一个方向，在这个方向上的函数的方向导数如黑色实线 问题1：不是很理解这里，紫色的方向是如何映射到函数上去的？这里不明白紫色的是一个点的方向还是只是一个方向，如果是一个方向的话，可以拓展成一个超平面？对应好函数上是一个降维 问题2：梯度和方向导数的关系 如果说梯度方向上的方向导数最大，能否推出方向导数最大的方向为梯度方向（存在梯度的情况）？如果梯度不存在，是否方向导数最大的方向就是变化率最大的方向 对于一个存在梯度的函数$f(x, y)$，其具有一阶连续偏导数，即可微，就意味着函数$f(x,y)$在某个点$(x_0, y_0)$处的所有方向导数在一个平面上。以下图为例，红色平面即为所有方向导数构成的平面【这里的红色平面是紫色和黑色构成平面】 论文中提到了，对于目标函数由一个光滑的损失函数和$L1/L_{2,1}$正则，如上面的公式(2)，对于所有的参数$\\Theta$就一定存在一个方向导数$f’(\\Theta; d)$，其方向为$d$（论文的附页里证明这个方向导数一定存在），可分为三种情况 这里$s$和$v$里面的值需要参考一下附页的证明公式 3.2.2 参数更新计算出方向导数之后，参数的更新公式如下 更新思路主要用到了拟牛顿法，使用了LBFGS的框架，借鉴了OWLQN算法的思路 顺便提一下和拟牛顿法相关的一些算法 牛顿法用了二阶导数，计算量大，每次需要计算黑塞矩阵 x_{n+1}= x_n - \\alpha(\\bf {H_n^{-1}} \\bf {g_n}) 于是有了BFGS，拟牛顿法中的一种，近似计算了黑塞矩阵或者其逆矩阵的正定对称矩阵 继续优化，于是又了LBFGS ，L代表了Limited-Memory 根据拟牛顿法，给定一个方向$d$，即可计算出$H_n^{-1}$，不需要求出$H_n^{-1}$，且计算过程中会用到梯度信息。由于$L_1$和$L_{2,1}$不是处处可导，所以又参考了OWLQN算法的处理思路 这里提一下OWLQN算法，主要是改进了LBFGS的框架对于非凸非平滑函数的优化 在不可导处用次梯度代替梯度（好像在FTRL中也有看到类似优化） 模型参数不为零时，此时可导，正常计算 模型参数为零时，此时不可导，正则项的左偏导数为$\\partial^-f(x)$和右偏导数为$\\partial^+f(x)$ 如何确定在零的位置的时候用左导数还是右导数呢？做法是防止象限穿越，通俗的讲就是在负象限的时候减去正值，在正象限的时候减去负值（加上正值） 问题：不理解这里提到的象限穿越和象限的概念 在Line Search的时候做了象限约束 当参数不在零点时，Line Search保持参数所在象限内搜索 当参数再零点时候，Line Search保持参数在次梯度约束的象限内进行搜索 之前提到了MLR算法是非凸非光滑的，即不可导，不存在次梯度，所以OWLQN算法无法直接套用，需要优化一下，主要有三点 次梯度改成方向导数 对更新方向p进行了象限约束：非正定时直接用方向导数作为搜索方向，否则要进行象限约束在方向导数所在象限内 线性搜索的象限约束不同，当MLR参数不在零点时，Line Search保持在参数所在象限内搜索，在零点时，Line Search保持在方向导数约束的象限内搜索 4. References 官方论文-Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction 官方PPT-海量数据下的非线性模型探索 阿里妈妈大规模分片线性模型 如何直观形象的理解方向导数与梯度以及它们之间的关系？ 为什么梯度反方向是函数值局部下降最快的方向？ 直观理解梯度，以及偏导数、方向导数和法向量等","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"MLR","slug":"MLR","permalink":"http://chenson.cc/tags/MLR/"},{"name":"CTR","slug":"CTR","permalink":"http://chenson.cc/tags/CTR/"}]},{"title":"Softmax理解笔记","date":"2020-01-05T04:28:41.000Z","path":"2020/01/05/Softmax理解笔记/","text":"1. Introduction目前工作中，一直没接触过多分类，所以对于多分类也不是很了解。所以这里稍微整理一下多分类的Softmax函数，加深理解。 2. Softmax \\begin {align} Softmax(x_j, x_1, x_2, ..., x_n) = \\frac {e^{x_j}} {\\sum_{i=1}^ne^{x_i}} \\tag 1 \\end {align} 以上动图是一个三分类的例子，经过Softmax之后，输出的就是一个概率值了，此时对这个三个类别输出的是一个概率分布，之后可以用交叉熵的概念，来计算真实分布和输出的预测概率分布之间的距离 3. LogSumExp \\begin {align} LogSumExp(x_1, x_2, ..., x_n) &= \\log(\\sum_{i=1}^n e^{x_i}) \\\\ &= \\log(\\sum_{i=1}^n (e^{x_i-c}e^c)) \\\\ &= \\log(e^c \\sum_{i=1}^n e^{x_i-c}) \\\\ &= \\log(e^c) + \\log(\\sum_{i=1}^n e^{x_i-c}) \\\\ &= c + \\log(\\sum_{i=1}^n e^{x_i-c}) \\tag 2 \\end {align} \\begin {align} \\log(Softmax(x_j, x_1, x_2, ..., x_n)) &= \\log \\frac {e^{x_j}} {\\sum_{i=1}^ne^{x_i}} \\\\ &= \\log e^{x_j} - \\log{\\sum_{i=1}^ne^{x_i}} \\\\ &= x_j - LogSumExp(x_1,x_2, ..., x_n) \\\\ &= x_j - (c + \\log(\\sum_{i=1}^n e^{x_i-c})) \\\\ &= x_j - c - \\log(\\sum_{i=1}^n e^{x_i-c}) \\tag 3 \\end {align} 通常$c = \\max(x_1, x_2, …, x_n)$，即有$a = x_i -c &lt;= 0$一直成立 此时，$e^a$可以不用考虑计算溢出的问题 这里之所以提到LogSumExp，是想解释一下Softmax这部分，和后序的Cross Entropy没啥关系 4. Softmax损失函数Softmax在做多分类的时候，优化的目标是：真是标签（ground-truth label）所对应的分数比其他类别分数更大，这里真实标签所对应的预测分数为target score，非目标类别对应的分数为non-target score，用数学的表达式即为： \\begin {align} \\forall i &\\neq y, z_y > z_i \\\\ \\cal L &= \\sum_{i=1, i \\neq y}^C \\max(z_i - z_y, 0) \\tag 4 \\\\ \\cal L_{hinge} &= \\sum_{i=1, i \\neq y}^C \\max(z_i - z_y + m, 0) \\tag 5 \\end {align}上面的公式是表达的是，对于所有的非目标类别的预测分数$z_i$和目标类别的预测分数$z_y$，做比较 大于目标类别的预测分数，则计算一个对应的损失 小于目标类别的预测分数，则此时的损失值为0 最终的目标就是要优化$\\cal L$，使其最小化 同时可以借鉴SVM中的思想，不仅$z_i$要比$z_y$小，而且最好要有$m$值的差距，保证在训练集、测试集和验证集上面的稳定性，类似hinge loss的思想 以上公式表示的是计算所有非目标类别的类和目标类别分数的一个比较，然后取全局最优，这样计算量就比较大，需要比较的是C-1次，可以优化一下，只需要目标类别的分数（$z_y$）大于非目标类别中分数（$\\max_{i \\neq y}(z_i)$）最高的那类即可，此时需要比较的只需要一次，计算量大大的减少了。所以目标函数可以优化如下 \\cal L = \\max(\\max_{i \\neq y}(z_i) - z_y, 0) \\tag 6鉴于这样优化之后，网络收敛会变慢，每次最多只有一个+1/-1的梯度进入到网络，需要将max函数转换成smooth版本，即光滑的版本（这里不是很理解，需查看相关资料），可以用LogSumExp函数替代max函数，所以上面的损失函数可以表示如下 \\cal L_{lse} = \\max(\\log(\\sum_{i=1, i \\neq y}^C e^{z_i}) - z_y, 0) \\tag 7 上述提到的网络收敛会变慢，不算非常理解。我的理解是之前对目标函数进行优化，将之前的C-1次比较，降到了一次比较，且求导之后的值是+/-1，故进入到网络的+/-1由之前的C-1降到了一个，所以网络收敛会变慢 上面的解决方案是将max函数转换成smooth版本，即光滑的版本，用LogSumExp函数替代max函数，这里可以参考博客寻求一个光滑的最大函数 对损失函数求导，发现LogSumExp函数的倒是恰好为Softmax函数，具体如下 \\frac {\\partial \\log(\\sum_{i=1, i \\neq y}^C e^{z_i})} {\\partial z_j} = \\frac {e^{z_j}} {\\sum_{i=1, i \\neq y}^C e^{z_i}} \\tag 8这样转换的好处在于 给予非目标分数的1的梯度将会通过LogSumExp函数传播给所有的非目标分数，各个非目标分数得到的梯度通过softmax函数进行分配的，较大的非目标分数会得到更大的梯度使其更快地下降。这些非目标分数的梯度总和为1，目标分数得到的梯度为-1，总和为0，绝对值和为2，这样就有效地限制住了梯度的总幅度。 这里不是非常理解，大致觉得是，之前转换成公式(6)，此时对于一个样本的梯度只有1和-1，如果判断对了，则目标分数会得到梯度-1，否则的话会得到梯度1，然而这里有C -1个非目标类别，需要将这个梯度1分配给这C-1个类别，且分数越高的，应该得到的梯度越大，此时用Softmax的思想来分配这个梯度1 问题：如何将这个1的梯度，分配给这C-1个类 Sol:我的理解是将LSE求导后，发现是Softmax值，然后1刚好可以正比于其对应的值 但上面的公式并非机器学习中常用的Softmax交叉熵损失函数，具体公式如下（这里只是对某个输出转换成概率之后取$\\log$值） \\begin {align} \\cal L_{softmax} &= \\log(1 + e^{\\log(\\sum_{i=1, i \\neq y}^C e^{z_i})-z_y}) \\\\ &= \\log(1 + \\frac {\\sum_{i=1, i \\neq y}^C e^{z_i}} {e^{z_y}}) \\\\ &= \\log \\frac {\\sum_{i=1}^C e^{z_i}} {e^{z_y}} \\\\ &= -\\log \\frac {e^{z_y}} {\\sum_{i=1}^C e^{z_i}} \\tag 9 \\end {align}以下先回忆一下交叉熵的概念 5. Cross Enropy这里其实是假设了两个分布，分别为样本的真实分布$p$和样本的估计分布$q$，交叉熵刻画的是这两个概率分布之间的距离，则 交叉熵刻画的是两个概率分布之间的距离，或可以说它刻画的是通过概率分布$q$来表达概率分布$p$的困难程度，$p$代表正确答案，$q$代表的是预测值，交叉熵越小，两个概率的分布约接近 \\begin {align} H(p) &= \\sum_{i=1}^Cp_i \\log\\frac 1 p_i \\\\ &= -\\sum_{i=1}^Cp_i \\log p_i \\\\ \\tag {10} H(p,q) &= \\sum_{i=1}^Cp_i \\log\\frac 1 q_i \\\\ &= -\\sum_{i=1}^Cp_i \\log q_i \\tag {11} \\end {align} 公式(10)表示的是用样本的真实分布$p$来衡量识别一个样本所需要的编码长度的期望 公式(11)表示的是估计分布$q$来表示真是分布$p$的平均编码长度的期望 公式(9)表示的只是公式(11)中的$\\log q_i$这部分 至于为什么要用交叉熵而不用MSE等其他的损失函数，是由于在分类的场景下，MSE版的loss是一个非凸函数（因为只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解），而Cross Entropy版的loss则是一个凸函数，具体可以参考博客经典损失函数：交叉熵 6. Softmax-CrossEntropy对于公式(9)是怎么来的，看着有点头晕，需要仔细推导一下 首先介绍一下softplus函数，即$\\log(1 + e^x)$，然后将公式(7)代入到$x$中（这里只考虑$\\cal L_{lse}$中大于0的部分），此时即有了公式(9)中的第一步，化简一下，即有了完整的公式(9) \\begin {align} \\cal L_{softmax} &= \\log(1 + e^{\\log(\\sum_{i=1, i \\neq y}^C e^{z_i})-z_y}) \\\\ &= \\log(1 + \\frac {\\sum_{i=1, i \\neq y}^C e^{z_i}} {e^{z_y}}) \\\\ &= \\log \\frac {\\sum_{i=1}^C e^{z_i}} {e^{z_y}} \\\\ &= -\\log \\frac {e^{z_y}} {\\sum_{i=1}^C e^{z_i}} \\tag 9 \\end {align} 问题1：为什么要引入softplus Sol1：似乎可以理解为引入了一定的m，增加泛化性 问题2：公式(7)中的max是如何体现在公式(9)中的 7. 常见问题7.1 Softmax和Sigmoid的区别 Softmax-CrossEntropy重点在于前面有一个Softmax的操作，将C个类别的输出转换成了一组概率值，从结构上来看，应该是和Sigmoid那部分是等价的，都是到类似激活的作用，输出的[0,1]之间的概率值。只是二分类的情况下，只需要输出一个概率值即可，另外一类的概率由$1-p$可得到。至于后续部分，都是引入了CrossEntropy来计算真实概率和预测概率之间的差异，然后再优化 如果将上面的三分类例子，用Softmax二分类来表示的话，则 \\begin {align} g(z_1) &= \\frac {e^{z_1}} {e^{z_1} + e^{z_2}} \\\\ &= \\frac 1 {1 + e^{z_2 - z_1}} \\\\ g(z_2) &= \\frac {e^{z_2}} {e^{z_1} + e^{z_2}} \\\\ &= \\frac 1 {1 + e^{z_1 - z_2}} \\\\ z_i&= w_i^Tx \\\\ \\end {align}Sigmoid二分类的公式为 \\begin {align} g(z) &= \\frac 1 {1 + e^{-z}} \\\\ z &= w_i^Tx \\end {align}这里Softmax和Sigmoid的区别，仅在于 $z_1-z_2$和$z$，只需令$z$等于$z_1-z_2$或$z_2-z_1$即可 7.2 LogSumExp函数求导 \\begin {align} \\frac {\\partial \\cal L} {\\partial z_j} &= \\frac {\\partial \\log(\\sum_{i=1,y \\neq y}^C e^{z_i})} {\\partial z_j} \\\\ &= \\frac {\\partial \\log(\\sum_{i=1,y \\neq y}^C e^{z_i})} {\\partial \\sum_{i=1,y \\neq y}^C e^{z_i}} ·\\frac {\\partial \\sum_{i=1,y \\neq y}^C e^{z_i}} {\\partial e^{z_j}} ·\\frac {\\partial e^{z_j}} {\\partial z_j} \\\\ &= \\frac 1 {\\sum_{i=1,y \\neq y}^C e^{z_i}}·1· e^{z_j} \\\\ &= \\frac {e^{z_j}} {\\sum_{i=1, i\\neq y}^C e^{z_i}} \\\\ \\frac {\\partial \\cal L} {\\partial z_i} &= \\begin {cases} p_i - 1 \\qquad i = y \\\\ p_i \\qquad i \\neq y \\end {cases} \\end {align}以二分类为例，可以推导出$z_1$和$z_2$得到的梯度永远是反向且幅度是相等的，两类的分界面为$(w_2 - w_1)^T x = 0$ \\begin {align} \\cal L_1 &= -\\log \\frac {e^{z_1}} {e^{z_1} + e^{z_2}} \\\\ &= -\\log \\frac 1 {1 + e^{z_2-z_1}} \\\\ \\cal L_2 &= -\\log \\frac 1 {1 + e^{z_1-z_2}} \\\\ z_i &= w_i^Tx \\end {align}7.3 Softmax的一些性质以下几点参考知乎专栏王峰博士整理的内容:Softmax理解之二分类与多分类 非负性：$0 &lt;= p_i &lt;= 1$ 正则项：$\\sum_{i=1}^C p_i = 1$ Softmax交叉熵损失函数关于分数$z$的导数如下 \\frac {\\partial \\cal L} {\\partial z_i} = \\begin {cases} p_i - 1 \\qquad i = y \\\\ p_i \\qquad i \\neq y \\end {cases} Softmax交叉熵损失对分数的梯度之和为0（可根据上一点推导出），即$\\sum_{i=1}^n \\frac {\\partial \\cal L_s} {\\partial z_i} = 0$ 根据上面两点可知，目标类别的绝对值等于非目标类别绝对值之和，即有 $\\sum_{i=1, i \\neq y}^n |\\frac {\\partial \\cal L_s} {\\partial z_i}| = |\\frac {\\partial \\cal L_s} {\\partial z_y}|$ $\\sum_{i=1}^n |\\frac {\\partial \\cal L_s} {\\partial z_i}| = 2|\\frac {\\partial \\cal L_s} {\\partial z_y}|$ 图示表示如下 8. References LogSumExp这一机器学习中常见的模式 从最优化的角度看待Softmax损失函数 Softmax理解之二分类与多分类 经典损失函数：交叉熵 图示Softmax及交叉熵损失函数","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"Softmax","slug":"Softmax","permalink":"http://chenson.cc/tags/Softmax/"},{"name":"多分类","slug":"多分类","permalink":"http://chenson.cc/tags/多分类/"}]},{"title":"在线学习算法初探","date":"2019-12-30T06:36:26.000Z","path":"2019/12/30/在线学习算法初探/","text":"1. Introduction背景：CTR里面数据多，维度高，时效性短，故需要实时训练 带来的问题就是数据量大，单机可能无法求得最优解 什么是Online Learning 关于实时训练：先用前N天的数据训练，在实时训练 如何部署：在这样场景下的LR，其实模型就是一系列的w，把参数存在redis里面就ok了 Online Optimization 在线最优化问题 FTRL可以看做一种优化算法的改进，所以是一种优化算法，但可以等价于在线学习，因为算是为在线学习而生的。在线优化算法框架 腾讯开源的PS angle支持，后期可以看一下 和Batch GD的区别 一个是学习全局的最优解，优化的是全局的Loss 一个是随着样本的到来，学习一个样本的最优解，即类似SGD。无法保证权重最优解，只能优化单个样本的Regret【定义和Loss基本一致，只是针对一个样本一个样本累计的损失】，叫做Online GD（OGD），缺点是无法保证稀疏性。所以问题变得了最优化问题的求解？ 要认识到稀疏性在CTR领域里面的重要性，理解知乎某个回答里面的5%的稀疏性，30%稀疏性等，为什么，有什么好处？—》稀疏解 为什么没有稀疏解：可能认为是浮点型计算，导致没有完全的0的稀疏解。sol是clip一下，turncated一下 和优化相关的 无约束优化问题，比如说没有正则项 有等式约束的问题，比如加入正则并限制正则的大小 不等式约束的优化问题 其中有等式约束优化问题和不等式约束问题分别可以使用拉格朗日乘子和KKT条件来转换为无约束问题 次梯度 什么是次梯度？ Bloom Filter，过滤到一些频次很低次等，可以回顾9313里面的算法，针对特征分布的长尾效应 2. Compare 这几个算法大体可以分成三大部分 A部分：loss-func，根据梯度来计算的，其中FOBOS、FTRL-Proximal这里计算的是累积梯度？？【==不是RDA才是累积梯度么==】 (A) an approximation to $\\mathcal l_{1:t}$based on the gradients $g_t = ▽ \\mathcal l_{t}(x_t)$ B部分：正则项，这里$\\Psi$是正则项, $\\phi$ 则是正则项$\\psi$的次梯度（因为对于L1，在0处是不可导的） C部分：==限制条件，限制了想的变化不能离过去太远，或者离0点太远，这里x可以理解成模型的weight== (C) additional strong convexity to stabilize the algorithm, needed to guarantee low regret (the matrices $Q_s$ are generalized learning rates) 根据以上几个公式，可以提取两个问题 how the $\\Psi$ function is handled where additional strong convexity is centered 2.1 相关定义 loss function f_t(x) = l_t(x) + \\Psi(x) \\\\ l_t(x) = loss(\\theta_t, y_t)这里$\\Psi$是一个固定的正则项，通常是非光滑的，比如L1 样本表示 (\\theta_t, y_t)这里$\\theta_t \\in \\mathcal R^n$，是样本的特性向量，即常见的$X$ $y \\in \\{-1, +1\\}$ 这里$x$是权重 FOBOS x_{t+1}= \\arg \\min _x g_t·x + \\lambda ||x||_1 + \\frac 1 2 ||Q_{1:t}^{\\frac 1 2}(x - x_t)||^2_2 RDA x_{t+1}= \\arg \\min _x g_t·x + t\\lambda ||x||_1 + \\frac 1 2 \\sum_{s=1}^t ||Q_{s}^{\\frac 1 2}(x - 0)||^2_2 AOGD (Adaptive Online Gradient Descent) FTRL-Proximal x_{t+1}= \\arg \\min _x g_t·x + t\\lambda ||x||_1 + \\frac 1 2 \\sum_{s=1}^t ||Q_{s}^{\\frac 1 2}(x - x_s)||^2_2 5. References 在线学习（Online Learning）导读 机器学习算法系列（31）：在线最优化求解（online Optimization） 布隆过滤器","tags":[{"name":"FTRL","slug":"FTRL","permalink":"http://chenson.cc/tags/FTRL/"},{"name":"Online Learning","slug":"Online-Learning","permalink":"http://chenson.cc/tags/Online-Learning/"},{"name":"CTR","slug":"CTR","permalink":"http://chenson.cc/tags/CTR/"}]},{"title":"分布式机器学习初探","date":"2019-12-28T04:46:14.000Z","path":"2019/12/28/分布式机器学习初探/","text":"1. Introduction随着数据量不断增大，单机往往需要花比较长的时间才能把模型训练完，所以需要有分布式机器学习系统来支撑，通过多台机器来共同完成模型的训练，缩短训练时间。这些多台机器，可以根据机器的性能，分别被串行/并行在一起，各自完成各自负责的那部分。 根据划分的内容，可以将并行计算可分为两种 模型并行 - Model Parallelism 从较为宏观的角度，将模型拆解成不同的几个部分，然后不同的机器负责模型中的一部分，如神经网络中，不同层可能会被分配给不同的机器，同一层的不同参数也有可能被分配到不同的机器上。通常这种方式是因为模型比较复杂，参数特别多，一台机器实在是hold不住的情况，就分配给多台机器来完成的。比如在推荐算法汇总，一个LR模型可能有几千万个维度，那么可以按万维为单位，将参数分给不同的机器来训练，具体如何实现，后面补充 数据并行 - Data Parallelism 从数据的角度，将数据分成多份，然后每台机器有同一个模型的多份副本，不同机器负责不同的数据（如一个Batch size的数据），然后通过某种方式，将所有机器的训练结果汇总在一起，之后再分发给所有的机器，这样就完成一轮模型的迭代训练 当然，上述的模型并行和数据并行是也可以同时存在，目前先暂时关注数据并行相关方法，然后再介绍模型并行的相关方法。 2. 数据并行如上所述，采用数据并行的话，每台机器在开始训练前需要有一个模型的完整参数，然后用自己机器上的数据训练完成后，将相关结果以某种方式送出去等待汇总，然后再接收汇总后更新过后的参数。这里根据结果的汇总方式，可以将数据并行分成两种不同的架构，分别是P/S架构和Ring-All Reduce 2.1 P/S架构在PS架构中，集群中的机器可以分成两类 参数服务器 Parameter Server 存放模型的参数 工作服务器 Worker 负责计算参数的梯度 这种架构的思想比较简单，对于每个迭代过程 工作服务器从参数服务器中获得参数，然后将计算的梯度返回给参数服务器 参数服务器聚合从工作服务器传回的梯度，然后更新参数，并将新的参数广播给工作服务器 同时在P/S架构中，梯度参数的更新方式可以分成同步更新和异步更新两种方式 同步更新 除了PS参数服务器，其他的所有worker都在同一个Batch下，各自计算不同的mini-batch，PS参数服务器一定会等到所有的worker都计算完成后，才会进行一次参数更新，再下发到所有的worker上去。 缺点 这种方式一般要求所有的机器性能比较均衡，如果某台机器比较慢的话，那么其他机器都训练完了，一直在等待这台机器完成后才能更新参数。 PS参数服务器的带宽可能会成为瓶颈 性能估计 可参考博文腾讯机智团队分享—AllReduce算法的前世今生 异步更新 这种更新方式就是不需要等待所有的worker都训练完，一旦自己的数据训练完之后，就将结果发送给PS参数服务器，然后拿到一个新的参数继续训练 问题：其实一直不太理解这种更新方式，因为感觉这样训练过几次，参数就不同步了，存在梯度失效的问题，也不知道PS参数服务器如何保证一轮的参数更新，是用到了一个Batch内所有的训练数据。【待解决】 2.2 Ring-All Reduce架构除了P/S架构，也可以不需要PS参数服务器的存在，目前比较出名的有Ring-All Reduce架构。正如这种架构的名字Ring，即环，所有的机器之间的拓扑结构为环形。All Reduce，所有机器都负责Reduce的汇总工作，即不需要PS参数服务器的存在了。 除此之外，还有 Recursive Halving and Doubling，和P/S架构不同的是，这里可能会有多台PS参数服务器，然后所有的worker都是树形结构里面的叶子节点，然后计算结构从叶子几点不断汇总到Root节点后，更新好参数再从上到下广播到叶子节点，这样的好处好像是可以缓解单一台PS参数服务器的带宽问题。 Butterfly 这里重点介绍Ring-All Reduce架构的算法思想 所有worker将数据分成N块（这里N为所有worker的个数） 所有的worker的拓扑结构为 $a \\to b \\to c \\to d \\to a$ 然后接下去在数据传输阶段，以每个worker所在的idx为起点，将数据传输给下一个worker， 如 $data_{a_0} \\to data_{b_0}$， $data_{b_1} \\to data_{c_1}$等，一个环都传输完之后（并传输），指针减1，重复上面的步骤，具体如下图 在传输数据的N-1次后，每个worker都至少有一块数据是完整的，如下图中的Step2，然后将各自完整的数据块广播出去大概还需要N-1次，共总共需要2N-2次，所有的数据块都可以得到完整的训练结果 3. 模型并行4. 常见机器学习的并行计算4.1 Logistic Regression逻辑回归中的并行计算最主要的是对目标函数梯度计算的并行化 梯度下降法的梯度更新公式 \\begin {align} \\frac {\\partial J} {\\partial w}_j &= \\frac 1 m \\sum_{i=1}^m (y^{(i)} - \\hat y^{(i)})x^{(i)} \\\\ w^{k+1}_j &= w^k_j + \\alpha \\frac {\\partial J} {\\partial w}_j \\end {align} 牛顿法中的更新公式 w^{k+1} = w^k - \\frac {J'(w^k)} {J''(w^k)} = w^k - H^{-1}_k·g_k 上述的更新公式中，都需要向量之间的点乘和相加，这里也就是可以进行并行计算的地方有。 特征权重向量$W_t$和特征向量$X_j$的点乘，即为了后序计算$\\hat y$用的 \\hat y_{(i)} = \\sigma(\\frac 1 { 1 + e^{-XW}}) 标量$(y_i - \\hat y_i )$和特征向量$X_j$的相乘 假设有m个样本，n维特征，此时可以构建矩阵 数据按行分割，并行 即上面提到的数据并行，以P/S结构为例，一整个数据集分成m组，分给m个worker，每个worker完成对应的计算，然后PS参数服务器接收所有worker的结算结构，汇总会将更新好的参数广播给所有的worker，下一轮再重复这几个步骤 数据按行和列分割，并行 按上图，对矩阵进行分割，M行分成m组，N列分成n组，此时各个节点可以表示为$Node(r, c)$，对应节点上的数据可以表示为$X_{(r,c)}$ 步骤1：各节点上的并行计算第t次迭代，$Node_{(r,c)}$上的第k个特征向量与特征权重分量的点乘 步骤2：然后将行号相同的结果汇总在一起，为的就是实现数据按行的并行结果 以上两步数学表示如下 \\begin {align} d_{r,k,t} &= W_t^TX_{r,k} \\\\ &= \\sum_{c=1}^n d_{(r,c),k,t} \\\\ &= \\sum_{c=1}^n W_{c,t}^T X_{(r,c),k} \\end {align} 这里$W_{c,t}$表示为第t轮迭代中特征权重向量在第c列上的分量 k为节点$Node(r,c)$上第k个特征向量 步骤3：各个节点独立计算标量与特征向量相乘 步骤4：然后将列号相同的结果汇总在一起，这两步做的很像1、2两步，只是一个按行汇总，一个按列汇总 以上两步数学表示如下 \\begin {align} G_{c,t} &= \\sum_{r=1}^m G_{(r,c),t} \\\\ &= \\sum_{r=1}^m \\sum_{k=1}^{M/m}[\\sigma(y_{r,k}, d_{r,k,t}) - 1]y_{r,k}X_{(r,c),k} \\end {align} $G_{(r,c),t}$表示为第t轮迭代中，第r行节点上部分样本计算出的目标函数的梯度向量在第c列上的分量 $G_{c,t}$表示为第t轮迭代中，目标函数的梯度向量$G_t$在第c列上的分量 $G_t = $ 并行计算LR的流程 4.2 GBDTGBDT本身并没有并行计算的设计，可以通过MPI方式在建数的时候对选取最佳特征的最佳分割点进行并行化计算。具体啥是MPI可以参考这篇教程博客，感觉写的很好MPI教程介绍 4.3 XGBoost 特征并行 XGBoost依旧是Boosting结构的，所以tree维度是无法并行的，只能实现单棵树里面的特征维度并行。主要思想是预先将特征排好序，并存储为块结构，分裂节点的时候采用多线程并行查找每个特征的最优分割点（计算每个特征的最大收益）， level-wise叶子生产策略，多线程并行分裂同一层的节点。不加区分的对待同一层的节点，实际上很多节点的分裂收益是非常低的。下面的lightgbm对这种低效的并行做出了改进 4.4 LightGBM 数据并行 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点 特征并行 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点 4.5 NN5. References 分布式机器学习系统笔记（一）——模型并行，数据并行，参数平均，ASGD 并行计算视角下的常用机器学习模型 深度学习分布式训练相关介绍 - Part 2 详解分布式训练架构PS-Worker与Horovod 腾讯机智团队分享—AllReduce算法的前世今生 ring allreduce和tree allreduce的具体区别是什么？ 逻辑回归并行化 LR的分布式（并行化）实现 MPI教程介绍","tags":[{"name":"分布式","slug":"分布式","permalink":"http://chenson.cc/tags/分布式/"},{"name":"机器学习","slug":"机器学习","permalink":"http://chenson.cc/tags/机器学习/"}]},{"title":"读书笔记-白话统计","date":"2019-12-16T01:59:41.000Z","path":"2019/12/16/读书笔记-白话统计/","text":"1. Title References","tags":[]},{"title":"发明者量化平台部署笔记","date":"2019-12-13T04:21:36.000Z","path":"2019/12/13/发明者量化平台部署笔记/","text":"1. 托管者部署 1234567891011121314151617181920(base) [root@izwz9hxp0cmozj9nwrpbbqz FMZ]# wget https://www.fmz.com/dist/robot_linux_amd64.tar.gz--2019-12-13 15:27:50-- https://www.fmz.com/dist/robot_linux_amd64.tar.gzResolving www.fmz.com (www.fmz.com)... 121.41.115.19Connecting to www.fmz.com (www.fmz.com)|121.41.115.19|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 11000453 (10M) [application/octet-stream]Saving to: ‘robot_linux_amd64.tar.gz’100%[====================================================================================================================&gt;] 11,000,453 1.65MB/s in 5.3s2019-12-13 15:27:56 (1.96 MB/s) - ‘robot_linux_amd64.tar.gz’ saved [11000453/11000453](base) [root@izwz9hxp0cmozj9nwrpbbqz FMZ]# tar -xvf robot_linux_amd64.tar.gzrobot(base) [root@izwz9hxp0cmozj9nwrpbbqz FMZ]# chmod +x robot(base) [root@izwz9hxp0cmozj9nwrpbbqz FMZ]# nohup ./robot -s node.fmz.com/******** -p ***************** &gt; fmz_nohup.log 2&gt;&amp;1 &amp;[1] 8338(base) [root@izwz9hxp0cmozj9nwrpbbqz FMZ]# cat fmz_nohup.lognohup: ignoring input2019/12/13 15:30:13 Login OK, SID: 99761, PID: 8338, Name: ##################### 注意在运行nohup指令的时候，需要添加在FMZ上的账号和密码，才能运行机器人 2. 添加交易所 注册SimNow账号，并注册好的账号，在win平台下，先把初始化密码改了后续才可以用 进入交易所，点击添加交易所 填写相关参数，添加即可 下面的交易账号为上面注册的SimNow的账号 右边的协议选择【期货CTP协议】，因为这里我是为了期货的仿真盘 注：不确定这里的行情服务器相关的是不是公开的，暂时未给出AppID的授权码 3. 部署机器人策略代码调通之后，即可部署机器人了，具体如下，交易平台选用刚刚添加的交易所即可。 4. References FMZ平台API说明文档 上期所模拟仿真系统","tags":[{"name":"Quant","slug":"Quant","permalink":"http://chenson.cc/tags/Quant/"},{"name":"量化平台","slug":"量化平台","permalink":"http://chenson.cc/tags/量化平台/"}]},{"title":"发明者量化平台入门笔记","date":"2019-12-02T06:51:20.000Z","path":"2019/12/02/发明者量化平台入门笔记/","text":"1. Introduction其实2017年底的时候就注册了这个平台的账号了，那会还是叫BotVS，那会对数字货币和自动化交易有些感兴趣，但后来工作有点忙自己有点懒，就没怎么研究了，到再次接触已经是2019年底了，不免感叹时光飞逝。 这次接触是因为工作需要在这个平台上实盘模拟一些策略，从其他的量化平台迁移过来的，主要是因为团队研发的平台和这个比较接近，以后策略上了自己平台的话就比较方便。 以下就简单的做个入门笔记，方便以后自己查阅。 2. 基础说明2.1 框架说明 main()为入口函数。 onexit()为正常退出扫尾函数，最长执行时间为5分钟，可以不声明，如果超时会报错interrupt错误。 onerror()为异常退出函数，最长执行时间为5分钟，可以不声明，Python语言编写的策略不支持该函数。 init()为初始化函数，策略程序会在开始运行时自动调用，可不声明。 这里和其他的框架有些不同，比如确实了before_trading和after_trading等函数，可能的原因是这个平台一开始是数字货币出身的，都是7x24小时交易，所以就不需要上述的两个函数。 基本框架如下： 12345678910// 这里和米匡的handle_tick基本一致function onTick()&#123; //在这里写策略逻辑，将会不断调用。e.g: exchange.Buy(100,1);&#125;function main()&#123; while(true)&#123; onTick(); Sleep(60000); // Sleep函数主要用于数字货币策略的轮询频率控制，防止访问交易所API接口过于频繁 &#125;&#125; 这里exchange是一个交易所的对象，默认为策略参数中添加的第一个交易所，所有与交易所的数据交互，都通过这个对象里面的函数实现。如果策略里面有多个交易所的话，则对象就是exchanges，即一个exchange的list列表了。 可以认为米匡是只支持一个交易所，且就是框架里面的context。 2.2 相关接口函数2.2.1 全局变量2.2.1.1 exchange / exchanges和交易所相关的 Trade 获取所有交易历史(非自己),由exchange.GetTrades()函数返回。 123456&#123; Time : 1567736576000, // 时间(Unix timestamp 毫秒) Price : 1000, // 价格 Amount : 1, // 数量 Type : 0 // 订单类型,参考常量里的订单类型,0即为ORDER_TYPE_BUY，ORDER_TYPE_BUY的值为0&#125; Ticker 市场行情由exchange.GetTicker()函数返回。 12345678910&#123; Info : &#123;...&#125;, // rest Request 请求交易所接口后，交易所接口应答的原始数据。 High : 1000, // 最高价 Low : 500, // 最低价 Sell : 900, // 卖一价 Buy : 899, // 买一价 Last : 900, // 最后成交价 Volume : 10000000, // 最近成交量 Time : 1567736576000 // 毫秒级别时间戳&#125; Record 标准OHLC结构,用来画K线和指标分析用的，由exchange.GetRecords()函数返回此结构数组。 类似米匡上get_price的接口一样，可以指定周期为1m、5m、1d等等。 12345678&#123; Time : 1567736576000, // 一个时间戳,精确到毫秒，与Javascript的new Date().getTime()得到的结果格式一样 Open : 1000, // 开盘价 High : 1500, // 最高价 Low : 900, // 最低价 Close : 1200, // 收盘价 Volume : 1000000 // 交易量&#125; Order 订单结构，由exchange.GetOrder()，exchange.GetOrders()函数返回。 1234567891011&#123; Info : &#123;...&#125;, // rest Request 请求交易所接口后，交易所接口应答的原始数据 Id : 123456, // 交易单唯一标识 Price : 1000, // 下单价格 Amount : 10, // 下单数量 DealAmount : 10, // 成交数量 AvgPrice : 1000, // 成交均价，注意，有些交易所不提供该数据，不提供的设置为0 Status : 1, // 订单状态,参考常量里的订单状态ORDER_STATE_CLOSED Type : 0, // 订单类型,参考常量里的订单类型ORDER_TYPE_BUY Offset : 0 // 数字货币期货和商品期货的订单数据中，订单的开平仓方向，ORDER_OFFSET_OPEN为开仓，ORDER_OFFSET_CLOSE为平仓方向&#125; MarketOrder 市场深度单，即exchange.GetDepth()返回数据中Bids、Asks数组中的元素的数据结构。也就是买卖盘口信息。 1234&#123; Price : 1000, // 价格 Amount : 1 // 数量&#125; Depth Account Position 2.2.1.2 Orders和订单相关的 Status 和米匡类似，调用订单相关的函数都会返回一个状态 | 常量名 | 定义 | 值 || :—————————- | :—————————- | :—- || ORDER_STATE_PENDING | 未完成 | 0 || ORDER_STATE_CLOSED | 已经完成 | 1 || ORDER_STATE_CANCELED | 已经取消 | 2 || ORDER_STATE_UNKNOWN | 未知状态（其它状态） | 3 | Type 订单类型 | 常量名 | 定义 | 值 || :——————— | :—- | :—- || ORDER_TYPE_BUY | 买单 | 0 || ORDER_TYPE_SELL | 卖单 | 1 | Offset 期货的开仓方向 | 常量名 | 定义 | 值 || :————————- | :————- | :—- || ORDER_OFFSET_OPEN | 开仓的订单 | 0 || ORDER_OFFSET_CLOSE | 平仓的订单 | 1 | 2.2.1.3 Position和仓位相关的 Type 也就是当前持仓相关的 | 常量名 | 定义 | 值 || :————— | :————————————————- | :—- || PD_LONG | 多头仓位(CTP用closebuy_today平仓) | 0 || PD_SHORT | 空头仓位(CTP用closesell_today平仓) | 1 || PD_LONG_YD | 昨日多头仓位(用closebuy平) | 2 || PD_SHORT_YD | 昨日空头仓位(用closesell平) | 3 | 以上在整个策略代码里面都可以使用的，和米匡里的context类似，Python使用的时候需要加global关键字. 5. References 发明者量化平台","tags":[{"name":"Quant","slug":"Quant","permalink":"http://chenson.cc/tags/Quant/"},{"name":"量化交易","slug":"量化交易","permalink":"http://chenson.cc/tags/量化交易/"}]},{"title":"2015 MacBookPro SSD硬盘更换","date":"2019-11-23T05:09:21.000Z","path":"2019/11/23/2015-MacBookPro-SSD硬盘更换/","text":"1. 前期准备 【忽略被猫弄成这样的桌布。。。】 电脑是2015年的MacBook Pro，256的硬盘。用了差不多四年了，由于不够用，考虑自己换块硬盘，对比了下感觉Intel 760P比较合适。准备如下： Intel 760P SSD 512G M.2 NVME协议 双11京东购买的，650左右Intel官方旗舰店购买 nvme转M.2接口转换器 马云家，不到20块吧 米家wiha螺丝刀 马云家，85块 金手指高温胶带 马云家，三块，用于贴在上面转换器上的，防静电啥的。如果硬盘是马云家买，可能就送一点了，因为就用了2cm左右 台电32G USB3.1优盘 京东活动27块买的，用于制作U盘引导盘的 Time Machine数据备份 MacOS Mojave系统 需要High Sierra以上，不然不支持nvme协议的硬盘 2. 安装步骤 Time Machine做好数据备份 制作好U盘启动盘 拆机 先关机 D面用小米螺丝刀P5，然后有两颗螺丝比较短，拧回去的时候需注意 拆完如下图，需要断开电池和主板的线，然后拆硬盘，拆硬盘用T5H 硬盘对比，标红处是防静电胶布贴的地方 接下来就是插上硬盘，拧螺丝，然后将电源和主板的线合上 后盖可先不拧螺丝，防止有问题又要重拆 安装系统 插上U盘，开机按Commad + R， 进入工具程式 先进磁盘工具，将Intel的硬盘格式化成Mac的格式 选择左上角的“内置”INTEL，点击“磁盘工具&gt;抹掉”，在“名称”中自定义名称（此名称在系统启动后可随时更改），“格式”选择“Mac OS 扩展（日志式）”，方案选“GUID分区图”，点击“抹掉”，然后点“完成” 这样新插入的SSD就可以用来安装Mac系统了 安装系统 重新退到上一层目录，这次选安装macOS，然后就会读取U盘里面的系统，选择安装到新硬盘上，点击继续。大概新系统20mins左右安装完成。此时可以选择撸一下猫 安装完之后，检查一下硬盘格式是否OK 突然忘记在哪看了，自己找找吧 ==，NVME的链接宽带需要为x4 从Time Machine中恢复数据 插上备份了系统的移动硬盘，还是Command + R，进入之前的实用工具，这次点击“从时间机器备份进行恢复”。整个恢复过程比较耗时，我原先是256G，所以估计3小时+吧，我是让机器自己恢复就去睡觉了，具体耗时不清楚 恢复完之后，重新安装macOS 其实第4步似乎可以跳过的，直接从Time Machine中恢复。但由于恢复过程比较久，不确定硬盘安装是否ok，我是先快速重装了一下，检查没事后才重装的 重装之后数据基本是恢复了，但还是有些小问题，里面很多软件都无效了，包括系统自带的Safari、Preview等，还有一些自己按照的软件。重试了新建用户、重置SMC和NVRAM均无效 选取苹果菜单 &gt;“关机”。等 Mac 关机后，按下内建键盘左侧的 Shift-Control-Option，然后同时按下电源按钮。按住这些按键和电源按钮 10 秒钟。如果您的 MacBook Pro 带有触控 ID，则触控 ID 按钮也是电源按钮。松开所有按键。再次按下电源按钮以开启 Mac。 2）如何重置 NVRAM 将 Mac 关机，然后开机并立即同时按住以下四个按键：Option、Command、P 和 R。您可以在大约 20 秒后松开这些按键，在此 期间您的 Mac 可能看似在重新启动。 在发出启动声的 Mac 电脑上，您可以在两次启动声之后松开这些按键。在 iMac Pro 上，您可以在 Apple 标志第二次出现并消失后松开这些按键。如果您的 Mac 使用了固件密码，这个组合键将不起任何作用或导致您的 Mac 从 macOS 恢复功能启动。要重置 NVRAM，请先关闭固件密码。 在您的 Mac 完成启动后，您可能需要打开“系统偏好设置”并调整已重置的任何设置，例如音量、显示屏分辨率、启动磁盘选择或时区。 所以没办法，又关机了，Command + R进入之前的实用工具，选择第二个安装macOS。 这次安装是不会删除电脑上的数据的，只会安装一遍系统而已，且之前的配置等其他数据都不会有影响。这部分大概耗时1h左右吧，重装之后系统软件恢复正常，但自己安装的第三方软件依旧无法运行，需要重新安装 重置SMC和NVRAM 按照上面的方法重置SMC和NVRAM，虽然我也没去了解干嘛的。。。 3. 使用感受 测了下速度，写的好像没有达到别人的1500+Mb/s，大概800+的样子吧，读取的差不多1300+Mb/s。可能买了块假硬盘？？？不过波动一直比较大，有时候又1000多，可能也是正常吧 用电似乎没啥感觉，本来电池健康就74%左右了，平常一直接电源用。然后建立索引啥的也没啥感觉，CPU啥的也比较正常 睡眠没有死机的情况，目前使用两天 以上，用时几个小时，花费750左右，256升级到Intel 512G，Over。 4. References Macbook Pro15款，升级ssd MacBook Pro 2015升级Intel 760P 1TB NVME SSD固态硬盘，2018.6.20亲测成功 https://post.smzdm.com/p/ag89eved/","tags":[{"name":"MacBook","slug":"MacBook","permalink":"http://chenson.cc/tags/MacBook/"},{"name":"SSD硬盘更换","slug":"SSD硬盘更换","permalink":"http://chenson.cc/tags/SSD硬盘更换/"}]},{"title":"类别不平衡数据小结","date":"2019-11-09T05:39:56.000Z","path":"2019/11/09/类别不平衡数据小结/","text":"1. Introduction之前在某银行的手机银行交易反欺诈项目中，遇到了样本比较偏的问题。然后这几天也是刚好看到一篇比较好的，有关于数据不平衡的文章，所以就做个笔记，顺便总结一下之前项目中遇到的问题，以及哪些方面可以改进的。 假设问题都是关于二分类的，正类为极少数的部分，负类为数据量大的一类。 如果更关注正类的分类能力，则是这里想要讨论的类别不平衡的问题； 如果更关注负类的分类能力，问题则变为Anomaly Detection，即异常值检测问题。 在这种情况下，样本不平衡往往会影响模型的学习性能，可能的原因是在训练过程中，样本不平衡影响了目标函数的优化，比如对少数类的误分类影响不会太大。此时常见的一些评估模型性能的指标也会有一定的误导性，比如ACC，KS，AUC等，因为这些指标往往会比其他正常模型里面的值都会高一些，AUC往往都是0.9几了。此时可以考虑一下其他指标： 召回率Recall \\frac {TP} {TP + FN} 准确率Precision（有些场景下也会转换成误报率来看） \\frac {TP} {TP + FP} F1-Score 2 \\times \\frac {Recall \\times Precision } {Recall + Precision} G-Mean \\sqrt{(Recall \\times Precision)} MCC \\frac {TP \\times TN - FP \\times FN} {\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} 2. 解决方案对于此类不平衡的问题，可以从数据层面和算法层面来缓解。 2.1 数据层面数据层面上算是从问题的根本上出发，就是缓解数据不平衡，让数据稍微平衡一些。 减少多数类的样本 欠采样/下采样【RUS、NearMiss、ENN、Tomeklink等（但我都没试过 == ）】【可能会影响原始样本的分布】 增加少数类的样本 过采样/上采样【SMOTE、ADASYN、Borderline-SMOTE等（只了解SMOTE相关的 ==）】 由于样本的稀疏性，正类少数样本很难有个清晰的结构，所以过采样生成的样本可能带来一部分噪音，同时导致空间上的不平滑分布。引用一下文章里面的引用： In cases with such a high imbalance the minority class is often poorly represented and lacks a clear structure. Therefore, straightforward application of pre-processing methods that rely on relations between minority objects (like SMOTE) can actually deteriorate the classification performance. Using randomized methods may also be inadvisable due to a high potential variance induced by the imbalance ratio. Methods that will be able to empower the minority class and predict or reconstruct a potential class structure seem to be a promising direction. -Krawczyk, Bartosz. “Learning from imbalanced data: open challenges and future directions.” Progress in Artificial Intelligence 5.4 (2016): 221-232. 结合上述两种方法同时采样 采样可以分成随机和非随机的，随机的话就是随机从大类样本中抽取一定比例样本再加上小类样本训练。但随机的话可能会带来一些其他问题，如过采样中选取了一些噪音数据，欠采样改变了数据的分布等。 除此之还有的话，有些场景下会不太一样。比如基于某些维度来采样，比如按客户维度比例随机抽取等，或者对每天按一定比例抽取等，或者是可以通过前置的一些算法，按某些维度来抽取。 2.2 算法层面刚刚提到了，由于类别不平衡，对于目标函数优化可能有影响，因为对于少数类误分也没太大影响，所以从算法层面上有代价敏感学习（cost-sensitive learning），字面意思就是给少数类分配较高的误分类代价。我之前在项目中就是修改了正负样本的权重，在采样之后，同时给正类较高的权重。 或者结合Boosting的方法，比如在欠采样中，多数类可以分成几部分和少数类一起训练，然后再结合模型来使用。 2.3 数据+算法层面大概就是借鉴了上面两种思路，比如周志华老师的BalanceCascade，结合了欠采样+Ensemble的方法。 该方法是一种典型的undersampling + ensemble方法，基学习器为AdaBoost类决策树。BC在每一次迭代中从多数类采样空间中随机降采样得到一个1:1的训练集，以从该训练集得到的Adaboost模型输出为依据，每一次迭代都从多数类采样空间中丢弃一部分已经被很好地分类的多数类样本。该多次迭代过程在逐步缩小采样空间降低Random undersampling的随机性的同时，在BalanceCascade过程后期产生的学习器会更加关注不易分的样本，并通过ensemble的方式最终得到一个鲁棒的强分类器。 这种方法有点类似我之前在工程中用到的，就是先通过采样训练一个baseline模型出来，然后丢弃一些可以非常好分类的大类，增加一些分数非常高的大类作为选定的样本，然后重复几次，即可选出一批比较特殊的样本了。 上述方法还有一个好处就是，可以将抽样带来的样本分布影响降低，通过Ensemble的方法，但就是会带来了较高的计算量，增加模型的训练+迭代+优化的难度，同时最终Ensemble出来的结果有问题时，比较难排除。 但这个方法在我所接触到的项目里面又比较特殊，因为这个做法有个前提假设，就是大类一定是大类，但实际中可能不太一样。比如金融欺诈的场景下，还是存在一些欺诈样本未被发现，此时模型给了一个较高的分数，但是根据上述的做法，这类样本最终会被选进训练样本，且当初正常样本来使用，对于模型可能会带来一些噪音干扰，也降低了模型的泛化能力。但跟业务沟通之后，且根据实际模型的效果，这种做法还是有些提高的。 上述的做法，大概就是为了在下面b图中，找到和正类重叠部分的负类，即分类难度比较高的那些样本。 引用一下最近看的那篇文章里面提到的问题，也是我们之前在项目中比较担心的问题： 初始random sample导致的误差累积：在extremely imbalance的情况下初始的几轮1:1采样面临着相当高的降采样率，此时降采样得到的多数类样本子集并不一定能够较好地反应总体数据的真实分布(namely, Bad undersampling)，使用这些bad undersampling得到的数据集训练得到的学习器学习到的自然也是一个biased分布，这意味着使用该学习器输出所丢弃的那一部分样本可能并不是我们想要丢弃的”soft”样本(错误地丢弃了含有重要信息的样本)。又由于在前几轮我们丢弃了多数类样本中一些重要的结构信息，在后续迭代中得到bad undersampling的概率变得更高，从而陷入了(bad sampling -&gt; bad base learner -&gt; bad drop -&gt; even worse sampling)的恶性循环。 后期一味关注样本的分类难度并不可取：与extremely imbalance随之而来的通常有类别分布在样本空间的严重重叠(overlapping)。设想两个类别各自服从一个只有中心点不同的高斯分布Gmaj与Gmin，少数类样本在Gmin上采样100次，多数类样本在Gmaj上采样1e5次 (IR=1000)。我们可以观察到即使两个类别所属的分布看起来可以被轻易区分开来，但在两者的样本数量差距悬殊的情况下，即便在少数类分布最密集的地方也存在相当多的多数类样本(见下图)。如果仍然秉持单纯关注样本的hardness的方法论，最后几轮迭代中这部分overlapping area中多数类样本的占比只会越来越多，可想而知使用这些样本训练得到的模型会具有相当糟糕的泛化能力(根据我自己的实验结果几乎等于没有泛化能力 = =)。 3. 可改进之处 数据层面 采样方面其实还是可以在深入研究一下，包括用上面提到的几种算法试一下； 由于样本是非常的不平衡，其实是可以尝试一下过采样的。但由于我之前接触的反欺诈项目，欺诈样本是和反欺诈不断抗衡的一个过程，不像其他不平衡项目中，相对比较稳定。所以过采样出来的样本还是得double check一下； 正类样本中一些被选择的部分，也需要再检查一下，毕竟有一部分是遗漏了的欺诈样本，但这部分就比较耗时了。 算法层面 整个结构我觉得可以再优化一些，可以尝试使用一下上面周老师的那个算法，或者模型那个结构，把整个pipeline的梳理好，方便以后模型刷新迭代； 4. References 极端类别不平衡数据下的分类问题S01：困难与挑战 极端类别不平衡数据下的分类问题S02：问题概述，模型选择及人生经验","tags":[{"name":"Class-Imbalance","slug":"Class-Imbalance","permalink":"http://chenson.cc/tags/Class-Imbalance/"},{"name":"PU Learning","slug":"PU-Learning","permalink":"http://chenson.cc/tags/PU-Learning/"}]},{"title":"半监督算法初探","date":"2019-09-19T10:19:57.000Z","path":"2019/09/19/半监督算法初探/","text":"1. Introduction在许多业务场景中，标签往往是非常难获取的，人工标注会花费大量的人力和金钱，且还不能保证百分百的准确性。这类情况可以分成两种： 完完全全的没有标签数据 有一小部分的正类标签样本和绝大部分的无标签样本 而这个也是最近刚好在项目中遇到的，在某银行的涉黑项目中，有好几w的带标签的黑名单标注样本，来自外部，然后想要主动识别行方内部用户（上亿）中是否还有这些用户（即没有被标注出来的涉黑用户），也就是上面提到的第二点，有部分的正类标签的样本和大量的无标签样本。这里介绍一下这个项目中用到的半监督算法- Two-Step PU Learning。 2. PU Learning Two-Step PU Learning：假定我们面对的数据中，真实黑样本中的小部分已经被标记了出来，构成集合P(Positve)，剩下所有数据都无标记构成集合U(Unlabeled)，如何建设模型可以对未标注的样本进行黑白分类？ 把U中的样本标签视为缺失，那么我们便可以考虑使用EM(Expectation Maximization)的思想，EM可以理解为是存在隐变量时MLE(Maximum Likelihood Estimation)的一种改进方法，这里我们在E步对缺失值进行填充，M步基于上次填充结果进行迭代，如是多轮之后产出最终模型，这就是原始的PU。 问题：如果一开始的P是有偏的，那么多轮的EM反而会有负向的效果 方案：引入spy【哨兵】机制，提高生成的白样本可靠性 2.1 算法步骤 生成样本池：选取问题所需的样本集，并根据其他领域迁移而来的知识给部分样本打上正例标签 While 不满足停止条件 do 采样：基于特定的采样方法，在采样环节选取出待标注的样本 标注：对待标注样本进行人工标注【仅保留专家100%的正类】 更新样本：采用特定的方法更新样本库 更新模型：使用two-step PU Learning方法更新模型【重点】 End While 2.1.1 采样 采用min-batch的方法批量采用，即每次采样多个记录，采样全都标注完成后算法才更新，在相同标注数量下显著减少了时间成本。 采样的样式基于Uncertainty &amp; Diversity标准，即尽量选取出当前模型最不确定同时又有着丰富的多样性的样本集。具体流程为： 对新的数据集$D_{new}$，使用当前模型打分 选取出若干个模型最不确定的白样本构成$D_{uncertain}$，不确定性的衡量基准基于模型打分而来 对$D_{uncertain}$进行K-means聚类，在每个类中取出最不确定的若干个样本，构造最终待标注的样本 2.1.2 标注专家进行标注，由于我们的方法对于P集合的信息会充分的信赖与利用，因此要求专家判断时，仅把具有充分信心的样本标注为1，保证P集合的正确性。 问题：这里标注的是1么，单么对于非1d部分，是标注为0么？ 2.1.3 更新样本在这一环节，由于对于专家标注的0无法完全信任，因此会选择将标为0的部分放入U集合中，假装没有标注过。而对于标注为1的部分，则进行多倍的上采样后全都放入P集合，以强化这批样本在下一轮模型更新中的作用。 2.1.4 更新模型常规的Active Learning通常如图二左边所示，专家会多次标注，逐渐扩充L(Labeled)集合，Active learner则会在多次学习L集合时不停提升自己的性能，我们称之为LU setting。 然而在本场景，我们更像是一个PU setting，专家多次标注，扩充P(Positive)集合，Learner则在每次迭代的时候，基于PU Learning进行学习。 PU Learning这里选用的Two-Step PU Learning，具体步骤如下： 将P集合中部分样本作为Spy【哨兵】混入到U集合中，并进行多轮EM迭代 考场Spy样本的分值分布，将U集合中所有分值小于Spy中10%分为模型分值的样本标注为0，生成N(Negative)集合，并基于此进行多伦EM迭代。 EM迭代的思路在Two-Step PU 过程中都是一致的，将P集合的样本score标注为1，U集合的样本score继承上一轮模型打分，训练新的模型拟合样本score，并给出新的模型打分，即完成一轮迭代。 这里Active Learing的基分类器为GBRT(Gradient Boosting Regression Tree) 5. References Positive Unlabeled Learning 无偏PU Learning 主动学习与半监督算法结合在支付宝风控的应用 半监督学习","tags":[{"name":"PU Learning","slug":"PU-Learning","permalink":"http://chenson.cc/tags/PU-Learning/"},{"name":"Semi-supervised Learning","slug":"Semi-supervised-Learning","permalink":"http://chenson.cc/tags/Semi-supervised-Learning/"}]},{"title":"TensorFlow高级API之Dataset","date":"2019-08-26T12:04:38.000Z","path":"2019/08/26/TensorFlow高级API之Dataset/","text":"1. Introduction上一篇主要介绍的是tf的Estimator，属于模型部分的。这里主要讲的是Dataset相关的内容，主要是加载数据，也会讲到一些特征工程相关的。 为什么使用Dataset呢，主要考虑到以下几点 之前都是placeholder+feed-dict，低级API。某篇博主里面说feed-dict是最慢的方式【存疑，待查证】。适用于从python将数据喂给后端。 Dataset提供了pipeline的操作方式，方便，复用性和性能也比较高，也比较适合大批量，分布式的情况【原因】。 2. 加载数据2.1 从numpay加载123456789# 开启# 且需要以开始就开启，否则会报如下错误# ValueError: tf.enable_eager_execution must be called at program startup.tf.enable_eager_execution()dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))iterator = dataset.make_one_shot_iterator()for i in iterator: print(i) 如果未开启的话，就需要新建tf.session来查看上面数据的结果了，否则会报如下错 因为tf采用的是符号式编程(symbolic style programs)模式，而不是常见的命令式编程(imperative style programs)模式，需要创建一个Session才可以运行程序 符号式编程： 命令式编程 同时也可以将np.array以元组的形式输入进去，比如(feaures, labels) 问题： 能否直接从pd.DataFrame中直接加载 2.2 从tf.tensors/tf.placeholder中加载123456789# 从tensor直接创建dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))# 从placeholder加载# 如果开启了eager——execution模式的话，会报如下错误# RuntimeError: tf.placeholder() is not compatible with eager execution.# 问题：如何把数喂进去 feed_dict么？x = tf.placeholder(tf.float32, shape=[None,2])dataset = tf.data.Dataset.from_tensor_slices(x) 2.3 从gendrator中加载2.4 加载csv文件12345CSV_PATH = 'tf_dataset_test_data.csv'dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)for i in dataset.take(5): print(i) 上面是最简单读取csv的方式，也有处理复杂一点的情况 可以先将csv文件以text的形式读进来，在读成csv文件 123456# 可以传一个文件的path，也可以是文件path的list# 且不能带header# 不能处理mixted typesdataset = tf.data.TextLineDataset(CSV_PATH2)for i in dataset.take(2): tf.decode_csv(i, record_defaults=[[_] for _ in df.select_dtypes(include='float').iloc[0].fillna(-999).values], field_delim=',') 读取的时候遇到了些坑，一直报错，然后看了官方文档-decode_csv里面参数介绍，一个recoder里面，所有格式必须相同，所以上面混着object/float/int的就不行了 Args: records: A Tensor of type string. ==Each string is a record/row in the csv and all records should have the same format.== record_defaults: A list of Tensor objects with specific types. Acceptable types are float32, float64, int32, int64, string. One tensor per column of the input record, with either a scalar default value for that column or an empty vector if the column is required. field_delim: An optional string. Defaults to &quot;,&quot;. char delimiter to separate fields in a record. use_quote_delim: An optional bool. Defaults to True. If false, treats double quotation marks as regular characters inside of the string fields (ignoring RFC 4180, Section 2, Bullet 5). name: A name for the operation (optional). na_value: Additional string to recognize as NA/NaN. select_cols: Optional sorted list of column indices to select. If specified, only this subset of columns will be parsed and returned. 把不同的类型的先去了就可以了，之后再找解决方案吧。 tf.data.experimental.make_csv_dataset可以读混合类型的csv，可参考官方文档 然后可以把这个代码封装起来，方便之后训练模型使用。 12345678910111213141516171819202122232425262728293031323334353637# 利用dataset解析CSV文件_CSV_COLUMNS = ['field1', 'field2', 'field3', 'field4']# 解析的摸版，必须是二维数组，否则报错_CSV_COLUMN_DEFAULTS = [[''], [''], [0.0], [0.0]]def input_fn(data_file, shuffle=False, batch_size=64, label_col=None, field_delim='\\t'): \"\"\" :param data_file: :param shuffle: :param batch_size: \"\"\" def parse_csv(value, label_col=label_col, field_delim=field_delim): columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS, field_delim=',') features = dict(zip(_CSV_COLUMNS, columns)) if label_col: labels = features.pop(label_col) return features, tf.equal(labels, 1.0) else: return features # Extract lines from input files using the Dataset API. dataset = tf.data.TextLineDataset(data_file) if shuffle: dataset = dataset.shuffle(buffer_size=100000) dataset = dataset.map(parse_csv, num_parallel_calls=100) # We call repeat after shuffling, rather than before, to prevent separate # epochs from blending together. # dataset = dataset.repeat() dataset = dataset.batch(batch_size) return datasetdataset = input_fn(CSV_PATH2, field_delim=',')for i in dataset.take(2): print(i) 那么回顾一下之前遇到混合类型数据的csv，如果再复杂一点的情况呢，比如里面某一列是个json数据，那么 3. 读取数据之前是加载数据，当然也有读取看了一下里面的数据，这只是读取数据的其中一种方法。 3.1 One-shotone-shot很简单，就跟之前使用的一样 12345678910x = np.random.sample((100,2))# make a dataset from a numpy arraydataset = tf.data.Dataset.from_tensor_slices(x)# create the iteratoriter = dataset.make_one_shot_iterator()# 和正常的迭代器一样，可以用get_next()获取下一个# create the iteratoriter = dataset.make_one_shot_iterator()el = iter.get_next() 3.2 Initializable之前有提到，加载数据的时候也可以从placeholder中加载，但是并没有实际喂入数据，这边跟之前用feed_dict差不多。但是这里也和上面一样，placeholder不支持eager模式 12345678910# using a placeholderx = tf.placeholder(tf.float32, shape=[None,2])dataset = tf.data.Dataset.from_tensor_slices(x)data = np.random.sample((100,2))iter = dataset.make_initializable_iterator() # create the iteratorel = iter.get_next()with tf.Session() as sess: # feed the placeholder with data sess.run(iter.initializer, feed_dict=&#123; x: data &#125;) print(sess.run(el)) # output [ 0.52374458 0.71968478] 3.3 Reinitializable3.4 Feedable4. 使用数据123456789101112131415# using two numpy arraysfeatures, labels = (np.array([np.random.sample((100,2))]), np.array([np.random.sample((100,1))]))dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()x, y = iter.get_next()# make a simple modelnet = tf.layers.dense(x, 8) # pass the first value from iter.get_next() as inputnet = tf.layers.dense(net, 8)prediction = tf.layers.dense(net, 1)loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as labeltrain_op = tf.train.AdamOptimizer().minimize(loss) 5. 特征工程5 References How to use Dataset in TensorFlow tf.io.decode_csv 用 tf.data 加载 CSV 数据","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://chenson.cc/tags/TensorFlow/"}]},{"title":"TensorFlow分布式训练初探","date":"2019-08-25T09:21:49.000Z","path":"2019/08/25/TensorFlow分布式训练初探/","text":"由于工作中可能会用到简单的深度学习模型，且由于数据量巨大，可能需要分布式训练。虽然用的平台可能有现成的框架，但还是了解一下。 1. Introduction1.1 深度学习模型训练过程 1.2 分布式训练策略 模型并行（in-graph-replication） 模型很大，无法在一个设备上跑完，需要分开跑。比如一台机器跑encoder，一台机器跑decoder等 数据并行（between-graph-replication） 训练数据分割成小块，在多个不同的设备上，每个设备上有完整的模型，互不影响 问题：如何保证不同设备上的模型参数是一致的？ 同步（synchronous） 数据也是像之前分开在多个设备上，每个设备更新需要等该轮所有设备完成训练-迭代后，收集所有的参数，统一更新。 异步（asynchronous） 数据也是像之前分开在多个设备上，其中一个设备完成一轮迭代，不需要等待其他节点，直接去更新模型的参数。不同设备读取参数取值的时间不一样，所以得到的值也有可能不一样。 问题 - 梯度失效（stale gradients） 刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一轮训练后，可能发现模型参数已经被其它设备更新过了，此时这个设备计算出的梯度就过期了（我理解的过期是指当前反向传播计算出来的最优值已经不适应最新的参数了）。由于梯度失效问题，异步训练可能陷入次优解 1.3 分布式训练架构某日面试的时候被问到了，之前写这篇文章的时候简单看了一下有点印象，但没认真整理，现在稍微补充一下 Parameter Server架构 (PS架构) 集群中的节点被分为两类，分别是parameter server和worker，其中 parameter server是存放模型的参数的 worker则是计算参数的梯度的 每个迭代过程中，worker从parameter server中获取参数，然后计算好梯度之后，就放回给parameter server；之后parameter server将worker中的梯度进行聚合操作，然后更新参数，再将新的参数广播给worker。 优点和缺点 对parameter server的传输带宽压力比较大，随着模型的复杂度加深，参数之间的相互依赖增大，需要传输的参数也多， Ring AllReduce架构 在这个架构中，每个设备都是worker，然后前后组成一个环，每一个worker将属于自己的mini-batch的数据梯度算好，然后传给下一个worker，同时也接受上一个worker的梯度。则对于一个包含了N个worker的环，每个worker都需要接受到其他N-1个worker的梯度后，就可以进行模型的参数了。整个过程可以分为两个过程 scatter-reduce allgather 问题： 如何判断已经接受到了其他N-1个worker的梯度？ 如何判断在最后一步中，大家的更新是同步的？ 优点 带宽的优化，每个设备节点的带宽都被充分利用了 1.4 RPC - Remot Precedure Call 与HTTP的区别 消息队列 2. TensorFlow的高阶API 在了解的过程，看到很多文章都在介绍tf.train.ClusterSpec 和 tf.train.Server 这类API实现的，且部署的需要修改的地方比较多。然后看到在1.3之后，官方给出了高级API：Estimator，居然部署的时候单机和分布式的代码基本一致，工程化比较方便。且给人的感觉很像sklearn里面的接口，实际上好像也是受那个启发，统一了接口。 然后又有人说2.0之后官方开始推 tf.keras 了，由于tf不是很了解，先看看这些吧。 对于 tf 1.x来说，官方推荐的是 tf.data.Dataset + tf.estimator.Estimator 对于 tf 2.x 来说，官方推荐的是 tf.data.Dataset + tf.keras 如上图，高阶的API有Estimator，Dataset，等 2.1 Dataset2.2 Experment2.3 Estimator 主要的方法有train, predict和evaluate 按照官方的教程，步骤如下： 2.3.1 编写函数模型12345def my_model_fn( features, # This is batch_features from input_fn labels, # This is batch_labels from input_fn mode, # An instance of tf.estimator.ModeKeys params): # Additional configuration 前两个参数是从输入函数中返回的特征和标签；也就是说，features 和 labels 是模型将使用的数据的句柄。mode 参数表示调用程序是train、predict还是evaluate。 调用程序可以将 params 传递给 Estimator 的构造函数。传递给构造函数的所有 params 转而又传递给 model_fn。在 custom_estimator.py 中，以下行将创建 Estimator 并设置参数来配置模型。此配置步骤与我们配置 tf.estimator.DNNClassifier（在预创建的 Estimator 中）的方式相似。 12345678910111213141516171819202122232425# 创建特征列# Feature columns describe how to use the input.# train_x 这里是my_feature_columns = []for key in train_x.keys(): my_feature_columns.append(tf.feature_column.numeric_column(key=key))classifier = tf.estimator.Estimator( model_fn=my_model, params=&#123; 'feature_columns': my_feature_columns, # Two hidden layers of 10 nodes each. 'hidden_units': [10, 10], # The model must choose between 3 classes. 'n_classes': 3, &#125;)# 训练classifier.train(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500))# 预测classifier.predict(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500))# 评估classifier.evaluate(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 500)) 看到很多地方都提到了first-class functions，了解一下，即函数可作为参数来用。 In computer science, a programming language is said to have first-class functions if it treats functions) as first-class citizens. This means the language supports passing functions as arguments to other functions, returning them as the values from other functions, and assigning them to variables or storing them in data structures.[1] Some programming language theorists require support for anonymous functions (function literals) as well.[2] In languages with first-class functions, the names) of functions do not have any special status; they are treated like ordinary variables) with a function type.[3] The term was coined by Christopher Stracheyin the context of “functions as first-class citizens” in the mid-1960s.[4] First-class functions are a necessity for the functional programming style, in which the use of higher-order functions is a standard practice. A simple example of a higher-ordered function is the map function, which takes, as its arguments, a function and a list, and returns the list formed by applying the function to each member of the list. For a language to support map, it must support passing a function as an argument. 2.3.2 定义模型其实这里就跟之前用低级的API定义模型差不多，差不多是把整个模型的结构定下来，这是不再需要手动去做运算，然后求导，反向传播和更新参数之类的。 12345678910111213141516171819# 1. 定义输入层# Use `input_layer` to apply the feature columns.net = tf.feature_column.input_layer(features, params['feature_columns'])# 2. 定义隐藏层# Build the hidden layers, sized according to the 'hidden_units' param.# units 参数会定义指定层中输出神经元的数量。# activation 参数会定义激活函数 - 在这种情况下为 Relu。# 这里的变量 net 表示网络的当前顶层。在第一次迭代中，net 表示输入层。在每次循环迭代时，tf.layers.dense 都使用变量 net 创建一个新层，该层将前一层的输出作为其输入。# tf.layers.dense 提供很多其他功能，包括设置多种正则化参数的功能。不过，为了简单起见，我们只接受其他参数的默认值。for units in params['hidden_units']: net = tf.layers.dense(net, units=units, activation=tf.nn.relu) # 3. 定义输出层(不使用激活函数)# Compute logits (1 per class).logits = tf.layers.dense(net, params['n_classes'], activation=None)# Compute predictions.predicted_classes = tf.argmax(logits, 1) 2.3.4 分布式参数设置2.3.5 相关问题 estimator 和 session的关系？只是封装了session么？ dataset和feed_dict的区别 1.3之前，使用placeholder + feed_dict读内存中的数据 1.3之前，使用文件名队列（string_input_producer）与内存队列（reader）读硬盘中的数据 tf.slim / tf.keras的区别 keras和tf.keras的区别 3. References 创建自定义 Estimator 使用 Estimator 构建线性模型 Higher-Level APIs in TensorFlow Denoising Autoencoder as TensorFlow estimator 如何用tensorflow实现分布式训练 构建分布式Tensorflow模型系列:Estimator 分布式训练的方案和效率对比","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://chenson.cc/tags/TensorFlow/"},{"name":"分布式","slug":"分布式","permalink":"http://chenson.cc/tags/分布式/"}]},{"title":"NLP相关算法初探","date":"2019-08-11T06:33:38.000Z","path":"2019/08/11/NLP相关算法初探/","text":"1. Introduction主要是想了解下NLP中Embedding的相关的，然后顺手就看了下最近比较火的Attention和Bert等算法，也正好丰富一下知识体系，了解了解。笔记做得很简单，之后再好好丰富吧。 Embedding在机器学习中应该地位比较重要，对于很多非结构化的数据或者半结构化的数据，往往都要用到Embedding，转化成机器容易识别的一个结构化的数据，比如向量和矩阵等，这么做的同时，还可以提高模型的泛化能力，不仅仅是限制在历史出现过的样本。 介绍Embedding之前，先简单介绍一下初始阶段常见的概念 词编码 向量空间子结构 Bag of words(词袋模型) TF-IDF 语言模型 分布式表示 共现矩阵 NNLM - Neural Netword Language Model 神经网络语言模型 图像预训练 底层特征的可复用特性 $\\to$ frozen 高层特性任务相关性质 $\\to$ fine-tuning 2. Word-Embedding 将预训练这种思想应用到NLP中 Word2Vec只是实现Word-Embedding的一种工具或者框架，其他的还有Glove等 发展历程： NNLM $\\to$ WE $\\to$ ELMO(双向语音模型，解决了多义词) $\\to$ GPT(特征用了Transformer来抽取，预训练) $\\to$ Bert(集大成者的解决方案，不算一个新模型。双向，预训练+微调，即图像预训练中的思路) 缺点是无法解释多义词 3. Attention和Transformer的关系 Transformer是一种不同于RNN和CNN的新的网络结构，这里强调的是一种网络结构 Transformer包含了Encoder和Decoder(基本Seq2Seq的模型都在这么一种架构) 如何理解文本领域里的Encoder-Decoder框架？ 比如机器学习的分类问题中，训练数据通常是一个 pari，其中$X = $, y是类别，可以是0/1，或者是一个ont-hot的向量 那么文本领域里常常是，这里Source和Target可以是一篇文章、一个句子等，可以是同一种语言也可以是不同的语言，对应的任务可以是翻译，或者总结等； $Source = (x_1, x_2, … …, x_m)$, Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C，其中 $C = F(x_1, x_2, … …, x_m)$ $Target = (y_1, y_2, … …, y_n)$, 对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息，来生成i时刻要生成的单词，即 $(y_1, y_2, …, y_{i-1}) + C \\to y_i$ 每个$y_i$都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target 如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架 如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架 如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架 除了文本领域，在语音识别、图像处理也可以有类似的应用，比如实时翻译，或者图片描述等 3. Attention 上面提到了$y_i$是根据历史的输出$ + C$ 来生成的，即 \\begin {align} y_1 &= f(C) \\\\ y_2 &= f(C, y_1) \\\\ y_3 &= f(C, y_1, y_2) \\\\ & ... ... \\\\ y_i &= f(C, y_1, y_2, ..., y_{i-1}) \\end {align}配图 其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别这里，在生成C的时候，所有的输入单词$$的影响力都是一样的，但是实际生活中，我们的都知道有些单词是比较重要，有些可以忽略，即需要给这些输入分配一个概率权重，有了这个概率信息，意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。 这个就是Attention的关键，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci，具体如下 \\begin {align} y_1 &= f(C_1) \\\\ y_2 &= f(C_2, y_1) \\\\ y_3 &= f(C_3, y_1, y_2) \\\\ & ... ... \\\\ y_i &= f(C_i, y_1, y_2, ..., y_{i-1}) \\\\ \\end {align}配图 那么问题来了，如何根据输入信息X，计算这个概率分布值？ 4. Self-Attention 可以将attention机制看做一种query机制，即用一个query来检索一个memory区域。我们将query表示为key_q。这里memory是一个键值对集合（a set of key-value pairs），共有M项（M为词库的大小？），其中的第i项，即第i个词，表示为。通过计算query和key_m[i]的相关度，来决定查询结果中，value_m[i]所占的权重比例。注意，这里的key_q，key_m，value_m都是vector。 Attention的计算概括起来分三步：1）计算query和memory中每个key_m的相关度。2）对所有的相关度结果使用softmax函数进行概率归一化处理。3）根据概率归一化结果对memory中的所有value_m进行加权平均，得到最终的查询结果。计算过程，形式化为： ==注：== key_q指的是上图中的vector q key_m~i~指的是上图中的vector k~i~ value_m~i~指的是上图中的vector v~i~ 在self-attention中，Q=K=V，是一个由所有词的词向量构成的一个矩阵。 综上，self-attention是一种序列建模的方式，在对句子进行分布式表示的时候，句子中的所有的词都会发生直接的交互关系，相对于RNN/双向RNN和CNN而言，更能够捕获长距离依赖知识。 Q-matrix, K-matrix, V-matrix 是如何初始化，保证Q是Query, K是Key和V是Value?或者后期是如何向最佳方向优化的？ 位置向量是如何初始化以及优化的 左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量 可以在 get_timing_signal_1d()中看到生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。 残差模块 损失函数 - [交叉熵](https://colah.github.io/posts/2015-09-Visual-Information/) - [KL离散度](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) 5. Multi-Head6. Positional Encoding7. TODO：在tf中实现word-embedding8. References Paper: Attention Is All You Need The Illustrated Transformer The Illustrated Transformer【译】 Code: The Annotated Transformer 深度学习中的注意力模型（2017版） 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统","tags":[{"name":"NLP","slug":"NLP","permalink":"http://chenson.cc/tags/NLP/"},{"name":"Bert","slug":"Bert","permalink":"http://chenson.cc/tags/Bert/"},{"name":"Attention","slug":"Attention","permalink":"http://chenson.cc/tags/Attention/"},{"name":"Embedding","slug":"Embedding","permalink":"http://chenson.cc/tags/Embedding/"}]},{"title":"HJJ反欺诈分享笔记","date":"2019-08-07T15:16:57.000Z","path":"2019/08/07/HJJ反欺诈分享笔记/","text":"1. Introduction知乎网红黄姐姐来我司的分享会，挺早就有关注她的，还挺想听一听的。但由于人在外地出差，就将同事做的note整理一下，之后可能用的到。 2. 反欺诈特征衍生 LBS，ip网格划分（违约率地图） 紧急联系人通话记录衍生，订单衍生 拒绝的人特征挖掘（拒绝的人、黑样本、正常还款，三者对比） APPList 赌博类app，套现类app，提额类app，贷款类，彩票类，app组合（eg:1个赌博+3个套现），作业类app，学习类，网赚类（金牌试客） app分类是更详细，个推提供比较更详细的app分类，比如检测出清机软件、定位软件、模拟软件、通讯录同步助手 埋点数据ip的变动，二次登陆，动支时间差（沉默用户用款），点击行为序列（次数，占比，序列） 短信特征衍生：“网贷中介”，“小贷”，“赌”，不良信用卡还款信息，催收信息，注册网贷，“老师”，“作业”，“学校”，“作业没写完” 通讯录与运营商数据对比、详单和通讯录交叉占比 电话邦数据筛选通讯录信息（快递类，外卖类，中介类等） 3. 反欺诈方法 风险点（鱼骨图）(注册 — 申请 — 下单 — 还款) SNS社交网络模型 聚类模型 异常点检测（孤立森林，auto_encoder） 4. 渠道分析 x轴：申请完成率，y轴：动支金额，z轴：授信金额 划分 聚类分析 5. 其他 APP录屏需求 传销：风险点集中暴雷 宏观调控 走工作 号码+时间 设备指纹（sensorID） 黑白名单核对，需要确定是黑人的时间 6. References 黄姐姐HJJ知乎 运营商处女地 黄金数据（sensorID） 紧急联系人 那些list背后的玄机","tags":[{"name":"Anti-Fraud","slug":"Anti-Fraud","permalink":"http://chenson.cc/tags/Anti-Fraud/"}]},{"title":"Shell常用命令小结","date":"2019-07-28T05:43:00.000Z","path":"2019/07/28/Shell常用命令小结/","text":"1. Introduction整理一下常用的Shell工具，比如sed、awk和xargs等，方便以后查阅。 2. sed3. awk4. xargs5. others6. References xargs 命令教程 Linux and Unix xargs command tutorial with examples","tags":[{"name":"Shell","slug":"Shell","permalink":"http://chenson.cc/tags/Shell/"},{"name":"sed","slug":"sed","permalink":"http://chenson.cc/tags/sed/"},{"name":"awk","slug":"awk","permalink":"http://chenson.cc/tags/awk/"},{"name":"xargs","slug":"xargs","permalink":"http://chenson.cc/tags/xargs/"}]},{"title":"Snippets for Python","date":"2019-06-28T02:07:00.000Z","path":"2019/06/28/Snippets-for-Python/","text":"收集平常遇到的各种不常用的Python技巧，持续更新ing。 1. 窗口函数滑窗函数平常还是用的很多的。细分的话可以分按时间维度、次数维度或是排序维度滑窗计算。 1.1 Codes 时间维度滑窗 123df = pd.Series(np.random.randn(600), index=pd.date_range('7/1/2018', freq='D', periods=600))r = df.rolling(window=5) 同时也支持自定义的聚合函数 1.2 Refernces2. 自定聚合函数2.1 Codes 测试一个SUM的聚合函数 123from functools import reducedef self_define_sum(series): return reduce(lambda x, y : x + y, series) 过滤非空值 123def not_null(series): analyst_list = filter(lambda analyst: pd.notnull(analyst), series) return list(analyst_list) 统计空值 12def count_nulls(series): return len(series) — series.count() 2.2 References Writing custom aggregation functions with Pandas 3. sort - UDF3.1 Codes123456s = ['(1, 10)', '(100, 1000)', '(20, 30)', '(100, 10000)', '(50, 100)']print(sorted(s))s.sort(key=lambda x : int(x.split(',')[0][1:]))print(s) 3.2 References4. Sklearn并行计算4.1 Codes1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = 'all'import pandas as pdimport numpy as npimport time, os, subprocess# Tutorial: https://stackoverflow.com/questions/38601026/easy-way-to-use-parallel-options-of-scikit-learn-functions-on-hpcfrom sklearn.externals.joblib import Parallel, parallel_backend, register_parallel_backend, delayed# 不q'cudef reader(f): time.sleep(5) return pd.read_csv(f) # method 1def mlp_reader(file_list, n_job=20, typ='threading'): \"\"\" By default the following backends are available: ‘loky’: single-host, process-based parallelism (used by default), # 可能不是我装的0.19版本的参数，报错，文档来着官方0.22的文档 ‘threading’: single-host, thread-based parallelism, ‘multiprocessing’: legacy single-host, process-based parallelism. \"\"\" with parallel_backend(typ): df_tmp = Parallel(n_jobs=n_job)(delayed(reader, check_pickle=False)(f) for f in file_list) df_tmp = pd.concat(df_tmp) # return df_tmp# method 2def mlp_reader_2(file_list, n_job=20): df_tmp = Parallel(n_jobs=n_job)(delayed(reader, check_pickle=False)(f) for f in file_list) df_tmp = pd.concat(df_tmp) return df_tmp # 测算时间def measure_time_using(n=10, p=10): r = [] for i in range(1, n): file_list = ['20190525_spark_test_data.csv' for _ in range(i * p)] # print(file_list) t1 = time.time() df_result2 = mlp_reader_2(file_list) t2 = time.time() t3 = time.time() df_result = mlp_reader(file_list) t4 = time.time() r.append([i, i * 10, t2-t1, t4-t3]) return pd.DataFrame(r, columns=['iter', 'total_file', 'mlp_reader_2', 'mlp_reader']) 在我的单机Mac Pro上，不用parallel_backend会稍微慢一些，这和我在实际建模工程中相反了，不知道是因为当时使用的是服务器有关系不。 在我的单机Mac Pro上，多线程的稍微快一点点。 4.2 References Easy way to use parallel options of scikit-learn functions on HPC Model selection / joblib does not actually use dask / ipyparallel distributed backends #7168 Document - sklearn.utils.parallel_backend 5. Subprocess5.1 Codes12345678910import subprocess, os# 开一个子进程p = subprocess.Popen(['python', 'python_file_for_runing.py'], stdout='logfile_name', stderr=subprocess.STDOUT, preexec_fn=os.setsid)# 打印子进程号print(\"subprocess's ID :\", p.pid) 5.2 References Python multiprocessing within mpi 6. 计算函数运行时间的装饰器6.1 Codes123456789101112131415161718192021222324252627282930313233343536373839import timefrom functools import wrapsdef timefn(fn): \"\"\"计算性能的修饰器\"\"\" precision = 4 @wraps(fn) def measure_time(*args, **kwargs): t1 = time.time() result = fn(*args, **kwargs) t2 = time.time() print(f'INFO ：func &#123;fn.__name__&#125; using &#123;round(t2 - t1, precision)&#125; second.') return result return measure_time# 使用方式1## 在定义函数前进行装饰@timefndef foo(): s = 0 for i in range(1000000): s += 1 return sfoo()# 使用方式2## 作为参数传进装饰器def bar(): s = 0 for i in range(1000000): s += 1 return stimefn(bar)()&gt;&gt;&gt; INFO ：func foo using 0.0898 second.&gt;&gt;&gt; 1000000&gt;&gt;&gt; INFO ：func bar using 0.1557 second.&gt;&gt;&gt; 1000000 6.2 References Python multiprocessing within mpi 7. 自定聚合函数7.1 Codes12 2.2 References8. 自定聚合函数8.1 Codes12 8.2 ReferencesReferences Python语言小册","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.cc/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.cc/tags/Pandas/"}]},{"title":"Hive常用函数小结","date":"2019-06-17T01:29:05.000Z","path":"2019/06/17/Hive常用函数小结/","text":"1. 窗口函数 函数 说明 ROW_NUMBER() 从1开始，按照顺序，生成分组内记录的序列，比如，按照pv降序排列，生成分组内每天的pv名次，ROW_NUMBER()的应用 场景非常多，再比如，获取分组内排序第一的记录，获取一个session中的第一条refer等 RANK() 生成数据项在分组中的排名，排名相等会在名次中留下空位 DENSE_RANK() 生成数据项在分组中的排名，排名相等会在名次中不会留下空位 CUME_DIST() 小于等于当前值的行数除以分组内总行数。比如，统计小于等于当前薪水的人数所占总人数的比例 PERCENT_RANK() 分组内当前行的RANK值-1/分组内 总行数-1 NTILE(n) 用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN,比如NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PERCEDING AND CURRENT ROW) Hive窗口函数可以计算一定范围内、一定值域内、或者一段时间内的累积和以及移动平均值等；可以结合聚集函数SUM() 、AVG()等使用；可以结合FIRST_VALUE() 和LAST_VALUE()，返回窗口的第一个和最后一个值。 如果只使用PARTITION BY子句,未指定ORDER BY的话,我们的聚合是分组内的聚合. 使用了ORDER BY子句,未使用window子句的情况下,默认从起点到当前行.window子句： PRECEDING：往前 FOLLOWING：往后 CURRENT ROW：当前行 UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING：表示到后面的终点 1.1 计算累计和统计1-12月的累积和，即1月为1月份的值，2月为1、2月份值的和，3月为123月份的和，12月为1-12月份值的和。关键字解析： SUM(SUM(amount)) 内部的SUM(amount)为需要累加的值； ORDER BY month 按月份对查询读取的记录进行排序，就是窗口范围内的排序； ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW 定义起点和终点，UNBOUNDED PRECEDING 为起点，表明从第一行开始, CURRENT ROW为默认值，就是这一句等价于： ROWS UNBOUNDED PRECEDING PRECEDING：在前 N 行的意思。 FOLLOWING：在后 N 行的意思。 1.1.1 计算所有月份的累计和123456789101112131415 select pt_month, sum(amount) pay_amount, sum(sum(amount))over(order by pt_month) cumulative_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state=0group by pt_month; select pt_month, sum(amount) pay_amount, sum(sum(amount))over(order by pt_month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) cumulative_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state = 0group by pt_month; 1.1.2 计算前3个月和本月共4个月的累积和123456789101112131415 select pt_month, sum(amount) pay_amount, -- 从当前行开始，再往前数3行 sum(sum(amount))over(order by pt_month ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) cumulative_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state=0group by pt_month; select pt_month, sum(amount) pay_amount, sum(sum(amount))over(order by pt_month ROWS 3 PRECEDING) cumulative_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state = 0group by pt_month; 1.1.3 计算前1月后1月和本月共3个月的累积和1234567 select pt_month, sum(amount) pay_amount, sum(sum(amount))over(order by pt_month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) cumulative_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state = 0 group by pt_month; 1.2 计算平均值1.2.1 计算前1月后1月和本月共3个月各月总值的平均值1234567 select pt_month, sum(amount) pay_amount, avg(sum(amount))over(order by pt_month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) average_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state = 0 group by pt_month; 1.2.2 计算前3个月和本月共4个月各月总值的平均值1234567 select pt_month, sum(amount) pay_amount, avg(sum(amount))over(order by pt_month ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) cumulative_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state = 0 group by pt_month; 1.3 计算窗体第一条和最后一条的值12345678 select pt_month, sum(amount) pay_amount, first_value(sum(amount))over(order by pt_month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) first_amount, last_value(sum(amount))over(order by pt_month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) last_amount from data_chushou_pay_info where pt_month between '2017-01' and '2017-12' and state = 0 group by pt_month; 1.4 ROW_NUMBER在做row_number() over() 排序时，排序要严格确认顺序，不然有相同顺序条件，多次执行结果就可能不一致的(相同条件会随机取) 2. 分析函数3. 视图4. 常见CASES4.1 实现一个字段包含另一个字段的查询123select * from data_chushou_pay_info where name1 like concat('%', name2, '%'); 4.2 分组排序 取top N12345678910select t1.* from ( select 品牌, 渠道, 档期, count/sum/其它() as num row_number() over (partition by 品牌,渠道,档期 order by num desc ) rank from table_1 where 品牌,渠道 限制条件 group by 品牌,渠道,档期) where t1.rank &lt;= 10 4.3 聚合函数与COALESCE和IF组合使用 COALESCE(arg1, arg2, arg3…)：遇到非null参数即返回该值 123SELECT SUM(COALESCE(sex_age.age, 0)) AS age_sum, SUM(IF(sex_age.sex='Female', sex_age.age, 0)) AS female_age_sum FROM employee; 4.4 聚合函数与DISTINCT关键词组合使用123SELECT COUNT(DISTINCT sex_age.sex) AS sex_uni_cnt, COUNT(DISTINCT name) AS name_uni_cnt FROM employee; 注：如果COUNT和DISTINCT连用，Hive将忽略对reducer个数的设置（如：set mapred.reduce.tasks=20;）, 仅会有一个reducer！此时reduce将成为瓶颈，这时我们可以使用子查询的方式解决该问题。 12SELECT COUNT(*) AS sex_uni_cnt FROM (SELECT DISTINCT sex_age.sex FROM employee) a; 4.5 采样当数据集非常大的时候，我们需要找一个子集来加快数据分析。此时我们需要数据采集工具以获得需要的子集。在此可以使用三种方式获得采样数据：random sampling， bucket sampling， block sampling. 4.5.1 Random Sampling使用RAND()函数和LIMIT关键字来获取样例数据。使用DISTRIBUTE和SORT关键字来保证数据是随机分散到mapper和reducer的。ORDER BY RAND()语句可以获得同样的效果，但是性能没这么高 123SELECT * FROM &lt;Table_Name&gt; DISTRIBUTE BY RAND() SORT BY RAND() LIMIT &lt;N rows to sample&gt;; 4.5.2 Bucket Sampling该方式是最佳化采样bucket表。RAND()函数也可以用来采样整行。如果采样列同时使用了CLUSTERED BY，使用TABLESAMPLE语句会更有效率。 123SELECT * FROM &lt;Table_Name&gt; TABLESAMPLE(BUCKET &lt;specified bucket number to sample&gt; OUT OF &lt;total number of buckets&gt; ON [colname|RAND()]) table_alias; 4.5.3 Block Sampling4.6 GROUPING SETS12345678910111213141516171819202122232425262728293031323334-- /***** CASE-1 *****/ SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS (a,b) -- 相当于 SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b -- /***** CASE-2 *****/ SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ((a,b), a) -- 相当于 SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b UNION SELECT a, null, SUM( c ) FROM tab1 GROUP BY a -- /***** CASE-3 *****/ SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS (a,b) -- 相当于 SELECT a, null, SUM( c ) FROM tab1 GROUP BY a UNION SELECT null, b, SUM( c ) FROM tab1 GROUP BY b 注：似乎对Hive的版本有要求，具体版本号有点记不清了。 5. 常见优化5.1 基于条件的LEFT OUTER JOIN优化左连接时，左表中出现的JOIN字段都保留，右表没有连接上的都为空。对于带WHERE条件的JOIN语句，例如： 1234567SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key = b.key) WHERE a.ds = '2009-07-07' AND b.ds = '2009-07-07' 执行顺序是，首先完成2表JOIN，然后再通过WHERE条件进行过滤，这样在JOIN过程中可能会输出大量结果，再对这些结果进行过滤，比较耗时。可以进行优化，将WHERE条件放在ON后，例如 123456789101112131415SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON a.key = b.key AND b.ds = '2009-07-07' AND a.ds = '2009-07-07' -- 用子查询优化-- 将where条件放到子查询中，查询效率将会提高很多SELECT a.val, b.val FROM (SELECt * FROM a WHERE a.ds = '2009-07-07') LEFT OUTER JOIN (SELECT * FROM b WHERE b.ds = '2009-07-07') ON a.key = b.key 多用子查询，多用where，然后再join。 如果先join，就全表扫描，然后最后where最后筛选，比较耗时。 如果用子查询，就可以利用where过滤不相关的字段，不但增加了map 数量，还减少了数据量。 尽量尽早地过滤数据，减少每个阶段的数据量,对于分区表要加分区，同时只选择需要使用到的字段. 小表要注意放在join的左边（有的公司很多都小表放在join的右边）否则会引起磁盘和内存的大量消耗。hive会将join前面的表数据装载内存,所以较小的一个表在较大的表之前,减少内存资源的消耗 5.2 左半连接（LEFT SEMI JOIN）左半连接实现了类似IN/EXISTS的查询语义，使用关系数据库子查询的方式实现查询SQL，例如： 1234SELECT a.key, a.value FROM a WHERE a.key IN (SELECT b.key FROM b); 使用Hive对应于如下语句： 1234SELECT a.key, a.val FROM a LEFT SEMI JOIN b ON (a.key = b.key) 需要注意的是，在LEFT SEMI JOIN中，表b只能出现在ON子句后面，不能够出现在SELECT和WHERE子句中。关于子查询，这里提一下，Hive支持情况如下： 在0.12版本，只支持FROM子句中的子查询； 在0.13版本，也支持WHERE子句中的子查询。 6. 常见问题6.1 数据倾斜6.1.1 JOIN的KEY倾斜 空值 [Hive]Hive数据倾斜（大表join大表） 事实上“把小表放在前面做关联可以提高效率”这种说法是错误的。正确的说法应该是“把重复关联键少的表放在join前面做关联可以提高join的效率。”【缺确认】Hive中小表与大表关联(join)的性能分析 7. References Hive分析函数和窗口函数 Hive JOIN使用详解 hive子查询sql效率优化 hive 查询性能优化总结 hive-where和having的区别 WHERE和HAVING的区别? Hive聚合函数及采样函数详解","tags":[{"name":"Hive","slug":"Hive","permalink":"http://chenson.cc/tags/Hive/"}]},{"title":"常见降维算法小结（二）","date":"2019-06-08T02:49:29.000Z","path":"2019/06/08/常见降维算法小结（二）/","text":"1. Introduction 2. 流形学习3. t-SNE4. Embedding5. AutoEncoder6. Word2Vec7. References 流形学习-高维数据的降维与可视化","tags":[]},{"title":"常见降维算法小结","date":"2019-05-30T08:23:01.000Z","path":"2019/05/30/常见降维算法小结/","text":"1. Introduction由于数学不是很好，经常对一些矩阵分解之类相关的算法理解得有些模糊，大致知道算法的核心思想，往往却没理解到其数学角度的本质区别，所以有必要好好整理一下相关的知识点。就以降维为起点，梳理一下相关的算法。（可能有些并没有归类到降维相关的算法上，但最终却有着降维的效果） 2. PCA - Principal Components Analysis主成成分分析(PCA)属于线性、无监督的全局降维算法 2.1 最大化投影方差从最大方差角度理解PCA，就是将高维数据投影到了低维上，并保证了数据之间的的方差是尽可能最大的。因为从信号的角度上来看大方差往往具有较高的信噪比 假设数据的原始矩阵为$V = \\{\\overrightarrow {v_1}, \\overrightarrow {v_2}, …, \\overrightarrow {v_n}\\}$，维度为$n\\times m$，其均值为$\\overrightarrow u = \\frac 1 n \\sum_{i=1}^n \\overrightarrow {v_i}$ 算法步骤： 2.1.1 对样本数据进行中心化处理，得到处理后样本$X$ \\begin {align} X &= [\\overrightarrow {v_1} - \\overrightarrow {u_1}, \\overrightarrow {v_2} - \\overrightarrow {u_2},..., \\overrightarrow {v_n} - \\overrightarrow {u_n}] \\\\ &= [{\\overrightarrow {x_1}, \\overrightarrow {x_2}, ..., \\overrightarrow {x_n}} ] \\end {align}向量的内积在几何上表示为第一个向量到第二个向量上的长度，向量$\\overrightarrow {x_i}$在单位向量$\\overrightarrow {w}$上的投影可以表示为$ (\\overrightarrow {x_i}, \\overrightarrow {w}) = x_i^Tw$， 且具体值为 \\begin {align} x_i^T w &= \\frac 1 n \\sum_{i=1}^n x_i^Tw \\\\ &= (\\frac 1 n \\sum_{i=1}^n x_i^T)w \\\\ &= 0 \\end {align}因为内部的数据中心化后和为0 2.1.2 求中心化处理后样本的协方差矩阵投影后的方差可以表示为 \\begin {align} D(x) &= \\frac 1 n \\sum_{i=1}^n (x_i^Tw)^2 \\\\ &= \\frac 1 n \\sum_{i=1}^n (x_i^Tw)^T(x_i^Tw) \\\\ &= \\frac 1 n \\sum_{i=1}^n w^Tx_ix_i^Tw \\\\ &= w^T(\\frac 1 n \\sum_{i=1}^n x_ix_i^T)w \\tag 1 \\end {align}公式(1)的括号内部其实是样本协方差矩阵，将其简化写作$\\Sigma$，则有 \\left\\{ \\begin{array}{} \\max\\{w^T\\Sigma w\\} \\\\ s.t \\space\\space\\space\\space w^Tw=1 \\end{array} \\right.以上问题就变成了带约束的最大化问题，之前整理的常见优化算法时提到拉格朗日乘子就是用来解这样问题的。 拉格朗日(Lagrangian)函数 L(x, y, \\lambda) = f(x, y) - \\lambda(g(x, y) - c)这里$f(x, y)$是目标函数，$g(x, y) = c$是约束条件。 将上面的式子代入到拉格朗日函数，有（这里$\\lambda$是拉格朗日乘子） J(w) = w^T\\Sigma w - \\lambda(w^Tw - 1) \\tag 2将上述公式(2)对$w$求导，并令其为0，则有 \\Sigma w - \\lambda w = 0即有$\\lambda$为$\\Sigma$的特征值，此时有 w^T\\Sigma w = w^T\\lambda w = \\lambda w^Tw = \\lambda即有$x$投影后的方差就是协方差矩阵的特征值$\\rarr$最大化方差就是最大化协方差矩阵最大的特征值 $\\rarr$最佳投影方向就是最大特征值所对应的特征向量。 特征值和特征向量 数学表示$Mv_i = \\lambda_i v_i$，这里$M$是个$n\\times n$的方阵，$v_i$是个n维的特征向量，$\\lambda_i$是方阵$M$的一个特征值，而$v_i$是方阵$M$的特征值$\\lambda_i$所对应的特征向量 特征值分解主要将一个方阵分解成左边格式$M=Q\\Sigma Q^{-1}$ 这里$Q$是方阵$M$的特征向量组成的矩阵 $\\Sigma$是一个对角矩阵，每一个对角线上的元素就是一个特征值 一般矩阵A，将A与其转置相乘$A^TA$，将会得到一个方阵，便可求得该方阵的特征值$A^TAv_i=\\lambda_iv_i$ 特征值表示的是这个特征到底有多重要，类似于权重，而特征向量在几何上就是一个点，从原点到该点的方向表示向量的方向 特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么 特征向量的代数上含义是：将矩阵乘法转换为数乘操作 特征向量的几何含义是：特征向量通过方阵A变换只进行伸缩，而保持特征向量的方向不变 这里同时需要强调一下：矩阵乘法其实是一个线性变换的过程 比如下面的式子变化如下图 \\left[\\begin{array}{} 3 &0\\\\ 0&1 \\end{array} \\right] \\left[\\begin{array}{} x\\\\ y \\end{array} \\right] = \\left[\\begin{array}{} 3x\\\\ y \\end{array} \\right] 同理，之前得到的对角矩阵$\\Sigma$，每一个对角线上的元素就是一个特征值，描述的也如上面的操作，描述了这个矩阵的变化方向，同时从大到小的排列代表了重要程度 2.2 最小回归误差2.1.3 对协方差矩阵进行特征值分解，将特征值从大到小排列2.1.4 取特征值前d大对应的特征向量，将n维数据投影到d维上2.2 最小平方误差3. SVD - Singular Value Decomposition3.1 图解SVD之前在讲PCA的时候，有提到特征值和特征值分解，这些都是针对行与列值相等的方阵而言的。但么对于非方阵，就可以用奇异值分解(SVD)来分解，分解出来的奇异值相当于方阵中的特征值。 奇异值分解即对任意矩阵M表示为下面这种形式 M = U\\Sigma V^T \\tag 3这里有 $U$是左奇异(singular)矩阵，$m \\times m$的正交阵，表示原始域的标准正交基 V是右奇异矩阵$n \\times n$的正交阵，表示经过M 变换后的co-domain的标准正交基 $\\Sigma$是$m \\times n$的对角矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值(降序排列)。表示了V 中的向量与u中相对应向量之间的关系 3.2 求解$U, \\Sigma, V$三个矩阵 V 根据之前提到的$M^TMv_i = \\lambda_i v_i$，此时即可求出矩阵V = {$v_1, v_2, …, v_n$} U 将矩阵$M$和$M$的转置做矩阵乘法，会的到一个$m \\times m$的方阵$MM^T$，此时有$MM^Tu_i = \\lambda_i u_i$，矩阵U={$u_1, u_2, …, u_m$} 问题1：为什么$M^TM$对应的是矩阵$M$，$MM^T$对应的是矩阵$U$?仅仅根据对应矩阵的大小么？ 问题2：这里的特征值$\\lambda_i$是之前公式里面的特征值么？ $\\Sigma$ 由于$\\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\\sigma$可以了。 将上面的等式(3)两边同时乘以矩阵V，则有（V的转置矩阵$V^T$等于V的逆矩阵$V^{-1}$，$VV^T=I$，对角线值为1的单位矩阵，单位矩阵$I$×任意矩阵$A$=$A$） \\begin {align} MV &= U \\Sigma V^TV \\\\ &= U \\Sigma \\\\ Mv_i &= \\sigma_i u_i \\\\ \\sigma_i &= \\frac {M v_i} {u_i} \\end {align}这里将矩阵V表示为{$v_1, v_2, …, v_n$}，矩阵U表示为{$Av_1, Av_2, …, Av_n$} 问题：矩阵U大小为$m\\times m$，这里似乎$m \\times n$，不对？ 之前就已经求出了$V$和$U$了，即可以求出奇异值矩阵$\\Sigma$ 这里证明一下之前提出的问题，为什么为什么$M^TM$对应的是矩阵$M$，$MM^T$对应的是矩阵$U$。 先证明前者，根据之前的公式 \\begin {align} M &= U\\Sigma V^T \\tag 4\\\\ M^T &= V\\Sigma^TU^T \\tag 5\\\\ M^TM &= V\\Sigma^TU^T U\\Sigma V^T \\tag 6\\\\ &= V\\Sigma^TI\\Sigma V^T \\\\ &= V\\Sigma^2V^T \\\\ M^TM V &= V\\Sigma^2V^TV \\tag 7\\\\ &= V\\Sigma^2 \\end {align}将公式(4)(5)相乘可得公式(6)，两边同时乘以矩阵$V$，得到公式(7) 问题：关于公式里面的$\\Sigma^2$是怎么来的，以及和上面提到的$\\lambda_i$的关系？ 回答：$\\Sigma$是奇异值矩阵，$\\lambda_i$是特征值，两者关系如下 \\begin {align} M^TMV &= V\\Sigma^2 \\\\ M^TMv_i &=\\sigma_i^2v_i \\\\ &= \\lambda_i v_i \\\\ \\sigma_i &= \\sqrt \\lambda_i \\tag 8 \\end {align}所以$\\sigma_𝑖$可以通过$\\frac {M𝑣_𝑖} {𝑢_𝑖}$来计算，也可以通过求出$M^TM$的特征值$\\lambda_i$取平方根来求奇异值 证明后面的公式，同理有 \\begin {align} M &= U\\Sigma V^T \\tag 4\\\\ M^T &= V\\Sigma^TU^T \\tag 5\\\\ MM^T &= U\\Sigma V^TV\\Sigma^TU^T \\tag 9\\\\ &= U\\Sigma^TI\\Sigma U^T \\\\ &= U\\Sigma^2U^T \\\\ M^TM U &= U\\Sigma^2U^TU \\tag {10}\\\\ &= U\\Sigma^2 \\end {align}所以梳理一下求解步骤 M \\rarr M^TM , MM^T \\rarr \\lambda_i \\rarr v_i, u_i \\\\ \\lambda_i \\rarr \\sigma_i \\rarr \\Sigma具体计算例子，在博客奇异值分解(SVD)原理与在降维中的应用里有，就不举例了。 3.3 SVD常见用途上面的计算推导公式中，最后得到了奇异值矩阵$\\Sigma$，这和矩阵分解中的特征值很像，代表了这个特征的重要性，且值从左上到右下是降序的，经常前top 10%的值就占了奇异值的99%，也就是说可以用这10%替代整个矩阵了。根据这一特性，SVD常见用途如下。 3.3.1 推荐算法以前的博客里面有提到过，选取奇异值前k个，具体应用如下图 3.3.2 PCA降维3.3.3 NLP-潜在语义索引（LSI）3.3.4 CV中用于图像压缩具体例子见知乎专栏奇异值的物理意义，还挺有意思的。并且这篇文章后面关于SVD的几何含义也写得很不错，值得一看。 3.4 SVD的几何含义3.5 SVD改进一下几种算法主要是SVD应用在推荐系统中的一些改进 3.5.1 SVD++3.5.2 BiasSVD3.5.3 FunkSVD4. LDA - Linear Discriminant Analysis 将带上标签的数据/点（有监督学习），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，一簇一簇的情况，相同类别的点，将会在投影后的空间中更接近。 5. References Relationship between SVD and PCA. How to use SVD to perform PCA? 矩阵分解（Matrix Decomposition） 矩阵论笔记（5）——矩阵分析和应用 拉格朗日乘法子 主成分分析（PCA）原理总结 奇异值分解(SVD)原理与在降维中的应用 特征值分解 和 SVD分解 奇异值的物理意义 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"PCA","slug":"PCA","permalink":"http://chenson.cc/tags/PCA/"},{"name":"LDA","slug":"LDA","permalink":"http://chenson.cc/tags/LDA/"},{"name":"SVD","slug":"SVD","permalink":"http://chenson.cc/tags/SVD/"}]},{"title":"机器学习与量化交易课程笔记","date":"2019-05-12T06:51:59.000Z","path":"2019/05/12/机器学习与量化交易课程笔记/","text":"评价：★☆☆☆☆大概就只能给一颗星。整个课程基本都是在讲机器学习，其实和量化已经没有什么关系了，只是套了一个量化交易的外壳而已。其中我觉得和量化 (当然指的是低频) 相关的几点都没有提到（目前听完第六课）： 量化交易中比较重要的时间序列数据及相关算法没有提及； 如何针对这类的时间序列特征做特征衍生也一点没有提到。从头到尾关于特征部分就提到了几个指标，反而在模型上大做文章。怎么的也得介绍一些特征衍生相关的包吧，比如ta-lib等； 如何回测数据或跟回测系统相关的内容。 以下是整个课程的笔记，非常的简单，单纯的记录一下而已。 第一课第二课第三课1. 数据获取 2. 时间序列分析时间序列：小数据量，基于均值，基于动量。 机器学习： 随机游走(与上一刻没有关系)，没法预测和统计套利。 找出平稳的随机过程的序列（在某个均值周围波动） 目标是：找出一个投资组合，这个组合是平稳的 Mean Reversion and Ornstein-Uhlenbeck process dx_t = \\theta(\\mu - x_t)d_t + \\sigma dW_t $\\theta$ ：回归到mean值的速率 $\\mu$ ： 均值 $\\sigma$ ：时间序列的方差 d ：布朗运动 时间序列价格波动 判断是否平稳 $x_1, x_2, …, x_t$ ADF - Test Hurst Exponent 作业 从单一股票到投资组合 Cadf.py y(t) = \\beta x(t) - \\epsilon(t) $\\beta$ $\\epsilon$ 随机噪声 统计套利 找出两只相关的股票 p-value t-statistics 泛化能力 - OOT &amp; 回测 其他 归一化的重要性 正则化：结构性风险，越复杂风险越高，奥卡姆剃刀 第四课好像这节课没啥好记的，基本都是一些机器学习相关的基础内容和介绍。 1. OLS - Ordinary Least Square2. Ridge常见线性回归，泛化能力较好，但没有特征选择的过程。 3. Lasso几何意义 4. Kernels对输入的数据做了非线性变化，拟合的时候还是使用的线性模型 核不一定是高斯函数，只是一种相似度的度量。 可通过k-means确定核的个数，RBF需要指定多少个核的。 机器学习技法课程学习笔记14— Radial Basis Function Network RBF - Radial basis function 径向基函数 Kernel 5. Cross-Validation第五课 - 策略建模综述1. Transform data into training set原始的金融数据到结构化的训练集 选择特征是quant对问题的一种理解，故因子的选择取决于对问题的理解。 e.g. 你认为沪深三百股指和什么事情有关？ 常见的Features Time lags CCI MA - Moving Average ROC Bollinger Bands … … More Feature Indicators 如何验证 计算R-square值 文本数据 TF_IDF Word2Vector 指标数 2. Building predictive models3. building event driving back - test pipelines基于事件的回测 第六课1. 特征选择2. 遗传算法3. 深入理解BP算法4. RNNReferences Youtube课程链接","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"Quant","slug":"Quant","permalink":"http://chenson.cc/tags/Quant/"}]},{"title":"地理解析与逆解析","date":"2019-05-12T01:07:36.000Z","path":"2019/05/12/地理解析与逆解析/","text":"1. 两个经纬度点之间的距离和方位角1.1 计算距离12345678910111213141516171819from math import radians, cos, sin, asin, sqrt, atan, tan, acos, pidef haversine(lngA, latA, lngB, latB): \"\"\" 计算给定A、B两点之间的距离 lng: 经度 -180~180 lat: 纬度 -90~90 返回单位为公里 \"\"\" # 地球平均半径，单位为公里 r = 6370.99681 # 将十进制度数转化为弧度 # 如在Hive或MySQL中无radians函数，可手动计算 radiasn(lat) == lat / 180 * pi lngA, latA, lngB, latB = map(radians, [lngA, latA, lngB, latB]) return r * acos(sin(latA) * sin(latB) + cos(latA) * cos(latB) * cos(lngA - lngB))haversine(110.0123, 23, 110.0123, 23.02) 1.2 计算方位角12345678910111213141516def cal_degree(lngA, latA, lngB, latB, d=360): \"\"\" 计算给定A、B两点之间的方位角 lng: 经度 -180~180 lat: 纬度 -90~90 返回单位为公里 \"\"\" lngA, latA, lngB, latB = map(radians, [lngA, latA, lngB, latB]) y = sin(lngB - lngA) * cos(latB) x = cos(latA) * sin(latB) - sin(latA) * cos(latB) * cos(lngB - lngA) # 返回的角度为0~360°，可根据需要改为180°或90°等 return(degrees(atan2(y, x)) + 360) % dcal_degree(110.0123, 23.32435, 110.1344, 25.5465)cal_degree(110.1344, 25.5465, 110.0123, 23.32435) 2. 百度地图API3. 高德地图API3.1 addr2lnglat123456789101112131415161718def addr2lnglat(addr): \"\"\" addr: 地址 \"\"\" import requests parameters = &#123;'address': addr, 'key': '*****'&#125; base = 'https://restapi.amap.com/v3/geocode/geo' res = requests.get(base, parameters, timeout=50) try: if res.status_code == 200: loc = res.json()['geocodes'][0]['location'] return loc else: return res.status_code except Exception as e: return repr(e) addr2lnglat('北京市朝阳区阜通东大街6号') 3.2 lnglat2addr12345678910111213141516171819def lnglat2addr(lng, lat): \"\"\" lng: 经度 -180~180 lat: 纬度 -90~90 \"\"\" from urllib.request import urlopen, quote parameters = &#123;'location': f'&#123;lng&#125;,&#123;lat&#125;', 'key': '*****', 'output' : 'json'&#125; base = 'https://restapi.amap.com/v3/geocode/regeo' res = requests.get(base, parameters, timeout=50) try: if res.status_code == 200: addr = res.json()['regeocode']['formatted_address'] return addr else: return res.status_code except Exception as e: return repr(e) lnglat2addr(101.3879394531, 31.2028176382) 4. References Python 地址编码工具箱geopy(能根据经纬度算距离，包含Vincenty) 高德地图官方文档 百度地图官方文档 Python调用高德地图API实现经纬度换算、地图可视化 python调用百度地图API实现经纬度换算、热力地图全流程指南 GPS查询","tags":[{"name":"LBS","slug":"LBS","permalink":"http://chenson.cc/tags/LBS/"}]},{"title":"常见优化算法小结","date":"2019-05-12T01:02:06.000Z","path":"2019/05/12/常见优化算法小结/","text":"1. Preview 在References里面的第一篇文章里，对于优化算法的分类总结的挺好的，分为公式解、数值优化和其他方法。这里重点总结的是数值优化法。 举个例子，在机器学习中对于有监督学习而言，通常都会有一个假设的Hypothesis函数，通过这样一个函数，我们输入一个数据x，会输出一个预测值$\\hat y$，且为了能够使得$\\hat y$和真实值$y$之间的差距非常小，即通过损失函数$L(w, x_i, y_i)$来计算。且需要考虑到整体的情况。会需要计算某一批量数据的预测值与真实值之间的误差，且需要不断最小化这个误差（降低训练误差）。即有了如下公式 \\min_w \\frac 1 N \\sum_{i=1}^N L(w, x_i, y_i) + \\lambda ||w||_2^2 上面这个公式的目的就是希望调整模型的参数权重$w$，使得这个批量的数据的损失函数达到最小值。 当然也可以从另外一个角度思考，两边取对数，将求最小值的问题，转换成求最大值的问题。即找到一个最优的概率密度函数$p(x)$，使得对训练样本的对数似然函数极大化（最大似然估计）。 \\max \\sum_{i=1}^l \\ln p(x_i; \\theta)这里 $\\theta$ 是需要求解的模型参数，是概率密度函数的参数。 所以机器学习中很重要的就是，基于给定的这些数据，如何找到最优的参数，使得整体的损失最小。 以下总结了机器学习中常见的优化算法。 2 梯度下降法2.1 Hypothesis h(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx2.2 评价函数 J(\\theta) = \\frac 1 2 \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2引用自吴恩达老师课件的内容，以最小平方误差为例子的评价函数为例，其中$\\theta$ 是模型的参数，$J(\\theta)$可以看成关于$\\theta$ 的多元目标函数。 对目标函数进行求偏导，可得到该函数的梯度，沿着该梯度最大方向，函数的值增长最快；反之沿着负梯度的方向，该函数的值下降最快（后面会证明）。所以对于这样的一个多元函数，其迭代公式为 \\theta_j := \\theta_j - \\alpha \\frac \\partial {\\partial\\theta} J(\\theta)2.3 迭代公式代入上面的公式有 \\begin {align} \\frac {\\partial} {\\partial \\theta_j} {J(\\theta)} &= \\frac {\\partial} {\\partial \\theta_j} \\frac 1 2(h_{\\theta}(x) - y)^2 \\\\ &= 2 \\cdot \\frac 1 2 (h_{\\theta}(x) - y) \\cdot \\frac {\\partial} {\\partial \\theta_j}(h_{\\theta}(x) - y) \\\\ &= (h_{\\theta}(x) - y) \\cdot \\frac {\\partial} {\\partial \\theta_j}(\\sum_{i=0}^n \\theta_i x_i - y) \\\\ &= (h_{\\theta}(x) - y)x_j \\\\ \\\\ \\theta_j &:= \\theta_j - \\alpha(h_{\\theta}(x^{(i)}) - y^{(i)}) x^{(i)} \\end {align}可以看到上面在求偏导的时候，是求的函数的一阶导数信息，所以是属于一阶优化算法。根据函数的一阶泰勒展开，在负梯度方向，函数值是会下降的（后面有尝试证明）。 泰勒展开式 [需要看王伟的PPT] 机器学习笔记1—泰勒展开式和牛顿法 2.4 推导过程理论推导先以一维的梯度下降来推导理解，然后再理解多维的。首先对于一个可导的一元函数，它的泰勒公式展开如下: f(x+\\Delta x) =f(x) + f'(x)\\Delta x + \\frac 1 2 f''(x)(\\Delta x)^2 + ... + \\frac 1 n f^{(n)}(x)(\\Delta x)^n根据已知的一元函数导数的结论： 如果在某一点处导数的值大于0，则函数在此处是增函数，加大$x$的值函数值会增加，减小$x$的值函数会减小； 如果在某一点处导数的值小于0，则函数在此处是减函数，增加$x$的值函数值会减小，减小$x$的值函数会增加； 前面里有个结论： 沿着该梯度最大方向，函数的值增长最快 反之沿着负梯度的方向，该函数的值下降最快 结合上面的两点理解： 当$f’(x) &gt; 0$时，想要减少函数的值的话，只能减少$x$的值； 当$f’(x)&lt;0$时，想要减少函数的值的话，只能增大x的值。 即当 $x := x - \\eta f’(x)$ 时函数的值下降最快。 公式推导对于泰勒展开公式太复杂了，我们可以先考虑一阶展开，即有 问题：泰勒公式中对于$\\epsilon$是否有取值方向的限定，如必须大于0 f(x+\\epsilon) \\approx f(x) + \\epsilon f'(x)此时可以令$\\epsilon = -\\eta f’(x) \\approx 0$，且$\\eta &gt; 0$，代入到上面的公式中有 f(x - \\eta f'(x)) \\approx f(x) - \\eta f'(x)^2此时当$f’(x) \\neq 0$时，有$\\eta f’(x)^2 &gt;0$（因为前面假设了$\\eta &gt; 0$），所有有如下公式： f(x - \\eta f'(x)) ≲ f(x)所以通过$x := x - \\eta f’(x)$来迭代$x$，函数$f(x)$的值可能会降低 问题1：假设$\\epsilon = -\\eta f’(x) \\approx 0$怎么来的，如果没有负号的情况呢？ 答1：推导出来的公式就是$f(x) ≲ f(x + \\eta f’(x))$，即沿着梯度方向，函数值是上升的。 问题2：如何理解沿着梯度的反方向，下降是最快的？ 答2：下面尝试着证明 多维梯度下降上面讨论的仅仅是一维情况，对于多维度$x$的情况（假设维度为$d$），目标函数$f(x)$的梯度就是一个维度为$d$的偏导数向量，具体如下 \\nabla_xf(x) = [\\frac {\\partial f(x)} {\\partial x_1}, \\frac {\\partial f(x)} {\\partial x_2},..., \\frac {\\partial f(x)} {\\partial x_d}]且目标函数$f(x)$在$x$上沿着$u$方向的方向导数为： \\begin{align} D_uf(x) &= \\lim_{h\\rightarrow 0} \\frac {f(x + hu) - f(x)} {h} \\\\ &= \\nabla f(x) \\cdot u \\\\ &= ||\\nabla f(x)|| \\cdot ||u|| \\cdot \\cos(\\theta) \\\\ &= ||\\nabla f(x)|| \\cdot \\cos(\\theta) \\end{align} 这里$\\nabla$是梯度算子, $u$是单位向量，且$||u|| = 1$ 方向导数$D_uf(x)$给出了$f$在$x$上沿着所有可能方向的变化率 因为$||\\nabla f(x)||$的值是固定的，只要当$\\theta = \\pi$时候，$cos(\\theta)=-1$，方向导数$D_uf(x)$此时值是最小的 x := x - \\eta \\nabla f(x)以下三种采用的基本思想都是GD，只是在更新时选用的数据有些许不同。 1.2 批量梯度下降 - Batch Gradient Descent 伪代码 1234# 重点在于whole_datasetfor i in range(n_epochs): params_grad = evaluate_gradient(loss_func, whole_dataset, params) params = params - learning_rate * params_grad 优缺点 1.3 随机梯度下降 - Stochastic Gradient Descent 伪代码 123456# 重点在于single_datafor i in range(n_epochs): np.random.shuffle(whole_dataset) for single_data in whole_dateset: params_grad = evaluate_gradient(loss_func, single_data, params) params = params - learning_rate * params_grad 优缺点 1.4 最小批梯度下降 - Mini-Batch Gradient Descent 伪代码 123456# 重点在于epoches + batch_size=50for i in range(n_epochs): np.random.shuffle(whole_dataset) for batch in get_batches(whole_dataset, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad 优缺点 2. 基于梯度下降法衍生的算法2.1 动量法2.2 AdaGrad算法2.3 RMSProp算法2.4 AdaDelta算法2.5 Adam算法3. 牛顿法牛顿法和梯度下降法一样，也是利用导数的思想，不断迭代来找到目标函数的极值点。不同的是，梯度下降法中，我们用到的是函数的一阶导数，而牛顿法用到了函数的二阶导数。牛顿法的核心思想是 在某点处用二次函数来近似目标函数，得到导数为0的方程，求解该方程，得到下一个迭代点。因为是用二次函数近似，因此可能会有误差，需要反复这样迭代，直到到达导数为0的点处。 3.1 牛顿法同上，这里我们先从一元函数开始推导理解，对一元的目标函数在$x_0$点出做泰勒展开，有 \\begin {align} f(x) &= f(x_0) + f'(x_0)(x-x_0) + \\frac 1 2 f''(x_0)(x-x_0)^2 + ... + \\frac 1 n f^{(n)}(x_0)(x-x_0)^n \\\\ &= f(x_0) + f'(x_0)(x-x_0) + \\frac 1 2 f''(x_0)(x-x_0)^2 \\end {align}因为这里只用到二阶导数，上面公式忽略了二阶以上的项。因为目的是为了找到导数为0的方程，对上面的式子对$x$求导并等于0，有 f'(x) = f'(x_0) + f''(x_0)(x-x_0) = 0简化后有 x = x_0 - \\frac {f'(x_0)} {f''(x_0)}这里$x$是$x_0$下一个点的位置，即迭代公式如下 x_{t+1} = x_t - \\frac {f'(x_t)} {f''(x_t)} 问题1：这里迭代公式看着有点像梯度下降法里的，只是多用了一个二阶导数？ 同理推广到多元函数上，有（忽略二阶以上的项） \\begin {align} f(x) &= f(x_0) + \\nabla f(x_0)^T(x - x_0) + \\frac 1 2 (x-x_0)^T\\nabla^2f(x_0)(x-x_0) \\end {align}同上，同时对上面式子两边对$x$求梯度，并令左右两边等于0，得到函数的导数（梯度向量）为 \\nabla f(x) = \\nabla f(x_0) + \\nabla^2f(x_0)(x-x_0) = 0这里$\\nabla^2f(x_0)$就是这章的重点黑塞矩阵)，通常用$H$来表示，黑塞矩阵具体表示如下： 简化上面的式子，有 \\begin {align} x_{t+1} &= x_t - \\frac {\\nabla f(x_t)} {\\nabla^2 f(x_t)} \\\\ &= x_t - \\frac {g(x_t)} {H(x_t)} \\\\ &= x_t - H^{-1}g \\end {align}沿着上述的迭代公式，函数会最终会到达函数的驻点处。其中$-H^{-1}g$称为牛顿方向。迭代终止的条件是梯度的模接近于0，或者函数值下降小于指定阈值。 直观理解以上是通过公式的推导，但是不够直观，可以从几何意义上理解一下 【莫名其妙这里写的一大段不见了，崩溃。下面就直接看Reference博客里的内容，下次复习的时候再补上吧:(】 3.2 拟牛顿法上面讲了牛顿法在每次迭代的时候，计算黑塞矩阵的逆，然后求解一个以该矩阵为系数矩阵的线性方程组，这需要大量的计算非常耗时，另外Hessian矩阵可能不可逆。 拟牛顿法的核心思想就是通过用一个$n$阶矩阵$G_k=G(x^{k})$来近似代替$H^{-1}(x^{k})$，即构造一个近似Hessian矩阵或其逆矩阵的正定对称矩阵。 在公式推导牛顿法的时候令$x_0$为点$x_k$，下一个点为$x_{k+1}$，有如下公式（考虑忽略二次项以上的值，取$\\approx$） \\begin {align} \\nabla f(x) - \\nabla f(x_0) \\approx& \\nabla^2f(x_0)(x -x_0) \\\\ g(x) - g(x_0) \\approx& H_{x}(x - x_0) \\\\ g_{k+1} - g_k \\approx& H_{k+1}(x_{k+1} - x_k) \\end {align}令 \\begin {align} s_k &= x_{k+1} - x_k \\\\ y_k &= g_{k+1} - g_k \\end {align}这里$s_k$保存的是输入的变化量，$y_k$保存的是梯度的变化量 有$y_k \\approx H_{k+1}s_k$ 或 $H^{-1}_{k+1}y_k \\approx s_k$，且该等式称之为拟牛顿条件，用来近似代替Hessian矩阵的矩阵需要满足此条件。根据这个条件，有不同的拟牛顿解法，具体如下： 以上算法看着都挺复杂的，还是以后有时间再另起篇幅理解吧。 4. 常见问题 局部最小值点 鞍点 泰勒展开公式 Hessian矩阵 关于有监督/无监督学习中，是如何应用如上的一些优化函数的？ 5. References 机器学习中的最优化算法总结 An overview of gradient descent optimization algorithms 一个框架看懂优化算法之异同 SGD/AdaGrad/Adam 一文看懂常用的梯度下降算法 监督学习之梯度下降——Andrew Ng机器学习笔记（一） 理解梯度下降法 理解牛顿法 牛顿法及其几何意义理解","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"优化算法","slug":"优化算法","permalink":"http://chenson.cc/tags/优化算法/"}]},{"title":"Metabase初探","date":"2019-04-21T07:22:36.000Z","path":"2019/04/21/Metabase初探/","text":"1. Introduction1.1 为什么选择Metabase Metabase是一款功能丰富、操作简单的BI工具，非常注重非技术人员（如产品、运营等）在使用这个工具时的体验，让他们能够自由探索数据，回答自己的问题，同时还是开源免费的，这点非常棒。而其他的BI工具要么是付费的（如Tableau），要么就只能看预先建好的Dashboard，对于不懂SQL或者数据库结构的他们来说，是很难自己去摸索的。同时对于技术人员来说，官网的文档写的挺棒的，比较全，然后设计也好看。 当然了，还存在其他更加高大上的BI报表工具，只是这个这个对于我目前的需求来说，是最不费时效果还比较OK的一款。 1.2 Metabase优点 支持以下数据库 Postgresql MySQL Druid SQL Server Redshift MongoDB Google BigQuery SQLite H2 Oracle Vertica Presto Snowflake 权限管理 Metabase有自己的权限管理系统，用户分为用户与用户组，权限分为数据源与集合。集合可以理解为个人或者公共的目录，管理员可以为用户与用户组分配不同的数据查询与集合权限。 便捷的数据探索方式 Metabase数据探索方式简单易用，可直接使用“托拉拽”的方式实现数据分析以及图表的需求，这个对于那些非技术人员（如产品，运营等）是非常友好的，不需要用户具备SQL及其他的专业知识。同时这个也比较适合组内内部的一些BI报表开发，可以一定程度避免与开发团队交涉，提高效率。 友好的Dashboard展示 1.3 Metabase缺点2. 安装略，这部分感觉看官网的文档比较好，写的很全。 目前我选择的安装方式是通过Jar文件安装（Mac有对应Metabase的Application），同时也支持Docker、Heroku等方式使用。我的安装环境是 OSX 10.14 java version “1.8.0_92” 3. 快速上手3.1 Custome3.2 Advance SQL4. References Metabase官方文档 Metabase使用教程 数据可视化的开源方案: Superset vs Redash vs Metabase (一) 数据可视化的开源方案: Superset vs Redash vs Metabase (二)","tags":[{"name":"Metabase","slug":"Metabase","permalink":"http://chenson.cc/tags/Metabase/"},{"name":"BI报表","slug":"BI报表","permalink":"http://chenson.cc/tags/BI报表/"}]},{"title":"遗传算法初探","date":"2019-04-09T11:37:00.000Z","path":"2019/04/09/遗传算法初探/","text":"1. Introduction 遗传算法 (genetic algorithm) 进化策略 (evolution strategy) 遗传规划 (genetic programming) / 进化规划 (evolution programming) GP常用于关于模型结构的自动化设计和优化的领域，如电路设计、机器结构设计和生物学中的药物分子设计等。 在数据相关领域，常用于符号回归、特征提取和选择等。 遗传算法并不是用来优化模型参数的 2. 初始化在遗传算法中，均需要把问题的可能答案编码成一个容易被进化算法操纵的形式，最常使用的二进制01编码。若采用生物学中的术语，则称之为表现型(phenotype)与基因型(genotype)之间的互相映射，如下图所示。 以下以背包算法作为一个简单的例子。 在机器学习中，以上三个概念对应的是： Gene - 单个特征 Chromosome - 特征子集 Population - 所有特征子集的集合 3. 适应度函数适应度函数就是为了计算Chromosome的对应的适应度分数，分数越高的即越有可能被留下来。 那么对应机器学习里面呢，就是这个特征子集对应训练出模型的分数，这个指标可以根据需求来定，AUC，KS，准确率等等。 以上面图片上为例（背包问题），计算前两条染色体的适应度分数（这里以survival points为准，当然也可以考虑背包的重量） A1【100110】有 A2【001110】有 4. 选择 - SELECT假设有m组染色体，且计算好了其对应的survival point，则这个survival point的总和就是分母，单个染色体的survival point就是分子。然后可以计算每条染色体survival point的占比。然后做选择的时候就类似转盘转一下，占比越大的就越有可能被选出来。这个也是make sense的。可以像第一个转盘一下选两次，也可以像第二个转盘一样，同时选两次。 5. 交叉 - CROSSOVER就是生物学中说的染色体交叉，有单点交叉和多点交叉。 单点交叉 多点交叉 简单的说就是根据这些特征，做一些固定的特征组合(根据特征的idx) 6. 变异 - MUTATE 变异的话就是染色体上发生随机的变化，也就是机器学习，深度学习中常所的引入一定的随机性。 7. 进化 - EVALUATE 完成变异之后，就会产生一个新的染色体，然后再用适应度函数去评估是否保留，或者替换已存在的一些染色体。 8. 终止条件跟机器学习算法中迭代的停止条件也比较相似，有 固定的迭代次数N，即进化的次数 X轮之内总体的评估指标没有发生变化，即early stopping 适应度函数已经达到了预先定义的值 … … 9. 编码实现9.1 Types - 选好解决问题的类型选择你要解决的问题类型,确定要求解的问题个数,最大值还是最小值 1234567from deap import base, creator# creating class# weights 1.0, 求最大值,-1.0 求最小值# (1.0,-1.0,)求第一个参数的最大值,求第二个参数的最小值creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))creator.create(\"Individual\", list, fitness=creator.FitnessMin) 9.2 Initialization - 注册个体、种群等函数初始化基因编码位数,初始值,等基本信息 12345678910111213141516from deap import basetoolsbox = base.Toolbox()# 种群数IND_SIZE = 300 # toolbox.register(\"attr_bool\", random.randint, 0, 1) toolbox.register(\"attribute\", random.random, 0, 1)# 调用randon.random为每一个基因编码编码创建 随机初始值 也就是范围[0, 1]toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attribute, n=IND_SIZE)# 是把之前创建的individual放入了population中toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) 9.3 Operator - 算子选择，交叉、变异、选择、进化等待操作,设计evaluate函数,在工具箱中注册参数信息:交叉,变异,保留个体,评价函数 12345678# 评估函数def evaluate(individual): return sum(individual),toolbox.register(\"mate\", tools.cxTwoPoint) # 两个基因之间交叉toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1) # 基因变异toolbox.register(\"select\", tools.selTournament, tournsize=3) # 选择保留的最佳个体toolbox.register(\"evaluate\", evaluate) # 进化 9.4 Algorithms - 将上面注册的函数等应用起来1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def main(): pop = toolbox.population(n=300) CXPB = 0.5 # the probability with which two individuals are crossed MUTPB = 0.2 # the probability for mutating an individual NGEN = 40 # the number of generations for which the evolution runs # Evaluate the entire population fitnesses = map(toolbox.evaluate, pop) for ind, fit in zip(pop, fitnesses): ind.fitness.values = fit # Begin the evolution for g in range(NGEN): print(\"-- Generation %i --\" % g) # Select the next generation individuals offspring = toolbox.select(pop, len(pop)) # Clone the selected individuals offspring = map(toolbox.clone, offspring) # Apply crossover and mutation on the offspring for child1, child2 in zip(offspring[::2], offspring[1::2]): # cross two individuals with probability CXPB if random.random() &lt; CXPB: toolbox.mate(child1, child2) # fitness values of the children # must be recalculated later del child1.fitness.values del child2.fitness.values for mutant in offspring: # mutate an individual with probability MUTPB if random.random() &lt; MUTPB: toolbox.mutate(mutant) del mutant.fitness.values # Evaluate the individuals with an invalid fitness invalid_ind = [ind for ind in offspring if not ind.fitness.valid] fitnesses = map(toolbox.evaluate, invalid_ind) for ind, fit in zip(invalid_ind, fitnesses): ind.fitness.values = fit # The population is entirely replaced by the offspring pop[:] = offspring # Gather all the fitnesses in one list and print the stats fits = [ind.fitness.values[0] for ind in pop] length = len(pop) mean = sum(fits) / length sum2 = sum(x*x for x in fits) std = abs(sum2 / length - mean**2)**0.5 print(\" Min %s\" % min(fits)) print(\" Max %s\" % max(fits)) print(\" Avg %s\" % mean) print(\" Std %s\" % std) print(\"-- End of (successful) evolution --\") best_ind = tools.selBest(pop, 1)[0] print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values)) return pop 9.5 代码封装整合将GA的代码封装成sklearn的接口，目前还需要修改的有以下几个地方 Fitness Function支持更多的指标，不仅仅只是f1_score 速度太慢了，需要提升一下性能。大概看了下耗时，以下几个地方可以优化一下 fitnesses = list(map(self.toolbox.evaluate, pop))，这边初始化耗时太久了 迭代进化的过程中，是否可以多核、多进程等并行优化 需要增加自定义终止条件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280import randomimport numpy as npfrom deap import base, creator, toolsclass FitenessFunction: def __init__(self, n_splits=5, *args, **kwargs): \"\"\" :param n_splits: int, Number of splits for cv :param *args: array-like, shape (n_samples,) :param **kwargs: Other estimator specific parameters :return : \"\"\" self.n_splits = n_splits self.estimator_ = None def calculate_fitness(self, estimator, X, y): \"\"\" :param estimator: :param X: :param y: \"\"\" self.estimator_ = clone(estimator) cv_set = np.repeat(-1., X.shape[0]) skf = StratifiedKFold(n_splits=self.n_splits) for train_index, test_index in skf.split(X, y): x_train, x_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] if x_train.shape[0] != y_train.shape[0]: raise Exception() self.estimator_.fit(x_train, y_train) y_pred = self.estimator_.predict(x_test) cv_set[test_index] = y_pred return f1_score(y, cv_set) class FeatureSelectionGA: \"\"\" FeaturesSelectionGA This class uses Genetic Algorithm to find out the best features for an input estimator using Distributed Evolutionary Algorithms in Python(DEAP) package. Default toolbox is used for GA but it can be changed accordingly. \"\"\" def __init__(self, estimator, X, y, cv_split=5, verbose=0): \"\"\" :param estimator: scikit-learn supported estimator, :param X: &#123;array-like&#125;, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. :param y: &#123;array-like&#125;, shape = [n_samples] :param cv_split: int, Number of splits for cross_validation to calculate fitness. :param verbose: 0 or 1 \"\"\" self.estimator_ = clone(estimator) self.n_features = X.shape[1] self.toolbox = None self.creator = self._create() self.cv_split = cv_split self.X = X self.y = y self.verbose = verbose if self.verbose == 1: print(\"estimator &#123;&#125; will select best features among &#123;&#125; features using cv_split :&#123;&#125;.\".format( estimator, X.shape[1], cv_split)) print(\"Shape od train_x: &#123;&#125; and target: &#123;&#125;\".format(X.shape, y.shape)) self.final_fitness = [] self.fitness_in_generation = &#123;&#125; self.best_ind = None def evaluate(self, individual): fit_func = FitenessFunction(self.cv_split) np_ind = np.asarray(individual) if np.sum(np_ind) == 0: fitness = 0.0 else: feature_idx = np.where(np_ind == 1)[0] # 计算适应度 fitness = fit_func.calculate_fitness(self.estimator_, self.X[:, feature_idx], self.y) if self.verbose == 1: print(\"Individual: &#123;&#125; Fitness_score: &#123;&#125; \".format(individual, fitness)) return fitness, def _create(self): creator.create(\"FeatureSelect\", base.Fitness, weights=(1.0,)) creator.create(\"Individual\", list, fitness=creator.FeatureSelect) return creator def create_toolbox(self): \"\"\" Custom creation of toolbox. :return Initialized toolbox \"\"\" self._init_toolbox() return toolbox def register_toolbox(self, toolbox): \"\"\" Register custom created toolbox. Evalute function will be registerd in this method. :param toolbox: Registered toolbox with crossover,mutate,select tools except evaluate \"\"\" toolbox.register(\"evaluate\", self.evaluate) self.toolbox = toolbox def _init_toolbox(self): toolbox = base.Toolbox() toolbox.register(\"attr_bool\", random.randint, 0, 1) # Structure initializers toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, self.n_features) toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) return toolbox def _default_toolbox(self): toolbox = self._init_toolbox() toolbox.register(\"mate\", tools.cxTwoPoint) toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1) toolbox.register(\"select\", tools.selTournament, tournsize=3) toolbox.register(\"evaluate\", self.evaluate) return toolbox def get_final_scores(self, pop, fits): self.final_fitness = list(zip(pop, fits)) def generate(self, n_pop, cxpb=0.5, mutxpb=0.2, ngen=5, set_toolbox=False): \"\"\" Generate evolved population :param: n_pop: int, population size :param: cxpb: float, crossover probablity :param: mutxpb: float, mutation probablity :param: n_gen: int, number of generations :param: set_toolbox: boolean, If True then you have to create custom toolbox before calling method. If False use default toolbox. :return: Fittest population \"\"\" import time t = time.time() if self.verbose == 1: print(\"Population: &#123;&#125;, crossover_probablity: &#123;&#125;, mutation_probablity: &#123;&#125;, total generations: &#123;&#125;\".format( n_pop, cxpb, mutxpb, ngen)) if not set_toolbox: self.toolbox = self._default_toolbox() else: raise Exception( \"Please create a toolbox.Use create_toolbox to create and register_toolbox to register. Else set set_toolbox = False to use defualt toolbox\") pop = self.toolbox.population(n_pop) CXPB, MUTPB, NGEN = cxpb, mutxpb, ngen print('&gt;&gt;&gt; time 1 =', time.time() - t) t = time.time() # Evaluate the entire population print(\"EVOLVING.......\") fitnesses = list(map(self.toolbox.evaluate, pop)) for ind, fit in zip(pop, fitnesses): ind.fitness.values = fit print('&gt;&gt;&gt; time 2 =', time.time() - t) t = time.time() # TODO: 迭代，需要优化次数 for g in range(NGEN): print(\"-- GENERATION &#123;&#125; --\".format(g + 1)) offspring = self.toolbox.select(pop, len(pop)) self.fitness_in_generation[str( g + 1)] = max([ind.fitness.values[0] for ind in pop]) # Clone the selected individuals offspring = list(map(self.toolbox.clone, offspring)) # Apply crossover and mutation on the offspring for child1, child2 in zip(offspring[::2], offspring[1::2]): if random.random() &lt; CXPB: self.toolbox.mate(child1, child2) del child1.fitness.values del child2.fitness.values for mutant in offspring: if random.random() &lt; MUTPB: self.toolbox.mutate(mutant) del mutant.fitness.values # Evaluate the individuals with an invalid fitness weak_ind = [ind for ind in offspring if not ind.fitness.valid] fitnesses = list(map(self.toolbox.evaluate, weak_ind)) for ind, fit in zip(weak_ind, fitnesses): ind.fitness.values = fit print(\"Evaluated %i individuals\" % len(weak_ind)) # The population is entirely replaced by the offspring pop[:] = offspring print('&gt;&gt;&gt; time g =', time.time() - t) t = time.time() # Gather all the fitnesses in one list and print the stats fits = [ind.fitness.values[0] for ind in pop] length = len(pop) mean = sum(fits) / length sum2 = sum(x * x for x in fits) std = abs(sum2 / length - mean**2)**0.5 if self.verbose == 1: print(\" Min %s\" % min(fits)) print(\" Max %s\" % max(fits)) print(\" Avg %s\" % mean) print(\" Std %s\" % std) print(\"-- Only the fittest survives --\") self.best_ind = tools.selBest(pop, 1)[0] print(\"Best individual is %s, %s\" % (self.best_ind, self.best_ind.fitness.values)) self.get_final_scores(pop, fits) return pop from sklearn.base import BaseEstimator, MetaEstimatorMixin, clonefrom sklearn.utils import check_array, safe_maskfrom sklearn.feature_selection.base import SelectorMixinclass SelectFromGA(BaseEstimator, SelectorMixin, MetaEstimatorMixin): def __init__(self, estimator, n_pop=50, cv_split=5, cxpb=0.5, mutxpb=0.2, ngen=5, set_toolbox=False, verbose=0): self.estimator = estimator # self.threshold = threshold # # self.prefit = prefit # select model can be used directly if prefit == True # self.norm_order = norm_order # used in getting importance self.n_pop = n_pop self.cv_split = cv_split self.cxpb = cxpb self.mutxpb = mutxpb self.ngen = ngen self.set_toolbox = set_toolbox self.verbose = verbose def fit(self, X, y=None, **fit_params): \"\"\" :param X : array-like of shape (n_samples, n_features) :param y : array-like, shape (n_samples,) :param **fit_params : Other estimator specific parameters :return : \"\"\" self.estimator_ = clone(self.estimator) self.fsga = FeatureSelectionGA(self.estimator_, np.array(X), np.array(y), self.cv_split, self.verbose) self.population = self.fsga.generate(self.n_pop, self.cxpb, self.mutxpb, self.ngen, self.set_toolbox) self.best_ind = self.fsga.best_ind def _get_support_mask(self): if hasattr(self, 'fsga'): fsga = self.fsga else: raise ValueError('Please fit SelectFromGA before transform.') return np.array(fsga.best_ind) &gt; 0 def transform(self, X): \"\"\" Reduce X to the selected features. :param X: array of shape [n_samples, n_features] The input samples. :return X_r: array of shape [n_samples, n_selected_features] The input samples with only the selected features. \"\"\" # 考虑移除，需添加 force_all_finite=False 后可支持缺失值 X = check_array(X, accept_sparse='csr', force_all_finite=False) mask = self.get_support() if not mask.any(): warn(\"No features were selected: either the data is\" \" too noisy or the selection test too strict.\", UserWarning) return np.empty(0).reshape((X.shape[0], 0)) if len(mask) != X.shape[1]: raise ValueError(\"X has a different shape than during fitting.\") return X[:, safe_mask(X, mask)] 10. References 一文读懂遗传算法工作原理 使用Python和遗传规划(Genetic Programming)玩转Flappy Bird python遗传算法（GA）DEAP-Overview学习摘要 Deap官方文档 Deap: python中的遗传算法工具箱 Python遗传算法工具箱的使用（一）求解带约束的单目标优化","tags":[{"name":"Genetic Algorithm","slug":"Genetic-Algorithm","permalink":"http://chenson.cc/tags/Genetic-Algorithm/"}]},{"title":"推荐系统小结","date":"2019-03-25T02:35:17.000Z","path":"2019/03/25/推荐系统小结/","text":"1. Factorization Machine (FM)1.1 Introduction传统用的One-Hot编码会带来数据稀疏的问题。 比如数据如下（栗子来自美团技术博客）： 做了One-Hot编码后 当里面某些特征的维度比较高的时候，数据维度可能达到百万级别。 而传统的LR模型也并没考虑到特征之间的关联 y = w_0 + \\sum_{i=1}^n w_i x_i 如需要表述特征之间的相关性，可采用多项式模型 ( Poly2，只考虑两个特征之间的相互关系) y = w_0 + \\sum_{i=1}^nw_ix_i + \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n w_{ij}x_ix_j相比之前的单一特征，考虑特征之间的两两关系，此时多出了 $n * (n-1) / 2$ 个二阶特征。 这样会带了另外一些问题 由于做了One-Hot编码，数据非常系数，$x_i * x_j$ 很多都是为0的，这样就会导致对应的权重 $w_{ij}$ 训练不出来，或者学出来的权重不靠谱 训练和存储的开销会增大很多 对于二阶特征的权重 $\\hat W$，此时引入了辅助向量 $V$，有 \\begin {align} \\hat W &= VV^T \\\\ V &= \\left [ \\begin{matrix} \\boldsymbol{v}_1 \\\\ \\boldsymbol{v}_2 \\\\ ... \\\\ \\pmb{v}_n \\end{matrix} \\right ] = \\left [ \\begin{matrix} v_{1,1} & v_{1,2} & ... & v_{1,k}\\\\ v_{2,1} & v_{2,2} & ... & v_{2,k} \\\\ ... & ... & ... & ... \\\\ v_{n,1} & v_{n,2} & ... & v_{n,k} \\end{matrix} \\right ] _{n\\times k} \\end {align}此时就可以用分解后产生的$V$来表达$\\hat W$，将参数由$n^2$降低到 $kn$个。其中$V$矩阵的每一行$v_i$就是第$i$维特征的隐向量，一个隐向量包含了$k$个描述第$i$维特征的因子，所以叫因子分解。 问题 和SVD奇异矩阵分解的区别？ 和MF矩阵分解的区别？ 1.2 公式推导（二阶表达式） 矩阵分解 It’s well known that for any ==positive definite matrix $W$==, there exists a matrix $V$ such that $W$ = $V$·$V^T$ provided that $k$ is sufficiently large. This shows that a $FM$ can express any interaction matrix $W$ if $k$ is chosen large enough. Nevertheless in sparse settings, typically a small $k$ should be chosen because there is not enough data to estimate complex interactions $W$. Restricting $k$ - and thus the expressiveness of the $FM$ - leads to better generalization and thus improved interaction matrices under sparsity. s 即引用某博客里面总结的： 当$k$值足够大时，对于任意正定对称的实矩阵$\\hat W \\in \\mathbb R^{n \\times n}$，均存在实矩阵 $V \\in \\mathbb R^{n\\times n}$，使得成立$\\hat W = VV^T$ 这里有几个条件： 对称矩阵：$A = A^T$ 对称矩阵（symmetric matrix）是一个方形矩阵，其转置矩阵和自身相等。 正定【不理解，待深入了解】 定义 广义：设$M$为$N$阶方阵对任意非零向量$x$有$ xMx^T&gt;0$则称$M$是正定矩阵 狭义：对于$n$阶实对称矩阵$M$，当且仅当对于所有非零实系数向量$z$，都有$z^TMz&gt;0$ $k$足够大 但通常在高度稀疏矩阵的场景下，$k$值不会取得很大的，因为没有足够的样本去估计复杂的交互矩阵。且$k$值教小，可以一定程度上提高FM在稀疏场景下的泛化性。 求解向量V【需要深入了解】 基本公式 ((a + b + c)^2 - a^2 - b^2 - c^2) 推导过程（需要手动实现一遍） \\begin {align} \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n w_{ij}x_ix_j &= \\sum_{i=1}^{n-1}\\sum_{j=i+1}^nx_i x_j \\tag 1 \\\\ &= \\frac 1 2 \\sum_{i=1}^{n}\\sum_{j=1}^nx_i x_j - \\frac 1 2 \\sum_{i=1}^nx_i x_i \\tag 2\\\\ &= \\frac 1 2 (\\sum_{i=1}^{n}\\sum_{j=1}^n\\sum_{f=1}^k v_{i,f} v_{j,f}x_i x_j - \\sum_{i=1}^{n}\\sum_{f=1}^k v_{i,f} v_{i,f}x_i x_j) \\tag 3\\\\ &= \\frac 1 2 \\sum_{f=1}^k((\\sum_{i=1}^nv_{i,f}x_i)(\\sum_{j=1}^nv_{j,f}x_j) - \\sum_{i=1}^nv_{i,f}^2x_i^2) \\tag 4\\\\ &= \\frac 1 2 \\sum_{f=1}^k((\\sum_{i=1}^n v_{i,f}x_i)^2 - \\sum_{i=1}^nv_{i,f}^2 x_i^2) \\tag 5 \\end {align}公式（1）到（2）如下图 公式（2）到（3）里面的$k$的是由$(v_i, v_j)$来的，因为之前已经有 \\begin {align} V &= \\left [ \\begin{matrix} \\boldsymbol{v}_1 \\\\ \\boldsymbol{v}_2 \\\\ ... \\\\ \\pmb{v}_n \\end{matrix} \\right ] = \\left [ \\begin{matrix} v_{1,1} & v_{1,2} & ... & v_{1,k}\\\\ v_{2,1} & v_{2,2} & ... & v_{2,k} \\\\ ... & ... & ... & ... \\\\ v_{n,1} & v_{n,2} & ... & v_{n,k} \\end{matrix} \\right ] _{n\\times k} \\\\ \\hat W &= VV^T \\\\ w_{i,i} &= \\\\ w_{i, j} &= \\\\ &= (v_{i,1}v_{j,1} + v_{i,2}v_{j,2} + ... + v_{i,k}v_{j,k}) \\\\ &= \\sum_{f=1}^kv_{i,f}v_{j,f} \\end {align} 公式（3）到（5），是将公共项$k$相关的提取出来，然后化简里面的项，最终就得到一个和$n$相关的求和项，此时复杂度是$\\mathcal O(kn)$，若未化简，在公式（1）的时候复杂度是$\\mathcal O (kn^2)$ 以下是图解上述公式 问题：为什么FM能够解决参数训练的问题 经过因子化后，组合特征$x_ix_j$和$x_jx_k$的权重系数分别为$&lt;\\boldsymbol{v_i},\\boldsymbol{v_j}&gt;$和$&lt;\\boldsymbol{v_j},\\boldsymbol{v_k}&gt;$，观察里面这俩个权重系数，里面有个相同的量$\\boldsymbol{v_j}$，因此所有包含$x_j$特征的非零组合特征的样本都能拿来训练，学习隐向量$\\boldsymbol v_j$。 本质上是在对特征进行embedding化表征，和目前非常常见的各种实体embedding本质思想是一脉相承的。因为FM是学习单个特征的embedding，并不依赖某个特定的特征组合是否出现过，所以只要特征$x_i$和其它任意特征组合出现过，那么就可以学习自己对应的embedding向量。于是，尽管$(x_i, x_j)$这个特征组合没有看到过，但是在预测的时候，如果看到这个新的特征组合，因为$x_i$和$x_j$都能学会自己对应的embedding，所以可以通过内积算出这个新特征组合的权重。这是为何说FM模型泛化能力强的根本原因。其实本质上，这也是目前很多花样的embedding的最核心特点，就是从0/1这种二值硬核匹配，切换为向量软匹配，使得原先匹配不上的，现在能在一定程度上算密切程度了，具备很好的泛化性能。 1.3 模型目标函数和优化算法1.3.1 目标函数 \\hat y(X) = w_0 + \\sum_{i=1}^nw_ix_i + \\sum_{i=1}^{n-1}\\sum_{j=i+1}^nx_ix_j根据上面的公式，FM需要学习的参数有如下（根据参数类型，可以分成三组不同的参数，即分组策略，为了是以后优化的时候方便考虑正则项，即模型的结构化风险） \\begin {align} \\Theta &= (w_0, w_1, w_2, ..., w_n, v_{1,1}, v_{1,2}, ..., v_{1,k}, ..., v_{n,1})^T \\\\ &= (w_0, \\boldsymbol w, \\boldsymbol V) \\end {align}此时，$\\hat y$公式也可以根据参数的不同，有三种形式（偏置，一次项，二次项三种情况）： $\\theta = w_0$ \\hat y(X) = \\sum_{i=1}^nw_ix_i + \\sum_{i=1}^{n-1}\\sum_{j=i+1}^nx_ix_j + w_0*1 $\\theta = w_l(l=1,…,n)$ \\hat y(X) = w_0+ \\sum_{i=1,i\\ne l}^nw_ix_i + \\sum_{i=1}^{n-1}\\sum_{j=i+1}^nx_ix_j + w_l*x_l $\\theta = {v_{lm}}$ \\hat y(X) = w_0 + \\sum_{i=1}^nw_ix_i + \\sum_{i=1}^{n-1}\\sum_{j=i+1}^nx_ix_j 当$\\hat y$对$\\theta$求导的时，有： 即上面分情况讨论$\\theta$取值时候蓝色的部分。 所以FM的优化问题是 \\Theta^* = \\arg \\min_{\\Theta} {}\\sum_{i=1}^N(loss(\\hat y^{(i)}, y^{(i)}) + \\sum_{\\theta \\in \\Theta} \\lambda_{\\theta}\\theta^2)1.3.2 优化算法 SGD(Stochastic Gradient Descent) 以下内容取自博客，这篇博客整理很系统。 ALS(Alternating Least-Squares) - 交替最小二乘法，即最小二乘法 MCMC(Markov Chain Monte Carlo) - 马尔科夫链蒙特卡洛算法 1.4 和其他模型对比 二阶多项式的SVM MF(Matrix Factorization)模型 MF方法可以看作是FM模型的一种特例，比如在MF版的推荐系统中，MF可以看作特征只有userId和itemId的FM模型。MF的核心思想是通过两个低维小矩阵（一个代表用户embedding矩阵，一个代表物品embedding矩阵）的乘积计算，来模拟真实用户点击或评分产生的大的协同信息稀疏矩阵，本质上是编码了用户和物品协同信息的降维模型。但是MF只能使用两个实体的二阶特征，而FM的优势是能够将更多的特征融入到这个框架中，并且可以同时使用一阶和二阶特征或者更高阶特征。 SVD(Singular Value Decomposition) 模型 1.5 代码实现 手撸一遍 调用xLearn包 2. Field-aware Factorization Machine (FFM)2.1 Introduction之前有提到在FM中，$\\boldsymbol V$矩阵的每一列$\\boldsymbol v_i$就是第i维特征的隐向量，一个隐向量包含了k个描述第i维特征的因子，此时$\\boldsymbol v_i$是个一维的隐向量。 FFM是在FM的基础上，进一步提出了Field的概念，即域/类别的概念。继续以FM中的例子理解 对于这三个特征Country、Day和Ad_type Country衍生出的One-Hot特征单独属于一个域 Day衍生出的One-Hot特征单独属于一个域 Ad_type衍生出的One-Hot特征也单独属于一个域 这个很好理解，因为由一个特征衍生出来的One-Hot特征其实都是属于一类的，比如都是国家类型、日期和广告类型等。 同时对于做完One-Hot的特征$x_i$，对每一个Field都会学习出一个隐向量$v_{i, f_j}$，不同的特征在跟同一个Field进行关联时，使用的是不同的隐向量。而在FM中，只有一个$v_i$的隐向量。这个是他们最大的不同。 \\begin {align} \\hat W &= VV^T = \\\\ \\hat y &= w_0 + \\sum_{i=1}^nw_ix_i + \\sum_{i=1}^{n-1}\\sum_{j=i+1}^nx_ix_j \\end {align}这里==$f_j$ 是第$j$个特征所属的field==。如果隐向量的长度为$k$，那么FFM的二次参数有$nfk$个，远多于FM模型的$ nk$个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 $\\mathcal O(kn^2)$。 继续之前的栗子 One-Hot Label $x_1$ $x_2$ $x_3$ $x_4$ $x_5$ $x_6$ $x_7$ Clicked C=USA C=China D=26/11/15 D=1/7/14 D=19/2/15 A=Movie A=Game 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 Field-Table Field Name Field Index Feature Name Feature Index Country 1 USA 1 China 2 Day 2 26/11/15 3 1/7/14 4 19/2/15 5 Ad_type 3 Movie 6 Game 7 二阶特征交叉 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n w_{ij}x_ix_j FFM表示 由于二阶交叉有点多，每个都写好累，就挑其中四个看吧。 w_{1, 3} ⋅ x_1 x_3 = ⋅ x_1 x_3 \\\\ w_{1, 6} ⋅ x_1 x_6 = ⋅ x_1 x_6 \\\\ w_{1, 7} ⋅ x_1 x_7 = ⋅ x_1 x_7 \\\\ w_{3, 7} ⋅ x_3 x_7 = ⋅ x_3 x_7可以看出来，一个特征$x_i$ 对于同一个Field的特征，使用的隐向量是一样的； 对于不同的Field的特征，使用的隐向量是不同的。 2.2公式推导2.3 模型目标函数和优化算法2.4 其他 特征处理细节 样本归一化：FFM默认是进行==样本数据的归一化==，即 为真；若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。 特征归一化：CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 是非常必要的。 省略零值特征：从FFM模型的表达式可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。 2.5 代码实现 手撸一遍- libffm-python 调用libffm 数据格式 $label$ $field_idx_1$：$feature_idx_1$：$value$ $field_idx_2$：$feture_idx_2$：$value$ 所以上面的栗子可以表示为 1 1:1:1 2:3:1 3:6:1 0 1:2:1 2:4:1 3:7:1 1 1:2:1 2:5:1 3:7:1 可能这里会困惑为啥最后都是1还要保留着？能否全部省略了？原因是可能特征存在数值型号的特征，比如人口特征中的年龄等，最后的value值此时为数值型，非0/1变量。(所有的数值特征共享同一个feature_idx) DEMO 调用xLearn 12345678910111213141516171819202122232425import xlearn as xl# Training taskffm_model = xl.create_ffm() # Use field-aware factorization machine (ffm)ffm_model.setTrain(\"./small_train.txt\") # Set the path of training datasetffm_model.setValidate(\"./small_test.txt\") # Set the path of validation dataset# Parameters:# 0. task: binary classification# 1. learning rate: 0.2# 2. regular lambda: 0.002# 3. evaluation metric: accuracyparam = &#123;'task':'binary', 'lr':0.2, 'lambda':0.002, 'metric':'acc'&#125;# Start to train# The trained model will be stored in model.outffm_model.fit(param, './model.out')# Prediction taskffm_model.setTest(\"./small_test.txt\") # Set the path of test datasetffm_model.setSigmoid() # Convert output to 0-1# Start to predict# The output result will be stored in output.txtffm_model.predict(\"./model.out\", \"./output.txt\") 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970(base) [root@izwz9hxp0cmozj9nwrpbbqz criteo_ctr]# python3 run_demo_ctr.py---------------------------------------------------------------------------------------------- _ | | __ _| | ___ __ _ _ __ _ __ \\ \\/ / | / _ \\/ _` | '__| '_ \\ &gt; &lt;| |___| __/ (_| | | | | | | /_/\\_\\_____/\\___|\\__,_|_| |_| |_| xLearn -- 0.40 Version ------------------------------------------------------------------------------------------------[------------] xLearn uses 32 threads for training task.[ ACTION ] Read Problem ...[------------] First check if the text file has been already converted to binary format.[------------] Binary file (./small_train.txt.bin) found. Skip converting text to binary.[------------] First check if the text file has been already converted to binary format.[------------] Binary file (./small_test.txt.bin) found. Skip converting text to binary.[------------] Number of Feature: 9991[------------] Number of Field: 18[------------] Time cost for reading problem: 0.00 (sec)[ ACTION ] Initialize model ...[------------] Model size: 5.56 MB[------------] Time cost for model initial: 0.00 (sec)[ ACTION ] Start to train ...[------------] Epoch Train log_loss Test log_loss Test Accuarcy Time cost (sec)[ 10% ] 1 0.603320 0.547672 0.770000 0.00[ 20% ] 2 0.542222 0.531491 0.770000 0.00[ 30% ] 3 0.524161 0.530445 0.770000 0.00[ 40% ] 4 0.506338 0.539100 0.770000 0.00[ 50% ] 5 0.491458 0.527948 0.770000 0.00[ 60% ] 6 0.486182 0.530082 0.775000 0.00[ 70% ] 7 0.474895 0.537433 0.775000 0.00[ 80% ] 8 0.465618 0.530379 0.775000 0.00[ 90% ] 9 0.458462 0.532626 0.775000 0.00[ 100% ] 10 0.451552 0.530528 0.775000 0.00[ ACTION ] Start to save model ...[------------] Model file: ./model.out[------------] Time cost for saving model: 0.00 (sec)[ ACTION ] Finish training[ ACTION ] Clear the xLearn environment ...[------------] Total time cost: 0.03 (sec)---------------------------------------------------------------------------------------------- _ | | __ _| | ___ __ _ _ __ _ __ \\ \\/ / | / _ \\/ _` | '__| '_ \\ &gt; &lt;| |___| __/ (_| | | | | | | /_/\\_\\_____/\\___|\\__,_|_| |_| |_| xLearn -- 0.40 Version ------------------------------------------------------------------------------------------------[------------] xLearn uses 32 threads for prediction task.[ ACTION ] Load model ...[------------] Load model from ./model.out[------------] Loss function: cross-entropy[------------] Score function: ffm[------------] Number of Feature: 9991[------------] Number of K: 4[------------] Number of field: 18[------------] Time cost for loading model: 0.00 (sec)[ ACTION ] Read Problem ...[------------] First check if the text file has been already converted to binary format.[------------] Binary file (./small_test.txt.bin) found. Skip converting text to binary.[------------] Time cost for reading problem: 0.00 (sec)[ ACTION ] Start to predict ...[------------] The test loss is: 0.530528[ ACTION ] Clear the xLearn environment ...[------------] Total time cost: 0.01 (sec) 3. DeepFM 出现背景 FM只计算了二阶特征组合，如需要考虑高阶特征的组合，可能采用DNN模型。但由于特征做了One-Hot，输入一般是非常高的维度，导致输入的参数太多了。 考虑到用FFM中Field的概念，可以将同一个特征One-Hot出来的特征转换成一个Dense Vector，然后在再基于Field的Dense数据，再输入到DNN中学习不同特征之间的高阶组合。(那为啥不叫DeepFFM？) 模型结构 输入的Embedding层 Embedding层的输出为 a^{(0)}=[e_1, e_2, ..., e_m]其中$e_i$是嵌入的第i个filed，m是field的个数，前向过程将嵌入层的输出，输入到隐藏层为 a^{(l+1)} = \\sigma (W^{(l)}a^{(l)} + b^{(l)}) FM部分 DNN部分 预测输出 \\hat y = sigmoid(y_{FM} + y_{DNN}) 4. Wide &amp; Deep Network (WDN)之前博客有总结了一下，可参考Wide-and-Deep模型初探 5. Deep &amp; Cross Network (DCN)DCN进阶版之xDeepFM 推荐系统遇上深度学习(二十二)—DeepFM升级版XDeepFM模型强势来袭 6. References 推荐系统遇上深度学习(一)—FM模型理论和实践 Factorization Machines 学习笔记（一）预测任务 分解机(Factorization Machines)推荐算法原理 Factorization Machines 一文读懂FM算法优势 FM系列算法解读（FM+FFM+DeepFM） 因子分解机（libffm+xlearn） 美团技术博客 - 深入FFM原理与实践 FFM模型在点击率预估中的应用实践 云脑-电商推荐系统(特征工程部分) libffm-python libffm-C++ DeepFM: A Factorization-Machine based Neural Network for CTR Prediction 推荐系统召回四模型之：全能的FM模型","tags":[{"name":"Wide&Deep","slug":"Wide-Deep","permalink":"http://chenson.cc/tags/Wide-Deep/"},{"name":"推荐系统","slug":"推荐系统","permalink":"http://chenson.cc/tags/推荐系统/"},{"name":"FM","slug":"FM","permalink":"http://chenson.cc/tags/FM/"},{"name":"FFM","slug":"FFM","permalink":"http://chenson.cc/tags/FFM/"},{"name":"DeepFM","slug":"DeepFM","permalink":"http://chenson.cc/tags/DeepFM/"},{"name":"DCN","slug":"DCN","permalink":"http://chenson.cc/tags/DCN/"}]},{"title":"AutoML平台试用体验","date":"2019-03-20T10:44:55.000Z","path":"2019/03/20/AutoML平台试用体验/","text":"1. 蚂蚁金服 Morse2. 蚂蚁金服 Pi3. 百度天链 4. 第四范式","tags":[{"name":"Auto-ML","slug":"Auto-ML","permalink":"http://chenson.cc/tags/Auto-ML/"}]},{"title":"Mac OSX安装Fiddler","date":"2019-03-07T09:59:54.000Z","path":"2019/03/07/Mac-OSX安装Fiddler/","text":"1. 安装Mono首先，Mac下需要使用.Net编译后的程序，需要用到跨平台的方案Mono(现阶段微软已推出跨平台的方案.Net Core，不过暂时只支持控制台程序)。以上是网上的描述，给我感觉有点类似一个虚拟机或者docker一下的东西，这样就能在OSX的环境下，运行exe文件了。 下载Mono，并安装 安装完后，执行指令 12345# version == 5.16.0/Library/Frameworks/Mono.framework/Versions/5.16.0/bin/mozroots --import --sync/Library/Frameworks/Mono.framework/Versions/5.16.0/bin/cert-sync --import --sync/Library/Frameworks/Mono.framework/Versions/5.18.0/bin/cert-sync --import --sync/Library/Frameworks/Mono.framework/Versions/5.18.0/bin/mozroots --import --sync 此步是为了从Mozilla LXR上下载所有受信任的root证书，存于Mono的证书库里。root证书能用于请求https地址。 报错 修改环境变量，并激活修改后的文件 sudo vi ~/.bash_profile 或者/etc/profile 12export MONO_HOME=/Library/Frameworks/Mono.framework/Versions/5.16.0export PATH=$PATH:$MONO_HOME/bin 2. 安装Fiddler 下载Fiddler，并安装（解压到非中文字符的路径下） 安装完后，进入到刚刚解压的Fiddler的路径，并命令执行 1sudo mono Fiddler.exe 以上都是网上的教程，在运行最后一步的时候，失败了。试了各种方法都没成功，不确定是我操作的问题还是这方法比较久了。所以我的解决方案是，改用Charles : ) 3. References Mac OS 安装Fiddler mac下Fiddler的安装-启动 Mono 官网 Fiddler 官网 macOS10.14 安装Fiddler","tags":[{"name":"Mac","slug":"Mac","permalink":"http://chenson.cc/tags/Mac/"},{"name":"Fiddler","slug":"Fiddler","permalink":"http://chenson.cc/tags/Fiddler/"}]},{"title":"Centos7安装Nvida驱动和显卡驱动","date":"2019-03-07T09:54:49.000Z","path":"2019/03/07/Centos7安装Nvida驱动和显卡驱动/","text":"1. 安装1.1 安装方式有三种 CUDA（.run）下载以后安装带上显卡驱动， 下载如下 1234567891011121314# 查看显卡版本lspci | grep -i nvidia# 下载# https://www.nvidia.cn/Download/index.aspx?lang=cn 可在这个网站看出对应的版本wget http://cn.download.nvidia.com/tesla/410.79/NVIDIA-Linux-x86_64-410.79.run# 安装sh NVIDIA-Linux-x86_64-410.79.run# 卸载sh NVIDIA-Linux-x86_64-410.79.run --uninstallnvidia-uninstall 显卡驱动.run文件安装 集成软件包安装（yum等） 12345# 确认是否为yum安装yum list installed | grep nvidia# 卸载yum remove nvidia-detect.x86_64 1.2 安装步骤 添加EIRepo源 1234rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 是否指定版本？rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 安装显卡驱动检查 1yum install nvidia-detect 运行显卡检测驱动 1234nvidia-detect -v# 运行结果kmod-nvidia 系统准备工作 12345678910111213# 注意这是升级系统相关的包，耗时较长yum -y updateyum -y groupinstall \"GNOME Desktop\" \"Development Tools\"# 安装基础包yum install kernel-devel kernel-doc kernel-headers gcc* glibc* glibc-*# 安装刚才检测到的驱动yum install kmod-nvidia yum -y install epel-releaseyum -y install dkms 编辑grub文件 123456vim /etc/default/grub# 在“GRUB_CMDLINE_LINUX”中添加# rd.driver.blacklist=nouveau nouveau.modeset=0# 随后生成配置grub2-mkconfig -o /boot/grub2/grub.cfg 创建blacklist 12345# /lib/modprobe.d/dist-blacklist.confvim /etc/modprobe.d/blacklist.conf# 添加, 屏蔽默认带有的nouveaublacklist nouveau 更新配置 (重建initramfs image) 12mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.imgdracut /boot/initramfs-$(uname -r).img $(uname -r) 重启 1reboot 确认禁用了nouveau 12# 若无输出则禁用成功lsmod | grep nouveau 开始安装驱动 1sh NVIDIA-Linux-x86_64-410.79.run 查看显卡使用情况 12# nvidia-smi是用来查看GPU使用情况的nvidia-smi 1.3 常用指令 查看安装的nvidia模块命令 12# rpm -qarpm -qa|grep -i nvid|sort 查看系统版本 123456uname -alsb_release -a# 以下方法适用于RedHat,CentOScat /etc/redhat-release 2. 安装CUDA 检测GPU是否OK 1lspci | grep -i nvidia 下载cuda 12# https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=CentOS&amp;target_version=7&amp;target_type=runfilelocalwget https://developer.nvidia.com/compute/cuda/10.0/Prod/local_installers/cuda_10.0.130_410.48_linux 安装gcc、g++编译器 12yum install gccyum install g++ 安装kernel-devel和kernel-headers 12yum install kernel-develyum install kernel-headers 赋予权限，和安装 12chmod 755 cuda_9.0.176_384.81_linux.run./cuda_9.0.176_384.81_linux.run 安装过程 1234567891011121314install NVIDIA Accelerated Graphics Driver ... -no # 不需要再安装显卡驱动Install OpenGL ... -no # 这个组件一定不要安装，如果出现选择，选 NoInstall CUDA 9.0 Toolkit ... -yesToolkit location /usr/local/cuda-9.0 ... [Enter] # 默认安装位置即可Install a symbolic link at ... -yesInstall CUDA 9.0 Samples ... -yes # 安装一些例程Enter CUDA Samples Location ... [Enter] # 默认安装位置...Finished Driver : Not SelectedToolkit : Installed in /usr/local/cuda-9.0Samples : Installed in /root, but missing recommended libraries***WARNING: Incomplete installation! 出现警告，安装不完全，但是没有影响 配置系统路径 123456# vim /etc/profile...export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH# source /etc/profile ; 使环境变量立即生效 CUDA测试 12345678# cuda ; 按两下 tab 键cudafe cuda-gdb cuda-install-samples-9.0.shcudafe++ cuda-gdbserver cuda-memcheck# nvcc --versionnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2017 NVIDIA CorporationBuilt on Fri_Sep__1_21:08:03_CDT_2017Cuda compilation tools, release 9.0, V9.0.176 3. 安装cuDNN 下载 1234567wget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/cudnn-9.0-linux-x64-v7.4.2.24.tgz# Runtime Libyarywget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/RHEL7_3-x64/libcudnn7-devel-7.4.2.24-1.cuda9.0.x86_64.rpm# Developer Librarywget https://developer.download.nvidia.com/compute/machine-learning/cudnn/secure/v7.4.2/prod/9.0_20181213/RHEL7_3-x64/libcudnn7-devel-7.4.2.24-1.cuda9.0.x86_64.rpm 4. Anaconda安装 下载 123# 所有历史版本：https://repo.continuum.io/archive/# anaconda 3.5.1wget https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh 路径加入到系统配置文件中 生成秘钥 12345from notebook.auth import passwdpasswd()Enter password: kn88888Verify password: kn88888'sha1:25f17927d3a2:74b098a697b5e7bd76931badf00df1000eccfe17' 生成jupyter的配置文件 12[root@s3-aidc-dl-prod-01 ~]# jupyter notebook --generate-configWriting default config to: /root/.jupyter/jupyter_notebook_config.py 修改配置文件 12345678910111213vim /root/.jupyter/jupyter_notebook_config.py# 约174行c.NotebookApp.ip = &apos;localhost&apos;# 约240行c.NotebookApp.port = 8888# 约220行c.NotebookApp.open_browser = False# 约229行c.NotebookApp.password = &apos;sha1:25f17927d3a2:74b098a697b5e7bd76931badf00df1000eccfe17&apos; 使用ssh私有通道 12ssh -N -f -L [remote port]:localhost:[local port] -p [ssh port] -l [username] [公网IP]ssh -N -f -L 7864:10.105.50.178:8888 fanchangxun@182.254.211.45 -p36000 安装JupyterHub 1conda install -c conda-forge jupyterhub jupyter 配置 5. References centos7 nvidia驱动安装失败问题的解决办法 我的AI之路 —— 从裸机搭建GPU版本的深度学习环境 【已解决】求助，N卡官方驱动安装不能 CentOS 7 安装 Cuda 的经历 Centos7 重装英伟达显卡驱动+Cuda9.0+Cudnn7 centos7系统 安装NVIDIA显卡驱动 CentOS 7.0安装Nvidia驱动 Centos7 上为kaldi安装/卸载 nvidia显卡驱动和CUDA CUDA之nvidia-smi命令详解 CentOS 7 安装 NVIDIA 显卡驱动和 CUDA Toolkit DKMS简介 6. Error Logs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114ERROR: Failed to run `/usr/sbin/dkms build -m nvidia -v 410.79 -k 3.10.0-862.11.6.el7.x86_64`: Error! echo Your kernel headers for kernel 3.10.0-862.11.6.el7.x86_64 cannot be found at /lib/modules/3.10.0-862.11.6.el7.x86_64/build or /lib/modules/3.10.0-862.11.6.el7.x86_64/source. # 错误：Google failed to run /usr/sbin/dkms centos# 解决方案sudo apt-get install dkms build-essential linux-headers-$(uname -r)ERROR: Failed to install the kernel module through DKMS. No kernel module was installed; please try installing again without DKMS, or check the DKMS logs for more information.ERROR: Installation has failed. Please see the file &apos;/var/log/nvidia-installer.log&apos; for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.nvidia-installer log file &apos;/var/log/nvidia-installer.log&apos;creation time: Mon Dec 24 15:39:55 2018installer version: 410.79PATH: /data1/anaconda3/bin:/usr/local/services/java/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/services/go/bin:/root/binnvidia-installer command line: ./nvidia-installerUnable to load: nvidia-installer ncurses v6 user interfaceUsing: nvidia-installer ncurses user interface-&gt; Detected 28 CPUs online; setting concurrency level to 28.-&gt; Installing NVIDIA driver version 410.79.-&gt; There appears to already be a driver installed on your system (version: 410.79). As part of installing this driver (version: 410.79), the existing driver will be uninstalled. Are you sure you want to continue? (Answer: Continue installation)-&gt; Would you like to register the kernel module sources with DKMS? This will allow DKMS to automatically build a new module, if you install a different kernel later. (Answer: Yes)-&gt; Installing both new and classic TLS OpenGL libraries.-&gt; Installing both new and classic TLS 32bit OpenGL libraries.-&gt; Install NVIDIA&apos;s 32-bit compatibility libraries? (Answer: No)-&gt; Will install GLVND GLX client libraries.-&gt; Will install GLVND EGL client libraries.-&gt; Skipping GLX non-GLVND file: &quot;libGL.so.410.79&quot;-&gt; Skipping GLX non-GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLX non-GLVND file: &quot;libGL.so&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so.410.79&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so&quot;-&gt; Skipping EGL non-GLVND file: &quot;libEGL.so.1&quot;-&gt; Uninstalling the previous installation with /usr/bin/nvidia-uninstall.Looking for install checker script at ./libglvnd_install_checker/check-libglvnd-install.sh executing: &apos;/bin/sh ./libglvnd_install_checker/check-libglvnd-install.sh&apos;... Checking for libglvnd installation. Checking libGLdispatch... Checking libGLdispatch dispatch table Checking call through libGLdispatch All OK libGLdispatch is OK Checking for libGLX libGLX is OK Checking for libEGL libEGL is OK Checking entrypoint library libOpenGL.so.0 Checking call through libGLdispatch Checking call through library libOpenGL.so.0 All OK Entrypoint library libOpenGL.so.0 is OK Checking entrypoint library libGL.so.1 Checking call through libGLdispatch Checking call through library libGL.so.1 All OK Entrypoint library libGL.so.1 is OK Found libglvnd libraries: libGL.so.1 libOpenGL.so.0 libEGL.so.1 libGLX.so.0 libGLdispatch.so.0 Missing libglvnd libraries: libglvnd appears to be installed.Will not install libglvnd libraries.-&gt; Skipping GLVND file: &quot;libOpenGL.so.0&quot;-&gt; Skipping GLVND file: &quot;libOpenGL.so&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so.1.2.0&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so.1&quot;-&gt; Skipping GLVND file: &quot;libGLESv1_CM.so&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so.2.1.0&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so.2&quot;-&gt; Skipping GLVND file: &quot;libGLESv2.so&quot;-&gt; Skipping GLVND file: &quot;libGLdispatch.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1.7.0&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libGL.so&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1.1.0&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libEGL.so&quot;-&gt; Skipping GLVND file: &quot;./32/libGL.so.1.7.0&quot;-&gt; Skipping GLVND file: &quot;libGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libGL.so&quot;-&gt; Skipping GLVND file: &quot;./32/libGLX.so.0&quot;-&gt; Skipping GLVND file: &quot;libGLX.so&quot;-&gt; Skipping GLVND file: &quot;./32/libEGL.so.1.1.0&quot;-&gt; Skipping GLVND file: &quot;libEGL.so.1&quot;-&gt; Skipping GLVND file: &quot;libEGL.so&quot;Will install libEGL vendor library config file to /usr/share/glvnd/egl_vendor.d-&gt; Searching for conflicting files:-&gt; done.-&gt; Installing &apos;NVIDIA Accelerated Graphics Driver for Linux-x86_64&apos; (410.79): executing: &apos;/usr/sbin/ldconfig&apos;...-&gt; done.-&gt; Driver file installation is complete.-&gt; Installing DKMS kernel module:ERROR: Failed to run `/usr/sbin/dkms build -m nvidia -v 410.79 -k 3.10.0-862.11.6.el7.x86_64`: Error! echoYour kernel headers for kernel 3.10.0-862.11.6.el7.x86_64 cannot be found at/lib/modules/3.10.0-862.11.6.el7.x86_64/build or /lib/modules/3.10.0-862.11.6.el7.x86_64/source.-&gt; error.ERROR: Failed to install the kernel module through DKMS. No kernel module was installed; please try installing again without DKMS, or check the DKMS logs for more information.ERROR: Installation has failed. Please see the file &apos;/var/log/nvidia-installer.log&apos; for details. You may find suggestions on fixing installation problems in the README available on the Linux driver download page at www.nvidia.com.","tags":[{"name":"Centos7","slug":"Centos7","permalink":"http://chenson.cc/tags/Centos7/"},{"name":"Nvida","slug":"Nvida","permalink":"http://chenson.cc/tags/Nvida/"},{"name":"GPU","slug":"GPU","permalink":"http://chenson.cc/tags/GPU/"}]},{"title":"用户画像初探","date":"2019-02-27T02:33:21.000Z","path":"2019/02/27/用户画像初探/","text":"1. 什么是用户画像从海量的用户数据中，建模抽象出每个用户的属性标签体系，这些标签体系通常要有一定的商业价值，有明确的层级划分和可理解性，能够直接指导商业运营。常见的标签包括但不限于：人口属性、行为轨迹、用户分群、生活场景、消费偏好等（如图）。通常根据不同的领域，不同的业务场景和不同的商业目标，用户画像的层级划分差异会比较大。 举个例子，以电商的用户画像和券商的用户画像为例，如图。 2. 如何建立用户画像标签体系通常可以分为两类，第一类就是有明确的标签，然后进过策略的统计分析直接得到，最简单的例子，通常就是用户的人口属性这类的静态特征。另外一种是没有明确的数据标签，需要我们对用户的这些数据通过机器学习训练模型，然后基于模型做出预测得出用户的标签，比如说用户的兴趣爱好。 如果说比较网站的讨论如何建立用户画像的标签体系，我觉得可能需要从数据的最上游说到最下游，可大致分成8个部分来说。 数据收集，清洗，汇总到数仓 特征宽表的计算，从数据到特征是整个环境中至关重要的一步，这个决定了结果的上限。 特征宽表的监控：展示各种统计指标，包括但不限于：max-min值，均值，方差，覆盖率、缺失率等，且需要按小时、按天按周等进行统计展示。然后设定对应的报警阈值，如果超过这个阈值的话就做出预警通知相关人员。 模型训练预测，其中会包括特征选择、模型训练、效果评估、例行预测等。 应用接口 商业应用 3. 建立过程中可能遇到的问题3.1 如何选取用户的唯一标识3.2 如何管理特征库3.3 如何计算特征 离线特征 Spark + Hive 实时特征 Kafka + Stome Kafka 高吞吐量的分布式消息系统 Stome是Twitter开源的分布式实时大数据处理框架 3.4 小样本问题 采样 下采样 过采样 欠采样 半监督学习 已标注数据和无标注数据：Lu Learning (Learning from Labeled and Unlableled Exampels) 每个类别的数据包含少量的已标注数据和大量的无标注数据 朴素贝叶斯分类器的EM算法，使用最大似然估计的迭代算法 Co-Training 自学习算法 Transductive SVM 直推式支持向量机 基于图的方法 正例和无标注数据中学习：Pu Learning (Learning from Positive and Unlableled Exampels) 假设解决的是一个二分类问题，但训练数据是由正例和无标注数据组成，其中不含有反例数据 直接法 单样本建模 V-SVM、One-Class-SVM、Biased-SVM 两步法 发现可靠的负例：SPy、1DN、NB算法等 基于上述的数据，用常用的模型训练 以上的模型和方法都有比较多的假设 3.5 根据不同业务需求，选择不同的模型同一个需求，可能可以用不同的模型来实现。 统计问题 平滑/归一 策略打分 语义分析 分词 LDA DNN … … 高维偏好 CF MF/FM 回归问题 回归树 普通最小二乘 Lasso Ridge SVR … … 分类问题 二分类 LR / SVM / NB RF / AdaBoost / GBDT / XGBoost … … 多分类 RF/ GBDT / XGBoost MaxEn One V.S All 聚类问题 K-Means 高斯混合 One-Class-SVM EM … … 关联规则 Aprior FP-growth 4. References - 数据挖掘之部分监督学习 社交平台舆情分析项目的总结和感想（LU学习，EM，KNN）(二)","tags":[{"name":"用户画像","slug":"用户画像","permalink":"http://chenson.cc/tags/用户画像/"}]},{"title":"hexo错误记录","date":"2019-02-23T06:30:57.000Z","path":"2019/02/23/hexo错误记录/","text":"讲实话，我不是特别了解hexo。大概就知道是个用来生成静态的博客包or工具？依赖了npm(node package manager)。然后平常使用的时候有时会遇到一些比较头大的问题，在没有动hexo和npm的情况下。 所以为了以后遇到类似的问题方便处理，对hexo的错误处理做了一个简单的记录(持续更新ing)。 1. hexo not found in XXX 问题描述 大概是一个多月没写文章了，在运行hexo n title的时候报了这个错误，说是本地本地没有发现hexo？？？反正是一脸懵逼 解决方案 尝试过未成功的方法 更新npm和node 卸载node，重装 最终成功的方法 12345678# 强制删除博客下node_modules文件夹$ rm -rf node_modules# 重装$ npm install # 结果失败，改用了淘宝的镜像后成功$ npm install cnpm -g --registry=https://registry.npm.taobao.org 注意，经常会报permission denied的Error，改用sudo运行指令。 之后hexo n title可以正常生成新的文章。 问题描述 解决方案 按照上面给的解决方案并没啥用，依旧报这个错误。 尝试过但未成功的方法 关于Hex Local hexo not found in XXX的一种解决方案 1sudo npm install -g hexo 依旧报错！！！ 搜了另外一个方法，Hexo s 提示 Local hexo not found in XXX 的解决方法，还是报一开始N早前的一个错误。node.js真是个坑啊。 实在没办法，定位一下具体问题出现在哪 1hexo g --debug 搜了半天也没解决方案。 2. hexo Error: Cannot find module ‘highlight.js 问题描述 来自上一个问题的子问题，虽然可以生成新的文章，但是在gendrate生成静态博客文件的时候失败了，报了没有highlights.js这个模块的错误。 看问题应该是npm没有安装上highlights这个模块。 解决方案 尝试过未成功的方法 安装highlights.js模块 1$ npm install highlight.js --save 报错关于fsevents模块的问题，真的是头大😡！！！ 重装npm的模块 123$ sudo npm cache clean -f$ sudo npm install -g n$ sudo n stable 安装fsevents模块 123$ sudo npm i fsevents # 失败$ sudo npm install -D fsevents # 失败 同样报错Error: EACCES: permission denied, mkdir &#39;/Users/Chenson/Dropbox/PROJECTS/myBlog/node_modules/fsevents/build 1$ sudo npm install -g fsevents # 失败 注意到里面有一个报错 npm WARN babel-eslint@10.0.1 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself. 似乎手动安装几个包，试一下看 1$ sudo npm install eslint 妈了个鸡，又报上面同样的错误，要疯了😠。 同样还是错误gyp ERR! stack Error: EACCES: permission denied, mkdir node_modules/fsevents/build 最终成功的方法 看上面尝试的方法，关键在于权限问题，尝试改了node_moules的权限为777，依旧失败。 后面参考答案Installation error: permission denied for node-sas，里面以为老哥说设置node的权限为root 12345# 非常重要！！！$ npm config set user root$ sudo npm install eslint $ sudo npm install fsevents$ sudo npm install highlight.js --save 最后这俩居然安装上了！ 然后也可以正常运行指令hexo g，生成静态博客。 3. 渲染复杂公式 问题描述 目前hexo对于简单的公式渲染没什么问题，但复杂点的似乎就渲染失败了，具体如下。 尝试但未成功的解决方案【有bug】 Hexo构建blog时渲染LaTeX数学公式的问题 解决方案是更换markdown引擎 解决方案就不放了，因为我试了之后，hexo就挂了TAT。 4. 表格显示问题 问题描述 有时候在blog里面插入表格，markdown似乎无法渲染成功，具体如下图 解决方案 表格无法正确显示 #16","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chenson.cc/tags/hexo/"}]},{"title":"读书笔记-增长黑客","date":"2019-02-23T05:19:54.000Z","path":"2019/02/23/读书笔记-增长黑客/","text":"评价：★★★★☆断断续续看了前几章，没有预计的那么好的感觉。一方面可能是还没全部看完的原因，另一方面可能一直是从算法的角度来期待这本书，而非运营，所以和预计有些落差。且自己好像总是希望能从书中获取一些所谓的干货，类似实践项目的一些经验，总是想借鉴别人的经验走一些捷径，有时候觉得这样挺不好的，有点太急功近利了，反而忘记了去思考，特别是这种和业务比较贴近的项目，很需要自己去思考。","tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://chenson.cc/tags/读书笔记/"},{"name":"智能营销","slug":"智能营销","permalink":"http://chenson.cc/tags/智能营销/"}]},{"title":"读书笔记-数据挖掘与数据化运营实战","date":"2019-02-23T05:15:17.000Z","path":"2019/02/23/读书笔记-数据挖掘与数据化运营实战/","text":"评价：★★★★☆整体还是偏运营一些，通过一些经典的案例（如用户流失预警、目标用户特征分析等），介绍了一些经典算法在这些案例中的应用。但是这些案例给我感觉还是太模糊了点，太多地方都是点到为止，不够深入，虽然有些确实是涉及到了一些业务上的东西。另外就是从技术的角度，觉得内容有些陈旧，好像是14年的书了吧。不知这些年大数据的积累，深度学习能否在数据化运营上也有些应用呢？不过从算法的角度，这个本书确实还比较适合作为数据化运营的入门书籍，具有一定的启发意义，整体还是比较推荐的。","tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://chenson.cc/tags/读书笔记/"},{"name":"智能营销","slug":"智能营销","permalink":"http://chenson.cc/tags/智能营销/"}]},{"title":"读书笔记-重构大数据统计","date":"2019-02-23T04:41:05.000Z","path":"2019/02/23/读书笔记-重构大数据统计/","text":"评价：★★★☆☆粗略看了一些，似乎数据设计到开发的内容比较多，很多都是java的代码，不过实用性估计不会很高。但是有些计算的思路不错，还是有些启发意义的。比如有些统计变量可以应用在特征工程中，基于RFM模型进行衍生等。 1. 单变量基本统计量1.1 常见的数据统计量 均值 \\overline X = \\frac 1 n \\sum_{i=1}^n X_i 方差 S_n^2 = \\frac 1 {n-1} \\sum_{i=1}^n (X_i - \\overline X)^2分母为n-1，则称之为$S_n^2$的自由度(degree of freedom) 问题：不是很理解自由度这个概念 标准差 - Standard Deviation S_n = \\sqrt {\\frac 1 {n-1} \\sum_{i=1}^n (X_i - \\overline X)^2} 变异系数 - Coefficient of Variation, CV \\frac {S_n} {\\overline X}标准差和均值的比值，一个无量纲的量，用来刻画数据的相对分散性 标准误 - Standard Error \\frac {\\sqrt {\\frac 1 {(n-1)} \\sum_{i=1}^n(X_i - \\overline X)^2}} {\\sqrt n} = \\sqrt{\\frac 1 {n(n-1)}\\sum_{i=1}^n(X_i - \\overline X)^2}标准误是由样本的标准差除以样本个数的开方计算得到的，标准误代表的是样本均数对总体均数的相对误差。 k阶原点矩 - Moment \\frac 1 n \\sum_{i=1}^nX_i^kk=1的时候，就是均值 k阶中心矩 \\frac 1 n \\sum_{i=1}^n（X_i - \\overline X）^kk=1的时候，一阶中心矩恒等于0 下面两个统计量可用来描述样本数据分布形状 偏度 - Skewness \\frac {\\sqrt n \\sum_{i=1}^n(X_i - \\overline X)^3} {[\\sum_{i=1}^n(X_i - \\overline X)^2]^{\\frac 3 2}}偏度是用来刻画数据对称性的指标。关于均值对称的数据，其偏度系数为0； 若左侧数据比较分散，则偏度系数小于0； 若右侧数据比较分散，则偏度系数大于0。 峰度 - Kurotsis \\frac { n \\sum_{i=1}^n(X_i - \\overline X)^4} {[\\sum_{i=1}^n(X_i - \\overline X)^2]^{2}} - 3峰度可以描述样本数据分布形态相对于正态分布的陡缓成都 若 Kurotsis = 0，则与状态分布的相同陡缓程度相同； 若 Kurotsis &gt; 0，则比状态分布的高峰更加陡峭，表现为尖顶峰； 若 Kurotsis &lt; 0，则比状态分布的高峰显得平缓，表现为平顶峰。 1.2 关于上面公式的思考 🤔看到上面的一些公式，发现一个共同点，基本都是会用到均值 $\\overline X$，这就意味着我们在计算这些数据的时候，得先扫描一遍所有数据，求出均值后再第二次扫描所有的数据，计算这些统计量。 如果我们想扫一次数据之后，就算出这些统计变量的话，就得实时计算出当前的均值，但很多的统计量都是需要实时将当前的变量与$\\overline X$ 做差，如果我们能够在需要时候这些统计变量的时候，再计算出这些值的话，是不是就可以边X的均值边缓存所有的X就可扫描一遍计算出结果。但这是这样的代价就是得缓存所有的X，可以说是非常大的，特别是对数据量很大的时候。能不能有其他的方式呢？其实可以思考一下下面这个公式的变化 \\sum_{i=1}^n = (X_i - \\overline X)^2 \\\\ = \\sum_{i=1}^n (X_i^2 - 2X_i\\overline X + \\overline X^2) \\\\ = \\sum_{i=1}^n X_i^2 - 2\\overline X \\sum_{i=1}^n X_i + \\sum_{i=1}^n \\overline X^2 \\\\ = \\sum_{i=1}^n X_i^2 - 2\\overline X · n\\overline X + n\\overline X^2 \\\\ = \\sum_{i=1}^b X_i^2 - n \\overline X^2 最终我们其实可以在扫一遍数据的情况下，计算出$\\sum_{i=1}^n X_i^2$、 $\\overline X$ 和n 同理可以推导公式 所以根据上面的思考结果，基本我们就可以将常见的几个数据统计量给计算出来。基本我们在扫一遍的时候，计算出如下几个值，n，sum，sum2，sum3，sum4等 count = n \\\\ sum = \\sum_{i=1}^n X_i \\\\ sum2 = \\sum_{i=1}^n X_i^2 \\\\ sum3 = \\sum_{i=1}^n X_i^3 \\\\ sum4 = \\sum_{i=1}^n X_i^41.3 不需要就可计算的次序统计量 目的：读取一遍数据同时就获取最大的前K个值和最小的前K个值 解决方案：同时维护两个队列 一个是按照从大到小的顺序排列，每次读取到新的数据加到队列中，然后只保留前k个数，将多出的数据删除 另一个队列是按照从小到大的顺序排列，方法同上，但是保留前k个数。 1.4 基于频数信息计算次序统计量 目的：统计某个数据的的频率（假设已经求出频率），并排序 例子： Items Counts A 4 B 2 C 1 D 3 解决方案： 对频率按元素由小到大的顺序排序（若已排好序，则可忽略），设排好序的频率对为（items[i], counts[i]） | Itmes | Counts | | ----- | ------ | | C | 1 | | B | 2 | | D | 3 | | A | 4 | 对每个元素items[i] 计算 cntScan[i], 其数值上等于count[0], count[1], …, count[i-1]的和,实际意义是将全部数据由小到大排序，第一次出现元素items[i]时对应的位置。 对于要去的分位数，首先是计算出对应排好序的数列的位置k，利用cntScan，使用二分法搜索出对应的数据元素编号j，则items[j]即为所求。 1.5 中位数、众数和均值之间的关系 当数据为对称分布的时候，这三个数是相同的 当数据为单峰的偏态分布的时候，则在均值的两侧，数据的个数不同。中位数会在数据个数校对的一侧，均值位于平衡点。 阅读中，持续更新 … … 2. References","tags":[{"name":"Big Data","slug":"Big-Data","permalink":"http://chenson.cc/tags/Big-Data/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://chenson.cc/tags/读书笔记/"}]},{"title":"Spark笔记-应用篇","date":"2019-01-24T09:36:58.000Z","path":"2019/01/24/Spark笔记-应用篇/","text":"","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.cc/tags/Spark/"}]},{"title":"Spark笔记-基础操作篇","date":"2019-01-24T09:36:35.000Z","path":"2019/01/24/Spark笔记-基础操作篇/","text":"本篇主要是为了熟悉PySpark的基本操作，没啥特别的内容。 1. 准备工作 启动Hadoop 启动Spark 在Jupyter中测试PySpark 123456789101112131415161718import pysparkfrom pyspark import SparkContext as scfrom pyspark import SparkConffrom pyspark.sql import SparkSession# conf = SparkConf().setAppName('spark-basic-oprations') \\ .setMaster('local[*]')sc = sc.getOrCreate(conf)# spark = SparkSession.builder \\ .appName('spark-basic-oprations') \\ .master('local[*]') \\ .config('spark.some.config.option', 'some-value') \\ .config('spark.debug.maxToStringFields', '50') \\ .getOrCreate() 2. RDD相关操作2.1 RDD的创建 读取外部数据 数据位于HDFS上 数据位于本地磁盘 需要注意的是，所有的worker nodes上都需要能够访问这个本地路径 代码 12345678# 数据位于HDFS上hdfs_file = '/user/hadoop/google_news.txt'# 数据位于本地磁盘，格式为(file:// + 本地的绝对路径)local_file = 'file:///usr/local/spark/data/google_news.txt'lines = sc.textFile(hdfs_file)lines = sc.textFile(local_file) textFile的数据可以是文件名、目录和压缩文件等。后面还可以跟一个参数，用来指定分区的数目 读取Driver中已经存在的数据集合 123nums = list(10)data = sc.parallelize()data.collect() 创建Key-Values-Pair RDD 这个跟MapReduce中的Key-Vlaue键值对是差不多的，经常都是 (key, value) 这样的格式。之所以介绍这样的一种格式，是因为这个之后很多都会将数据转换成类似的数据格式，然后对RDD做各种transformation做统计，比如reduceByKey、groupByKey等。 123lines = sc.textFile(local_file)kv_pairs = lines.flatMap(lambda x : x.split(' ')).map(lambda x : (x, 1))kv_pairs.take(10) 2.2 RDD常用操作 Transformation map(func): 将每个元素传递到函数func中，并将结果返回为一个新的数据集 filter(func): 筛选出满足函数func的元素，并返回一个新的数据集 1234567lines = sc.textFile(local_file)# 判断是否为空行，这里返回值为True/Falselines.filter(lambdas x : len(x) &gt; 0).collect()# 过滤非空行的，这里返回非空的内容lines.filter(lambdas x : len(x) &gt; 0).collect() flatMap(func): 与map()相似，但每个输入元素都可以映射到0或多个输出结果 reduceByKey(func): 应用于 (key, value) 键值对的数据集时，返回一个新的 (key, value) 形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合。如果之前有了解过MapReduce的话，发现和里面的Reduce操作会比较相似。 groupByKey(): 应用于 (key, value) 键值对的数据集时，返回一个新的(key, iterable)形式的数据集，这个和reduceByKey有那么一点区别，入参不需要func。 sortByKey(): 字面意思，根据键排序的RDD mapValues(func): 入参的func只应用于value上，不对key做修改。 join: 这个操作来自于database中，所以还可以细分为内连接()、右外连接(rightOuterJoin)和左外连接(leftOuterJoin)等。 Action count(): 返回数据集中的元素个数 collect(): 以数组的形式返回数据集中的所有元素 first(): 返回数据集中的第一个元素 take(): 以数组的形式返回数据集中的前n个元素 reduce(func): 通过函数func(输入两个参数并返回一个值)聚合数据集中的元素 1234567lines = sc.textFile(local_file)# 计算单行的长度lines_length = lines.map(lambdas x : len(x))# 计算总长度total_length = lines_length.reduce(lambda a, b : a + b) foreach(func): 将数据集中的每个元素传递到函数func中运行 2.3 RDD持久化 非持久化 1234567891011data = [\"Hadoop\", \"Spark\", \"Hive\"]rdd = sc.parallelize(data)# 非持久化，重复计算了## 行动操作，触发一次真正从头到尾的计算print(rdd.count()) # &gt;&gt; 3## 行动操作，触发一次真正从头到尾的计算print(', '.join(rdd.collect())) # &gt;&gt;&gt; Hadoop, Spark, Hive 持久化 persist() persist接受一个参数 MEMORY_ONLY 表示将RDD作为反序列化的对象存储于JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容 MEMORY_AND_DISK 表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将会被存放在硬盘上 代码 12345678910111213# 持久化data = [\"Hadoop\", \"Spark\", \"Hive\"]rdd = sc.parallelize(data)# 会调用persist(MEMORY_ONLY)，但是，语句执行到这里，并不会缓存rdd，这是rdd还没有被计算生成，接下来第一次action的时候才会缓存结果rdd.cache() # 第一次行动操作，触发一次真正从头到尾的计算，这时才会执行上面的rdd.cache()，把这个rdd放到缓存中print(rdd.count()) # &gt;&gt;&gt; 3# 第二次行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存中的rddprint(', '.join(rdd.collect())) # &gt;&gt;&gt; Hadoop, Spark, Hive 移除持久化 可以使用unpersist()方法手动地把持久化的RDD从缓存中移除 2.4 分区 RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上。RDD分区的一个分区原则是使得分区的个数尽量等于集群中的CPU核心(core)数目。 对于不同的Spark部署模式而言(分别是本地模式、Standalone模式、YARN模式、Mesos模式)，都可以通过设置spark.default.parallelism这个参数的值，来配置默认的分区数目，一般而言： 本地模式：默认为本地机器的CPU数目，若设置了local[N],则默认为N； Apache Mesos：默认的分区数为8； Standalone或YARN：在“集群中所有CPU核心数目总和”和“2”二者中取较大值作为默认值； 2.5 共享变量在Spark分布式计算中，不同的计算节点上的不同人物会调用同一个函数，里面会设计到一些数据的使用。 如果每个节点只是进行读操作，那么可以将该份数据拷贝到每台节点的不同任务上。 如果需要在不同节点不同任务之间，该变量，即存在读写的操作。此时应该设置这些变量为共享变量。 根据以上需求，Spark中提供了两种类型的变量 广播变量(broadcast variables) 每台机器缓存一个只读变量(非每个任务)，这样可以非常高效的给每个节点提供一个大的输入数据集的副本。 需要被缓存的数据在被广播后，会先结果序列化，然后在被调用的时候再进行反序列化。 Question 为什么haodoop需要序列化数据？ 当需要多次多地方使用的时候，多次的反序列化是否影响效率？ 具体操作 1234broadcast_data = sc.broadcast([1, 2, 3])broadcast_data.value&gt;&gt;&gt; [1, 2, 3] 上面的SparkContext.broadcast(v)是在普通的变量中创建一个广播变量 累加器(accumulators) 3. DataFrame相关操作3.1 Dataframe v.s. RDD RDD RDD是分布式的Java对象的集和，如上图的RDD是以为Person为类型参数，但Person类的内部结果对于RDD是不可知的(有校验机制么？) DataFrame DF看着和Python中的DataFrame差不多，是以RDD为基础的分布式数据集，也是分布式的Row对象的集和，提供了数据的详细结构信息，即Schema。 同时RDD也是惰性机制的，只记录了各种的逻辑转换的路线，即DAG图。等需要计算结果的时候，再计算。 3.2 创建DataFrame 直接创建 12345678910111213141516171819202122232425262728spark = SparkSession.builder \\ .appName('spark-basic-oprations') \\ .master('local[*]') \\ .config('spark.some.config.option', 'some-value') \\ .config('spark.debug.maxToStringFields', '50') \\ .getOrCreate() df = spark.read.text(file)df.show()# 和 Python 中的DataFrame很相似&gt;&gt;&gt; +--------------------+ | value| +--------------------+ |As of 2013, Googl...| | | |The service cover...| | | |On December 1, 20...| | | |The layout of Goo...| | | |On July 14, 2011,...| | | |Additionally in J...| | | |In June 2017, the...| +--------------------+ RDD转换成DataFrame 利用反射机制推断包含特定类型对象的RDD的Schema，适用对已知的数据结构 使用编程接口，构造一个Schema并将其应用在已知的RDD上 3.2 DataFrame常用操作下面这些操作和Python中队DataFrame 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 打印模式信息df.printSchema()&gt;&gt;&gt; root |-- age: long (nullable = true) |-- name: string (nullable = true) # 选择多列df.select(df.name,df.age + 1).show()&gt;&gt;&gt; +-------+---------+ | name|(age + 1)| +-------+---------+ |Michael| null| | Andy| 31| | Justin| 20| +-------+---------+ # 条件过滤df.filter(df.age &gt; 20 ).show()&gt;&gt;&gt; +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ # 分组聚合df.groupBy(\"age\").count().show()&gt;&gt;&gt; +----+-----+ | age|count| +----+-----+ | 19| 1| |null| 1| | 30| 1| +----+-----+ # 排序df.sort(df.age.desc()).show()&gt;&gt;&gt; +----+-------+ | age| name| +----+-------+ | 30| Andy| | 19| Justin| |null|Michael| +----+-------+ # 多列排序df.sort(df.age.desc(), df.name.asc()).show()&gt;&gt;&gt; +----+-------+ | age| name| +----+-------+ | 30| Andy| | 19| Justin| |null|Michael| +----+-------+ # 对列进行重命名df.select(df.name.alias(\"username\"),df.age).show()&gt;&gt;&gt; +--------+----+ |username| age| +--------+----+ | Michael|null| | Andy| 30| | Justin| 19| +--------+----+ 4. Spark Streaming 数据总体上可以分为静态数据和流数据。对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算。 批量计算以“静态数据”为对象，可以在很充裕的时间内对海量数据进行批量处理，计算得到有价值的信息。Hadoop就是典型的批处理模型，由HDFS和HBase存放大量的静态数据，由MapReduce负责对海量数据执行批量计算。 流数据必须采用实时计算，实时计算最重要的一个需求是能够实时得到计算结果，一般要求响应时间为秒级。当只需要处理少量数据时，实时计算并不是问题；但是，在大数据时代，不仅数据格式复杂、来源众多，而且数据量巨大，这就对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生。 处理Spark Streaming，其他流计算框架有 Twitter Storm(可实现毫秒级流计算)、Yahoo S4。 流计算处理过程包括如下几部分： 数据实时采集 常见的开源分布式日志采集系统 Facebook的Scribe LinkedIn的Kafka 淘宝的TimeTunnel 基于Hadoop的Chukwa和Flume 实际实时计算 实时查询服务 4.1 创建StreamingContext对象1234567891011121314# 在pyspark中创建from pyspark import SparkContextfrom pyspark.streaming import StreamingContextssc = StreamingContext(sc, 1)# 编写一个独立的Spark Streaming程序，而不是在pyspark中运行from pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextconf = SparkConf().setAppName('spark-streaming').setMaster('local[*]') sc = SparkContext(conf=conf)ssc = StreamingContext(sc, 1) 监听本地文件流 上面例子中，开启ssc的监听后，每隔5秒(之前初始化设定的值)会查询一次，但只处理新增的文件，对已经存在的历史文件不做处理。 监听套接字流 12345678910111213141516171819202122from __future__ import print_functionimport sysfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextif __name__ == \"__main__\": if len(sys.argv) != 3: print(\"Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;\", file=sys.stderr) exit(-1) sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\") ssc = StreamingContext(sc, 1) lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2])) counts = lines.flatMap(lambda line: line.split(\" \")) \\ .map(lambda word: (word, 1)) \\ .reduceByKey(lambda a, b: a + b) counts.pprint() ssc.start() ssc.awaitTermination() 监听RDD队列流 12345678910111213141516171819202122232425import time from pyspark import SparkContextfrom pyspark.streaming import StreamingContext if __name__ == \"__main__\": sc = SparkContext(appName=\"PythonStreamingQueueStream\") ssc = StreamingContext(sc, 1) # Create the queue through which RDDs can be pushed to # a QueueInputDStream rddQueue = [] for i in range(5): rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)] # Create the QueueInputDStream and use it do some processing inputStream = ssc.queueStream(rddQueue) mappedStream = inputStream.map(lambda x: (x % 10, 1)) reducedStream = mappedStream.reduceByKey(lambda a, b: a + b) reducedStream.pprint() ssc.start() time.sleep(6) ssc.stop(stopSparkContext=True, stopGraceFully=True) 监听Kafka 监听Flume","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.cc/tags/Spark/"},{"name":"PySpark","slug":"PySpark","permalink":"http://chenson.cc/tags/PySpark/"}]},{"title":"Spark笔记-原理篇","date":"2019-01-24T08:28:11.000Z","path":"2019/01/24/Spark笔记-原理篇/","text":"还在UNSW读研的时候，修过一门COMP9313，大数据相关课程的时候有学过一点Spark，但那会只是做过一些简单的Projects，简单的应用过Spark和MapReduce这俩计算框架，并没有实际工程上的经验。最近因为工作的原因，需要使用Spark做一些特征工程的计算，因为数据量还是挺大的，好几十亿条的数据吧。用SQL的话，不是那么的灵活，用Python计算这些的话，估计还是挺呛的，所以Spark是最好的选择。 关于学习计划，打算先熟悉一下Spark的一些基本原理和PySpark的基本操作，然后接下来再记录一下做特征工程相关的内容。 1. Spark的设计与运行原理1.1 Spark生态系统 Spark Core - 基本功能模块，如内存计算、任务调度、部署模式、故障恢复、存储管理等 Spark SQL - 可以直接操作Spark中的RDD Spark Streaming - 用于计算流数据的，支持高吞吐量、可容错处理的实时流数据处理，其核心思路是将流式计算分解成一系列短小的批处理作业 Spark MLLib - 机器学习的包，高级的接口有ML GraphX - 用于图计算 1.2 Spark基本概念 RDD - 弹性分布式数据集(Resilient Distributed Dataset)，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型（高度受限不是很理解） DAG - 有向无环图 (Directed Acyclic Graph)，主要反映了RDD之间的依赖关系 Executor - 运行在工作节点(Work Node)上的一个进程 应用 - 用户编写的Spark应用程序 任务 - 运行在Excutor上的工作单元 作业 - 一个作业包含多个RDD及作用于相应RDD上的各种操作 阶段 - 作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”，或者也被称为“任务集” 1.3 Spark架构设计一下两部分就看看就好了，初期不一定需要很深入的了解。初期的重点我觉得应该是先理解RDD部分。 1.4 Spark运行基本流程 1.5 Spark核心部分之一RDDSpark相比MapReduce更加高效，很重要一点是建立在统一的抽像RDD之上。MapReduce在计算的时候，会拆分成多个Maps和Reduces，这些都会被分配在不同的节点上，每个节点会将自己计算好的中间结果写入到HDFS上。那么这个过程就造成很多的重复计算、数据复制、磁盘IO和序列化开销。而在Spark中，RDD的存在就会避免了上面提到的这几点，从而使Spark更加高效。 那么什么是RDD呢？从最后一个单词看，这个是一个Dataset，也就是一个数据集合，一个分布式对象集合(不是很理解)。在一开始提到RDD的时候，说了RDD提供了一种高度受限的共享内存模型，对高度受限不是很理解，看了下好像是因为这个集合是只读的。即一个RDD里面是可以有多个记录分区的，也可以分布在集群中不同的节点上，但是这些数据集是只读的，不能直接修改。(有点像C++中的共享内存模型) 1.5.1 RDD的操作 Transformation 转换的意思，从字面上理解就是这个转到那个，那个转到另外一个。可以理解为一个类似流程图一样，那么每走一步就是指定了其依赖关系，RDD之间的相互依赖关系(不太理解为什么是RDD之间，而不是RDD之内)。 常见的Transformation有Map、Filter、Groupby、Join等，其输入是一个RDD，输出也是一个RDD。 Questions: 如果对一个RDD连续执行Map、Filter、Join等，那么中间过程会有多少个RDD呢？如果有多个这些RDD会不会很占内存等？ Action 字面意思就是行动，大致就是需要输出一个结果。常见的Action有Count，Collect等，其输入是一个RDD，但是输出为一个值或者结果。 1.5.2 RDD的执行流程 RDD读入外部或者内存的集合进行创建(读完之后当前只是一个RDD)； 经过多个Transformations，每次转换都会产生不同的RDD，供下一次Transformation使用； 最后一个是Action操作，然后输出到外部数据源或者Scala集合或者标量。 回答一下上面的困惑，RDD采用的惰性调用，字面意思是只有在需要的时候才会去做，即在Action的时候，才会去计算之前的Transformations操作。那么意味着不会输出到外部数据源或者Scala集合等，不会实际计算出中间结果，减少数据IO等操作。 新的困惑：所以RDD只是一个转换记录的集合，那之前怎么说是分布式对象集合，到底会不会占用HDFS磁盘或者内存呢？ 1.5.3 RDD特性 高效的容错性(分布式共享内存、键值存储、内存数据库) 中间结果持久化到内存(Spark是基于内存计算的，中间数据是持久化到内存，而不是磁盘，避免读写IO开销，但当T级的数据是如何内存足够存下数据的呢？) 存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化开销(记得Hadoop中是有自己的一套序列化的，和Java不一样) 1.5.4 RDD之间的依赖关系 窄依赖(Narrow Dependency) 一个或多个父RDD分区对应一个子RDD分区，即一对一或者多对一； 生成窄依赖关系的Transformations有Map，Filter，Union等 宽依赖(Wide Dependency) 一个父RDD分区对应多个子RDD分区，即一对多； 生成宽依赖关系的Transformations有Groupby，Sortbykey等 Join比较特殊，两者都有可能是。 Questions： 不是很理解不同Transformations生成的依赖关系不同 其实上面的理解点应该是分区，父分区和子分区的关系。根据MapReduce的计算框架，Groupby的操作肯定是会造成一对多的分区。同时在MapReduce中Map给我感觉也是会有一对多，是否这里的Map和MapReduce中的Map不太一样？ 2. References 子雨大数据之Spark入门教程(Python版)","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.cc/tags/Spark/"}]},{"title":"jupyterhub配置多用户","date":"2019-01-13T12:42:12.000Z","path":"2019/01/13/jupyterhub配置多用户/","text":"最近组里申请了一台带GPU的服务器，Nvidia的M40，虽然性能一般，但玩一玩也够用了。因为是Centos的系统，组里的小伙伴觉得安装不如Ubuntu方便，都不想鼓捣环境，然后就由我来安装了。之前已经安装了CUDA和显卡驱动，因为安装时候的笔记没整理好，就先整理安装jupyter多用户的教程。后续有空的时候再写前置的环境教程吧（其实如果一开始就给一个干净的环境，安装超简单的）。 1. 系统环境我的系统：CentOS Linux release 7.3.1611 (Core) 1234567# 查看系统版本lsb_release -a# 以下二种方法适用于RedHat,CentOScat /etc/redhat-releaserpm -q centos-releaserpm -q redhat-release 2. 安装步骤2.1 安装依赖包1234yum install sqlite-devel npm nodejs-legacy zlib-devel openssl-devel# 这个开启了，导致我后面一直报错，头疼，就kill相关进程就ok了npm install -g configurable-http-proxy 2.2 安装Anaconda312345wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.shbash Anaconda3-4.4.0-Linux-x86_64.sh# 安装完后应该会提示你是否要将anaconda3路径加入到系统环境变量中去，当然选择是啦，然后更新环境变量source ~/.bashrc 检测是否安装成功 1conda list 2.3 安装jupyterhub1conda install -c conda-forge jupyterhub 检测是否安装成功 12jupyterhub -hconfigurable-http-proxy -h 2.4 创建jupyterhub的配置文件123# jupyterhub是在anaconda3/bin路径下# 这条指令会生成一个jupyterhub_config.py的配置文件jupyterhub --generate-config 修改配置文件 123456789101112131415161718# ip和使用的端口号195:c.JupyterHub.ip = '10.105.xx.xxx'219:c.JupyterHub.port = 9999294:c.JupyterHub.statsd_prefix = 'jupyterhub'375:c.Spawner.cmd = ['jupyterhub-singleuser'] # 管理用户623: #c.Authenticator.admin_users = &#123;'root'&#125; # 白名单用户673:c.Authenticator.whitelist =&#123;'user_name_1','user_name_2','user_name_3', ..., 'user_name_x'&#125;708:c.LocalAuthenticator.create_system_users = True722:c.PAMAuthenticator.encoding = 'utf8'# 非必要749:c.InteractiveShellApp.exec_lines = ['import os;os.environ.update(&#123;\"JAVA_HOME\": \"/usr/local/jdk1.8.0_74\"&#125;)'] 注意：这里在白名单和管理用户都提到的访问的用户，这里的用户实际上就是我们Centos服务器的用户，如果我们配置的用户没在系统当中，则会自动创建，创建后需要我们手工的指定用户密码，用于用户后期登录jupyterhub。 2.5 开启服务1jupyterhub --config=/etc/jupyterhub/jupyterhub.py --no-ssl 然而失败了 TAT 一直报403错误，然后google了一下这个问题 Jupyterhub sevice unavailable error and http :403 forbidden. 根据上述的问题找到了一个解决方案 @carlurips aux | grep configurable-http-proxyif you find any processes, shut them down 然后再启动一开始的那条指令就可以完美运行了。 可能有些童鞋不知道怎么在本地访问服务器上的服务，那么此时就需要将端口进行转发啦，运行如下指令 12# 下面的意思是大概我先登入到fanchangxn这个跳板机，然后再连到10.105这台服务器上，同时将这台机器上的端口7864和本地端口7865挂载到一起ssh -N -f -L 7865:10.105.xx.xxx:7864 fanchangxun@182.254.xx.xxx -p36000 完美登入 此时发现要账号和密码怎么办？似乎刚刚只是配置了用户名字，没有要求配置密码啊。 由于之前我一直是在root账号下运行的，而我们本次配置的目的是允许多用户隔离运行自己的代码，而其他用户没有权限查看/修改/运行。所以先在本台机器下创建一些用户和密码（需在root账号下） 12345678910111213# 添加用户adduser user1# 设置密码，太简单系统为报一个bad password的warnning，可以忽略# 对于root用户来说，可以直接修改其他用户的密码passwd user1# 然后输入密码，比如 pwd12345678# 检查是否创建成功cut -d: -f1 /etc/passwd# 如果不小心把账号创建错了，需要删除也是ok的(永久删除)userdel user1 创建成功后，系统就会在/home下创建相应的user1这个文件夹，这个就是以后user1用的文件夹，然后用刚刚的账号和密码登入上述的jupyter就ok了（前提已经添加到了whitelist里面去），在里面创建文件夹或是notebook，就会在对应的/home/user1下创建相应的文件夹等。 2.4 开启后台运行和关闭因为这样的服务通常需要24h运行的，所以需要后台运行，通常可以用nohup、screen都可以。 12345678# 后台运行nohup jupyterhub --config=/data1/jupyterhub/jupyterhub_config.py --no-ssl &gt; /data1/jupyterhub/nohup.out 2&gt;&amp;1 &amp;# 查看是否正常运行，只对当前终端生效jobs# 删除运行的任务，后面为任务号fg num_of_jobs 12345# 然而jobs只看当前终端生效的，关闭终端后，在另一个终端jobs已经无法看到后台跑得程序了，此时利用ps（进程查看命令）# a:显示所有程序 # u:以用户为主的格式来显示 # x:显示所有程序，不以终端机来区分ps aux | egrep node 12# 或者查看哪个端口号被占用lsof -i:7864 12# 或者netstat -ap | grep 7864 12# 杀死进程kill -9 进程号 然而不知为何一直杀不掉这个进程，只能去看看他的父进程，杀掉父进程试试看 先杀了4773进程，再把17959进程杀了就OK了，完美。 3. 安装常用插件3.1 nb_extension12345678910111213# 为jupyter安装extensionpip install jupyter_contrib_nbextensionsconda install -c conda-forge jupyter_contrib_nbextensions# 安装jupyter nb_extensionjupyter contrib nbextension install --user# 开启/关闭extensionsjupyter nbextension enable &lt;nbextension require path&gt;jupyter nbextension disable &lt;nbextension require path&gt;# e.gjupyter nbextension enable codefolding/main 以上我在root用户下运行发现并没有效果，后来发现需要先切到用户账号下运行才行。 然后会在用户的路径下面生成不同的配置文件，然后重启服务器即可。 3.2 notedown使用notedown插件来读写github源文件 12# 安装nodtedown插件pip install https://github.com/mli/notedown/tarball/master 修改jupyterhub_config.py文件 12# 将配置添加到文件的末尾c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager' 使用方法 1234# 相互转化notedown input.md &gt; output.ipynbnotedown input.ipynb --to markdown --strip &gt; output.mdnotedown input.ipynb --to markdown &gt; output_with_outputs.md 在IPython中使用 12345678import reimport sysimport argparsefrom IPython.nbformat.v3.rwbase import NotebookReaderfrom IPython.nbformat.v3.nbjson import JSONWriterimport IPython.nbformat.v3.nbbase as nbbase 3. Daily Errors3.1 04/16/2019莫名其妙无法启动kernel 错误如下 1The kernel has died, and the automatic restart has failed. It is possible the kernel cannot be restarted. If you are not able to restart the kernel, you will still be able to save the notebook, but running code will no longer work until the notebook is reopened. 解决方案 卸载重装 ipykernel References 5. References Jupyterhub安装流程 如何建立一個團隊用的 Jupyter-Hub CentOS 7中添加一个新用户并授权 centos系统添加/删除用户和用户组 Installing jupyter_contrib_nbextensions Github-notedown","tags":[{"name":"jupyterhub","slug":"jupyterhub","permalink":"http://chenson.cc/tags/jupyterhub/"}]},{"title":"Mixed-Logistic-Regression模型初探","date":"2018-12-22T09:05:55.000Z","path":"2018/12/22/Mixed-Logistic-Regression模型初探/","text":"","tags":[{"name":"MLR","slug":"MLR","permalink":"http://chenson.cc/tags/MLR/"}]},{"title":"大数据数仓建设笔记","date":"2018-12-20T05:02:15.000Z","path":"2018/12/20/大数据数仓建设笔记/","text":"虽然不是从事大数据开发的，但我觉得还是有必要对大数据这边有一些了解，简单的可能是如何使用大数据的一些工具，比如是Hive、Spark、Presto等，以及如何日更一张宽表。所以就按照我平常接触和了解的做了个简单的总结。 1. 为什么要建设数据仓库 数据集成 不同业务系统数据集中、统一管理 面向主题 对业务数据分主题建设—参与人、行为域、黑名单 时间维度 按天存储历史快照数据，数据可追溯 相对稳定 提升计算能力 数据预清洗处理，预加工存储 支持多方面应用 OLAP、BI 2. 数据仓库规划2.1 数据仓库分层介绍为什么要对数据进行层次设计呢？主要有一下原因： 清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。 数据血缘追踪：简单来讲可以这样理解，我们最终给业务诚信的是一能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。 减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。 把复杂问题简单化：讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。 屏蔽原始数据的异常。 屏蔽业务的影响，不必改一次业务就需要重新接入数据 2.2 简单介绍这里主要简单介绍上面的分层架构，实际上对于大点的公司，可能数仓在设计上还更为详细一些，不会如上面那搬简单。这里可能就按照我平常工作中接触的架构来介绍了。 源数据层 - BDM 这部分一般由数据网关去拉去数据，尽量保留最㡳的数据信息，一般为最原始的json数据。 数据准备层 - ODS(Operational Data Store) 这部分的数据一般是由BDM那边的数据解析来了，会对json进行拆解，将json拆解成结构化的表。在数据处理过程中尽可能会保留json中所有的信息，比如表名和字段值之类的。不过也有时候会对数据进行处理，比如说数据脱敏可能在这一层就开始完成了。 为了保留原始数据，通常这边是每天全量更新的，也就是会保留了历史变更的数据，按天分区储存。这个非常重要方便以后回溯数据。 存储 尽可能保留原始数据信息，包括字段名称 数据处理 结构化，拆表（json复杂结构） 脱敏（身份证、手机号拆字段） 去噪、去重等。 禁止 筛选、汇总、舍去字段 数据明细层 - DWD 这一层会在存储方面对数据进行分类存储，同时也会对数据做一些简单的处理，比如去噪啊，格式化处理啊之类的。 存储 分主题、全量数据 数据处理 字段命名规范化、关键维度映射 时间维度的处理 维度转代理键、空值处理、非必要字段的舍弃与转换 脏数据处理 业务表合并 增量转全量 轻度汇总层 - DWS 这一层对已经分类和处理好的数据做一些轻度的汇总。 存储 分主题、维度汇总数据 数据处理 维度汇总、不同颗粒度的维度汇总 各类业务统计度量值的计算 业务口径逻辑的实现 数据集市层 - DM 作为一个调包侠，经常接触的可能就是这部分了。因为这部分基本就是已经汇总好的宽表，通常都是可以直接入模型的数据了，或对这部分数据简处理一下就可以直接用的。这一层通常是T+1的日更宽表，宽表逻辑通常由模型这边开发好提供给大数据开发，或者是运营、业务、BI那边根据他们的需求，将逻辑整理好给大数据开发。 存储 跨主题模型实现、宽表 数据处理 跨主题的汇总统计计算 展开维度形成宽表 业务口径逻辑的实现 3. 架构总览整个数仓就是根据上面的分层来设计的，从网关到ODS到DWD到DWS最后到DM。 4. DM层宽表设计与维护12 5. References 大数据环境下该如何优雅地设计数据分层","tags":[{"name":"Big Data","slug":"Big-Data","permalink":"http://chenson.cc/tags/Big-Data/"}]},{"title":"拒绝图床，从我做起","date":"2018-12-01T04:29:20.000Z","path":"2018/12/01/拒绝图床，从我做起/","text":"前段时间想整理一下以前的博文笔记，因为很多都是早期自己记录的笔记，条理比较乱。想稍微整理一下，方便以后自己回顾复习以及他人阅读。但发现一件比较头大的事情就是，以前插图都是上传到图床上然，而现在很多图片都已经挂掉了，链接不可访问。这简直就是灾难性的一幕。 早期之所以用图床是因为方便，因为Chrome有个插件可以直接上传，采用本地的话需要把图片保存到本地路径然后引用对应的图片，至于如何同步到github上不影响显示效果也是比较麻烦的。好在现在Typora更新了几个版本后，对于插入图片可以时候非常友好（这里裂墙推荐Typora，最好用的MarkDown工具，没有之一）。具体使用步骤如下： 1. 安装步骤 安装插件，修改Hexo配置 修改主页配置文件_config.yml里面的post_asset_folder : true 在hexo目录下执行命令npm install hexo-asset-image --save，即安装一个可以上传本地图片的插件 安装成功后，以后再新建博文hexo n “XXX”的时候，除了生成对应的.md文件的同时，也会在同一层目录下，建一个同名的文件夹，用于存放这篇博文引用的本地图片。 12345XXX.mdXXX|--img1.jpg|--img2.jpg|--img3.jpg 生成静态页面，对应的html是 1&lt;img src=\"/year/month/day/XXX/img1.jpg\" alt=\"img1\"&gt; 修改Typora配置，具体如图 配置完之后，可以在截图完之后，将剪切板的图片直接复制到对应文件目录下的文件夹内，即我们上面新建好的文件夹。有木有非常方便！！！当然这里也可以设置其他路径，只是我这里设置的是这样，为了便于图片管理。 发布文章 文章写好之后，执行指令hexo g; hexo d之后，就会生成对应的静态页面，放到public对应的文件夹下。OK，我们可以打开自己的网站看一下就可以啦。这个时候你会惊喜的发现，图片全都无法显示。哈哈哈，为什么嘞？ 那么看一下原始makrdown目录下的结构 再看一下对应public下的目录结构 发现图片对应的文件夹是不一样的（此时可回头看生成对应的html路径地址）原始md是引用同名文件夹下的图片，而html却和应用的图片在同一层目录下，所以读取失败了。 生成的html路径 1&lt;img src=\"/year/month/day/XXX/img1.jpg\" alt=\"img1\"&gt; 但我比较想要的路径是 1&lt;img src=\"/year/month/day/XXX/year-month-day-XXX/img1.jpg\" alt=\"img1\"&gt; 原始图片和md文章所在的路径是 12PATH-TO-BLOG/source/_posts/year-month-day-XXX/img1.jpgPATH-TO-BLOG/source/_posts/year-month-day-XXX.md 当然我们可以愚蠢的设置本地的md文件，把图片前面的XXX先去了，但是推上去的时候，图片的相对路径就是正常的。这样虽然线上可以正常显示，但是同时会导致线下无法显示图片，所以这方法真的是有点蠢。 网上找了会解决方案似乎都没有成功，可能是我的操作问题，没设置好。于是就自己写了个简单脚本解决这个问题。 这段脚本主要就是在public下对应blog所在的文件下新建一个日期-文章名的文件夹，然后将图片移到这个文件夹下，那么图片和文章的相对路径就和md文件里面是一致的了，所以在更新hexo推上去，图片就显示正常啦。以后写文章的时候只需要在部署hexohexo d的之前，运行一下下面代码即可。妈妈以后再也不用担心我图片挂掉了！！！ hexo g python relocated_img.py hexo d 运行代码后的文件夹结构 具体代码如下【我知道写了这么for循环很丑 ==！】： 123456789101112131415161718192021222324252627282930313233343536import os_PUBLIC_ = 'public'_PATH_ = os.getcwd()list_year = [year for year in os.listdir(_PUBLIC_) if year.isdigit() and len(year) == 4]_EXT_ = ['.jpg','.jpeg', '.gif','.png']for year in list_year: p_year = os.path.join(_PATH_, _PUBLIC_, year) list_month = [month for month in os.listdir(p_year) if month.isdigit() and len(month) == 2] for month in list_month: p_month = os.path.join(_PATH_, _PUBLIC_, year, month) list_day = [day for day in os.listdir(p_month) if day.isdigit() and len(day) == 2] for day in list_day: p_day = os.path.join(_PATH_, _PUBLIC_, year, month, day) list_blog = [blog for blog in os.listdir(p_day) if not blog.startswith('.')] for blog in list_blog: print('blog :', blog) p_blog = os.path.join(_PATH_, _PUBLIC_, year, month, day, blog) list_file = os.listdir(p_blog) dir_img = '-'.join([year, month, day, blog]) dir_img = os.path.join(p_day, blog, dir_img) if not os.path.exists(dir_img): os.makedirs(dir_img) for file in list_file: if file.endswith(tuple(_EXT_)): p_file = os.path.join(_PATH_, _PUBLIC_, year, month, day, blog, file) print(' &gt;&gt;&gt; img:', file) print(' &gt;&gt;&gt; img_path:', p_file, end='\\n\\n') cmd = f'mv &#123;p_file&#125; &#123;dir_img&#125;' os.system(cmd) 2. Bugs虽然这方法挺好用的，但发现有个小bug，就是图片在放到对应的XXX文件夹下，会重复一张。如果在Typora中设设置指定的custom fold下倒不会，设置成同名文件夹下就会多出一张，目前没找到解决方案，暂且先这样吧。 当然另外一个可能的问题就是会占用github的空间，但量小的情况下问题不大。 3. References hexo博客图片问题","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chenson.cc/tags/hexo/"},{"name":"Typora","slug":"Typora","permalink":"http://chenson.cc/tags/Typora/"}]},{"title":"聚类算法：K-Means及扩展算法K-Modes、K-Prototype初探","date":"2018-11-27T05:23:30.000Z","path":"2018/11/27/聚类算法：K-Means及扩展算法K-Modes、K-Prototype初探/","text":"由于最近正在参与的自动化建模平台需要用到这一算法，但sklearn里面的聚类算法只支持数值型的，无法用到类别型的特征上，所以就研究了K-Modes和K-Prototypes这两个算法。具体算法的思想和源码如下。 1. k-Means Algorightm K-Means K-Means++ K-Means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321class KMeans(BaseEstimator, ClusterMixin, TransformerMixin): \"\"\"K-Means clustering Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : &#123;'k-means++', 'random' or an ndarray&#125; Method for initialization, defaults to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. n_init : int, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, default: 300 Maximum number of iterations of the k-means algorithm for a single run. tol : float, default: 1e-4 Relative tolerance with regards to inertia to declare convergence precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances verbose : int, default 0 Verbosity mode. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. Attributes ---------- cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : Labels of each point inertia_ : float Sum of squared distances of samples to their closest cluster center. Examples -------- &gt;&gt;&gt; from sklearn.cluster import KMeans &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], ... [4, 2], [4, 4], [4, 0]]) &gt;&gt;&gt; kmeans = KMeans(n_clusters=2, random_state=0).fit(X) &gt;&gt;&gt; kmeans.labels_ array([0, 0, 0, 1, 1, 1], dtype=int32) &gt;&gt;&gt; kmeans.predict([[0, 0], [4, 4]]) array([0, 1], dtype=int32) &gt;&gt;&gt; kmeans.cluster_centers_ array([[1., 2.], [4., 2.]]) See also -------- MiniBatchKMeans Alternative online implementation that does incremental updates of the centers positions using mini-batches. For large scale learning (say n_samples &gt; 10k) MiniBatchKMeans is probably much faster than the default batch implementation. Notes ------ The k-means problem is solved using Lloyd's algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, 'How slow is the k-means method?' SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That's why it can be useful to restart it several times. \"\"\" def __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=1e-4, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto'): self.n_clusters = n_clusters self.init = init self.max_iter = max_iter self.tol = tol self.precompute_distances = precompute_distances self.n_init = n_init self.verbose = verbose self.random_state = random_state self.copy_x = copy_x self.n_jobs = n_jobs self.algorithm = algorithm def _check_test_data(self, X): X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES) n_samples, n_features = X.shape expected_n_features = self.cluster_centers_.shape[1] if not n_features == expected_n_features: raise ValueError(\"Incorrect number of features. \" \"Got %d features, expected %d\" % ( n_features, expected_n_features)) return X def fit(self, X, y=None, sample_weight=None): \"\"\"Compute k-means clustering. Parameters ---------- X : array-like or sparse matrix, shape=(n_samples, n_features) Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) \"\"\" random_state = check_random_state(self.random_state) self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \\ k_means( X, n_clusters=self.n_clusters, sample_weight=sample_weight, init=self.init, n_init=self.n_init, max_iter=self.max_iter, verbose=self.verbose, precompute_distances=self.precompute_distances, tol=self.tol, random_state=random_state, copy_x=self.copy_x, n_jobs=self.n_jobs, algorithm=self.algorithm, return_n_iter=True) return self def fit_predict(self, X, y=None, sample_weight=None): \"\"\"Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" return self.fit(X, sample_weight=sample_weight).labels_ def fit_transform(self, X, y=None, sample_weight=None): \"\"\"Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" # Currently, this just skips a copy of the data if it is not in # np.array or CSR format already. # XXX This skips _check_test_data, which may change the dtype; # we should refactor the input validation. return self.fit(X, sample_weight=sample_weight)._transform(X) def transform(self, X): \"\"\"Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by `transform` will typically be dense. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to transform. Returns ------- X_new : array, shape [n_samples, k] X transformed in the new space. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) return self._transform(X) def _transform(self, X): \"\"\"guts of transform method; no input validation\"\"\" return euclidean_distances(X, self.cluster_centers_) def predict(self, X, sample_weight=None): \"\"\"Predict the closest cluster each sample in X belongs to. In the vector quantization literature, `cluster_centers_` is called the code book and each value returned by `predict` is the index of the closest code in the code book. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data to predict. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return _labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[0] def score(self, X, y=None, sample_weight=None): \"\"\"Opposite of the value of X on the K-means objective. Parameters ---------- X : &#123;array-like, sparse matrix&#125;, shape = [n_samples, n_features] New data. y : Ignored sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) Returns ------- score : float Opposite of the value of X on the K-means objective. \"\"\" check_is_fitted(self, 'cluster_centers_') X = self._check_test_data(X) x_squared_norms = row_norms(X, squared=True) return -_labels_inertia(X, sample_weight, x_squared_norms, self.cluster_centers_)[1] k_means 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239def k_means(X, n_clusters, sample_weight=None, init='k-means++', precompute_distances='auto', n_init=10, max_iter=300, verbose=False, tol=1e-4, random_state=None, copy_x=True, n_jobs=1, algorithm=\"auto\", return_n_iter=False): \"\"\"K-means clustering algorithm. Read more in the :ref:`User Guide &lt;k_means&gt;`. Parameters ---------- X : array-like or sparse matrix, shape (n_samples, n_features) The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,), optional The weights for each observation in X. If None, all observations are assigned equal weight (default: None) init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. precompute_distances : &#123;'auto', True, False&#125; Precompute distances (faster but takes more memory). 'auto' : do not precompute distances if n_samples * n_clusters &gt; 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. verbose : boolean, optional Verbosity mode. tol : float, optional The relative increment in the results before declaring convergence. random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\" K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn't support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. return_n_iter : bool, optional Whether or not to return the number of iterations. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). best_n_iter : int Number of iterations corresponding to the best results. Returned only if `return_n_iter` is set to True. \"\"\" if n_init &lt;= 0: raise ValueError(\"Invalid number of initializations.\" \" n_init=%d must be bigger than zero.\" % n_init) random_state = check_random_state(random_state) if max_iter &lt;= 0: raise ValueError('Number of iterations should be a positive number,' ' got %d instead' % max_iter) # avoid forcing order when copy_x=False order = \"C\" if copy_x else None X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32], order=order, copy=copy_x) # verify that the number of samples given is larger than k if _num_samples(X) &lt; n_clusters: raise ValueError(\"n_samples=%d should be &gt;= n_clusters=%d\" % ( _num_samples(X), n_clusters)) tol = _tolerance(X, tol) # If the distances are precomputed every job will create a matrix of shape # (n_clusters, n_samples). To stop KMeans from eating up memory we only # activate this if the created matrix is guaranteed to be under 100MB. 12 # million entries consume a little under 100MB if they are of type double. if precompute_distances == 'auto': n_samples = X.shape[0] precompute_distances = (n_clusters * n_samples) &lt; 12e6 elif isinstance(precompute_distances, bool): pass else: raise ValueError(\"precompute_distances should be 'auto' or True/False\" \", but a value of %r was passed\" % precompute_distances) # Validate init array if hasattr(init, '__array__'): init = check_array(init, dtype=X.dtype.type, copy=True) _validate_center_shape(X, n_clusters, init) if n_init != 1: warnings.warn( 'Explicit initial center position passed: ' 'performing only one init in k-means instead of n_init=%d' % n_init, RuntimeWarning, stacklevel=2) n_init = 1 # subtract of mean of x for more accurate distance computations if not sp.issparse(X): X_mean = X.mean(axis=0) # The copy was already done above X -= X_mean if hasattr(init, '__array__'): init -= X_mean # precompute squared norms of data points x_squared_norms = row_norms(X, squared=True) best_labels, best_inertia, best_centers = None, None, None if n_clusters == 1: # elkan doesn't make sense for a single cluster, full will produce # the right result. algorithm = \"full\" if algorithm == \"auto\": algorithm = \"full\" if sp.issparse(X) else 'elkan' if algorithm == \"full\": kmeans_single = _kmeans_single_lloyd elif algorithm == \"elkan\": kmeans_single = _kmeans_single_elkan else: raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\" \" %s\" % str(algorithm)) if n_jobs == 1: # For a single thread, less memory is needed if we just store one set # of the best results (as opposed to one set per run per thread). for it in range(n_init): # run a k-means once # 可选：kmeans_single_lloyd or kmeans_single_elkan labels, inertia, centers, n_iter_ = kmeans_single( X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, precompute_distances=precompute_distances, tol=tol, x_squared_norms=x_squared_norms, random_state=random_state) # determine if these results are the best so far if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia best_n_iter = n_iter_ else: # parallelisation of k-means runs seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(kmeans_single)(X, sample_weight, n_clusters, max_iter=max_iter, init=init, verbose=verbose, tol=tol, precompute_distances=precompute_distances, x_squared_norms=x_squared_norms, # Change seed to ensure variety random_state=seed) for seed in seeds) # Get results with the lowest inertia labels, inertia, centers, n_iters = zip(*results) best = np.argmin(inertia) best_labels = labels[best] best_inertia = inertia[best] best_centers = centers[best] best_n_iter = n_iters[best] if not sp.issparse(X): if not copy_x: X += X_mean best_centers += X_mean distinct_clusters = len(set(best_labels)) if distinct_clusters &lt; n_clusters: warnings.warn(\"Number of distinct clusters (&#123;&#125;) found smaller than \" \"n_clusters (&#123;&#125;). Possibly due to duplicate points \" \"in X.\".format(distinct_clusters, n_clusters), ConvergenceWarning, stacklevel=2) if return_n_iter: return best_centers, best_labels, best_inertia, best_n_iter else: return best_centers, best_labels, best_inertia _kmeans_single_lloyd 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127def _kmeans_single_lloyd(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): \"\"\"A single run of k-means, assumes preparation completed prior. Parameters ---------- X : array-like of floats, shape (n_samples, n_features) The observations to cluster. n_clusters : int The number of clusters to form as well as the number of centroids to generate. sample_weight : array-like, shape (n_samples,) The weights for each observation in X. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional Method for initialization, default to 'k-means++': 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (k, p) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. tol : float, optional The relative increment in the results before declaring convergence. verbose : boolean, optional Verbosity mode x_squared_norms : array Precomputed x_squared_norms. precompute_distances : boolean, default: True Precompute distances (faster but takes more memory). random_state : int, RandomState instance or None (default) Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary &lt;random_state&gt;`. Returns ------- centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i'th observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). n_iter : int Number of iterations run. \"\"\" random_state = check_random_state(random_state) sample_weight = _check_sample_weight(X, sample_weight) best_labels, best_inertia, best_centers = None, None, None # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) if verbose: print(\"Initialization complete\") # Allocate memory to store the distances for each sample to its # closer center for reallocation in case of ties distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype) # iterations for i in range(max_iter): centers_old = centers.copy() # labels assignment is also called the E-step of EM labels, inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, centers, precompute_distances=precompute_distances, distances=distances) # computation of the means is also called the M-step of EM if sp.issparse(X): centers = _k_means._centers_sparse(X, sample_weight, labels, n_clusters, distances) else: centers = _k_means._centers_dense(X, sample_weight, labels, n_clusters, distances) if verbose: print(\"Iteration %2d, inertia %.3f\" % (i, inertia)) if best_inertia is None or inertia &lt; best_inertia: best_labels = labels.copy() best_centers = centers.copy() best_inertia = inertia center_shift_total = squared_norm(centers_old - centers) if center_shift_total &lt;= tol: if verbose: print(\"Converged at iteration %d: \" \"center shift %e within tolerance %e\" % (i, center_shift_total, tol)) break if center_shift_total &gt; 0: # rerun E-step in case of non-convergence so that predicted labels # match cluster centers best_labels, best_inertia = \\ _labels_inertia(X, sample_weight, x_squared_norms, best_centers, precompute_distances=precompute_distances, distances=distances) return best_labels, best_inertia, best_centers, i + 1 _kmeans_single_elkan 123456789101112131415161718192021222324252627def _kmeans_single_elkan(X, sample_weight, n_clusters, max_iter=300, init='k-means++', verbose=False, x_squared_norms=None, random_state=None, tol=1e-4, precompute_distances=True): if sp.issparse(X): raise TypeError(\"algorithm='elkan' not supported for sparse input X\") random_state = check_random_state(random_state) if x_squared_norms is None: x_squared_norms = row_norms(X, squared=True) # init centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) centers = np.ascontiguousarray(centers) if verbose: print('Initialization complete') checked_sample_weight = _check_sample_weight(X, sample_weight) centers, labels, n_iter = k_means_elkan(X, checked_sample_weight, n_clusters, centers, tol=tol, max_iter=max_iter, verbose=verbose) if sample_weight is None: inertia = np.sum((X - centers[labels]) ** 2, dtype=np.float64) else: sq_distances = np.sum((X - centers[labels]) ** 2, axis=1, dtype=np.float64) * checked_sample_weight inertia = np.sum(sq_distances, dtype=np.float64) return labels, inertia, centers, n_iter 2. K-Modes Algorithm step1：随机确定k个聚类中心$C_1$, $C_2$ … $C_k$，$C_i$是长度为M的向量，$C_i$ = [$C_{1i}$, $C_{2i}$, … , $C_{mi}$] step2：对于样本$x_j$ (j=1,2,…,N)，分别比较其与k个中心之间的距离 这里的**距离为不同属性值的个数**，假如$x_1$=[1, 2, 1, 3], $C_1$=[1, 2, 3, 4]，那么x1与C1之间的距离为2 step3：将$x_j$划分到距离最小的簇，在全部的样本都被划分完毕之后，重新确定簇中心，向量$C_i$中的每一个分量都更新为簇i中的众数 step4：重复步骤二和三，直到总距离（各个簇中样本与各自簇中心距离之和）不再降低，返回最后的聚类结果 KModes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152class KModes(BaseEstimator, ClusterMixin): \"\"\"k-modes clustering algorithm for categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. cat_dissim : func, default: matching_dissim Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : &#123;'Huang', 'Cao', 'random' or an ndarray&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centroids. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. verbose : int, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, cat_dissim=matching_dissim, init='Cao', n_init=1, verbose=0, random_state=None, n_jobs=1): self.n_clusters = n_clusters # 簇的个数 self.max_iter = max_iter # self.cat_dissim = cat_dissim # 类别间距离的计算方法 self.init = init # 初始中心点的选取方法 &#123;'Huang', 'Cao', 'random' # or an ndarray&#125; self.n_init = n_init # 运行次数取最佳值 self.verbose = verbose # self.random_state = random_state # self.n_jobs = n_jobs # if ((isinstance(self.init, str) and self.init == 'Cao') or hasattr(self.init, '__array__')) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method and algorithm are deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, **kwargs): \"\"\"Compute k-modes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] \"\"\" random_state = check_random_state(self.random_state) self._enc_cluster_centroids, self._enc_map, self.labels_,\\ self.cost_, self.n_iter_ = k_modes(X, self.n_clusters, self.max_iter, self.cat_dissim, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def fit_predict(self, X, y=None, **kwargs): \"\"\"Compute cluster centroids and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X). \"\"\" return self.fit(X, **kwargs).predict(X, **kwargs) def predict(self, X, **kwargs): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" if self.verbose and self.cat_dissim == ng_dissim: print(\"Ng's dissimilarity measure was used to train this model, \" \"but now that it is predicting the model will fall back to \" \"using simple matching dissimilarity.\") X = check_array(X, dtype=None) X, _ = encode_features(X, enc_map=self._enc_map) return _labels_cost(X, self._enc_cluster_centroids, self.cat_dissim)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return decode_centroids(self._enc_cluster_centroids, self._enc_map) else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_modes 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def k_modes(X, n_clusters, max_iter, dissim, init, n_init, verbose, random_state, n_jobs): \"\"\"k-modes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-modes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values X = check_array(X, dtype=None) # Convert the categorical values in X to integers for speed. # Based on the unique values in X, we can make a mapping to achieve this. X, enc_map = encode_features(X) n_points, n_attrs = X.shape assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = unique results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_modes_single)(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best] k_modes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596def k_modes_single(X, n_clusters, n_points, n_attrs, max_iter, dissim, init, init_no, verbose, random_state): \"\"\" X : n_clusters : n_points : n_points, n_attrs = X.shape n_attrs : max_iter : Maximum number of iterations of the k-modes algorithm for a single run. dissim : func, default: matching_dissim, &#123;matching_dissim, ng_dissim&#125; Dissimilarity function used by the k-modes algorithm for categorical variables. Defaults to the matching dissimilarity function. init : inti_no : verbose : random_state : \"\"\" random_state = check_random_state(random_state) # _____ INIT _____ # 初始化，选取中心点 if verbose: print(\"Init: initializing centroids\") # 1. method huang if isinstance(init, str) and init.lower() == 'huang': centroids = init_huang(X, n_clusters, dissim, random_state) # 2. method cao elif isinstance(init, str) and init.lower() == 'cao': centroids = init_cao(X, n_clusters, dissim) # 3. 随机选取 elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = X[seeds] # 4. 使用提供好的array elif hasattr(init, '__array__'): # Make sure init is a 2D array. if len(init.shape) == 1: init = np.atleast_2d(init).T assert init.shape[0] == n_clusters, \\ \"Wrong number of initial centroids in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[0], n_clusters) assert init.shape[1] == n_attrs, \\ \"Wrong number of attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init.shape[1], n_attrs) centroids = np.asarray(init, dtype=np.uint16) else: raise NotImplementedError if verbose: print(\"Init: initializing clusters\") # rows : n_clusters, cols : n_points membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # cl_attr_freq is a list of lists with dictionaries that contain the # frequencies of values per cluster and attribute. # rows : n_clusters, cols : n_attrs cl_attr_freq = [[defaultdict(int) for _ in range(n_attrs)] for _ in range(n_clusters)] for ipoint, curpoint in enumerate(X): # Initial assignment to clusters # 返回距离最近的中心点 clust = np.argmin(dissim(centroids, curpoint, X=X, membship=membship)) membship[clust, ipoint] = 1 # Count attribute values per cluster. for iattr, curattr in enumerate(curpoint): cl_attr_freq[clust][iattr][curattr] += 1 # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(n_attrs): if sum(membship[ik]) == 0: # Empty centroid, choose randomly centroids[ik, iattr] = random_state.choice(X[:, iattr]) else: centroids[ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_modes_iter(X, centroids, cl_attr_freq, membship, dissim, random_state) # All points seen in this iteration labels, ncost = _labels_cost(X, centroids, dissim, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, cost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, cost)) return centroids, labels, cost, itr 3. K-prototype Algorithm KPrototypes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160class KPrototypes(kmodes.KModes): \"\"\"k-protoypes clustering algorithm for mixed numerical/categorical data. Parameters ----------- n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. max_iter : int, default: 300 Maximum number of iterations of the k-modes algorithm for a single run. num_dissim : func, default: euclidian_dissim Dissimilarity function used by the algorithm for numerical variables. Defaults to the Euclidian dissimilarity function. cat_dissim : func, default: matching_dissim Dissimilarity function used by the kmodes algorithm for categorical variables. Defaults to the matching dissimilarity function. n_init : int, default: 10 Number of time the k-modes algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of cost. init : &#123;'Huang', 'Cao', 'random' or a list of ndarrays&#125;, default: 'Cao' Method for initialization: 'Huang': Method in Huang [1997, 1998] 'Cao': Method in Cao et al. [2009] 'random': choose 'n_clusters' observations (rows) at random from data for the initial centroids. If a list of ndarrays is passed, it should be of length 2, with shapes (n_clusters, n_features) for numerical and categorical data respectively. These are the initial centroids. gamma : float, default: None Weighing factor that determines relative importance of numerical vs. categorical attributes (see discussion in Huang [1997]). By default, automatically calculated from data. verbose : integer, optional Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. n_jobs : int, default: 1 The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. Attributes ---------- cluster_centroids_ : array, [n_clusters, n_features] Categories of cluster centroids labels_ : Labels of each point cost_ : float Clustering cost, defined as the sum distance of all points to their respective cluster centroids. n_iter_ : int The number of iterations the algorithm ran for. gamma : float The (potentially calculated) weighing factor. Notes ----- See: Huang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), 1998. \"\"\" def __init__(self, n_clusters=8, max_iter=100, num_dissim=euclidean_dissim, cat_dissim=matching_dissim, init='Huang', n_init=10, gamma=None, verbose=0, random_state=None, n_jobs=1): super(KPrototypes, self).__init__(n_clusters, max_iter, cat_dissim, init, verbose=verbose, random_state=random_state, n_jobs=n_jobs) self.num_dissim = num_dissim self.gamma = gamma self.n_init = n_init if isinstance(self.init, list) and self.n_init &gt; 1: if self.verbose: print(\"Initialization method is deterministic. \" \"Setting n_init to 1.\") self.n_init = 1 def fit(self, X, y=None, categorical=None): \"\"\"Compute k-prototypes clustering. Parameters ---------- X : array-like, shape=[n_samples, n_features] categorical : Index of columns that contain categorical data \"\"\" random_state = check_random_state(self.random_state) # If self.gamma is None, gamma will be automatically determined from # the data. The function below returns its value. self._enc_cluster_centroids, self._enc_map, self.labels_, self.cost_,\\ self.n_iter_, self.gamma = k_prototypes(X, categorical, self.n_clusters, self.max_iter, self.num_dissim, self.cat_dissim, self.gamma, self.init, self.n_init, self.verbose, random_state, self.n_jobs) return self def predict(self, X, categorical=None): \"\"\"Predict the closest cluster each sample in X belongs to. Parameters ---------- X : array-like, shape = [n_samples, n_features] New data to predict. categorical : Index of columns that contain categorical data Returns ------- labels : array, shape [n_samples,] Index of the cluster each sample belongs to. \"\"\" assert hasattr(self, '_enc_cluster_centroids'), \"Model not yet fitted.\" Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) Xcat, _ = encode_features(Xcat, enc_map=self._enc_map) return _labels_cost(Xnum, Xcat, self._enc_cluster_centroids, self.num_dissim, self.cat_dissim, self.gamma)[0] @property def cluster_centroids_(self): if hasattr(self, '_enc_cluster_centroids'): return [ self._enc_cluster_centroids[0], decode_centroids(self._enc_cluster_centroids[1], self._enc_map) ] else: raise AttributeError(\"'&#123;&#125;' object has no attribute 'cluster_centroids_' \" \"because the model is not yet fitted.\") k_prototypes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677def k_prototypes(X, categorical, n_clusters, max_iter, num_dissim, cat_dissim, gamma, init, n_init, verbose, random_state, n_jobs): \"\"\"k-prototypes algorithm\"\"\" random_state = check_random_state(random_state) if sparse.issparse(X): raise TypeError(\"k-prototypes does not support sparse data.\") # Convert pandas objects to numpy arrays. if 'pandas' in str(X.__class__): X = X.values if categorical is None or not categorical: raise NotImplementedError( \"No categorical data selected, effectively doing k-means. \" \"Present a list of categorical columns, or use scikit-learn's \" \"KMeans instead.\" ) if isinstance(categorical, int): categorical = [categorical] assert len(categorical) != X.shape[1], \\ \"All columns are categorical, use k-modes instead of k-prototypes.\" assert max(categorical) &lt; X.shape[1], \\ \"Categorical index larger than number of columns.\" ncatattrs = len(categorical) nnumattrs = X.shape[1] - ncatattrs n_points = X.shape[0] assert n_clusters &lt;= n_points, \"Cannot have more clusters (&#123;&#125;) \" \\ \"than data points (&#123;&#125;).\".format(n_clusters, n_points) Xnum, Xcat = _split_num_cat(X, categorical) Xnum, Xcat = check_array(Xnum), check_array(Xcat, dtype=None) # Convert the categorical values in Xcat to integers for speed. # Based on the unique values in Xcat, we can make a mapping to achieve this. Xcat, enc_map = encode_features(Xcat) # Are there more n_clusters than unique rows? Then set the unique # rows as initial values and skip iteration. unique = get_unique_rows(X) n_unique = unique.shape[0] if n_unique &lt;= n_clusters: max_iter = 0 n_init = 1 n_clusters = n_unique init = list(_split_num_cat(unique, categorical)) init[1], _ = encode_features(init[1], enc_map) # Estimate a good value for gamma, which determines the weighing of # categorical values in clusters (see Huang [1997]). if gamma is None: gamma = 0.5 * Xnum.std() results = [] seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) if n_jobs == 1: for init_no in range(n_init): results.append(k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seeds[init_no])) else: results = Parallel(n_jobs=n_jobs, verbose=0)( delayed(k_prototypes_single)(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, seed) for init_no, seed in enumerate(seeds)) all_centroids, all_labels, all_costs, all_n_iters = zip(*results) best = np.argmin(all_costs) if n_init &gt; 1 and verbose: print(\"Best run was number &#123;&#125;\".format(best + 1)) # Note: return gamma in case it was automatically determined. return all_centroids[best], enc_map, all_labels[best], \\ all_costs[best], all_n_iters[best], gamma k_prototypes_single 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132def k_prototypes_single(Xnum, Xcat, nnumattrs, ncatattrs, n_clusters, n_points, max_iter, num_dissim, cat_dissim, gamma, init, init_no, verbose, random_state): # For numerical part of initialization, we don't have a guarantee # that there is not an empty cluster, so we need to retry until # there is none. random_state = check_random_state(random_state) init_tries = 0 # 初始化几个中心点 # 可能存在失败的情况 while True: init_tries += 1 # _____ INIT _____ if verbose: print(\"Init: initializing centroids\") if isinstance(init, str) and init.lower() == 'huang': centroids = kmodes.init_huang(Xcat, n_clusters, cat_dissim, random_state) elif isinstance(init, str) and init.lower() == 'cao': centroids = kmodes.init_cao(Xcat, n_clusters, cat_dissim) elif isinstance(init, str) and init.lower() == 'random': seeds = random_state.choice(range(n_points), n_clusters) centroids = Xcat[seeds] elif isinstance(init, list): # Make sure inits are 2D arrays. init = [np.atleast_2d(cur_init).T if len(cur_init.shape) == 1 else cur_init for cur_init in init] assert init[0].shape[0] == n_clusters, \\ \"Wrong number of initial numerical centroids in init \" \\ \"(&#123;&#125;, should be &#123;&#125;).\".format(init[0].shape[0], n_clusters) assert init[0].shape[1] == nnumattrs, \\ \"Wrong number of numerical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[0].shape[1], nnumattrs) assert init[1].shape[0] == n_clusters, \\ \"Wrong number of initial categorical centroids in init (&#123;&#125;, \" \\ \"should be &#123;&#125;).\".format(init[1].shape[0], n_clusters) assert init[1].shape[1] == ncatattrs, \\ \"Wrong number of categorical attributes in init (&#123;&#125;, should be &#123;&#125;).\" \\ .format(init[1].shape[1], ncatattrs) centroids = [np.asarray(init[0], dtype=np.float64), np.asarray(init[1], dtype=np.uint16)] else: raise NotImplementedError(\"Initialization method not supported.\") if not isinstance(init, list): # Numerical is initialized by drawing from normal distribution, # categorical following the k-modes methods. meanx = np.mean(Xnum, axis=0) stdx = np.std(Xnum, axis=0) centroids = [ meanx + random_state.randn(n_clusters, nnumattrs) * stdx, centroids ] if verbose: print(\"Init: initializing clusters\") # 计算对应组内的成员 membship = np.zeros((n_clusters, n_points), dtype=np.uint8) # Keep track of the sum of attribute values per cluster so that we # can do k-means on the numerical attributes. cl_attr_sum = np.zeros((n_clusters, nnumattrs), dtype=np.float64) # Same for the membership sum per cluster cl_memb_sum = np.zeros(n_clusters, dtype=int) # cl_attr_freq is a list of lists with dictionaries that contain # the frequencies of values per cluster and attribute. cl_attr_freq = [[defaultdict(int) for _ in range(ncatattrs)] for _ in range(n_clusters)] for ipoint in range(n_points): # Initial assignment to clusters # 计算初始的归属类，分别由类别的和数值型数据组成 # gamma是类别特征的权重 clust = np.argmin( num_dissim(centroids[0], Xnum[ipoint]) + gamma * cat_dissim(centroids[1], Xcat[ipoint], X=Xcat, membship=membship) ) membship[clust, ipoint] = 1 cl_memb_sum[clust] += 1 # Count attribute values per cluster. for iattr, curattr in enumerate(Xnum[ipoint]): cl_attr_sum[clust, iattr] += curattr for iattr, curattr in enumerate(Xcat[ipoint]): cl_attr_freq[clust][iattr][curattr] += 1 # If no empty clusters, then consider initialization finalized. # 如果每一组的成员个数都大于0的话，则算是初始化成功，否则需要重新初始化 if membship.sum(axis=1).min() &gt; 0: break # TODO: 如果不修改随机种子或者其他的数据，每次跑的结果都是一样的 if init_tries == MAX_INIT_TRIES: # Could not get rid of empty clusters. Randomly # initialize instead. init = 'random' elif init_tries == RAISE_INIT_TRIES: raise ValueError( \"Clustering algorithm could not initialize. \" \"Consider assigning the initial clusters manually.\" ) # Perform an initial centroid update. for ik in range(n_clusters): for iattr in range(nnumattrs): centroids[0][ik, iattr] = cl_attr_sum[ik, iattr] / cl_memb_sum[ik] for iattr in range(ncatattrs): centroids[1][ik, iattr] = get_max_value_key(cl_attr_freq[ik][iattr]) # _____ ITERATION _____ if verbose: print(\"Starting iterations...\") itr = 0 labels = None converged = False cost = np.Inf while itr &lt;= max_iter and not converged: itr += 1 centroids, moves = _k_prototypes_iter(Xnum, Xcat, centroids, cl_attr_sum, cl_memb_sum, cl_attr_freq, membship, num_dissim, cat_dissim, gamma, random_state) # All points seen in this iteration labels, ncost = _labels_cost(Xnum, Xcat, centroids, num_dissim, cat_dissim, gamma, membship) converged = (moves == 0) or (ncost &gt;= cost) cost = ncost if verbose: print(\"Run: &#123;&#125;, iteration: &#123;&#125;/&#123;&#125;, moves: &#123;&#125;, ncost: &#123;&#125;\" .format(init_no + 1, itr, max_iter, moves, ncost)) return centroids, labels, cost, itr","tags":[{"name":"Cluster","slug":"Cluster","permalink":"http://chenson.cc/tags/Cluster/"}]},{"title":"关于Python的Mixin模式初探","date":"2018-11-19T23:25:13.000Z","path":"2018/11/20/关于Python的Mixin模式初探/","text":"1. 经典类 / 新式类 经典类 (Python2.2之前的版本) 经典类是一种没有继承的类，实例类型都是type类型(???)。如果经典类被作为父类，子类在调用父类的构造函数时就会出错。这时候MRO的方法为DFS（深度优先搜索（子节点顺序：从左到右）） 1234import inspect # inspect.getmro（A）可以查看经典类的MRO顺序class A: def __init__(self): print('This is a classic class.') http://python.jobbole.com/85685/ 新式类 为了使类和内置类型更加统一(???)，引入了新式类。新式类的每个类都继承于一个基类，可以是自定义类或者其它类，默认承于object。子类可以调用父类的构造函数。 这时有两种MRO的方法 如果是经典类MRO为DFS（深度优先搜索（子节点顺序：从左到右））。 如果是新式类MRO为BFS（广度优先搜索（子节点顺序：从左到右））。 123class A(object): def __init__(self): print('This is a new-style class.') 1234567891011121314151617181920class D(object): pass class E(object): pass class F(object): pass class C(D, F): pass class B(E, D): pass class A(B, C): pass if __name__ == '__main__': print A.__mro__ 2. 多重继承python中的MRO与多继承 3. Mix-inMixin表示的是Mix-in，表示这个类是作为功能添加到子类中，而不是作为父类，它的作用同Java中的接口。 普通类多重继承下，只能有一个普通父类和若干个Mixin类（保持主干单一） Mixin类不能继承普通类（避免钻石继承，重复调用） Mixin 类应该单一职责（参考 Java 的 interface 设计，Mixin 和此极其相似，只不过附带实现而已） 使用Mixin类实现多重继承要非常小心 首先必须是表示一种功能，而不是某个物品 必须责任单一，如果有多个功能，那么就写多个Mixin类 然后不能依赖于子类实现 即使子类没有继承这个Mixin类，也可以照样工作，就是缺失某个功能","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"}]},{"title":"Wide_and_Deep模型初探","date":"2018-11-18T06:01:03.000Z","path":"2018/11/18/Wide-and-Deep模型初探/","text":"1. BackgroundWide and Deep是Google在16年左右提出的模型，主要应用在了Google Play的应用推荐系统中，同时国内的一些大厂也在各自的业务中有使用到这一模型，比如美团。而该算法的核心主要在于结合了线性模型的记忆能力Memorization 和深度神经网络的泛化能力Generaization，且在训练过程中，同时优化这两个模型，即Jointly Training。 2. Overview 如上图所示，该模型的主要结构是中间Wide&amp;Deep Models那个部分，其主要由左边的Wide Models 和右边的Deep Models组成。 3. Wide Part最左端的就是一个普通的广义线性模型，不做过多的解释。 该部分的模型，输入的特征可以是连续特征，也可以是稀疏的离散特征。但该模型可以对离散特征之间做特征交叉，即Cross-Product，为了可以从历史数据中发现特征之间的相关信息，然后对交叉过后的特征做One-Hot处理。 4. Deep Part最右部分是一个DNN模型，一个前馈的深度神经网络。 该部分的模型，输入的特征同样可以是连续特征，也可以是稀疏的离散特征。但和上面部分的离散特征处理方式不同，这里对离散特征做了Embedding，即将高维稀疏的离散特征转化为低纬的稠密特征。 Embedding的初始权重是随机赋予的，然后在训练过程中，不断参与优化。 所以训练的过程大致是需要做Embedding的向量先转化成Dense Vector，然后和其他的连续型或者数值型的特征合并一起，喂给接下去的几层DNN。 其中隐含层的计算方法为 a^{l + 1} = f(W^{(l)}a^{(l)} + b^{(l)})这里 l是隐含层数， f是激活函数，通常使用ReLUs作为激活函数 5. Wide&amp;Deep Part如上图中间部分所示，两个模型最后是合并到一起训练的，并将两个模型的结果的加权和作为最终的预测结果（最终喂给了同一个LR模型去训练），具体如下公式所示。 P(Y=1 | x) = \\sigma(w^T_{wide}[x, \\phi(x)] + w^T_{deep}a^{(l_f)} + b)这里$\\sigma(·)$是sigmoid激活函数。 在论文中作者也特别的提到，这里的Joinly Training 和Ensemble模型是不一样的。对于Ensemble模型，各个子模型是单独训练的，各自的预测结果不影响其他的子模型，只是最后大家的预测结果通过某一种方式，比如投票，加权等，最终得出一个预测结果。 Jointly Training的预测结果，最终会通过Back-Propagating the Gradients，传播到Wide Part和Deep Part。这里两部分模型采用了不同的训练方法： Wide Models采用了FTRL + L1正则 Deep Models采用了AdaGrad 6. Conclusion在最终的测试部分，我自己准备了一份数据用于测试该模型。 Wide Part Deep Part Wide &amp; Deep Part 从上面的结果可以看出，从Wide Models到Deep Models，训练效果提升还是比较大的，但从Deep Models到Wide&amp;Deep Models训练结果非但没有提升，还有了一丝丝的下降，且这个我也已经测试了很多测，主要修改了Embedding中和Wide Models中的一些参数，后者始终没有超过前者的效果。但在网上找的一些测试数据中，Wide&amp;Deep Models还是有一丝丝的提升。原因我分析了一些可能由如下造成 数据集本身可能不太适合这个算法 数据处理方式，因为有部分数据有空值，我基本只是简单的填充了一下 Wide Models训练数据 训练参数 7. References 深度学习在美团点评推荐平台排序中的运用 Wide &amp; Deep Learning for Recommender Systems 简单易学的深度学习算法——Wide &amp; Deep Learning","tags":[{"name":"Wide&Deep","slug":"Wide-Deep","permalink":"http://chenson.cc/tags/Wide-Deep/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://chenson.cc/tags/Pytorch/"}]},{"title":"Neo4j数据导入笔记","date":"2018-08-31T15:23:32.000Z","path":"2018/08/31/Neo4j数据导入笔记/","text":"1. 导入数据导入数据大概有以下几种方式 Cypher的CREATE语句 Cypher的LOAD CSV语句 官方的Java API导入 - Batch Inserter 官方提供的neo4j-import工具 网友编写的Batch-import工具 2. 具体操作2.1 CREATE2.2 LOAD CSV2.3 Batch Insert2.4 neo4j-import 需要停止Neo4j 只能生成新的数据库，即无法在已存在的数据库中插入数据 12345bin/neo4j-import --into retail.db --id-type string \\ --nodes:Customer customers.csv --nodes products.csv \\ --nodes orders_header.csv,orders1.csv,orders2.csv \\ --relationships:CONTAINS order_details.csv \\ --relationships:ORDERED customer_orders_header.csv,orders1.csv,orders2.csv 参数说明 --into retail.db显然是目标数据库，其中不能包含现有数据库。 重复--nodes和--relationships参数是同一实体的多个（可能分裂的）csv文件的组，即具有相同的列结构。 每组的所有文件都被视为可以连接成一个大文件。一个标题行的组的第一个文件是必需的，它甚至可能被包含在其中可能比一个多GB的文本文件更易于处理和编辑一个单行文件。也支持压缩文件。 customers.csv直接作为带有:Customer标签的节点导入，属性直接从文件中获取。对于从:LABEL列中获取节点标签的产品也是如此。订单节点取自3个文件，一个标题和两个内容文件。输入:CONTAINS的order_details.csv订单项关系是通过其ID 来创建的，包含与所包含产品的订单。订单通过再次使用订单csv文件连接到客户，但这次使用不同的标头，其中：IGNORE是不相关的列这–id-type string表示所有:ID列都包含字母数字值（对仅数字ID进行优化）。列名用于节点和关系的属性名称，特定列有一些额外的标记 name:ID - 全局id列，通过该列查找节点以便以后重新连接，如果保留属性名称，它将不会被存储（临时），这就是–id-type所指的如果你有跨实体的重复id，你必须在括号中提供实体（id-group） :ID(Order)如果您的ID是全球唯一的，您可以将其关闭 :LABEL - 节点的标签列，多个标签可以用分隔符分隔 :START_ID，:END_ID- 关系文件列，引用节点ID，用于id-groups使用:END_ID(Order) :TYPE - 关系型列所有其他列都被视为属性，但如果为空或在注释时跳过：IGNORE 类型转换可以通过后面添加的名称，例如通过:INT，:BOOLEAN等 https://neo4j.com/docs/operations-manual/current/tools/import/file-header-format/?_ga=2.242662919.407071438.1562662054-675208984.1561960585#import-tool-header-format-header-files 1234neo4j-admin import --database=graph_test.db \\ --nodes=\"import/movies3-header.csv,import/movies3.csv\" \\ --nodes=\"import/actors3-header.csv,import/actors3.csv\" \\ --relationships=\"import/roles3-header.csv,import/roles3.csv\" 导入数据数据movies.csv（header未分离） 1234movieId:ID,title,year:int,:LABELtt0133093,&quot;The Matrix&quot;,1999,Moviett0234215,&quot;The Matrix Reloaded&quot;,2003,Movie;Sequeltt0242653,&quot;The Matrix Revolutions&quot;,2003,Movie;Sequel movies3-header.csv 1movieId:ID,title,year:int,:LABEL 因为ID此时是全局的，所以如果其他nodes都是从0开始自增的ID，那么就会重复了。所以可以指定一个命名空间 1movieId:ID(Movie-ID),title,year:int,:LABEL movies3.csv 123tt0133093,&quot;The Matrix&quot;,1999,Moviett0234215,&quot;The Matrix Reloaded&quot;,2003,Movie;Sequeltt0242653,&quot;The Matrix Revolutions&quot;,2003,Movie;Sequel 对应到Neo4j里面的数据格式是 12345678910&gt;&gt;&gt; MATCH (n:Movie) RETURN n LIMIT 3╒════════════════════════════════════════════════════════════════════╕│\"n\" │╞════════════════════════════════════════════════════════════════════╡│&#123;\"title\":\"The Matrix\",\"year\":1999,\"movieId\":\"tt0133093\"&#125; │├────────────────────────────────────────────────────────────────────┤│&#123;\"title\":\"The Matrix Reloaded\",\"year\":2003,\"movieId\":\"tt0234215\"&#125; │├────────────────────────────────────────────────────────────────────┤│&#123;\"title\":\"The Matrix Revolutions\",\"year\":2003,\"movieId\":\"tt0242653\"&#125;│└────────────────────────────────────────────────────────────────────┘ actors3-header.csv 1personId:ID,name,:LABEL actors3.csv 123keanu,&quot;Keanu Reeves&quot;,Actorlaurence,&quot;Laurence Fishburne&quot;,Actorcarrieanne,&quot;Carrie-Anne Moss&quot;,Actor 对应到Neo4j里面的数据格式是 1234567891011&gt;&gt;&gt; MATCH (n:Actor) RETURN n LIMIT 3╒═══════════════════════════════════════════════════╕│\"n\" │╞═══════════════════════════════════════════════════╡│&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125; │├───────────────────────────────────────────────────┤│&#123;\"name\":\"Laurence Fishburne\",\"personId\":\"laurence\"&#125;│├───────────────────────────────────────────────────┤│&#123;\"name\":\"Carrie-Anne Moss\",\"personId\":\"carrieanne\"&#125;│└───────────────────────────────────────────────────┘ roles3.csv （header未分离） 12345678910:START_ID,role,:END_ID,:TYPEkeanu,&quot;Neo&quot;,tt0133093,ACTED_INkeanu,&quot;Neo&quot;,tt0234215,ACTED_INkeanu,&quot;Neo&quot;,tt0242653,ACTED_INlaurence,&quot;Morpheus&quot;,tt0133093,ACTED_INlaurence,&quot;Morpheus&quot;,tt0234215,ACTED_INlaurence,&quot;Morpheus&quot;,tt0242653,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0133093,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0234215,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0242653,ACTED_IN roles3-header.csv 1:START_ID,role,:END_ID,:TYPE 如果上面使用了ID的命名空间的话 需要在关系中指明，否则关系会出现错乱 1:START_ID(Actor-ID),role,:END_ID(Movie-ID),:TYPE roles3.csv 123456789keanu,&quot;Neo&quot;,tt0133093,ACTED_INkeanu,&quot;Neo&quot;,tt0234215,ACTED_INkeanu,&quot;Neo&quot;,tt0242653,ACTED_INlaurence,&quot;Morpheus&quot;,tt0133093,ACTED_INlaurence,&quot;Morpheus&quot;,tt0234215,ACTED_INlaurence,&quot;Morpheus&quot;,tt0242653,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0133093,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0234215,ACTED_INcarrieanne,&quot;Trinity&quot;,tt0242653,ACTED_IN 1234567891011121314&gt;&gt;&gt; MATCH p=()-[r:ACTED_IN]-&gt;() RETURN p LIMIT 3╒══════════════════════════════════════════════════════════════════════╕│\"p\" │╞══════════════════════════════════════════════════════════════════════╡│[&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125;,&#123;\"role\":\"Neo\"&#125;,&#123;\"title\":\"T││he Matrix Revolutions\",\"year\":2003,\"movieId\":\"tt0242653\"&#125;] │├──────────────────────────────────────────────────────────────────────┤│[&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125;,&#123;\"role\":\"Neo\"&#125;,&#123;\"title\":\"T││he Matrix Reloaded\",\"year\":2003,\"movieId\":\"tt0234215\"&#125;] │├──────────────────────────────────────────────────────────────────────┤│[&#123;\"name\":\"Keanu Reeves\",\"personId\":\"keanu\"&#125;,&#123;\"role\":\"Neo\"&#125;,&#123;\"title\":\"T││he Matrix\",\"year\":1999,\"movieId\":\"tt0133093\"&#125;] │└──────────────────────────────────────────────────────────────────────┘ 关于上面的数据的header格式，可以，冒号：前面是字段名，后面跟着的是字段的数据类型，这边比较特殊的有ID和LABLE,其他的可以跟stirng，intz之类的 导入成功 如这边导入到的是一个新的数据库，需要修改配置文件 conf/neo4j.conf 123# The name of the database to mount# dbms.active_database=graph.dbdbms.active_database=graph_test.db 2.5 Batch-import导入自己准备的数据 1234567Exception in thread \"Thread-4\" java.lang.RuntimeException: org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.DuplicateInputIdException: Id '12370' is defined more than once in group 'global id space' at org.neo4j.unsafe.impl.batchimport.staging.AbstractStep.issuePanic(AbstractStep.java:150) at org.neo4j.unsafe.impl.batchimport.staging.AbstractStep.issuePanic(AbstractStep.java:142) at org.neo4j.unsafe.impl.batchimport.staging.LonelyProcessingStep.lambda$receive$0(LonelyProcessingStep.java:58) at java.lang.Thread.run(Thread.java:745)Caused by: org.neo4j.unsafe.impl.batchimport.cache.idmapping.string.DuplicateInputIdException: Id '12370' is defined more than once in group 'global id space' at 这边有重复数据，检查一下用户ID 123456789101112&gt;&gt;&gt; df_cus.customer_id.value_counts().head()12370 212431 212394 212429 212457 2Name: customer_id, dtype: int64&gt;&gt;&gt; df_cus.query(&apos;customer_id == 12370&apos;) customer_id country sex age20 12370 Austria female 2221 12370 Cyprus male 32 里面有重复的数据，解决方案是忽略有重复的数据或者修改底层数据 12345neo4j-admin import --ignore-duplicate-nodes\\ --database=graph_online_retail.db \\ --nodes=\"import/online_retail/node_stocks.csv\" \\ --nodes=\"import/online_retail/node_customers.csv\" \\ --relationships=\"import/online_retail/relationships.csv\" 3. References neo4j(二).使用neo4j-import导入数据及关系 Importing CSV Data into Neo4j Neo4j | 数据导入之neo4j-admin和neo4j-import的区别","tags":[{"name":"图数据库","slug":"图数据库","permalink":"http://chenson.cc/tags/图数据库/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"}]},{"title":"图数据库-关系网络设计","date":"2018-08-18T06:45:01.000Z","path":"2018/08/18/图数据库-关系网络设计/","text":"1. 简单介绍目前知识图谱可以分成两大类：一类是开放领域的知识图谱，比如谷歌搜索引擎所建立的知识图谱。另一类是垂直领域的知识图谱，根据字面意思来看，就是特定某一个领域，比如金融风控领域。今天介绍一下关系网络在金融风控领域的应用。 主要应用可以分成两大类。 第一类 网络特征的直接提取，提取出中心度或者一度/二度关联特征供上层规则系统或者风险评估模型使用。 比如说在银行贷款申请的业务场景下，在贷款申请人中，第一度是联系人，则与目标申请共享相同联系人的申请数目是一度关联度，如果这些申请还与另外一些申请共享相同的住址或其他特征，这些申请就和目标申请形成二度关联。一些简单的指标，比如一度关联节点或者二度关联节点是否触黑/触黑个数，在实际的反欺诈实践中效果是非常显著的。如果特征不涉及深度的关系，传统的关系型数据库就能够满足需求，因为计算量是随着度数的增加呈指数增加。 第二类 网络信息的深度挖掘。深度挖掘通常始于对连通子图的计算，对于社交属性较弱的金融应用，较大的连通子图可能对欺诈网络有揭示作用。在此基础上，还可以进行进一步展开社区的发现，这里面社区的发现不等于连通子图，汇聚性是一个更严格的指标。除此之外，通过欺诈比例传播，或者说染色，将已知的欺诈标注扩撒开来，从而获得更多的欺诈标注，也是关系图谱的一个重要应用。 2. 应用 - 基于通话详单的关系网络设计以用户的通话关系网络为例，节点、属性和关系设计如下： 2.1 节点及属性 User (业务场景内的用户) 手机号 是否注册 是否申请 是否通过 (最近一次) 是否触黑 (历史) 是否为坏用户 (有D7|M1|M2逾期记录等) 是否为好用户 (无逾期记录) 是否处于逾期中 (任在逾期中) 评分 (业务场景内分评分，可以是反欺诈分或者信用分等) 为了节省内存，以上关系(不包括手机号)可表示为 1101000576，具体代表意思如下： | Register | Apply | Pass | Balck | Bad | Good | Undue | Score || ———— | ——- | —— | ——- | —— | —— | ——- | ——- || 1 | 1 | 0 | 1 | 0 | 0 | 0 | 576 | 其中手机号可以将十进制数转换成62进制表示(即0-9,a-z,A-Z)，1️以节省内存。 所以一个节点的属性可表示为(P_num, Status)。 Phone (非用户) (P_num, Status) 这里Status就只有一个分数，至于如何计算一个非业务场景内的分数呢，可以采用在业务场景数据里，该非用户的联系人分数的加权评分分。比如说Phone2联系了业务场景内两个注册用户User1和User2，其分数分别为550，580，且之间的权重为W1和W2。则该非注册用户的评分为 W1 * User_1 + W2 * User2 问题：是否应该考虑非业务场景内的用户分数，如考虑了，好像存在递归调用的现象。 2.2 关系 E - 紧急联系人 B - 银行预留手机号 D - 存在通话记录 W - 权重 2.3 一度关系 根据上图显示 User和User1之间存在E/B|D|W1 之间的关系，则说明User1是User的紧急联系人或者银行预留手机号，且两人之间存在通话记录。同时可以看到User和User1之间关系是单向的，所以U-&gt;U1和U1-&gt;U之间的权重是不一样的。 2.4 二度关系 根据上图显示 User与User3、User4、User5和Phone2都是二度联系人。其中User与User3构成强对强关系，User与Phone2构成强对弱关系，User与User4/User5构成弱对弱关系。 在该业务场景下，强关系代表了是否为对方紧急联系人或者银行预留手机号，可体现出两者之间的关系(这个不是必须的)。 2.5 评分这个评分在很多第三方提供的数据中会有看到类似的 3. 特征衍生3.1 直接提取3.2深度挖掘4. 建议及思考在上述的设计当中，还是存在很多设计上的缺点可以改善的，具体如下 4.1 数据准备上的建议 是否可以考虑只导入手机号 对通话详单进行清洗，减少数据量。比如营销、电子欺诈号码等。 导入通讯录会带来大量的冗余节点(当用户基数很庞大的时候)，可考虑不导入通讯录 如果业务场景内允许，比如时候User和Phone之间其实一一对应的话，我们即可考虑删除User节点，简化图的结构 减少内存，即手机号、状态和关系以最节省内存的方式存放。 增加每条关系的权重 4.2 细化关系种类 call_in - 有来电记录 call_out - 有去电记录 msg_in - 有收短信记录 msg_out - 有发短信记录 emergency_call - 紧急联系人号码 contact (keep) - 通讯录记录 user_phone - 用户注册电话 4.3 考虑其他数据源来搭建网络 IP地址 MAC地址 LBS地址 身份证前6位对应地址/归属地 家庭住址 工作地址 手机号对应地址/归属地 等等 5. References 如何构建知识图谱 大数据和人工智能在P2P中的应用 这是一份通俗易懂的知识图谱技术与应用指南","tags":[{"name":"图数据库","slug":"图数据库","permalink":"http://chenson.cc/tags/图数据库/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"},{"name":"关系网络","slug":"关系网络","permalink":"http://chenson.cc/tags/关系网络/"}]},{"title":"图数据库-Neo4j-常用算法","date":"2018-08-18T06:03:23.000Z","path":"2018/08/18/图数据库-Neo4j-常用算法/","text":"本次主要学习图数据库中常用到的一些算法，以及如何在Neo4j中调用，所以这一篇偏实战，每个算法的原理就简单的提一下。 1. 图数据库中常用的算法 PathFinding &amp; Search 一般用来发现Nodes之间的最短路径，常用算法有如下几种 Google Search Results Dijkstra - 边不能为负值 Folyd - 边可以为负值，有向图、无向图 Bellman-Ford SPFA Centrality 一般用来计算这个图中节点的中心性，用来发现比较重要的那些Nodes。这些中心性可以有很多种，比如 Degree Centrality - 度中心性 Weighted Degree Centrality - 加权度中心性 Betweenness Centrality - 介数中心性 Closeness Centrality - 紧度中心性 Community Detection 基于社区发现算法和图分析Neo4j解读《权力的游戏》 用于发现这个图中局部联系比较紧密的Nodes，类似我们学过的聚类算法。 Strongly Connected Components Weakly Connected Components (Union Find) Label Propagation Lovain Modularity Triangle Count and Average Clustering Coefficient 2. 路径搜索算法 Shortest Path 1234567MATCH (start:Loc&#123;name:&quot;A&quot;&#125;), (end:Loc&#123;name:&quot;F&quot;&#125;)CALL algo.shortestPath.stream(start, end, &quot;cost&quot;)YIELD nodeId, costMATCH (other:Loc) WHERE id(other) = nodeIdRETURN other.name AS name, cost Single Source Shortest Path 123456MATCH (n:Loc &#123;name:&quot;A&quot;&#125;)CALL algo.shortestPath.deltaStepping.stream(n, &quot;cost&quot;, 3.0YIELD nodeId, distanceMATCH (destination) WHERE id(destination) = nodeIdRETURN destination.name AS destination, distance All Pairs Shortest Path 1234567891011CALL algo.allShortestPaths.stream(&quot;cost&quot;,&#123;nodeQuery:&quot;Loc&quot;,defaultValue:1.0&#125;)YIELD sourceNodeId, targetNodeId, distanceWITH sourceNodeId, targetNodeId, distanceWHERE algo.isFinite(distance) = trueMATCH (source:Loc) WHERE id(source) = sourceNodeIdMATCH (target:Loc) WHERE id(target) = targetNodeIdWITH source, target, distance WHERE source &lt;&gt; targetRETURN source.name AS source, target.name AS target, distanceORDER BY distance DESCLIMIT 10 Minimum Weight Spanning Tree 12345MATCH (n:Place &#123;id:&quot;D&quot;&#125;)CALL algo.spanningTree.minimum(&quot;Place&quot;, &quot;LINK&quot;, &quot;cost&quot;, id(n), &#123;write:true, writeProperty:&quot;MINST&quot;&#125;)YIELD loadMillis, computeMillis, writeMillis, effectiveNodeCountRETURN loadMillis, computeMillis, writeMillis, effectiveNodeCount; CASE 123456789101112131415MERGE (a:Loc &#123;name:&quot;A&quot;&#125;)MERGE (b:Loc &#123;name:&quot;B&quot;&#125;)MERGE (c:Loc &#123;name:&quot;C&quot;&#125;)MERGE (d:Loc &#123;name:&quot;D&quot;&#125;)MERGE (e:Loc &#123;name:&quot;E&quot;&#125;)MERGE (f:Loc &#123;name:&quot;F&quot;&#125;)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(b)MERGE (a)-[:ROAD &#123;cost:50&#125;]-&gt;(c)MERGE (a)-[:ROAD &#123;cost:100&#125;]-&gt;(d)MERGE (b)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:40&#125;]-&gt;(d)MERGE (c)-[:ROAD &#123;cost:80&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:30&#125;]-&gt;(e)MERGE (d)-[:ROAD &#123;cost:80&#125;]-&gt;(f)MERGE (e)-[:ROAD &#123;cost:40&#125;]-&gt;(f); 3. 中心性算法 PageRank 123456CALL algo.pageRank.stream(&quot;Page&quot;, &quot;LINKS&quot;,&#123;iterations:20&#125;)YIELD nodeId, scoreMATCH (node) WHERE id(node) = nodeIdRETURN node.name AS page,scoreORDER BY score DESC Degree Centrality Betweenness Centrality 12345CALL algo.betweenness.stream(&quot;User&quot;, &quot;MANAGES&quot;, &#123;direction:&quot;out&quot;&#125;)YIELD nodeId, centralityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user,centralityORDER BY centrality DESC; Closeness Centrality 123456CALL algo.closeness.stream(&quot;Node&quot;, &quot;LINK&quot;)YIELD nodeId, centralityMATCH (n:Node) WHERE id(n) = nodeIdRETURN n.id AS node, centralityORDER BY centrality DESCLIMIT 20; CASE 12345678910111213141516171819202122MERGE (home:Page &#123;name:&quot;Home&quot;&#125;)MERGE (about:Page &#123;name:&quot;About&quot;&#125;)MERGE (product:Page &#123;name:&quot;Product&quot;&#125;)MERGE (links:Page &#123;name:&quot;Links&quot;&#125;)MERGE (a:Page &#123;name:&quot;Site A&quot;&#125;)MERGE (b:Page &#123;name:&quot;Site B&quot;&#125;)MERGE (c:Page &#123;name:&quot;Site C&quot;&#125;)MERGE (d:Page &#123;name:&quot;Site D&quot;&#125;)MERGE (home)-[:LINKS]-&gt;(about)MERGE (about)-[:LINKS]-&gt;(home)MERGE (product)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(product)MERGE (links)-[:LINKS]-&gt;(home)MERGE (home)-[:LINKS]-&gt;(links)MERGE (links)-[:LINKS]-&gt;(a)MERGE (a)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(b)MERGE (b)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(c)MERGE (c)-[:LINKS]-&gt;(home)MERGE (links)-[:LINKS]-&gt;(d)MERGE (d)-[:LINKS]-&gt;(home) 4. 社区发现算法 Strongly Connected Components 1234CALL algo.scc.stream(&quot;User&quot;,&quot;FOLLOWS&quot;)YIELD nodeId, partitionMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS name, partition Weakly Connected Components (Union Find) 1234CALL algo.unionFind.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId,setIdMATCH (u:User) WHERE id(u) = nodeIdRETURN u.id AS user, setId Label Propagation 12CALL algo.labelPropagation.stream(&quot;User&quot;, &quot;FOLLOWS&quot;, &#123;direction: &quot;OUTGOING&quot;, iterations: 10&#125;) Lovain Modularity 12345CALL algo.louvain.stream(&quot;User&quot;, &quot;FRIEND&quot;, &#123;&#125;)YIELD nodeId, communityMATCH (user:User) WHERE id(user) = nodeIdRETURN user.id AS user, communityORDER BY community; Triangle Count and Average Clustering Coefficient 123456CALL algo.triangle.stream(&quot;Person&quot;,&quot;KNOWS&quot;)YIELD nodeA,nodeB,nodeCMATCH (a:Person) WHERE id(a) = nodeAMATCH (b:Person) WHERE id(b) = nodeBMATCH (c:Person) WHERE id(c) = nodeCRETURN a.id AS nodeA, b.id AS nodeB, c.id AS node 5. References Neo4j in deep 官方文档：Comprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"图数据库","slug":"图数据库","permalink":"http://chenson.cc/tags/图数据库/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"}]},{"title":"图数据库-Neo4j-初探","date":"2018-08-17T05:15:46.000Z","path":"2018/08/17/图数据库-Neo4j-初探/","text":"本次初探主要学习如何安装Neo4j，以及Cypher的基本语法。 1. 安装Neo4j Desktop版本 neo4j-desktop Server版本（Community版) 比较建议安装这个版本，因为Desktop版本的老是闪退，且要激活之类的。 下载Neo4j数据库 neo4j-server-community 下载常用算法的插件 graph-algorithms neo4j-graph-algorithms apoc-procedures neo4j-apoc-procedures 将下载下来的算法插件放入到$NEO4J_HOME/plugins文件夹下 Service版修改配置文件$NEO4J_HOME/conf/neo4j.conf 1234567891011# 解决登入的时候报没有授权的错误dbms.security.auth_enabled=false# 添加下载的算法插件dbms.security.procedures.unrestricted=apoc.*,algo.*apoc.import.file.enabled=true#增加页缓存到至少4G，推荐20G:dbms.memory.pagecache.size=4g#JVM堆保存留内存从1G起，最大4G:dbms.memory.heap.initial_size=1gdbms.memory.heap.max_size=4g 启动/停止 (把server所在的路径添加到系统的PATH) 1234567# 建议将neo4j所在的路径条件到系统$PATH当中，# export NEO4J_HOME=\"path-to-neo4j\"$NEO4J_HOME/bin/neo4j start$NEO4J_HOME/bin/neo4j console$NEO4J_HOME/bin/neo4j stop$NEO4J_HOME/bin/neo4j start -u neo4j -p neo4j$NEO4J_HOME/bin/cypher-shell 1CALL dbms.procedures() // 查看neo4j可用的进程，包括刚刚安装的插件 2. Cypher基本语法 Nodes基本语法 在Cypher里面通过一对小括号代表一个节点 () 代表匹配任意一个节点 (node1) 代表匹配任意一个节点，并给它起了一个别名 (:Lable) 代表查询一个类型的数据 (person:Lable) 代表查询一个类型的数据，并给它起了一个别名 (person:Lable {name:”小王”}) 查询某个类型下，节点属性满足某个值的数据 (person:Lable {name:”小王”,age:23}) 节点的属性可以同时存在多个，是一个AND的关系 Relationship基本语法 系用一对-组成，关系分有方向的进和出，如果是无方向就是进和出都查询 —&gt; 指向一个节点 -[role]-&gt; 给关系加个别名 -[:acted_in]-&gt; 访问某一类关系 -[role:acted_in]-&gt; 访问某一类关系，并加了别名 -[role:acted_in {roles:[“neo”,”Hadoop“]}]-&gt; 创建/删除节点 1234567891011121314151617181920212223// 插入一个Artist类别的节点，而且这个节点有一个属性为Name，值为Lady GagaCREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;&#125;)// 创建并返回CREATE (a:Artist &#123;Name:&quot;Lady Gaga&quot;, Gemder:&quot;Femal&quot;&#125;) return a// 一次性创建多个CREATE (a:Album &#123; Name: &quot;Killers&quot;&#125;), (b:Album &#123; Name: &quot;Fear of the Dark&quot;&#125;) RETURN a, bCREATE (a:Album &#123; Name: &quot;Piece of Mind&quot;&#125;) CREATE (b:Album &#123; Name: &quot;Somewhere in Time&quot;&#125;) RETURN a, b// 删除节点，如果这个节点和其他节点有连接的话，不能单单删除这个节点MATCH (a:Album &#123;Name: &quot;Killers&quot;&#125;) DELETE a// 一次性删除多个节点MATCH (a:Artist &#123;Name: &quot;Iron Maiden&quot;&#125;), (b:Album &#123;Name: &quot;Powerslave&quot;&#125;) DELETE a, b // 删除所有节点MATCH (n) DELETE n 创建/删除关系 123456789101112131415161718192021222324252627282930// 对Lady Gaga和专辑PieceOfMind之间创建一个released的关系MATCH (a:Artist), (b:Album)WHERE a.Name = &quot;Lady Gaga&quot; AND b.Name = &quot;Piece of Mind&quot;CREATE (a)-[r:RELEASED]-&gt;(b)RETURN rMATCH (a:Artist), (b:Album), (p:Person)WHERE a.Name = &quot;Strapping Young Lad&quot; AND b.Name = &quot;Heavy as a Really Heavy Thing&quot; AND p.Name = &quot;Devin Townsend&quot; CREATE (p)-[pr:PRODUCED]-&gt;(b), (p)-[pf:PERFORMED_ON]-&gt;(b), (p)-[pl:PLAYS_IN]-&gt;(a)RETURN a, b, p // 删除指定的关系MATCH (:Artist)-[r:RELEASED]-(:Album) DELETE r MATCH (:Artist &#123;Name: &quot;Strapping Young Lad&quot;&#125;)-[r:RELEASED]-(:Album &#123;Name: &quot;Heavy as a Really Heavy Thing&quot;&#125;) DELETE r // 删除所有的关系MATCH ()-[r:RELEASED]-() DELETE r // 清除所有节点和关系 MATCH (n)OPTIONAL MATCH(n)-[r]-()DELETE n,r // 删除整个数据库MATCH (n) DETACH DELETE n 创建/删除约束 同SQL一样，Neo4j数据库支持对Node或relationship的属性的UNIQUE约束 123CREATE CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUEDROP CONSTRAINT ON (a:Artist) ASSERT a.Name IS UNIQUE 创建/删除索引 123456CREATE INDEX ON :Album(Name) // View the schema:schemaDROP INDEX ON :Album(Name) 更新一个节点/边 12MATCH (n:Person &#123; name: &quot;Andres&quot; &#125;)SET n.name = &quot;Taylor&quot;; 筛选过滤 123456789// WHEREMATCH (p1: Person)-[r:friend]-&gt;(p2: Person) WHERE p1.name=~&quot;K.+&quot; or p2.age=24 or &quot;neo&quot; in r.rels RETURN p1, r, p2 // NOT MATCH (p:Person)-[:ACTED_IN]-&gt;(m)WHERE NOT (p)-[:DIRECTED]-&gt;()RETURN p, m 结果集返回 12345MATCH (p:Person)RETURN p, p.name AS name, upper(p.name), coalesce(p.nickname,&quot;n/a&quot;) AS nickname, &#123; name: p.name, label:head(labels(p))&#125; AS person MATCH (n) RETURN DISTINCT n.name; 聚合函数 Cypher支持count, sum, avg, min, max 聚合的时候null会被跳过 count 语法 支持 count( distinct role ) 123456MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations// 收集聚合结果MATCH (m:Movie)&lt;-[:ACTED_IN]-(a:Person)RETURN m.title AS movie, collect(a.name) AS cast, count(*) AS actors 排序和分页 123MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a, count(*) AS appearancesORDER BY appearances DESC SKIP 3 LIMIT 10; Union 联合 12345MATCH (actor:Person)-[r:ACTED_IN]-&gt;(movie:Movie)RETURN actor.name AS name, type(r) AS acted_in, movie.title AS titleUNION （ALL）MATCH (director:Person)-[r:DIRECTED]-&gt;(movie:Movie)RETURN director.name AS name, type(r) AS acted_in, movie.title AS title With语句 with语句给Cypher提供了强大的pipeline能力，可以一个或者query的输出，或者下一个query的输入 和return语句非常类似，唯一不同的是，with的每一个结果，必须使用别名标识。 使用with我们可以在查询结果里面在继续嵌套查询。 1234MATCH (p:Person)-[:ACTED_IN]-&gt;(m:Movie)WITH p, count(*) AS appearances, COLLECT(m.Title) AS moviesWHERE appearances &gt; 1RETURN p.name, appearances, movies 有点类似SQL中的having，这里是with + where两个一起来实现的。 查询最短路径 12MATCH (ms:Person &#123; name: &quot;Node A&quot; &#125;),(cs:Person &#123; name:&quot;Node B&quot; &#125;), p = shortestPath((ms)-[r:Follow]-(cs)) RETURN p; 加载数据 Cypher Neo4j Couldn’t load the external resource neo4j初探 加载存在本地server上的数据，会在路径前面自动加个前缀 /path-to-neo4j/neo4j-community-3.4.5/import，即Server对应所在的路径下的import 12345678910111213141516// 加载addressLOAD CSV WITH HEADERS FROM &quot;file:///data/addresses.csv&quot; AS csvLineCREATE (p:Person &#123;id: toInt(csvLine.id), email: csvLine.address &#125;)// 加载emailLOAD CSV WITH HEADERS FROM &quot;file:///data/emails.csv&quot; AS csvLineCREATE (e:Email &#123;id: toInt(csvLine.id), time: csvLine.time, content: csvLine.content &#125;) // 创建收发关系USING PERIODIC COMMIT 500 // 分段加载LOAD CSV WITH HEADERS FROM &quot;file:///data/relations.csv&quot; AS csvLineMATCH (p1:Person &#123;id: toInt(csvLine.fromId)&#125;),(e:Email &#123; id: toInt(csvLine.emailId)&#125;),(p2:Person&#123; id: toInt(csvLine.toId)&#125;)CREATE UNIQUE (p1)-[:FROM]-&gt;(e)CREATE(e)-[:TO]-&gt;(p2) 如果需要导入其他地方的，可以使用 123456789LOAD CSV FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine[0], Name: csvLine[1]&#125;)// 使用csv中的header LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLineCREATE (:Genre &#123;GenreId: csvLine.Id, Name: csvLine.Track, Length: csvLine.Length&#125;) // 自定义csv文件中的分隔符LOAD CSV WITH HEADERS FROM &quot;https://path-to-csv&quot; AS csvLine FIELDTERMINATOR &quot;;&quot; 使用 neo4j-import 导入数据 使用neo4j-import导入数据 使用条件 需要先关闭neo4j 无法再原有的数据库添加，只能重新生成一个数据库 导入文件格式为csv 参数 —into：数据库名称 —bad-tolerance：能容忍的错误数据条数（即超过指定条数程序直接挂掉），默认1000 —multiline-fields：是否允许多行插入（即有些换行的数据也可读取） —nodes：插入节点 —relationships：插入关系 更多参数可允许命令bin/neo4j-import 1bin/neo4j-import --multiline-fields=true --bad-tolerance=1000000 --into graph.db --id-type string --nodes:person node.csv --relationships:related relation_header.csv,relation.csv 运行完后，将生成的graph.db放入data/databases，覆盖原有数据库，启动运行即可 3. References Neo4j的简单搭建与使用 Neo4j Tutorial Neo4j的查询语法笔记 官方文档：Comprehensive-Guide-to-Graph-Algorithms-in-Neo4j-ebook","tags":[{"name":"图数据库","slug":"图数据库","permalink":"http://chenson.cc/tags/图数据库/"},{"name":"Neo4j","slug":"Neo4j","permalink":"http://chenson.cc/tags/Neo4j/"}]},{"title":"深度学习笔记-RNN常见网络","date":"2018-05-25T11:10:06.000Z","path":"2018/05/25/深度学习笔记-RNN常见网络/","text":"说实话，只是大概了解一下这些常见的RNN模型，但是这些模型为什么这么设计，为什么能够抓取网络中可能有用的长短期信息其实我还是比较懵的。这一块以后也是需要去深入理解的一部分，先mark一下。 1. 循环神经网络 - RNN (Recurrent Neural Network) H_t = \\phi(X_tW_{xh} + H_{t-1}W_{hh} + b_h)上面的公式是RNN相关拓展网络的基础，基本都是用到了这个结构。 2. 门控循环单元 - GRN (Gated Recurrent Neural Networks)按时间顺序上来看，GRN是在LSTM之后提出来的。但是这个相对来说可能没那么复杂，所以先总结这个，然后再总结LSTM。 门控循环单元，重点是在于门的，即Gated。这里指的是 $\\sigma$ = sigmod func，通过$\\sigma$制造出来。 那么为什么要出现这些门呢，相比原先的RNN模型，这些门的出现能够解决什么样的问题？ 之前讨论过，RNN在(BPTT)计算的时候，梯度容易出现衰减或爆炸，特别是梯度衰减的问题。由于出现了梯度衰减，那么对于一些时间序列上，距离较长的数据，模型就比较难捕捉到这两个时间点上的关系。 模型设计如下： 重置门 - R (reset) 有助于捕捉时序数据中短期的依赖关系。 根据$sigmod$函数的性质，值介于0-1之间，可能丢弃过去的一些不相关的信息： R == 0，丢弃了上一轮的隐含层的信息（极端情况，下同）； R == 1，则保留相关的信息。 根据上面的模型图，我们可以看到重置门其实是有两个输入的，分别是： $X_t$ $H_{t-1}$ 即当前和上一轮的信息，则是短期内的信息。 R_t = \\sigma(X_t W_{xr} + H_{t-1}W_{hr} + b_r) 更新门 有助于捕捉时序数据中长期的依赖状态。 用于更新当前隐含层的状态，为了捕捉到之前比较久、长期之前的状态 Z_t = \\sigma(X_t W_{xz} + H_{t-1}W_{hz} + b_z) $Z_t$ == 0，则不保留之前的信息； $Z_t$ == 1，保留之前的信息。 候选隐含状态 \\tilde H= tanh(X_tW_{xh} + R_t \\bigodot H_{t-1}W_{hh} + b_h 当前隐含状态 H_t = Z_t\\bigodot H_{t-1} + (1 - Z_t) \\bigodot \\tilde H_t 问题：相比LSTM，有什么实质上的提升？ 3. 长短期记忆 - LSTM (Long and Short-Term Memory)结构比之前的GRU更为复杂一些，LSTM的隐含层包含隐含层变量H和记忆细胞C，它们的设计和形状相同（可以看到后面的几个func设计上其实是一样的。 LSTM引入了三个门（输入门、遗忘门和输出门），这里三个门的结构都是一样的。和两个信息部件（记忆细胞和候选记忆细胞） 输入门 短期记忆 I_t= \\sigma(X_t W_{xi} + H_{t-1}W_{hi} + b_i) 遗忘门 长期记忆 F_t= \\sigma(X_t W_{xf} + H_{t-1}W_{hf} + b_f) 输出门 O_t= \\sigma(X_t W_{xo} + H_{t-1}W_{ho} + b_o) 候选记忆细胞 Memory Cell \\tilde C_t = \\tanh(X_t W_{xc} + H_{t-1}W_{hc} + b_c) 记忆细胞 这里$\\bigodot$是矩阵里面的元素一一对应相乘，而$F_t$是遗忘门，经过$\\sigma$激活函数出来后，取值都是在[0,1]之间，和上一个记忆细胞$C_{t-1}$相乘的话，就能选择对应重要的信息保留下来。然后输入门和本次的候选记忆细胞相乘，选择本次相对重要的信息保留来，这两个相加，就是本次的记忆细胞。 (对信息进行筛选通过sigmoid之后的tensor和记忆细胞做元素乘实现，取出多少比例就代表保留多少信息) C_t = F_t \\bigodot C_{t-1} + I_t \\bigodot \\tilde C_t 当前隐含状态 H_t = O_t \\bigodot \\tanh(C_t) 问题：候选记忆细胞的激活函数为什么用tanh函数 (-1, 1) 之间？ 4. 深度循环神经网络5. 双向循环神经网络6. References 动手学深度学习-循环神经网络 RNN LSTM与GRU深度学习模型学习笔记","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"RNN","slug":"RNN","permalink":"http://chenson.cc/tags/RNN/"}]},{"title":"深度学习笔记-RNN","date":"2018-05-14T03:35:41.000Z","path":"2018/05/14/深度学习笔记-RNN/","text":"1. 一些预备知识1.1 Markov Chain 2. 循环神经网络 - RNN (Recurrent Neural Network)3. 门控循环单元 - GRN (Gated Recurrent Neural Networks)4. 长短期记忆 - LSTM (Long and Short-Term Memory)5. References","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.cc/tags/CNN/"}]},{"title":"深度学习笔记-CNN常见网络","date":"2018-04-27T02:32:41.000Z","path":"2018/04/27/深度学习笔记-CNN常见网络/","text":"上次笔记主要记录了卷积神经网络总卷积Block中的基本概念，本次笔记打算整理一下常见的CNN，从开山鼻祖的LeNet到目前比较流行的ResNet和DenseNet等。 1. LeNetLeNet是早起用来识别手写数字图像的卷积神经网络，算是卷积神经网络的开山鼻祖。 LeNet主要分为卷积层块和全连接层块两大部分。 1.1 卷积层块基本单位是卷积层后接最大池化层（实际上里面还可以额外的添加一些其他的模块），然后卷积层块就由这俩基本单位重复的堆叠构成。 卷积层 用来识别图像里的空间模式，识别一些局部的locally特征，比如线条和物体的局部。 卷积层的输入为4D数据，形状为(批量大小，通道，高，宽)，输出使用了Sigmoid激活函数。 最大池化层 用来降低卷积层对位置的敏感性，池化层注意设置好pool_size和strides大小。 1.2 全连接层块在Conv和Dense交界的部分，需要将输出的数据flatten才可以给Dense用。激活函数使用Sigmod 1.3 MxNet实现代码12345678910111213141516171819202122232425import d2lzh as d2limport mxnet as mxfrom mxnet import autograd, gluon, init, ndfrom mxnet.gluon import loss as gloss, nnimport time# 定义网络net = nn.Sequential()net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'), nn.MaxPool2D(pool_size=2, strides=2), nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'), nn.MaxPool2D(pool_size=2, strides=2), # Dense会默认将(批量大小,通道,高,宽)形状的输入转换成(批量大小,通道 * 高 * 宽)形 # 状的输入 nn.Dense(120, activation='sigmoid'), nn.Dense(84, activation='sigmoid'), nn.Dense(10))# 训练部分(有些代码不完整，详情见沐神Github代码)lr, num_epochs = 0.9, 5# 其实这里想展示的是net权重的初始化时，使用的是Xavier随机初始化net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())trainer = gluon.Trainer(net.collect_params(), 'sgd', &#123;'learning_rate': lr&#125;)train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs) 运行结果 下图有俩结果，一个是沐神笔记里面用K80跑的结果，一个是我笔记本跑的结果，配置是2015年的中配RMBP，cpu是i5 8G，大概速度差了10倍左右。 调参练习 这部分呢，其实就是找一下在深度学习中调参的一些感觉，毕竟我只是一枚炼丹工程师。 改为普通的随机初始化网络权重可以看到下图基本不收敛。 接着尝试一下把lr给调小一些，调到0.05，似乎也没啥提升，继续调小到0.005，且增加num_epochs到20，基本也没提升。然后想想我是改了一个较为合适的权重，所以应该把lr调高一些，方便快速调整到合适的权重，就把lr提高到1.5，用的随机初始化，一直等到epoche到了10左右的时候acc才开始提升，可以说收敛得非常慢了。看来这个权重的初始化非常重要，需要单独调研一下，到时候看啥原因。 往小调 往大调 其实lr调太小了(0.05/0.005, 原先是0.9)，虽然改回去Xavier初始化权重也基本是上面效果，估计是太小了不收敛？若是调到0.5的话，可以收敛，但相比速度上会收敛得慢一些，所以选定一个合适的lr也是很重要的。 调整卷积窗口的大小 调整输出通道数 调整激活函数 调整全连接层输出个数 2. AlexNet（2013） 总共六个阶段 kernel，strides相对大很多 Xavier是怎么初始化的？与默认的区别？ LeNet V.S AlexNet 3. VGG使用重复元素的非常深的网络 使用很多相对加多的kernel，比如3x3的卷积，然后接上一个池化层，之后再将这个模块重复很多次。 VGG Block 相同叠加 VGG Stack 相似叠加 VGG11, VGG13, VGG16, VGG19 … … 4. Bath Norm批量归一化，将数据均值变为0，方差变为1，使数值更加稳定。 $\\lambda$ $\\beta$ 全连接2D数据 每个样本之间，均值变为0，方差变为1 如果是卷积3D数据 对每个channel做变换，使得均值变为0，方差变为1 Q&amp;A: batchNorm 加的位置？ 为什么是Conv2D的后面，Activation的前面，而不是Conv2D的前面？ 每个batch之间的归一化不一样？ 训练的时候ok，测试的时候如何做归一化？ 保留训练时候算出来的结果做归一化 mean, variance, $\\lambda$ , $\\beta$ 5. Network in Networ (NiN) AlexNet 卷积层和全连接层分别加深加宽从而得到深度网络 NiN 串联数个卷积层块和全连接层块来构建深度网络 Q&amp;A： 如何解决Conv和Dense之间输入维度的问题，比如Conv的输入为4D，但是Dense可能无法输出如此维度的数据 把Dense层换成kernel为1x1的Conv 4D数据到2D数据通常会编程非常大 6. GoogLeNet(2014) Inception 7. References","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.cc/tags/CNN/"}]},{"title":"深度学习笔记-CNN","date":"2018-04-20T06:45:37.000Z","path":"2018/04/20/深度学习笔记-CNN/","text":"最近正在学习李沐沐神的动手学深度学习，目前听到了第五课，真的是非常好的课程。其实在学习之前也有断断续续的学过一些深度学习相关的东西，也会用PyTorch跑一些简单的模型，但仅限于此，对理论知识理解还是比较浅薄的，只是一个调包侠，所以这次学习打算认真学习一下相关的理论知识。 在学之前也有听了一下NG刚发布的深度学习课程，但对比了一下，还是觉得沐神的比较适合我，毕竟教学用的是中文，方便理解，且理论知识深度合适，所以打算先听完这门课，以后理论方面有需要的，再听NG相关的课程。 1. 卷积神经网络 CNN - (Convolutional Neural Network)1.1 神经网络 - Neural Network 单个神经元 神经网络 1. 2 卷积神经网络的基本结构基本结构可以分为三个block 输入层 卷积层+激活函数+池化层 输出层 输入层 —-&gt; 卷积层 —-&gt; 激活函数 —-&gt; 池化层 —-&gt; 全连接层 中间三个部分可以看做是一个block，复杂的模型可以重复多个这样的block，里面参数可以是一样或者相似的。 1. 3 卷积 - Convolutional相对普通的神经网络，卷积神经网络在中的核心就是在于卷积计算层了，卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。对于如何理解卷积，裂墙推荐知乎上的这篇文章通俗理解『卷积』——从傅里叶变换到滤波器，从信号处理这个本质的角度来理解卷积，非常棒👍。 卷积的意思就是，神经网络是对图片上的一小块区域进行处理，这种做法加强了图片信息的连续性，使得神经网络可以看到图片上的图形，而非是一个个离散的点。下图是一个二维数组的卷积运算，中间的核是卷积中的kernel。 根据图片示意，我们将输入层蓝色的框框部分与kernel相乘，有： 0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19当然此时只运算了一小部分，左边的输入数组中，蓝色的四格小框框可以逐步的往右边一格一格的挪动，然后到达最右边后，又往下移动一个格子，从左往右重复上面的运算，即有最右边的输出矩阵 0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19 \\\\ 1 \\times 0 + 2 \\times 1 + 4 \\times 2 + 5 \\times 3 = 25 \\\\ 3 \\times 0 + 4 \\times 1 + 6 \\times 2 + 7 \\times 3 = 37 \\\\ 4 \\times 0 + 5 \\times 1 + 7 \\times 2 + 8 \\times 3 = 43以下是一个方便理解的动图 以上就是卷积最基本的运算过程。在这个过程中，我们有一下几个假设 kernel size 的大小 在上述我们假设了kernel的size是 $2 \\times 2$ 的，当然也可以是其他维度的kernel，比如说 $3 \\times 3$ 之类的，或者是 $4 \\times 5$ 这样的非正方形矩阵 周围的填充值 padding 其实这一部分上图中并没有体现出来这假设，padding的中文意思是填充，顾明思议就是在输入的二维矩阵的高和宽两侧填充元素（通常是0）。比如下图 这里假设padding为0，即在给定输入的二维矩阵的高和宽两侧分别填充了一圈0元素（是否可以是两圈或者多圈？）。 相比上一张图，我们可以看到输出的二维矩阵也变大了一圈。 其实对于给定的输入的二维矩阵、kernel大小，我们可以计算出输出矩阵的大小的 没有设定padding (n_h - k_h + 1)/1 \\times (n_w - k_w + 1)/1这里h代表的是高度、w代表了宽度： 输出宽度 = 输入宽度 - kernel宽度 + 1 输入高度 = 输入高度 - kernel高度 + 1 设定了padding (n_h - k_h + p_h + 1)/1 \\times (n_w - k_w + p_w + 1)/1 输出宽度 = 输入宽度 - kernel宽度 + 填充的宽度 + 1 输入高度 = 输入高度 - kernel高度 + 填充的高度 + 1 这里回答一下上面我括号内加粗的问题，当给周围填充一圈0的时候，padding的值为 $p_w \\&amp; p_h = 1$，当两圈0的时候为 $ p_w \\&amp; p_h=2 $。通常对于padding的取值为： p_w = k_w - 1 \\\\ p_h = k_h - 1这样我们就可以将输出的矩阵变大，尽量保持和和输入大小比较相近，避免多层卷积的时候，矩阵越来越小，相当于信息越来越少。 如果需要保持输入和输出相同的大小，则可以采用如下设定： kernel大小为奇数，在矩阵的高宽两侧都填充p值 kernel大小为偶数，在矩阵的高宽一侧中填充p值 滑动的步长 stride 这里我们假设输入框框从左往右和从上到下的移动步伐是1个小格子，实际上也可以是2个小格子，3个小格子等，这个参数在CNN中叫做stride。下图stride的大小为(2, 3)，即向右滑动的时候，每次2个格子，向下滑动的时候，每次3个格子。 注意到了在padding部分有一个常数项为1了吗，其实那个就是我们设定的stride的滑动大小，如果stride不为(1, 1)的时候，此时输入大小计算公式为： (n_h - k_h + p_h + s_h)/s_h \\times (n_w - k_w + p_w + s_w)/s_w 1.4 激活函数先说结论，激活函数主要的作用是加入非线性因素，因为线性的表达能力不够（如何体现？）。 其实从上面卷积的计算过程中，过程大致可以理解为： 从一个输入的矩阵中抽取出一个局部矩阵，然后跟kernel线性相乘，然后输出一个结果值。通过从左向右，从上至下的滑动，遍历了整个输入矩阵的。 上述过程中，一个很重要的概念是线性相乘，卷积的操作其实是线性的。此时加入激活函数可以给模型加入非线性因素，且激活函数需要具备以下几个性质 非线性 线性激活层对于深层神经网络没有作用，因为其作用以后仍然是输入的各种线性变换 连续可微 因为神经网络使用的梯度下降法 范围最好不饱和（不是很理解） 当有饱和的区间段时，若系统优化进入到该段，梯度近似为0，网络的学习就会停止。 单调性 当激活函数是单调时，单层神经网络的误差函数是凸的，好优化 在原点处近似线性 样当权值初始化为接近0的随机值时，网络可以学习的较快，不用可以调节网络的初始值 常用的几个激活函数有： Sigmoid 在CNN卷积部分中基本被淘汰，因为饱和时梯度值非常小，当层数比较多的时候，用BP算法方向传播的时候，靠近前面层部分基本得不到更新，即梯度耗散。且输出的值不是以0为中心。 Tanh 如下图，基本缺点同Sigmoid函数 ReLu （最常用的激活函数） Alex在2012提出AlexNet时候提出的 当x&gt;0的时候，梯度恒为1，不存在梯度耗散的问题，收敛快。 当x&lt;0的时候，输出为0，增加网络的稀疏性，这样提取出的特征具有代表性，泛化性强。（如何保证x&lt;0的部分值是无意义的？） 缺点就是具有死亡特性（有点感觉，但不是很理解），需要注意调整Learning Rate来避免死亡节点过多。 Leaky ReLu 改散了ReLU的死亡特性，同时也损失了一部分稀疏性，增加一个超参。 1.5 池化层 - Pooling先说结论：池化层的几个主要作用： 缓解了卷积层对位置的过度敏感性 能够压缩图片，使特征图变小，同时保留了有用的信息，提取主要特征 池化大致就是一个筛选过滤的过程，能过将左边的输出层中有用的信息筛选出给到输出层。计算方式和kernel的计算方式比较相似，也是给定一个(w, h)大小的pooling矩阵，从左向右，从上至下缓慢滑动计算输出一个值给到输出层。与kernel不同的时候，这不做矩阵运算，只是计算输入矩阵中当前局部矩阵(w, h)中的最大值或是均值，对应使用的就是最大池化或是均值池化。（滑动的大小时候如何设定？） 回答一下上述括号中的问题，池化层同样也有padding和stride的概念，方式和卷积中比较相似，就不一一解释了。 1.6 其他一些概念 感受野 - Receptive Field 中文名真的是… … 太太太难理解了，直接看英文名Receptive Field比较直观一些。 基本定义就是 卷积神经网络的每一层输出的特征图（Feature ap）上的像素点在原图像上映射的区域大小。 第一层卷积 输入为10x10的图片，经过3x3的kernel，输出为8x8的大小（这里只考虑单层） 且在output1中的每一个像素点，都受到原始图像对应的3x3区域内的影响，第一场的感受野为3，用字母表示RF1=3 第二层卷积 在第一层卷积的输出output1下，经过第二层3x3的kernel卷积，输出的大小为6x6 如果从output2往回推的话，output2上的一个像素点，受到output1上三个像素点的影响，而这三个像素点又总共受到输入层五个像素点的影响，所以第二层的感受野RF2=5 第三层卷积 此时kernel3依旧为3x3，根据上面推，RF3=7 第四次卷积 第五次池化运算 通道 - Channel 多输入通道 上面我们讨论的都是二维的数据矩阵，其实在图像处理过程中，图片通常都是三维的。除了图片的高度和宽度外，我们可以从颜色的角度，分为RGB三个颜色通道。即一个三维的矩阵，这里的第三维，我们称之为Channel，可以说的相互独立的(计算用到的kernel可以是独立的)。 依旧是以上面的例子为例，从Channel一维升级到二维，计算过程如下： channel_1:1 \\times 1 + 2 \\times 2 + 3 \\times 4 + 5 \\times 4 = 37 \\\\ channel_2:0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 4 = 19 \\\\ output = 37 + 19 = 56 多输出通道 上面的例子中，输出通道为1，即最终对每个独立channel运算的通道输出的结果做累加运算。 如果是需要输出多通道呢？相似的，我们在计算独立channel的时候，就输出一个多通道的的输出，然后在不同channel之间，继续做累加运算。 $1 \\times 1$ 卷积层 这个卷积层很有意思，沐神在上课的过程中多次提到了该卷积。 当kernel的size为(1, 1)的时候，说明此时kernel不对输入层做采样然后计算，即失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。 可以说此时一开始的二维层面上，kernel是没啥用的，但是在第三维channel的角度还是有用的，即我们可以用1x1的矩阵做维度变换。 上图中，输入通道的维度为(3, 3, 3)， kernel的维度为(2, 1, 1), 此时的输出通道为(2, 3, 3)。 具体计算过程如下 1234567def corr2d_multi_in_out_1_times_1(X， K)：c_i, h, w = X.shape # 3, 3, 3 c_o = K.shape[0] # 2X = X.reshape((c_o, h * w)) # (2, 3 * 3)K = K.reshape((c_o, c_i)) # (2, 3)Y = nd.dot(K, X) # 全连接层的矩阵乘法return Y.reshape((c_o, h, w)) # (2, 3, 3) 2. PyTorch代码实现对不起沐神，我还是用的PyTorch，而不是MxNet。 🤦‍♂️ 🤦‍♂️ 🤦‍♂️ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class CNN(nn.Module): def __init__(self, in_dim, n_class): super(CNN, self).__init__() # 表示将一个有序的模块写在一起 # 相当于神经网络的层按顺序放一起方便结构显示 self.conv = nn.Sequential( # 卷积层，有五个参数： # in_channels: 表示的是输入卷积层的图片厚度 # out_channels: 比偶奥数的是要输出的厚度 # kernel_size: 表示的是卷积核的大小，一个数字的话表示长宽相等的卷积核 # stride: 表示卷积核滑动的步长 # padding: 表示在图片走位填充0的多少，padding=0表示不填充，padding=1四周都填充1维 nn.Conv2d(in_dim, 6, 3, stride=1, padding=1), # 激活函数，里面有一个参数inplace # False，表示新创建一个对象对其修改 # True，表示直接对这个对象进行修改 nn.ReLU(True), # 最大池化层，也有平均池化层等，里面的参数有: # kernel_size: 表示池化的窗口的大小，和卷积里面的kernel_size是一样的 # stride: 也和卷积层里面一样，需要自己设置滑动步长 # padding 和卷积层一样，默认是0 nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5, stride=1, padding=0), nn.ReLU(True), nn.MaxPool2d(2, 2) ) # 全连接层 self.fc = nn.Sequential( nn.Linear(400, 120), nn.Linear(120, 84), nn.Linear(84, n_class) ) def forward(self, x): out = self.conv(x) # batch_size out = out.view(out.size(0), -1) out = self.fc(out) return out # 定义loss和optimizercriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 # 开始训练 for epoch in range(num_epoches): print('epoch &#123;&#125;'.format(epoch + 1)) print('*' * 10) running_loss = 0.0 running_acc = 0.0 for i, data in enumerate(train_loader, 1): img, label = data if use_gpu: img = img.cuda() label = label.cuda() img = Variable(img) label = Variable(label) # 向前传播 out = model(img) loss = criterion(out, label) running_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() accuracy = (pred == label).float().mean() running_acc += num_correct.data[0] # 向后传播 optimizer.zero_grad() loss.backward() optimizer.step() # ========================= Log ====================== step = epoch * len(train_loader) + i # (1) Log the scalar values info = &#123;'loss' : loss.data[0], 'accuracy' : accuracy.data[0]&#125; for tag, value in info.items(): logger.scalar_summary(tag, value, step) # (2) Log values and gradients of the parameters (histogram) for tag, value in model.named_parameters(): tag = tag.replace('.', '/') logger.histo_summary(tag, to_np(value), step) logger.histo_summary(tag + '/grad', to_np(value.grad), step) # (3) Log the images info = &#123;'images': to_np(img.view(-1, 28, 28)[:10])&#125; for tag, images in info.items(): logger.image_summary(tag, images, step) if i % 300 == 0: print('[&#123;&#125;/&#123;&#125;] Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, num_epoches, running_loss / (batch_size * i), running_acc / (batch_size * i))) print('Finish &#123;&#125; epoch, Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( epoch + 1, running_loss / (len(train_dataset)), running_acc / (len( train_dataset)))) model.eval() eval_loss = 0 eval_acc = 0 for data in test_loader: img, label = data if use_gpu: img = Variable(img, volatile=True).cuda() label = Variable(label, volatile=True).cuda() else: img = Variable(img, volatile=True) label = Variable(label, volatile=True) out = model(img) loss = criterion(out, label) eval_loss += loss.data[0] * label.size(0) _, pred = torch.max(out, 1) num_correct = (pred == label).sum() eval_acc += num_correct.data[0] print('Test Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format(eval_loss / (len( test_dataset)), eval_acc / (len(test_dataset)))) print()# 保存模型torch.save(model.state_dict(), './model/cnn.pth') 3. References 莫烦 - 什么是卷积神经网络 CNN (Convolutional Neural Network) 什么是感受野 通俗理解『卷积』——从傅里叶变换到滤波器 一文读懂卷积神经网络CNN 一句话CNN：如何理解padding的作用和算法 CNN入门讲解：什么是激活函数（Activation Function）","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"},{"name":"CNN","slug":"CNN","permalink":"http://chenson.cc/tags/CNN/"}]},{"title":"Mac系统下matplotlib显示中文","date":"2018-04-10T07:08:20.000Z","path":"2018/04/10/Mac系统下matplotlib显示中文/","text":"1. Mac系统下让matplotlib显示中文现在的时间是2018.04.10，更新一下Mac系统下的解决方法 我的环境：anaconda3 + Python 3.6.3 添加字体 添加 SimHei 字体（simhei.ttf文件）到 ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf 中； 下载地址：黑体字体simhei.ttf 我用的是 anaconda3 下的 python 环境，这个地址对应你正在使用的 python 安装地址 ​ 修改matplotlib配置文件 12cd ~/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-datavi matplotlibrc # 编辑配置文件 找到 font.sans-serif 添加 SimHei 到字体列表 （如图，大约在211行） 同时修改 axes.unicode_minus，将 True 改为 False，作用就是解决负号’-‘显示为方块的问题（大约在330行） 删除缓存文件 Mac 系统下删除 ~/.matplotlib/ 下的所有缓存文件 12rm -rf ~/.matplotlib/*.cacherm -rf ~/.matplotlib/fontList.json Linux、CentOS 删除 ~/.cache/matplotlib目录下的两个缓存文件（同上） 注意 rm -rf 命令，确认路径没错在用 ​ 画图测试 未修改配置文件，需要添加如下代码： 1234567891011121314#coding:utf-8 import matplotlib #指定默认字体 matplotlib.rcParams['font.sans-serif'] = ['SimHei'] matplotlib.rcParams['font.family']='sans-serif' #解决负号'-'显示为方块的问题 matplotlib.rcParams['axes.unicode_minus'] = False from matplotlib.font_manager import _rebuild_rebuild()plt.plot([-1,2,-5,3]) plt.title(u'中文',fontproperties=myfont) plt.show() ​ 如果已经修改了 matplotlib 配置文件，则不需要上述代码，直接画图即可。 ​ 2. References 彻底解决matplotlib中文乱码问题 matplotlib图例中文乱码?","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"http://chenson.cc/tags/Jupyter/"}]},{"title":"PyTorch学习笔记","date":"2018-03-22T07:52:02.000Z","path":"2018/03/22/PyTorch学习笔记/","text":"趁着公司最近不是很忙，认真学习一下这两年比较火的PyTorch，虽然没有实打实的、认认真真的学过深度学习，但也稍微懂一点。打算先简单学习一下基本操作，搭个简单的DNN、CNN之类的。看看后面有没有时间，然后再上个吴恩达或者李飞飞的课程，补充一下理论知识。所以这篇文章就记录一些简单的概念和上手过程中可能遇到的问题。 1. 损失函数1.1 二分类问题损失函数用 criterion = nn.BCELoss() 此时，y的类型应该为float类型， 且y_pred用的时候需要在该层前面加上 Sigmoid 函数 123456import torch.nn.functional as Floss_fn = torch.nn.BCELoss(reduce=False, size_average=False)y_true = Variable(torch.randn(3, 4)) # 限制了target类型为floaty_pred = Variable(torch.FloatTensor(3, 4).random_(2))loss = loss_fn(F.sigmoid(y_pred), y_true) 1.2 多分类的问题 (&gt;= 2，二分类为特殊的多分类)损失函数用 nn.CrossEntropyLoss, 多分类用的交叉熵损失函数，用这个 loss 前面不需要加 Softmax 层 123456789# 类别之间的样本均衡问题weight = torch.Tensor([1, 2, 1, 1, 10])loss_fn = torch.nn.CrossEntropyLoss(reduce=False, size_average=False, weight=weight) # 考虑到样本不均衡的问题y_pred = Variable(torch.randn(3, 5)) y_true = Variable(torch.FloatTensor(3).random_(5)) # 限制了target类型为longloss = loss_fn(y_pred, y_true) 1.3 其他常见的损失函数2. 优化器优化器的选择 3. 遇到的问题 Learnning Rate 之前因为接触深度学习比较少，所以对深度学习的调参不是很熟悉，所以遇到了这个坑。之前在搭一个比较简单的DNN的时候，发现函数一直不会收敛，找了好久发现是Learning rate设置太大了，改小了就比较好了，从0.05改到0.001效果就比较好。 然后更改网络结构的时候，也是遇到类似的问题，输出经常都是为同一个值，可以尝试一下把学习率增大或者调小试试看，多试几个值。 优化函数 损失函数 4. 可视化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import torch.nn.functional as f# 建造数据集data = torch.ones((100, 2))x0 = torch.normal(2 * data, 1)y0 = torch.zeros(100) # y0是标签 shape(100,),是一维x1 = torch.normal(-2 * data, 1)y1 = torch.ones(100) # y1也是标签 shape(100,)，是一维x = torch.cat((x0, x1), 0).type(torch.FloatTensor)y = torch.cat((y0, y1), 0).type(torch.LongTensor)x, y = Variable(x), Variable(y) # 训练神经网络只能接受变量输入，故要把x, y转化为变量plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], # 这两个参数分别代表x,y轴坐标 c=y.data.numpy(), s=100, cmap='RdYlGn') # c为color,y有两种标签，代表两种颜色的点，'RdYlGn'红色和绿色plt.show()# 建造神经网络模型class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) self.out = torch.nn.Linear(n_hidden, n_output) def forward(self, x): x = f.relu(self.hidden(x)) y = self.out(x) return y# 训练神经网络模型并将训练过程可视化net = Net(n_feature=2, n_hidden=10, n_output=2)optimizer = torch.optim.SGD(net.parameters(), lr=0.02)loss_func = torch.nn.CrossEntropyLoss()plt.ion()for i in range(100): out = net(x) loss = loss_func(out, y) optimizer.zero_grad() loss.backward() optimizer.step() # 绘图 if i % 10 == 0: plt.cla() # torch.max(a,1) 返回每一行中最大值的那个元素，且返回其索引（返回最大元素在这一行的列索引 # f.softmax(out）是将out的内容以概率表示。 # torch.max()返回的是两个Variable，第一个Variable存的是最大值，第二个存的是其对应的位置索引index。这里我们想要得到的是索引，所以后面用[1]。 prediction = torch.max(f.softmax(out), 1)[1] pred_y = prediction.data.numpy().squeeze() target_y = y.data.numpy() plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[ :, 1], c=pred_y, s=100, cmap='RdYlGn') accuracy = sum(pred_y == target_y) / len(y_train) print('&gt;&gt;&gt; accuracy :', accuracy) plt.text(1.5, -4, 'accuracy=%.2f' % accuracy, fontdict=&#123;'size': 10, 'color': 'red'&#125;) plt.pause(0.1)plt.ioff()plt.show() 5. Utils in PyTorch12345678910111213141516171819202122232425262728293031323334def _reshape(df_meta, features, label='14d', shape=(6, 4, 32)): \"\"\"将一维的数据转换成多维的\"\"\" df = df_meta.reset_index(drop=True) matrix, label = [], [] for i in range(df.shape[0]): x = df.iloc[i, :][features] y = df.iloc[i, :][features] m = np.array(x).reshape(shape) matrix += [m] label += [y] return np.array(matrix), np.array(label)def data2tensor(X, y=None, x_dtype='float32', y_dtype='float32'): \"\"\"将训练用的X和y合并成一个TonsorDataset\"\"\" X2 = torch.from_numpy(np.array(X).astype(dtype=x_dtype)) if y is None: return TensorDataset(X2) else: y2 = torch.from_numpy(np.array(y).astype(dtype=y_dtype)) return TensorDataset(X2, y2)def df2var(df, dtype='float32'): \"\"\"把df转成PyTorch中的variable\"\"\" from torch.autograd import Variable return Variable(torch.from_numpy(np.array(df).astype(dtype=dtype)))def adjust_learning_rate(optimizer, epoch): \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\" lr = args.lr * (0.1 ** (epoch // 30)) for param_group in optimizer.param_groups: param_group['lr'] = lr 6. References Pytorch Loss Function 总结 PyTorch中的Loss Fucntion","tags":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://chenson.cc/tags/PyTorch/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chenson.cc/tags/Deep-Learning/"}]},{"title":"Docker常用指令","date":"2018-03-13T05:12:33.000Z","path":"2018/03/13/Docker常用指令/","text":"1. docker镜像构建12cd analyze-serverdocker build -t analyze-server . 2. 创建12345docker create analyze-server # 创建容器，但处于停止状态docker run analyze-server # 创建并启动容器# 启动容器，并挂载文件docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server 3. 查看12docker ps # 查看当前运行的容器docker ps -a # 查看所有的容器，包括停止的 4. 启动1docker start [NAME]/[CONTAINER ID] # 通过docker start来启动之前已经停止的docker_run镜像 12docker exec -it analyze /bin/bash # 在运行的容器中执行后面的指令docker exec -it -u root spark-notebook /bin/bash # 以root用户运行指令 交互型容器：运行在前台，容器中使用exit命令或者调用docker stop、docker kill命令，容器停止。 docker -it --name analyze -i -t -i 打开容器的标准输入 -t 告诉docker为容器创建一个命令行终端 --name analyze 指定容器名称，可以不填(随机) analyze-server 告诉我们使用什么镜像来启动容器 /bin/bash 告诉docker要在容器里面执行此命令 后台型容器：运行在后台，创建后与终端无关，只有调用docker stop、docker kill命令才能使容器停止。 1docker run --name=analyze -p 5000:5000 -p 8888:8888 -p 6006:6006 -v /Users/Chenson/Github/analyze-server/app/:/analyze-server/app/ -v /Users/Chenson/:/home/ -d analyze-server -d 使容器在后台运行。 -c 可以调整容器的CPU优先级。默认情况下，所有的容器拥有相同的CPU优先级和CPU调度周期，但你可以通过Docker来通知内核给予某个或某几个容器更多的CPU计算周期。比如，我们使用-c或者–cpu-shares =0启动了C0、C1、C2三个容器，使用-c/–cpu-shares=512启动了C3容器。这时，C0、C1、C2可以100%的使用CPU资源（1024），但C3只能使用50%的CPU资源（512）。如果这个主机的操作系统是时序调度类型的，每个CPU时间片是100微秒，那么C0、C1、C2将完全使用掉这100微秒，而C3只能使用50微秒。 -c 后的命令是循环，从而保持容器的运行 5. 停止12docker stop [NAME]/[CONTAINER ID] # 退出容器docker kill [NAME]/[CONTAINER ID] # 强制停止一个容器 6. 删除12345docker rm [NAME]/[CONTAINER ID] # 不能够删除一个正在运行的容器，会报错。需要先停止容器docker rm 'docker ps -a -q' # 一次性删除所有容器 # -a标志列出所有容器， # -q标志只列出容器的ID，然后传递给rm命令，依次删除容器 7. References Docker 简易教程 Docker 命令大全","tags":[{"name":"docker","slug":"docker","permalink":"http://chenson.cc/tags/docker/"}]},{"title":"Hadoop2.7.4完全分布式集群搭建和测试","date":"2017-10-10T05:24:58.000Z","path":"2017/10/10/Hadoop2-7-4完全分布式集群搭建和测试/","text":"1. 环境配置1.1 环境说明 1.2 修改机器名称和hosts等 vi /etc/sysconfig/network 1234HOSTNAME=hadoop-masterHOSTNAME=hadoop-salve1HOSTNAME=hadoop-salve2HOSTNAME=hadoop-salve3 执行 reboot 后生效 sudo vi /etc/hostname 1234# 相应修改三台机器hadoop-masterhadoop-slave1hadoop-slave2 sudo vi /etc/hosts 1234567127.0.0.1 localhost localhost.localdomain VM-0-6-ubuntu# Hadoop Cluster# 【注意】：用内网IP，若用公网IP，则无法启动master上的9000监听端口172.17.6 hadoop-master172.17.11 hadoop-salve1 172.17.7 hadoop-salve2 1.5. SSH无密码验证配置 安装 ssh 123sudo apt-get install openssh-serverps -e | grep &quot;ssh&quot;ssh localhost 生成密钥 pair 12345678910# 查看权限ls -aldrwxr-x--x 2 root root 4096 Dec 23 2015 .ssh# 给用户权限# sudo chown ubuntu .sshchmod 700 .ssh # 生成密钥ssh-keygen -t rsa 在 master 上导入 authorized_keys 123456789# 重要sudo chmod 700 .ssh sudo chmod 640 .ssh/authorized_keyssudo chown $USER .sshsudo chown $USER .ssh/authorized_keyscat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# 同时给其他的 salver 在各个机器验证一下 2. 所需软件2.1 JDK软件tutorial 1 安装 JDK 123456sudo add-apt-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installersudo apt-get install openjdk-8-jdksudo apt-get install openjdk-8-jre 配置环境变量 vi /etc/profile 123456789101112131415# JAVA# 1. AWS EC2 Ubuntu 16export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64# 2. 腾讯云 Ubuntu 14export JAVA_HOME=/usr/lib/jvm/java-8-oracle# 3. MAC OSexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Homeexport JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 执行命令使之生效 source /etc/profile 2.2 Hadoop 软件 下载软件 123wget http://apache.uberglobalmirror.com/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gztar xvf hadoop-2.7.4.tar.gz 配置 vi ~/.bashrc 设置 Hadoop 的环境变量 1234export HADOOP_HOMEexport PATHexport HADOOP_CONF_DIRexport YARN_CONF_DIR 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# ~/.bashrc: executed by bash(1) for non-login shells.# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)# for examples# If not running interactively, don't do anythingcase $- in *i*) ;; *) return;;esac# don't put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don't overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern \"**\" used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)[ -x /usr/bin/lesspipe ] &amp;&amp; eval \"$(SHELL=/bin/sh lesspipe)\"# set variable identifying the chroot you work in (used in the prompt below)if [ -z \"$&#123;debian_chroot:-&#125;\" ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we \"want\" color)case \"$TERM\" in xterm-color|*-256color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the prompt#force_color_prompt=yesif [ -n \"$force_color_prompt\" ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it's compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ \"$color_prompt\" = yes ]; then PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ 'else PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h:\\w\\$ 'fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase \"$TERM\" inxterm*|rxvt*) PS1=\"\\[\\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h: \\w\\a\\]$PS1\" ;;*) ;;esac# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' alias grep='grep --color=auto' alias fgrep='fgrep --color=auto' alias egrep='egrep --color=auto'fi# colored GCC warnings and errors#export GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'# some more ls aliasesalias ll='ls -alF'alias la='ls -A'alias l='ls -CF'# Add an \"alert\" alias for long running commands. Use like so:# sleep 10; alertalias alert='notify-send --urgency=low -i \"$([ $? = 0 ] &amp;&amp; echo terminal || echo error)\" \"$(history|tail -n1|sed -e '\\''s/^\\s*[0-9]\\+\\s*//;s/[;&amp;|]\\s*alert$//'\\'')\"'# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don't need to enable# this, if it's already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi# HADOOPexport HADOOP_HOME=/home/ubuntu/workdir/hadoop-2.7.4export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport CLASSPATH=$CLASSPATH:$HADOOP_HOME/bin# JAVAexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib# LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/home/ubuntu/workdir/hadoop-2.7.4/lib/native 1export CLASSPATH=$CLASSPATH:/home/ubuntu/workdir/hadoop-2.7.4/etc/hadoop:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-auth-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/gson-2.2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jsch-0.1.54.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/common/hadoop-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/activation-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-client-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-api-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-registry-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.4-tests.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar:/home/ubuntu/workdir/hadoop-2.7.4/contrib/capacity-scheduler/*.jar 在 $HADOOP_CONF_DIR 中设置 JAVA_HOME 123vi /home/hadoop-2.7.4/etc/hadoop/hadoop-env.sh 设置JAVA_HOMEvi /home/hadoop-2.7.4/etc/hadoop/mapred-env.sh 设置JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 vi slaves 这里面填写的全是slaves 12hadoop-slave1hadoop-slave2 vi $HADOOP_CONF_DIR/core-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;!-- 注意这里的区别 一个适用于单机 一个适用于集群 --&gt; &lt;!-- 单机 --&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;!-- 集群 master --&gt; &lt;value&gt;hdfs://hadoop-master:9000&lt;/value&gt; &lt;description&gt;设定namenode的主机名及端口(建议不要更改端口号)&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;description&gt; 设置缓存大小 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop-2.7.4/tmp&lt;/value&gt; &lt;description&gt; 存放临时文件的目录 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/hdfs-site.xml 1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/name&lt;/value&gt; &lt;description&gt; namenode 用来持续存放命名空间和交换日志的本地文件系统路径 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/workdir/hadoop-2.7.4/hdfs/data&lt;/value&gt; &lt;description&gt; DataNode 在本地存放块文件的目录列表，用逗号分隔 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;description&gt; 设定 HDFS 存储文件的副本个数，默认为3 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; vi $HADOOP_CONF_DIR/mapred-site.xml (复制mapred-site.xml.template,再修改文件名) 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;hadoop-master:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop-master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://hadoop-master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi $HADOOP_CONF_DIR/yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;!-- master node 的名字 --&gt; &lt;!-- 单机和集群的区别？？ --&gt; &lt;value&gt;hadoop-master&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;hadoop-master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;hadoop-master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;hadoop-master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;hadoop-master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;hadoop-master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345dfs.nameservices —– HDFS NN的逻辑名称，使用上面设置的myhdfsdfs.ha.namenodes.myhdfs —– 给定服务逻辑名称myhdfs的节点列表dfs.namenode.rpc-address.myhdfs.nn1 —– myhdfs中nn1节点对外服务的RPC地址dfs.namenode.http-address.myhdfs.nn1 —– myhdfs中nn1节点对外服务的http地址dfs.namenode.shared.edits.dir —– 设置一组 journalNode 的 URI 地址，active NN 将 edit log 写入这些 12345在每个节点上创建数据存储目录/home/hadoop-2.7.4/hdfs 用来存放集群数据。在主节点node上创建目录/home/hadoop-2.7.4/hdfs/name 用来存放文件系统元数据。在每个从节点上创建目录/home/hadoop-2.7.4/hdfs/data 用来存放真正的数据。所有节点上的日志目录为/home/hadoop-2.7.4/logs所有节点上的临时目录为/home/hadoop-2.7.4/tmp 1上面的配置只需要在master中配好, 然后复制到其他的slaves节点中中去 格式化 namenode 和 datanode（在 master 上执行就可以了 不需要在 slaves 上执行） 12hdfs namenode -formathdfs datanode -format 分别在 master 和 slaves 中用 jps 查看进程","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"}]},{"title":"同步异步IO","date":"2017-09-29T03:54:40.000Z","path":"2017/09/29/同步异步IO/","text":"1. 大致区别所谓的同步和异步的区别，指的是Application和kernel之间的交互方式。 如果Application不需要等待kernel的回应，那么它就是异步的（kernel会发信号通知）。 如果Application提交完IO请求后，需要等待“回执”，那么它就是同步的。 而阻塞和非阻塞，指的是Application是否等待IO操作的完成。 如果Application必须等到IO操作实际完成以后再执行接下面的操作，那么它就是阻塞的。（从发出信号开始，会一直block对应的进程知道操作完成，） 反之，如果不需要等待IO操作的完成就开始执行其他的操作，那么就是非阻塞的。（在kernel还准备数据的情况下，就立刻返回了。） 叫车之后，就一直在路口等着，车来了自己上去 ——同步，阻塞 叫车之后，一边等着一边看美女，车来了自己上去 ——同步，非阻塞 叫车之后，光顾着看美女，司机到了之后打电话给你 ——异步，非阻塞 即是：同步就是你要自己检查车来了没有；异步就是车来了司机联系你。阻塞就是等车的时候老实等着，别干别的（被阻塞）；非阻塞就是等车的时候你可以做其他事情。 2. Blocking IO 3. Nonblocking IO 4. IO multiplexing / Event driven IO使用select 和epoll，单个process可以同时处理多个网络连接的IO。基本原理就是select/epol这个function会不断的轮询所负责的所有的socekt，当某个socket有数据到达了，就通知用户进程。它的流程如下图： 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 5. Asynchronous IO 用户进程发起read操作之后，就可以立刻做其他的事情。而另一方面，从kernel的角度，当它收到一个asynchronous read之后，首先它就会立刻返回一个信号，所以不会对用户进程产生任何的block。 然后，kernel会等待数据准备晚餐，将数据拷贝到用户内存，当这一切都完成后，kernel会给用户进程发送一个signal，告诉它read操作已经完成了。","tags":[{"name":"同步异步IO","slug":"同步异步IO","permalink":"http://chenson.cc/tags/同步异步IO/"}]},{"title":"多线程与多进程简单理解","date":"2017-09-22T06:28:08.000Z","path":"2017/09/22/多线程与多进程简单理解/","text":"1. 多进程直观来看，就是一个个pid，进程是程序在计算机上的一次执行活动。 创建子进程的调用是 fork() fork() 的功能就是产生子进程，调用一次，返回两次。 一次返回0，一次返回子进程的pid 子进程永远返回 0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用 getppid() 就可以拿到父进程的ID 失败返回-1 12345678910111213141516171819202122232425#include &lt;unistd.h&gt; #include &lt;sys/types.h&gt; #include &lt;stdio.h&gt; void print_exit() &#123; printf(\"the exit pid:%d/n\",getpid() ); &#125; main()&#123; pit_t, pid; atexit(print_exit); //注册该进程退出时的回调函数 pid = fork(); if (pid &lt; 0) printf(\"error in fork!\"); else if (pid == 0) // 子进程 printf(\"i am the child process, my process id is %d/n\", getpid()); else // 父进程 &#123; printf(\"i am the parent process, my process id is %d/n\", getpid()); sleep(2); wait(); &#125; &#125; 2. 多线程进程是由若干线程组成的，一个进程至少有一个线程。 线程就是把一个进程分成很多片，每一片都是可以独立的流程 linux提供的多线程的系统调用： 123456int pthread_create(pthread_t *restrict tidp, const pthread_attr_t *restrict attr, void *(*start_rtn)(void), void *restrict arg);Returns: 0 if OK, error number on failure 第一个参数为指向线程标识符的指针。第二个参数用来设置线程属性。第三个参数是线程运行函数的起始地址。最后一个参数是运行函数的参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include&lt;stdio.h&gt;#include&lt;string.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;pthread.h&gt; void* task1(void*);void* task2(void*);void usr();int p1,p2;int main()&#123; usr(); getchar(); return 1;&#125; void usr()&#123; pthread_t pid1, pid2; pthread_attr_t attr; void *p; int ret=0; pthread_attr_init(&amp;attr); // 初始化线程属性结构 pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_DETACHED); // 设置attr结构为分离 pthread_create(&amp;pid1, &amp;attr, task1, NULL); // 创建线程，返回线程号给pid1,线程属性设置为attr的属性，线程函数入口为task1，参数为NULL pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE); pthread_create(&amp;pid2, &amp;attr, task2, NULL); // 前台工作 ret=pthread_join(pid2, &amp;p); // 等待pid2返回，返回值赋给p printf(\"after pthread2:ret=%d,p=%d/n\", ret,(int)p); &#125;void* task1(void *arg1)&#123; printf(\"task1/n\"); // 艰苦而无法预料的工作，设置为分离线程，任其自生自灭 pthread_exit( (void *)1);&#125;void* task2(void *arg2)&#123; int i=0; printf(\"thread2 begin./n\"); // 继续送外卖的工作 pthread_exit((void *)2);&#125; 3. 线程 VS 进程 进程开销大，线程开销小。 线程安全：概念比较直观。一般说来，一个函数被称为线程安全的，当且仅当被多个并发线程反复调用时，它会一直产生正确的结果。 线程安全的条件：主要需要考虑的是线程之间的共享变量。属于同一进程的不同线程会共享进程内存空间中的全局区和堆，而私有的线程空间则主要包括栈和寄存器。因此，对于同一进程的不同线程来说，每个线程的局部变量都是私有的，而全局变量、局部静态变量、分配于堆的变量都是共享的。在对这些共享变量进行访问时，如果要保证线程安全，则必须通过加锁的方式 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。 多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题 多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程 4. 进程间通信4.1 单机多进程通信IPC（Inter Process Communication）包括：管道、文件、和消息传递 管道（Pipe）及有名管道（named pipe）：管道可用于具有亲缘关系进程间的通信，有名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信； 信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身；linux除了支持Unix早期信号语义函数sigal外，还支持语义符合Posix.1标准的信号函数sigaction（实际上，该函数是基于BSD的，BSD为了实现可靠信号机制，又能够统一对外接口，用sigaction函数重新实现了signal函数）； 报文（Message）队列（消息队列）：消息队列是消息的链接表，包括Posix消息队列system V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。 信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。 套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。起初是由Unix系统的BSD分支开发出来的，但现在一般可以移植到其它类Unix系统上：Linux和System V的变种都支持套接字。 4.2 分布式进程通信5. 线程通信多数的多线程都是在同一个进程下的，它们共享该进程的全局变量，我们可以通过全局变量来实现线程间通信。如果是不同的进程下的2个线程间通信，直接参考进程间通信。 6. References 多线程和多进程的区别（小结）","tags":[{"name":"线程","slug":"线程","permalink":"http://chenson.cc/tags/线程/"},{"name":"进程","slug":"进程","permalink":"http://chenson.cc/tags/进程/"}]},{"title":"Machine Learning - PCA","date":"2017-06-17T12:59:32.000Z","path":"2017/06/17/Machine-Learning-PCA/","text":"1. PCAPCA(Principal Components Analysis)是主成成分分析，之前也叫做Pricipal Factor Analysis。顾名思义就是分析数据里面的主要部分，是最常用的一种降维方法。那么为什么需要降维呢？ 因为在真实的训练数据中，总是会存在各种冗余的数据 比如说拿到一个汽车的样本，里面既有以“千米/每小时”度量的最大速度特征，也有“英里/小时”的最大速度特征，显然这两个特征有一个多余 拿到一个数学系的本科生期末考试成绩单，里面有三列，一列是对数学的兴趣程度，一列是复习时间，还有一列是考试成绩。我们知道要学好数学，需要有浓厚的兴趣，所以第二项与第一项强相关，第三项和第二项也是强相关。那是不是可以合并第一项和第二项呢？ 这个与第二个有点类似，假设在IR中我们建立的文档-词项矩阵中，有两个词项为“learn”和“study”，在传统的向量空间模型中，认为两者独立。然而从语义的角度来讲，两者是相似的，而且两者出现频率也类似，是不是可以合成为一个特征呢？ 因为这些冗余的数据，常常会导致我们的模型过度拟合。这个时候，就需要一种特征降维的方法来减少特征数，减少噪音和冗余，减少过度拟合的可能性。而PCA就是降维的算法之一，将原先的数据从n维映射到k维上(k&lt;n)。这里k维是全新的正交特征，这k维特征成为主元，是重新构造出出来的k维特征，而不是简单的从n维当中移去了(n-k)维，然后剩下主要的k维特征。 那么请思考一个问题：对于正交属性空间中的样本点，如何用一个超平面(直线的高维推广)，对所有的样本进行恰当的表达？那么需要找到怎么样的一个超平面来分割呢？一般需要具备如下特征： 最近重构性：样本点到这个超平面的距离都足够近 最大可分性：样本点在这个超平面上的投影尽可能的要分开，而不是重叠一起 2. 计算过程那么如何来构建呢？举个栗子： 现在有这么一组数据，x和y是两个特征 第一步：求出所有维度的平均值上面的例子中，x的平均值是1.81，y的平均值是1.91 第二步：中心化，即$\\sum x_i=0$.将所有的样本都减去这俩个平均值。比如第一个样本(2.5, 2.4) - (1.81, 1.91) = (0.69, 0.49) 第三步：求特征的协方差矩阵如果有x, y, z三个特征，分别需要求cov(x, x), cov(x, y), cov(x, z), cov(y, y), cov(y, z), cov(z, z)这几个。 当协方差大于0的时候，表示x和y若有一个增加，另一个也会增加。 当协方差下于0的时候，表示一个增加，另一个会减少 当协方差等于0的时候，表示来者之间是独立的。 协方差的绝对值越大，两者对彼此的影响也就越大 具体如下图 而我们上面的例子中只有x和y两个变量，即 第四步：求协方差的特征值和特征向量，得到 上面是两个特征值，下面是对应的特征向量。 比如特征值0.0490833989对应的特征向量为(-0.735178656, 0.677873399)T，这里的特征向量都归一化为单位单位向量 求协方差的步骤： 第五步：将特征值按照从大到小的顺序排列，选择其中个最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。 上面的例子中，特征值只有两个，我们需要选择其中最大的那个。所以这里我们选择1.28402771，对应的特征向量是 (-0.677873399, -0.735178656)T. 第六步：将样本投影到选取的特征向量上去。 假设样本数量为m，特征数量为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*m，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为 FinalData(m\\ast k) = DataAdjust(m\\ast n) \\times EigenVectors(n \\ast k)所以上面的例子中 FinalData(10\\ast 1) = DataAdjust(10\\ast 2) \\times EigenVectors(-0.677873399, -0.735178656)^T得到的结果是 这样就将原始的样本数据从n维特征变成了k维特征，这k维就是原始特征在k维上的投影。 下面的图描述了上面的过程： 原先所有数据是分布在x-y这个坐标系上，+表示的是样本数据。差不多是对角线上的两条线分别代表了两个正交的特征向量。由于协方差矩阵是对称的，因此其特征向量正交。然后我们将原有的样本数据投影到这个新的坐标系中去，得到转换后的数据： 从上面的图我们可以看出，在新的坐标系中，x轴基本就可以表示原有的样本数据特征。而整个过程，看起来有点像是将坐标系做了旋转。如果当k=1的时候，上面数据就只会变成一维的数据。而当原有的数据维度很高的时候，有时候为了可视化操作，我们就会对其行PCA降维。 3. PCA理论基础之前我们提到了，我们希望新的超平面具备最大可分性和最近重构性这两个特征。实际上根据这两个特征，能够分别得到主成分分析的两种等价推导。 最大可分性：最大方差理论 样本点在这个超平面上的投影尽可能尽量分开，即投影后的样本之间的方差要最大化。假设现在有五个样本分布在x-y坐标系下，如下图 投影的计算过程如下： 红色点表示某一个样本点$x^{(i)}$，蓝色点表示 $x^{(i)}$在超平面u上的投影，在这里u是直线的斜率，也是直线的方向向量，且是单位向量。蓝色的点$x^{(i)}$在u上的投影带你，离原点的距离是($x^{(i)}$, u)，即$(x^{(i)})^Tu$或者$u^Tx^{(i)}$，而蓝色点到原点的距离，就是在蓝色坐标轴上的坐标。由于这些样本点的每一维的特征均值都是0（之前归一化过），因此投影到u上的样本点的均值仍然是0. 如果我们将这这五个样本投影到某一个维度上，即从二维投影到一维上。这里为了比较不同效果，分别选了一条过原点的直线表示。如下图 从上面两幅图可以看出，左边的图上的样本间的距离比右边的要大，即左边投影后的样本点之间的方差最大，为： 最后的一个等式中，中间那部分恰好是求协方差的公式(这里用m，而不是m-1)，所以我们用$\\lambda$表示左边部分，即样本点之间的方差: \\lambda = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2用$\\sum$表示中间部分： \\sum = \\frac 1 m \\sum_{i=1}^m(u^Tx^{(i)})^2那么上面的式子可以改写为： \\lambda = u^T\\sum u之前提到了u是超平面的单位向量，即有$u^Tu = 1$，将上面左右两边的式子都乘以u，可得： u\\lambda = \\lambda u= uu^T\\sum u = \\sum u即$\\sum u = \\lambda u$，所以$\\lambda$就是$\\sum$的特征值，u是特征向量。最佳的投影直线是特征值$\\lambda$最大时对应的特征向量，其次是$\\lambda$第二大对应的特征向量，依次类推。 因此只需要对协方差矩阵进行特征值分解，得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，就可将样例$x^{(i)}$表示为下面的新样本： 其中的第j维就是$x^{(i)}$在$u_j$上的投影。通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。 最近重构性：最小平方误差理论 假设现在选的超平面是L(这个例子中是直线)，那么某一样本$x_k$到L的垂直距离为d’，那么有所有点到该直线的距离为： \\sum_{k=1}^n||(x_k' - x_k)||^2上面这个公式称作最小平方误差(Least Squared Erroe)","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"PCA","slug":"PCA","permalink":"http://chenson.cc/tags/PCA/"}]},{"title":"Machine Learning - 支持向量机","date":"2017-06-16T09:21:11.000Z","path":"2017/06/16/Machine-Learning-支持向量机/","text":"1. Margins Logistic Regression ​在logistic regression中, 如果我们要判断一个点是属于0还是1概率是θ p(y=1 | x; \\theta) \\\\ h_\\theta(x) = g(\\theta^T x) logistic regression和SVM的区别 由上图可知, C是非常靠近Boundary的, 而A是离Boundary最远的点. 所以我们非常有信心的说A是属于+, 而C比较有可能属于+. 所以对于那些距离边界比较近的点才是我们需要重点考虑的. 而这也正是logistic regression和SVM之间的区别. logistic regression考虑全局(如何考虑全局？ans：RMSE最小值)红线 SVM考虑局部(最小间隔要大于多少)绿线 2. SVM 符号说明 在Logstic中, 对于二分类问题, y ∈ {0, 1} 在SVM中, 对于二分类问题, y ∈ {-1, +1}, 且重新定义公式 h_{w,b} (x) = g(w^Tx + b) \\\\ g(z) = 1 \\space (z\\geq 0) \\\\ g(z) = -1 \\space (z < 0) 函数间隔 Functional Margins 第一个公式表示的是某个样本的函数间隔. 第二个公式表示的是全局的函数间隔, 也就是函数间隔为所有样本中函数间隔最小的那个函数间隔决定. 之前使用正负1来表示 y, 所以计算出来的距离都是一个非负数, 且这个数值的大小表示了对于预测结果的confidence. y 值越接近1, 表示对这个判断越肯定. \\hat {\\gamma}^{(i)} = y^{(i)} (w^Tx^{(i)} + b) \\\\ \\hat \\gamma = \\min_{i=1,...,m} \\hat \\gamma^{(i)}函数间隔越大, 代表了我们对于分类的结果非常的肯定, 所以希望函数间隔越大越好, 但需要对这个间隔加上一些限制条件(后面具体讲)才行. 因为我们可以在不改变这个超平面的情况下, 只要成比例增加w和b, 就能让函数间隔任意的大. 几何距离 Geometrix Margins 下图中, 如果我们知道B点所在的超平面(separating hyperplane)的解析式, 任何其他点到该面的距离都可以用上面定义过的函数间隔来表示. 决策边界：w^Tx + b = 0w是超平面的法向量, 垂直于决策边界, 也就是这个超平面. 若B是A在分割面上的投影 (AB垂直于超平面), 那么我们可以计算A到超平面的距离 γ . 假设A点是$x_i$, 那么B点为： \\overrightarrow {OB} = \\overrightarrow {OA} - \\overrightarrow {BA} \\\\ x^{(i)} - \\gamma^{(i)} · \\frac w {||w||}因为B点在这个超平面上, 所以我们将这个点带回超平面得到： w^T(x^{(i)} - \\gamma^{(i)} · \\frac w {||w||}) + b = 0通过上面的式子, 可以解出γ： w^Tx^{(i)} - \\gamma^{(i)} · \\frac {w^Tw} {||w||} + b = 0 \\\\ w^Tx^{(i)} + b = \\gamma^{(i)} ||w|| \\\\ \\gamma^{(i)} = (\\frac w {||w||})^Tx^{(i)} + \\frac b {||w||}加上前面的$y^{(i)}$, 于是我们就能得到了几何间隔： \\gamma^{(i)} = y^{(i)}(\\frac {w^T} {||w||}x^{(i)} + \\frac b {||w||})通过上面的式子, 发现当||w|| = 1时, 几何间隔就是函数间隔. 这个时候, 如果任意放大||w||, 几何间隔是不会改变的. 因为||w||也会随着被放大. 几何间隔与函数间隔的关系为： \\gamma^{(i)} = \\frac {\\hat \\gamma^{(i)}} {||w||}对于所有的训练样本, 几何间隔为： \\gamma = \\min_{i=1,...,m} \\gamma^{(i)} 2. 间隔最大化根据上一节, 我们可以求出几何间隔γ. 如果现在需要找到一个超平面S, 使得离超平面最近的点的几何间隔越大越好, 可以用下列优化问题表示： \\max_{w,b} \\gamma \\\\ s.t. \\space y_i(\\frac w {||w||}·x_i + \\frac b {||w||}) \\geq \\gamma, \\space i = 1, 2, ..., N即我们希望最大化超平面(w, b)关于训练数据集的几何间隔γ, 约束条件表示超平面(w,b)关于每个训练样本点的几何间隔至少是γ. 考虑到几何间隔与函数间隔的关系式, 可以将这个问题改写成函数间隔来表示, 即： \\max_{w,b} \\frac {\\hat \\gamma} {||w||} \\\\ s.t. \\space y_i(w·x_i + b) \\geq \\hat \\gamma, \\space i = 1, 2, ..., N上面式子中, 函数间隔的取值并不会影响到最优化问题的解. 事实上, 假设将w和b按比例改变为λw和λb, 这时函数间隔也会被当大λ倍, 所以这个对上面的最优化问题的不等式约束是没有影响的, 对目标函数的优化也没有影响. 这样, 可以去函数间隔γ=1, 代入到上面的最优化问题, 注意到最大化 $\\frac 1 {||w||}$和最小化 $\\frac 1 2$ $||w||^2$是等价的, 也是就得到厦门的线性可分支持向量机学习的最优化问题： \\min_{w,b} \\frac 1 2 ||w||^2 \\\\ s.t. \\space y_i(w·x_i + b) - 1 \\geq 0这个时候我们的问题就转化成了在线性约束下的二次规划, 可以使用二次规划的软件来解决这个优化问题,, 然后我们就可以得到我们的最优间隔分类器.实际上, 我们有更好的办法去解这个优化问题. 但在这之前, 我们需要补充一下其他的相关知识. 3. 拉格朗日对偶在约束最优化问题中, 常常利用拉格朗日对偶性(Lagrange dulity)讲原始问题转换为对偶问题, 通过解度偶问题而得到的原始问题的解. 该方法应用在许多统计学习方法中, 例如最大熵模型和支持向量机. 原始问题 \\min_w f(w) \\\\ s.t. \\space h_i(w) = 0, \\space i = 1,2,...,l下面是约束条件, 使用拉格朗日乘子法, 将问题转换为： L(w, b) = f(w) + \\sum_{i=1}^l\\beta_ih_i(w)这里面, βi为拉格朗日乘子(Lagrange Multipliers). 然后令偏导为0来解得w和β： \\frac {∂L} {∂w_i} = 0 \\\\ \\frac {∂L} {∂\\beta_i} = 0更加广泛的约束最优化问题： \\min_w f(w) \\\\ s.t. \\space g_i(x) ≤ 0, \\space i = 1, 2, ...,k \\\\ \\space \\space \\space \\space \\space \\space \\space h_i(w) = 0, \\space i = 1,2,...,l所以我们定义广义拉格朗日公式(Generalized Lagrangian)为： L(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^k \\alpha_ig_i(w) + \\sum_{i=1}^l \\beta_ih_i(w)其中, αi,βi为拉格朗日乘子(Lagrange Multipliers). 现在我们定义： \\theta_p(w) = \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} L (w, \\alpha, \\beta) ​ 其中下表P代表了“primal”. 若两个约束条件当中至少有一个得不到满足的时候, 则可令αi为无穷大或βi为无穷大使得： \\theta_P = f(w) \\space \\space \\space (w满足原始问题的约束)\\\\ \\theta_p = \\infty \\space \\space \\space (其他)\\​ 对于满足原始约束的w来说, θp与原始问题中的目标函数相同. 对于违反原始约束问题的w来说θp = ∞ ​ 如果考虑最小化： ​ \\min_w\\theta_p(w) = \\min_w \\max_{\\alpha, \\beta: \\alpha_i ≥ 0 } L(w, \\alpha, \\beta)​ 它是与原始最优化问题相等价的, 即它们有相同的解. 对偶问题 现在看另外一个问题： \\theta_D(\\alpha, \\beta) = \\min_w L(w, \\alpha, \\beta)其中下表D代表了对偶(dual). 在原始问题中, 我们先最大化关于α和β的函数, 再最小化关于w的函数； 而对偶问题中, 我们先最小化关于w的函数, 在最大化关于α和β的函数： \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} \\theta_D(\\alpha, \\beta) = \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} \\min_w L(w, \\alpha, \\beta)它们唯一的区别就在于min和max的顺序不同. 我们令: d^{\\*} = \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} \\min_w L(w, \\alpha, \\beta) \\leq \\min_w \\max_{\\alpha, \\beta: \\alpha_i ≥ 0} L(w, \\alpha, \\beta) = p^{\\*}也就是说, 在某种情况下, 会有d = p. 这个时候我们就可把求原始问题转化成求对偶问题. 假设f和g都是凸函数(convex function), h是仿射的, 并且存在w是对所有的i, 能够使得: g_i(w) < 0上述假设条件下, 一定存在w*, α*和β*使得w*是原始问题的解. α*和β*是对偶问题的解. 并且还有 p = d = L(w, α\\, β*). w*, α*和β*满足KKT条件(Karush-Kuhn-Tucker conditions)： 如果存在满足KKT条件的w*,α*,β*, 则原始条件与对偶问题一定有解. 公式(5)又称之为KKT对偶互补条件, 这个条件表明如果 α* &gt; 0, 那么就有g(w*) = 0. 即约束条件 g(w*) &lt;= 0 激活, w处于可行域的边界上. 而其他谓语可行域内部g(w*) &lt; 0的点都不起约束作用, 对应的α* = 0. 为什么要引入对偶 为什么要使用随机梯度下降 用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降(为什么？？), 只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 对偶的优势 对偶形式将权重向量w转化成实例$x_i$和$y_i$的线性组合形式。且对偶是以内积的形式出现的，可以预先使用Gram矩阵储存，用空间换时间的方法提高计算效率。原始形式每次判断误分类点时都需要进行向量点乘运算。 这里同时也为后面引入核函数做伏笔，因为感知机是神经网络和支持向量机的基础。 4. 最优间隔分类器根据上面的内容, 回顾SVM的问题 \\min_{\\gamma,w,b} \\frac 1 2 ||w||^2 \\\\ s.t. \\space y^{(i)}(w^Tx^{(i)} + b) \\geq 1 \\根据拉格朗日对偶问题, 修改约束条件为 g_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0那么就和拉格朗日公式是一样的了. 根据上面的KTT条件可知只有函数间隔是1, 线性约束式前面的系数大于0, g(w)=0, 其他的不在线上的点(g(w)&lt;0), 极值不会在他们所在的范围内取得, 因此前面的系数等于0. 考虑下图, 最大间隔分类超平面为实线： 其中一个正样本和两个负样本正好在平行于分类超平面的虚线上, 只有这三个样本对应的 αi&lt;0, 其他样本对应的 αi=0. 这三个样本就叫做支持向量机. 从这里我们可以看出, 支持向量的个数远远小于集训集的大小. 现在构造拉格朗日函数： L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1]所以接下去的任务就是求解对偶问题, 根据上面的知识, 有： d* = \\max_{\\alpha:\\alpha_i \\geq 0} \\theta_D(\\alpha) = \\max_{\\alpha:\\alpha_i \\geq 0} \\min_{w, b} L(w, b,\\alpha)首先, 求L(w,b,α)关于w, b 的最小值. 令偏导数为0： \\frac {∂L} {∂w} = w - \\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)} = 0 \\\\ \\frac {∂L} {∂b} = 0 - \\sum_{i=1}^m\\alpha_iy^{(i)}= 0可得： w = \\sum_{i=1}^m\\alpha_i y^{(i)} x^{(i)} \\\\ \\frac {∂} {∂b} L(w, b, \\alpha) = \\sum_{i=1}^m\\alpha_i y^{(i)} = 0将求得的w带回拉格朗日函数L(w,b,α), 可得到： L(w, b, \\alpha) = \\frac 1 2 ||w||^2 - \\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)} + b) - 1] 计算出了min L(w,b,α), 便可以继续进行max操作, 即： 可以证明该优化问题满足KKT条件, 求得$α^{*}_i$之后(后面将如何求解), 可通过: w = \\sum_{i=1}^m \\alpha_i y^{(i)}x^求得w*, 最后通过下面式子求得b* b^{\\*} = -\\frac {\\max_{i:y^{(i)}=-1}w^{\\*T}x^{(i)} + \\min_{i:y^{(i)} = 1}w^{\\*T}x^{(i)}}{2}当求出所有的参数, 就可以通过$w^T$x + b 来进行分类了： w^T + b = (\\sum_i^m \\alpha_iy_ix_i)^T x + b = \\sum_i^m \\alpha_iy_i⟨x_i, x⟩ + b通过上面式子发现, 现在新来一个新数据, 只需要计算它与训练样本的内积即可. 并通过前面的KKT条件我们知道, 只有除了支持向量的那些原本, 都有$α_i$ = 0. 所以, 我们只需要将新样本与支持向量机做内积运算, 即可求出$w^T$x + b Example 假设这里有三个样本点，正样本点x1=(3,3)^T, x2=(4,3)^T, 负样本点是x3=(1,1)^T，试用感知机学习算法对偶形式求感知机模型。 ANSWER 取αi = 0，这里i=1，2，3，b=0，n=1 计算Gram矩阵 G= \\begin {bmatrix} ||x_1||^2 & x_1·x2 & x_1·x3 \\\\ x2· x1 & ||x2||^2 & x_2·x3 \\\\ ||x_3· x1 & x_3·x2 & ||x_3||^2 \\end{bmatrix} = \\begin {bmatrix} 18 & 21 & 6 \\\\ 21 & 25 & 7 \\\\ 6 & 7 & 2 \\end{bmatrix} 误分条件 y_i(\\sum_{j=1}^N\\alpha_jy_jx_j ·x_i + b) \\leq 0参数更新 \\alpha_i \\leftarrow \\alpha_i + 1 \\\\ b \\leftarrow b + y_i​ 迭代过程如下，结果列于下表 | k | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 || :—: | :—: | :—: | :—: | :—: | :—: | :—: | :—: | :——: || | | x1 | x3 | x3 | x3 | x1 | x3 | x3 || α1 | 0 | 1 | 1 | 1 | 2 | 2 | 2 | 2 || α2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 || α3 | 0 | 0 | 1 | 2 | 2 | 3 | 4 | 5 || b | 0 | 1 | 0 | -1 | 0 | -1 | -2 | -3 | w, b分别为： w = 2x_1 + 0x_2 - 5x_3 = (1, 1)^T \\\\ b = -3分离超平面： x^{(1)} + x^{(2)} - 3 = 0 5. Kernels在之前的线性回归的章节中，有提到过polynomial regression。假设x是房子的面积，我们使用三个特征$x$, $x^2$, $x^3$来构造一个三次多项式。 这里面有两个概念要区分一下。x为原先房子的面积，是属性(attributes)。通过这个属性x映射出来的$x$, $x^2$, $x^3$叫做特征(features)。在这里使用ϕ来表示从属性到特征的特征映射(featuer mapping)。比如： ϕ(x) = \\begin{bmatrix} x \\\\ x^2 \\\\ x^3 \\end{bmatrix}那么在SVM中，如何使用这种特征映射呢？ 通过上面知识，我们只需要将所有出现$⟨x^{(i)}, x^{(j)}⟩$ 替换为 $⟨ϕ^{(i)}, ϕ^{(j)}⟩$ 看上去好像我们既在SVM中使用了特征映射, 又解决了数据在低维空间中线性不可分的情况. 但是, 这里有个问题. 如果我们通过特征映射得到的$ϕ(x)$是一个很高维甚至是无穷维的, 那么计算$⟨ϕ(x^{(i)}),ϕ(x^{(j)})⟩$就不是那么现实了, 计算时间会很久. 这里我们就要引出一个叫kernels的概念, 假设 Q: z在这里代表的什么？ K(x, z) = (x^Tz)^2 \\space \\space \\space \\space \\space \\space \\space \\space x, z \\in R^b展开K(x, z): K(x, z) = (\\sum_{i=1}^n x_iz_i) (\\sum_{j=1}^n x_iz_i) \\\\ = \\sum_{i=1}^n \\sum_{j=1}^n x_ix_j z_i z_j \\\\ = \\sum_{i,j=1}^n (x_ix_j) (z_iz_j)展开后我们发现，K(x, z)还可以写成$K(x, z) = ϕ(x)^T ϕ(z)$，其中： ϕ(x) = \\begin{bmatrix} x _1x_1 \\\\ x _1x_2 \\\\x _1x_3 \\\\ x _2x_1 \\\\ x _2x_2 \\\\ x _2x_3 \\\\ x _3x_1 \\\\ x _3x_2 \\\\ x _3x_3 \\\\ \\end{bmatrix}在这个例子中，映射后特征的内积和原始特征的内积的平方是等价的，也就是说我们只需要计算原始特征的内积再进行平方就可以了，并不需要先得到映射后再计算映射后特征的内积。计算原始特征内积的时间复杂度为O(n)，而计算映射特征的时间复杂度为O(n^2)。 再看另外一个kernels K(x, z) = (x^T z + c)^2 \\\\ = \\sum_{i,j=1}^n (x_ix_j) (z_iz_j) + \\sum_{i=1}^n (\\sqrt {scx_i})( \\sqrt {scx_j} ) + c^2同样我们也可以改写上面的式子 所以广泛的来说，我们有 K(x, z) = (x^T z + c)^d这个kernel将n维的特征映射为(d, n+d)维，即这里面对应的多项式$x_{i1}$, $x_{i2}$, …, $x_{ik}$最多到d维。尽管空间维度为O(n^d)，但计算时间仍然只是O(n)，因为我们并不需要将映射后的特征全部计算出来再计算内积。 但因为计算的是内积，有IR中的余弦相似度可孩子，如果x和z的向量夹角越小，那么核函数的值就越大。反之就越小。因此核函数值是ϕ(x)和ϕ(z)相似度。 再看另外一个很函数 K(x, z) = exp(\\frac {||x - z||^2} {2\\sigma^2})在这个核函数中，如果x和z很相近，则(||x-z||~= 0)，那么核函数值为1. 如果相差很大，则(||x-z||&gt;&gt; 0), 那么很函数的值约等于0. 由于这个核函数类似于高斯分布，因此成为高斯核函数，也叫做径向基函数(Radial Basis Function 简称RBF)，它能够把原始特征映射到无穷维。 类似于高斯很函数，比较x和z的相似度，并映射到0~1之间。logistic Regression中， sigmoid函数也可以，所以还有sigmoid核函数。 K(x, z) = tanh(\\beta · xz + b)下面有张图说明在低维线性不可分时，映射到高维后就可分了，使用高斯核函数。 在SVM中，对于训练样本学习出w和b参数后，对于新来的样本我们只需要计算$w^Tx + b$来判断。那么在使用了核函数之后，则需要相应的改为$w^Tϕ(x) + b$ 那么是需要先计算好ϕ(x)再进行预测呢？实际上不需要的, 之前计算过 w^Tx + b = (\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)})^T x + b \\\\ = \\sum_{i=1}^m \\alpha_i y^{(i)} ⟨x^{(i)}, x⟩ + b所以我们只需要将 ⟨x^{(i)}, x⟩ \\\\ 替换为 \\\\ K(x^{(i)}, x)6. 核函数的有效性判断核函数的有效性，即判断是否存在ϕ, 使得下面式子成立 K(x,z)=⟨ϕ(x)ϕ(z)⟩假设我们有核K和m个训练样本{x(1),x(2),…,x(m)}, 定义一个 (m×m) 的矩阵K K_{ij} = K(x^{(i)}, x^{(j)})如果此时K是一个有效的kerne，那么则有： K_{ij} = k(x^{(i)}, x^{(j)}) = ϕ(x^{(i)})ϕ(x^{(i)}) \\\\ = ϕ(x^{(j)})^T ϕ(x^{(i)}) = K(x^{(j)}, x^{(i)}) = K_{ji}即K是对称矩阵。现在我们用ϕk(x)不澳是向量ϕ(x)的第k个元素，对任意的向量z都有： 从上面的证明我们可以得到，如果K是一个有效的kernel，那么对于在训练集上的核矩阵K一点是半正定的。事实上，这不仅仅是个必要条件，也是充分条件。有效核也叫做Mercer Kernel 7. Reference http://zhihaozhang.github.io/2014/05/11/svm3/ http://www.cnblogs.com/bourneli/p/4199990.html http://www.cnblogs.com/90zeng/p/Lagrange_duality.html","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.cc/tags/SVM/"}]},{"title":"Machine Learning - Ensemble Learning","date":"2017-06-13T04:27:45.000Z","path":"2017/06/13/Machine-Learning-Ensemble-Learning/","text":"1. 集成学习 (Ensemble Learning)在对新的数据实例进行分类的时候，集成学习通过训练好多个学习器，把这些分类器的的分类结果进行某种组合 (比如投票) 决定分类结果，以取得更好的结果，就是我们生活中那句话“三个臭皮匠顶个诸葛亮”，通过使用多个决策者共同决策一个实例的分类从而提高分类器的泛化能力。 同质集成 (Homogeneous) 需要是同种类型，比如全部是决策树或神经网络等，每个个体学习器称之为基学习器 (base learner)，相应的学习算法称为基学习算法 (base learning algorithm) 异质集成 (Heterogenous) 可以由不同的学校算法生成，这时候就不再有基学习算法，每个个体学习器称为组件学习器 (component learner) 以下我们重点讨论同质集成。 2. 分类器的选择 差异性 问题：如何选择/构建差异性的基分类器？(ans: section 3) 精度 &gt; 0.5 精度略高于50%的分类器称之为弱学习器 (weak learner) 问题：如何投票选择出最佳的预测？(ans: section 4) 如何组合 假设在二分类器中，这里有三个分类器在测试三个样本，每个分类器的正确率都是66%，那么组合出的结果大致可以分成以下几种 集成性能提升 | | test1 | test2 | test3 || :—: | :—-: | :—-: | :—-: || h1 | right | right | wrong || h2 | wrong | right | right || h3 | right | wrong | right | 如果只是简单投票法的话，在每个test中，有两个分类器是对的，一个是错的，那么投票出来的结果是对的，最终精度可以达到100%。 集成不起作用 | | test1 | test2 | test3 || :—: | :—-: | :—-: | :—-: || h1 | right | right | wrong || h2 | right | right | wrong || h3 | right | right | wrong | 三个分类器对三个test进行预测，恰好三个分类器对test3的情况分类都是错的，而对test1和test2的结果都预测正确，那么最终的测试结果没有影响，都是66%。 集成起负作用 | | test1 | test2 | test3 || —— | ——- | ——- | ——- || h1 | right | wrong | wrong || h2 | wrong | right | wrong || h3 | wrong | wrong | right | 和第一种情况相反，最终的测试结果是33% 通过上面的例子，可以反映出一个问题，在同质集成中，如何构建多个误差是相互独立的基学习器。因为这些基学习器是用的同一种算法，基本也是使用同一组数据，解决的也是同一个问题。 根据个体学期的生成方式，集成学习方法大致可以分成两大类： Boosting 个体学习器之间存在强依赖关系，必须串行生成的序列化方法 Bagging 和 Random Forest 个体学习器之前不存在强依赖关系，可同时生成的并行化方法 3. 构建差异性基分类器在同一个数据集上，构建不同的，具有差异性的分类器 (好而不同)，就是通过抽样技术获取多个训练数据集，从而生成多个差异性分类器。目前主要的方法有：Bagging 和 Boosting。 3.1 Boosting提升方法是一个迭代的过程，通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重，这样错分的数据再下一轮的迭代就有更大的作用 (对错分数据进行惩罚)。 具体来说，先从初始训练集训练出一个基学习器，再根据这个基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，一直到基学习器的数目到达事先指定的T值，然后将所有的基学习器相结合。(问题：如何调整分布？) 数据的权重有两个作用，一方面我们可以使用这些权值作为抽样分布，进行对数据的抽样，另一方面分类器可以使用权值学习有利于高权重样本的分类器。把一个弱分类器提升为一个强分类器，大家可以参考AdaBoost算法 (西瓜书P173) Example 假设现在有一个线性分类器如下表 | | 预测 + | 预测 - | count || :———-: | :—: | :—: | :—-: || 实际 + | 24 | 16 | 40 || 实际 - | 9 | 51 | 60 || count | 33 | 67 | 100 | 错误率计算 \\epsilon = \\frac {(9 + 16)} {100} = 0.25 错分样本的权值更新 w_e = \\frac 1 {2*\\epsilon} = 2 正确样本的权值更新 w_r = \\frac 1 {2*(1 - \\epsilon)} = \\frac 2 3 所以讲权重乘以相应的数据量，得到新的数据分布，如下 预测 + 预测 - count 实际 + 24 * 2/3 = 16 16 * 2 = 32 16+32=48 实际 - 9 * 2 = 18 51 * 2/3 = 34 18 + 34 = 52 count 16 + 18 = 34 32 + 34 = 66 100 3.2 Bagging通过对原数据集进行有放回的采样 (bootstrap sampling) 构建出大小和原数据集大小一样的新数据集D1，D2，D3…..，然后用这些新的数据集训练多个分类器H1，H2，H3….。因为是有放回的采样所以一些样本可能会出现多次，而其他样本会被忽略，理论上初始训练集中约有63.2%的样本会出现在采样集中。(西瓜书P27) 自助采样法 (Bootstrap sampling) 假设给定一个包含了m个样本的数据集D，我们需要构建一个新的数据D1，大小和D一样的。那么每次我们从D中取出一个样本入到D1中，之后把样本放回D中，重新采样，总共采样m次，那么我们可以得到一个大小为m的D1。因为我们把样本又重新放入D中，下次还有可能抽取到，导致在D1中，有些样本会出现多次，而有些样本则一次也不会出现。所以根据如下公式： lim_{m -> \\infty} (1 - \\frac 1 m)^m -> \\frac 1 e \\approx 0.368最终在初始样本数据集D中，有36.8%的样本是一次也没有出现的，而63.2%重复出现了，这样我们就改变了初始样本数据集的分布。所以我们可以将D1作为训练集，D\\D1，即剩下的36.8%未出现在D1中的作为测试集。这样的测试结果称之为包外估计 (out-of-bag-estimate) Bagging通过降低基分类器方差改善了泛化能力，因此Bagging的性能依赖于基分类器的稳定性，如果基分类器是不稳定的，Bagging有助于减低训练数据的随机扰动导致的误差，但是如果基分类器是稳定的，即对数据变化不敏感，那么Bagging方法就得不到性能的提升，甚至会减低，因为新数据集只有63%。 3.3 Random Forest随机森林是Bagging的一个拓展变体，基于Bagging框架进一步降低了了模型的方差。在这里，我们用的决策树与传统的决策树略有不同。 传统的决策树，比如C45或者CART，每当需要划分一个属性的时候，我们会将所有的属性的都进行划分一遍，然后计算其划分之后与划分之前熵的偏差，或者计算Gini指数等，然后从中选取出最佳属性进行划分。 而随机森林，对基决策树的每个结点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选取一个最优属性用于划分 (从特征的不同子集来构建树)。这里的参数k控制了随机性的引入程度： 若k=d，则基决策树和传统决策树相同。 若k=1，则是随机选择一个属性用于划分 推荐值k=log_2 d 随机森林除了基于Bagging中对于样本的扰动，还同时对数据集中的属性划分进行了扰动，这样可以增加基学习器的多样性和差异性，使得最终集成的泛化性得到提升。 3.4 增强多样性 数据样本扰动 输入属性扰动 输出表示扰动 算法参数扰动 4. 组合策略4.1 归回预测 (数值预测) 简单平均值法 就是取各个分类器结果的平均值（会不会降低这里面最好的那个分类器的精度） H(x) = \\frac 1 T \\sum_{i=1}^T h_i(x) 加权平均法 给不同的分离器赋予不一样的权重值，然后求和 H(x) = \\sum_{i=1}^Tw_i · h_i(x) 4.2 分类预测 (类别预测) 简单投票法 每个分类器的权重大小都一样，少数服从多数 绝对多数投票法票数过半，否则拒绝预测 相对多数投票法票数最多的那个 加权投票法 给每个分类器赋予一个权重，然后根据权重来投票，得到票数高的最为输出结果 概率投票法（和简单分类有什么不同？） 有的分类器的输出是有概率信息的，比如 分类器A输出结果1的概率为75% 分类器B输出结果0的概率为80% 分类器C输出结果1的概率为52% 最终输出的结果为？？？（概率相加还是概率的平均值，相加可能性高） 4.3 学习法5. References 西瓜书第八章 - 周志华 非参考，推荐阅读 小菜鸟对周志华大神gcForest的理解 gcForest算法理解","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"Ensemble","slug":"Ensemble","permalink":"http://chenson.cc/tags/Ensemble/"}]},{"title":"Hadoop权威指南笔记（三）","date":"2017-06-09T06:08:52.000Z","path":"2017/06/09/Hadoop权威指南笔记（三）/","text":"1. 为什么要使用Secondary Sort在Hadoop中，从Map到Reduce的过程中，key是不断被sort的。所以从map出来的时候写入到一个intermediate output file的时候，key是有序的。然后reduce不断的从不同的cluster里面fetch里面的key-value pairs的时候，仍然多次sort这些pairs。所以最终进入到reducer的key是有序的，但是value是无序的。如果我们需要对value也进行排序呢？Google的MR内置了函数对value也可以排序，但是Hadoop不行，我们需要自己去定制partitioner等去实现这个功能。 举个栗子： 输入文件格式如下 12345678910112015,1,242015,3,542015,1,32015,2,-432015,4,52015,3,462014,2,642015,1,42015,1,212015,2,352015,2,20 ​ 期望的输出格式如下（value是有序的） 123452014-2 642015-1 3，4，21，242015-2 -43，0，352015-3 46，562015-4 5 ​ Hadoop默认的输出格式如下（value是无序的） 123452014-2 642015-1 21，4，3，242015-2 0，35，-432015-3 56，462015-4 5 2. 解决方案 传统的解决方法 就是进入到同一个reducer的时候，这些同一个可以的values是在一个list里面的，那么我们就可以先把这个list里面的value存到内存中去，然后在内存中将这些value排序。这个方法只适用于数据量较小的时候，当数据量很大的时候，内存并不能同时存入这些values，程序就报错无法正常运行。 利用Hadoop特点的方法 既然Hadoop可以对Key进行排序，那么可以利用这点，将之前的key-value pairs 组成一个新的key，让Hadoop对这些Key进行排序，栗子如下 12345// 原先的key-value pair(2015,1) 21// 新的composite-key-value pair((2015,1),21) 21 这里的(2015,1)是我们原先就有的key，为了区分新的key，将这个原有的key (k, v1) ,称为natural key，将新的key ((k, v1), v2)称为composite key。将v2称为natural value。具体如下图 ​ 为了实现Hadoop对新的composite key进行排序，我们需要自定义partitioner和grouping来确保这些composite key中natural key相同的会被分配到同一个reducer。因为在composite key中，即使(k, v1)相同，只要v2不同，默认的partitioner就会认为这是俩个不同的key，就很有可能讲这个composite key分配到不同的reducer里去。 ​ 3.实现过程 partitioner: 将natural key相同的发送到同一个reducer里去 在同一个[]里面说明是一个reducer，value任然是无序的 123456789[((2014-2,64),64)][((2015-1,24),24), ((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,56),56), ((2015-3,46),46)][((2015-4,5),5)] ​ grouping comparator: 将natural key相同的作为一个group排序 123456789[((2014-2,64),64)][((2015-1,3),3), ((2015-1,4),4), ((2015-1,21),21), ((2015-1,24),24)][((2015-2,-43),-43), ((2015-2,0),0), ((2015-2,35),35)][((2015-3,46),46), ((2015-3,56),56)][((2015-4,5),5)] ​ 进入到Reducer的格式 (有点里不理解此时composite key里面的natural value是如何确定的) 123456789((2014-2,64), (64))((2015-1,24), (2，4，21，24))((2015-2,35), (-43,0,35))((2015-3,46), (46,56))((2015-4,5), (5)) ​ 最终的输出 12342014-2 642015-1 3,4,21,242015-2 -43,0,352015-4 5 整个流程图如下（图内数据与上面数据不符合） 4. 代码 composite key 将旧的Key（natural key）和Value组合成新的Key（composite key）的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.secondarySort;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class Entry implements WritableComparable&lt;Entry&gt; &#123; private String yearMonth; private int count; public Entry() &#123; &#125; @Override public int compareTo(Entry entry) &#123; int result = this.yearMonth.compareTo(entry.getYearMonth()); if (result == 0) &#123; result = compare(count, entry.getCount()); &#125; return result; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(yearMonth); dataOutput.writeInt(count); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.yearMonth = dataInput.readUTF(); this.count = dataInput.readInt(); &#125; public String getYearMonth() &#123; return yearMonth; &#125; public void setYearMonth(String yearMonth) &#123; this.yearMonth = yearMonth; &#125; public int getCount() &#123; return count; &#125; public void setCount(int count) &#123; this.count = count; &#125; public static int compare(int a, int b) &#123; return a &lt; b ? -1 : (a &gt; b ? 1 : 0); &#125; @Override public String toString() &#123; return yearMonth; &#125;&#125; ​ Partitioner 1234567891011package com.secondarySort; import org.apache.hadoop.mapreduce.Partitioner; public class EntryPartitioner extends Partitioner&lt;Entry, Integer&gt; &#123; @Override public int getPartition(Entry entry, Integer integer, int numberPartitions) &#123; return Math.abs((entry.getYearMonth().hashCode() % numberPartitions)); &#125;&#125; ​ Grouping Compartor 1234567891011121314151617package com.secondarySort; import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class EntryGroupingComparator extends WritableComparator &#123; public EntryGroupingComparator() &#123; super(Entry.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; Entry a1 = (Entry) a; Entry b1 = (Entry) b; return a1.getYearMonth().compareTo(b1.getYearMonth()); &#125;&#125; ​ Mapper 1234567891011121314151617181920212223public class SecondarySortMapper extends Mapper&lt;LongWritable, Text, Entry, Text&gt; &#123; private Entry entry = new Entry(); private Text value = new Text(); @Override protected void map(LongWritable key, Text lines, Context context) throws IOException, InterruptedException &#123; String line = lines.toString(); String[] tokens = line.split(\",\"); // YYYY = tokens[0] // MM = tokens[1] // count = tokens[2] String yearMonth = tokens[0] + \"-\" + tokens[1]; int count = Integer.parseInt(tokens[2]); entry.setYearMonth(yearMonth); entry.setCount(count); value.set(tokens[2]); context.write(entry, value); &#125;&#125; ​ Reducer 123456789101112public class SecondarySortReducer extends Reducer&lt;Entry, Text, Entry, Text&gt; &#123; @Override protected void reduce(Entry key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder builder = new StringBuilder(); for (Text value : values) &#123; builder.append(value.toString()); builder.append(\",\"); &#125; context.write(key, new Text(builder.toString())); &#125;&#125; ​ Deriver 123456789101112131415Configuration conf = new Configuration();Job job = Job.getInstance(conf);job.setJarByClass(Iteblog.class);job.setJobName(\"SecondarySort\"); FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setOutputKeyClass(Entry.class);job.setOutputValueClass(Text.class); job.setMapperClass(SecondarySortMapper.class);job.setReducerClass(SecondarySortReducer.class);job.setPartitionerClass(EntryPartitioner.class);job.setGroupingComparatorClass(EntryGroupingComparator.class); 5. 常用的Secondary Sort代码 IntPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; int first, second; // public IntPair() &#123;&#125; // // public IntPair(int left, int right) &#123; // set(left, right); // &#125; public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; public String toString()&#123; return \"(\" + first + \",\" + second + \")\"; &#125; @Override public void readFields(DataInput arg0) throws IOException &#123; // TODO Auto-generated method stub first = arg0.readInt(); second = arg0.readInt(); &#125; @Override public void write(DataOutput arg0) throws IOException &#123; // TODO Auto-generated method stub arg0.writeInt(first); arg0.writeInt(second); &#125; // 关键：自定义类型的比较方法 @Override public int compareTo(IntPair arg0) &#123; // TODO Auto-generated method stub if (first != arg0.first) &#123; return first &lt; arg0.first ? -1 : 1; &#125; else if (second != arg0.second) &#123; return second &lt; arg0.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; public int hashCode() &#123; return first * 157 + second; &#125; public boolean equals(Object right) &#123; if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125;&#125;public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; &#123; // 类型要和Mapper输出的一样 @Override public int getPartition(IntPair arg0, IntWritable arg1, int arg2) &#123; // TODO Auto-generated method stub return Math.abs((arg0.getFirst() * 127) % arg2); &#125;&#125;/* * 第一种方法，实现接口RawComparator 数据类型的比较在MapReduce中式及其重要的, * Mapreduce中有一个排序阶段，key和其他的key相比较。 针对此，Hadoop 提供的一个优化是 RawComparator * * public static class GroupingComparator implements RawComparator&lt;IntPair&gt;&#123; * * @Override public int compare(IntPair arg0, IntPair arg1) &#123; // TODO * Auto-generated method stub int l = arg0.getFirst(); int r = * arg0.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; * * @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int * s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, * Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; &#125; */// 方法二public static class GroupingComparator extends WritableComparator &#123; protected GroupingComparator() &#123; super(IntPair.class, true);// 调用父类的构造函数 &#125; public int compare(WritableComparable w1, WritableComparable w2) &#123; IntPair i1 = (IntPair) w1; IntPair i2 = (IntPair) w2; int l = i1.getFirst(); int r = i2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125;&#125; ​ TextPair 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package TextPair;// cc TextPair A Writable implementation that stores a pair of Text objects// cc TextPairComparator A RawComparator for comparing TextPair byte representations// cc TextPairFirstComparator A custom RawComparator for comparing the first field of TextPair byte representations// vv TextPairimport java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;import org.apache.hadoop.io.WritableUtils;public class TextPair implements WritableComparable&lt;TextPair&gt; &#123; private Text first; private Text second; public TextPair() &#123; set(new Text(), new Text()); &#125; public TextPair(String first, String second) &#123; set(new Text(first), new Text(second)); &#125; public TextPair(Text first, Text second) &#123; set(first, second); &#125; public void set(Text first, Text second) &#123; this.first = first; this.second = second; &#125; public Text getFirst() &#123; return first; &#125; public Text getSecond() &#123; return second; &#125; @Override public void write(DataOutput out) throws IOException &#123; first.write(out); second.write(out); &#125; @Override public void readFields(DataInput in) throws IOException &#123; first.readFields(in); second.readFields(in); &#125; @Override public int hashCode() &#123; return first.hashCode() * 163 + second.hashCode(); &#125; @Override public boolean equals(Object o) &#123; if (o instanceof TextPair) &#123; TextPair tp = (TextPair) o; return first.equals(tp.first) &amp;&amp; second.equals(tp.second); &#125; return false; &#125; @Override public String toString() &#123; return first + \"\\t\" + second; &#125; @Override public int compareTo(TextPair tp) &#123; int cmp = first.compareTo(tp.first); if (cmp != 0) &#123; return cmp; &#125; return second.compareTo(tp.second); &#125; // ^^ TextPair // vv TextPairComparator public static class Comparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public Comparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); int cmp = TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); if (cmp != 0) &#123; return cmp; &#125; return TEXT_COMPARATOR.compare(b1, s1 + firstL1, l1 - firstL1, b2, s2 + firstL2, l2 - firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; &#125; static &#123; WritableComparator.define(TextPair.class, new Comparator()); &#125; // ^^ TextPairComparator // vv TextPairFirstComparator public static class FirstComparator extends WritableComparator &#123; private static final Text.Comparator TEXT_COMPARATOR = new Text.Comparator(); public FirstComparator() &#123; super(TextPair.class); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; try &#123; int firstL1 = WritableUtils.decodeVIntSize(b1[s1]) + readVInt(b1, s1); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); return TEXT_COMPARATOR.compare(b1, s1, firstL1, b2, s2, firstL2); &#125; catch (IOException e) &#123; throw new IllegalArgumentException(e); &#125; &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; if (a instanceof TextPair &amp;&amp; b instanceof TextPair) &#123; return ((TextPair) a).first.compareTo(((TextPair) b).first); &#125; return super.compare(a, b); &#125; &#125; // ^^ TextPairFirstComparator // vv TextPair&#125;// ^^ TextPair","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.cc/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems(2)","date":"2017-05-15T08:43:49.000Z","path":"2017/05/15/Machine-Learning-Recommender-Systems-2/","text":"1. 协同过滤，给用户推荐物品1.1 基于用户的协同过滤算法UserCF 主要通过分析用户的行为记录，计算物品之间的相似度 该算法认为物品A和物品B具有很大的相似度是因为喜欢物品A的用户大部分也喜欢物品B 1.1.1 步骤 找到和目标用户相似的用户集合 找到这个集合中的用户喜欢，且和目标用户没有听说过的的物品推荐给目标用户 1.1.2 计算相似度 得到用户之间的兴趣相似度之后，寻找最相近的K个用户 1.2基于物品的协同过滤算法1.2.1 计算步骤 计算物品之间的相似度 根据物品的相似度和用户的历史行为给用户生成推荐列表 1.2.2 计算相似度 1.3 隐语义模型 UserCF 找到和他们看了同样书的其他用户，即是兴趣相似用户。然后给用户推荐这些用户相似用户所喜欢的其他书籍 ItemCF 在已经看过的书中，寻找相似的书，即和这些看过的书，同时出现在其他用户的看过的书中，然后推荐这些其他的书 其他方法 可以对物品的兴趣进行分类，对于某个用户，首先得到他的兴趣分类，然后从这些分类中挑出他可能喜欢的物品 如何给物品进行分类 隐含语义分析技术(latent variable analysis) 如何确定用户的兴趣，即对哪些类的物品感兴趣，以及感兴趣的程度 对于一个给定的类，选择哪些属于这个类的物品推荐给用户？以及如何确定这些物品在这个中的权重。即如何在这个类中，挑选出合适的物品推荐给用户 1.3.1 LFM (latent factor model)1.4 用户标签数据 User Generated Content2. 需要解决的问题 - 评分预测（用户A对电影x的评分预测） 2.1 实验方法2.1.1 划分训练集 与时间无关，可以均匀分布随机换分数据集。即对每一个用户，随机选取一些评分记录作为训练集，剩下的作为测试集 与时间相关，那么需要将用户的旧行为作为训练集，讲用户的新行为作为测试集 例子 Netflix的评分预测系统中，将每个用户的评分记录按照从早到晚进行排序，然后将用户最后的10%的评分记录作为测试集，90%的评分记录作为训练集。 2.1.2 评分标准 RMSE = \\frac {\\sqrt {\\sum_{(u, i) \\in T} (r_{ui} - \\hat r_{ui})^2 }} {|Test|}2.1.3 评分预测算法 平均值 最简单的方法：利用平均值预测用户对物品的评分 全局平均值 计算在整个训练集中，所有评分记录的评分平均值 用户评分平均值 计算用户u在训练集中所给出的评分的平均值 物品评分平均值计算该物品w在训练集中被评价了的评分的平均值 用户对物品分类的平均值 假设这里有两个分类，一个是用户分类函数U，一个是物品分类W，U(u)定义了用户u所属的分类，W(w)定义了物品w所属的分类。那么我们可以利用训练集中同类用户对同类物品评分的平均值预测用户对物品的评分 之前是三种平均值其实是用户对物品分类的平均值的一种特例 U(u) = 0，W(w) = 0，那么就是全局平均值 U(u) = u，W(w) = 0，那么就是用户评分平均值 U(u) = 0，W(w) = w，那么就是物品评分平均值 在以上的方法中，我们并没有考虑到用户的活跃度和物品的流行程度。实际上可以将这两点考虑进去，对活跃用户和流行的物品给定一点penalty 2.1.4 基于领域的方法（基于用户的领域和基于物品的领域算法） 基于用户的领域算法 \\hat r_{ui} = \\overline r_u + \\frac {\\sum_{v \\in S(u, K) \\bigcap N(i)} w_{uv}*(r_{vi} - \\overline r_v)} {\\sum_{v \\in S(u, K) \\bigcap N(i)} |w_{uv}|}E(r_u) 是用户u对他点评过的所有物品评分的平均值 S(u, K) 是和用户u兴趣最相似的K个用户的集合 N(i) 是对物品i点评过分数的用户集合 r_vi 是用户v对物品i的评分 E(r_v) 是用户v对他评分过的所有物品评分的平均值 w_uv 是用户u和v之间的相似度 ​ 基于物品的领域算法 \\hat r_{ui} = \\overline r_i + \\frac {\\sum_{j \\in S(i, K) \\bigcap N(u)} w_{ij}*(r_{uj} - \\overline r_i)} {\\sum_{j \\in S(i, K) \\bigcap N(u)} |w_{ij}|}E(r_i) 是物品i的平均分，是所有用户对物品i点评过的分数的平均值 S(i, K) 是和物品i最相似的K个物品的集合 N(u) 是用户u点评多分数的物品集合 r_uj 是用户u对物品j的评分 w_ij 是物品i和j之间的相似度 2.1.5 计算相似度 余弦相似度（cosine similarity) w_{ij} = \\frac {\\sum_{u \\in U} r_{ui} * r_{uj}} {\\sqrt{\\sum_{u \\in U}r_{ui}^2 * \\sum_{u \\in U}r_{uj}^2}} 皮尔逊系数（pearson correlation） w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_i) * (r_{uj} - \\overline r_j)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_i)^2 * \\sum_{u \\in U}(r_{uj} - \\overline r_j)^2}} 修正余弦相似度（adjust cosine similarity）（在MovieLens数据集上效果最好） w_{ij} = \\frac {\\sum_{u \\in U} (r_{ui} - \\overline r_u) * (r_{uj} - \\overline r_u)} {\\sqrt{\\sum_{u \\in U}(r_{ui} - \\overline r_u)^2 * \\sum_{u \\in U}(r_{uj} - \\overline r_u)^2}} 2.1.6 隐语义模型的矩阵分解模型 （Latent Factor Model)隐含类别模型、隐语义模型等，在本质上都是为了找出某一东西的潜在的主题或者分类。在推荐系统中，可以基于用户的行为利用隐语义模型，对item进行自动聚类，这样可以避免了人为分类的偏差。 举例说明： ​ 用户A喜欢看数学，历史和计算机的书籍 ​ 用户B喜欢看机器学习，编程语言和离散数学方面的书籍 ​ 用户C喜欢看大师的作品，比如专门看Knuth或者Jiawei Han的书籍 那么系统在对用户的喜好进行推荐的时候，需要找出同属于用户兴趣圈子的书籍。对于之前提到过的三个用户来说 ​ 用户A的圈子：数学、计算机、历史 ​ 用户B的圈子：这三本书可以同时分到计算机的圈子，但是离散数学却又可以分到数学圈子去 ​ 用户C的圈子：根据作者不同来划分圈子，那么这个圈子就和之前用户A、B的角度是不同的。 假设让人工来完成之前书籍的分类，那么经常会碰到划分的粒度不同，角度不同等情况。 同时，需要注意一下两点： 用户A对这三个类别书籍感兴趣，不代表不对其他类别的数据感兴趣 同一本书可以属于多个类别，所以每本书在每个类别里面都有一个权重，权重值越大，说明属于这个类别的可能性越高 那么，LFM是如何解决上面的几个问题的呢？ 回答上面的问题前，我们可以思考一下我们需要做的哪些工作。 实际上，我们可以将所有的User看做列，把所有的Item看做行，构建一个mxn的二维矩阵，如下图 最左边的R矩阵是一个user-item矩阵，矩阵值Rij表示的是用户i对item j的兴趣度，或者是评分。那么我们的评分预测，就可以转换成对这个矩阵中的某些值（缺失值）的预测，同时需要保证我们的预测值对于这个矩阵的扰动的最小的。（即补全之后矩阵的特征值和补全之前的特征值相差不大，具体见SVD分解） 而右边的两个P和Q矩阵就是LFM所做的，LFM算法从数据集汇总抽出若干个class，计算出所有user对这些class的感兴趣长度，即P矩阵。同时计算出所有item在这些class中的权重值，即Q矩阵。P矩阵作为user和item之间连接的桥梁，所以R可以表示为P矩阵和Q矩阵相乘。 R_{UI} = P_U Q_I = \\sum_{k=1}^K = P_{U,k}Q_{k,I}以下是LFM的优点： 不需要关心矩阵P是怎么构建的，即不需要如何给物品进行聚类、划分等（角度，粒度等） Q矩阵中，对于一个item并不是明确给划分到某一个分类，而是计算这个item属于这些所有类别的概率，值越大可能性越高 同理，P矩阵中，对于一个user并没有限定在某些class中，而是计算这个user对于这些classes的感兴趣程度 虽然我们知道了LFM为我们做了哪些工作，但是我们还是不知道该如何求解出矩阵P和矩阵Q中的参数值，一般的做法是最优损失函数来求参数。 传统的SVD分解 传统方法中，给定一个user-item的矩阵R。 首先需要对评分矩阵R中的缺失值进行简单的补全，比如用全局平均值，或者用户/物品的平均值补全，得到补全后的矩阵R’ 得到补全后的矩阵R’，接着可以利用SVD分解，将R’分解成如下形式 R' = U^TSV \\\\ R \\in R^{m * n} \\\\ U \\in R^{k * m} \\\\ V \\in R^{k * n} \\\\ S \\in R^{k * k}U和V是两个正交矩阵，S是对角矩阵，对角线上的每一个元素都是矩阵的奇异值。 为了对R’进行降维，可以取最大的f个奇异值组成对焦矩阵Sf，并且找到这个f个奇异值汇中每个值在U、V矩阵中对应的行和列，得到Uf、Vf，从而得到一个降维后的评分矩阵： R_f'=U_f^TS_fV_f该方法的一些缺点： 在现实中，R矩阵基本上会是一个稀疏矩阵，即95%的数据是缺失的，同时该矩阵非常的大。一经补全，该矩阵就是一个稠密矩阵，储存开销非常的大 计算复杂度非常的高，特别是对于补全之后的稠密矩阵 ​ Funk-SVD分解，即 Latent Factor Model（LFM） http://sifter.org/~simon/journal/20061211.html 从矩阵的角度，将评分矩阵R分解成两个低纬度相乘： \\hat R = P^TQ \\\\ R \\in R^{m*n} \\\\ P \\in R^{f*m} \\\\ Q \\in R^{f*n}P、Q是两个降维后的矩阵，那么对于用户u对于物品i的评分的预测值可以^R(u, i) = ^r_ui，可以通过如下公式计算： \\hat r_{ui} =b_{ui} + \\sum_fp_{uf}q_{if} \\\\ p_{uf} = P(u, f) \\\\ p_{if} = Q(i, f)Simon Funk-SVD的思想是直接通过训练集中的观察值，利用最小化RMSE学习P、Q矩阵。 损失函数的计算： C(p, q) = \\sum_{(u, i) \\in Train} (r_{ui} - \\hat r_{ui})^2 = \\sum_{(u, i) \\in Train}(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})^2 + \\lambda(||p_u||^2 + ||q_i||^2) \\\\ \\hat r_{ui} = \\mu + b_u + b_i + p_u^Tq_i要最小化上面的损失函数，可以利用随机梯度下降法。以下是简单的推导该公式 上面的cost function中欧两个参数p和q，首先 对他们分别求偏导，求出最快下降的方向 \\frac {∂C} {∂p_{uf}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})*q_{ik} + 2\\lambda p_{uk} \\\\ \\frac {∂C} {∂q_{if}} = -2(r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if})*p_{uk} + 2\\lambda q_{ik} 然后根据随机梯度下降法，需要将参数沿最快的下降方向前进，即可得到如下的递推公式： p_{uf} = p_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) * q_{ik} - \\lambda p_{uk}) \\\\ q_{if} = q_{uf} + \\alpha ((r_{ui} - \\sum_{f=1}^Fp_{uf}q_{if}) * p_{uk} - \\lambda q_{ik}) 所以，执行LFM需要： 根据数据集初始化P和Q矩阵（如何初始化） 确定四个参数：分类书F，迭代次数N，学习速率α（α = 0.9） 和正则化参数*λ ​ 伪代码 1234567891011121314151617181920def LFM(user_items, F, N, alpha, lambda): #初始化P,Q矩阵 [P, Q] = InitModel(user_items, F) #开始迭代 For step in range(0, N): #从数据集中依次取出user以及该user喜欢的iterms集 for user, items in user_item.iterms(): #随机抽样，为user抽取与items数量相当的负样本，并将正负样本合并，用于优化计算 samples = RandSelectNegativeSamples(items) #依次获取item和user对该item的兴趣度 for item, rui in samples.items(): #根据当前参数计算误差 eui = eui - Predict(user, item) #优化参数 for f in range(0, F): P[user][f] += alpha * (eui * Q[f][item] - lambda * P[user][f]) Q[f][item] += alpha * (eui * P[user][f] - lambda * Q[f][item]) #每次迭代完后，都要降低学习速率。一开始的时候由于离最优值相差甚远，因此快速下降； #当优化到一定程度后，就需要放慢学习速率，慢慢的接近最优值。 alpha *= 0.9 Baseline Estimats 对比基线考虑到重口难调，有些user会给出比较高的分数，有些要去严格的users会给出比较低的分数，而有些质量好的商品会得到比较高的分数，质量差的分数较低。所以为了调整这些，引入了baseline estimate。比如为了估计某个用户u会给电影i打的评分： b_{ui} = u + b_u + b_i $\\mu$ 是该物品的整体平均值 $b_u$ 是用户打分相对整体用户打分平均值的偏差 $b_i$ 是该物品相对整体平均值的偏差 举个例子：预测豆瓣用户小明给电影泰坦尼克号的评分 泰坦尼克号在豆瓣上的平均分数是3.7分(u) 泰坦尼克号的平均分数又比所有电影在豆瓣上的平均分数高0.5分($b_i$) 但是小明是个电影爱好者，比平均用户打分偏低0.3分($b_u$) 所以不考虑regularized的话，预测小明给泰坦尼克号的评分应该是 3.7 - 0.3 + 0.5 = 3.9 所以这里，得到 $b_u$ 和 $b_i$ 的值很重要 这里对$b_u$和$b_i$加入了penality，为了防止overfitting。 这里$r_ui$是我们训练数据中的已知rating，可以直接使用 $\\mu$是整体平均值，也可以根据训练数据计算出来 $\\lambda_2$和$\\lambda_3$是我们手动设置的参数，MovieLens数据上，20比较合适 R(u)和R(i)为用户urating过的物品的集合，和物品i被rating过用户的集合 所以我们可以根据这些数据，计算出$b_u$和$b_i$b_i$（注意维度） b_i = \\frac {\\sum_{u \\in R(u)}(r_{ui} - \\mu)} {\\lambda_2 + |R(i)|} \\\\ b_u = \\frac {\\sum_{i \\in R(u)}(r_{ui} - \\mu - b_i)} {\\lambda_3 + |R(u)|}除了上面的方法，还有一种更为简便的方法计算$b_u$和$b_i$，就是直接使用user，item的rating的平均值估计 b_u = \\frac {\\sum R(u)} {len(R(u))} \\\\ b_i = \\frac {\\sum R(i)} {len(R(i))}Neighborhood Models item-oriented algorithm: a rating is estimated using known rating made by the same user on similarity items. user-oriented algorithm: estimate unknown ratings based on recorded ratings of like minded users. Similarity measure between items Pearson correlation 计算物品i和j的相似度 s_{ij} = \\frac {n_{ij}} {n_{ij} + \\lambda_2}ρ_{ij} n_ij 表示都对物品i和j评分过的用户的数量 ρij是皮尔逊系数，通常取？？？ λ2 通常取100 预测评分 \\hat r_{ui} = b_{ui} + \\frac{\\sum_{j \\in S^k(i; u)} s_{ij}(r_{ui} - b_{ui})} {\\sum_{j \\in S^k(i; u)}s_{ij}} 在用户u所有评分过的物品中，找到相似度和i最高的k个物品（k-neighbors），用$S^k(i; u)$表示 但是这种算法还是有一些局限性，比如对于两个完全没有关系的物品之间的预测。或者是对于某些物品，最为相似的k个物品缺失，所以可以修正以上的公式（不是很明白这里） \\hat r_{ui} = b_{ui} + \\sum_{j \\in S^k(i; u)} \\theta^u_{ij}(r_{ui} - b_{ui}) \\\\ \\{\\theta^u_{ij} | j \\in S^k(i; u)\\}","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"}]},{"title":"Hadoop权威指南笔记（二）","date":"2017-05-04T14:13:45.000Z","path":"2017/05/04/Hadoop权威指南笔记（二）/","text":"1. Mapper 2. Reducer 3. MapReduce数据都是以 key-values pairs 的形式在Mapper和Reduce之间传递的 Mapper输出的 key-value pairs 应该和 Reducer输入的 key-value pairs 类型是一样的 在Reducer中，是 key-valueLists pairs 的形式 4. Deriver也就是初始化配置MR然后调用执行，一般可以写成如下形式： 123456789101112131415161718192021222324252627282930313233// 老APIpublic void run(String inputPath, String outputPath) throws Exception &#123; JobConf conf = new JobConf(WordCount.class); conf.setJobName(\"wordcount\"); // the keys are words (strings) conf.setOutputKeyClass(Text.class); // the values are counts (ints) conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(MapClass.class); conf.setReducerClass(Reduce.class); FileInputFormat.addInputPath(conf, new Path(inputPath)); FileOutputFormat.setOutputPath(conf, new Path(outputPath)); JobClient.runJob(conf);&#125;// 新APIpublic void run(String IN, String OUT) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1);&#125; 5. Data Flow在hadoop中所有的Mapper和Reducer都是独立工作的，这也是hadoop分布式能够稳定运行的原因之一(有利于容错处理)。在MapReduce整个过程中，只有一次数据相互交互，就是Mapper到Reducer这个过程。从Mapper输出的所有的intermediate data会被统一shuffle(必须要等所有MR执行完毕吗？)，然后同一个key的key-value pairs 会被分配到同一个reducer中去。 6. A Closer Look第五部分中，我们看到的是MR的宏观流程，具体的流程具体可以分为 Map -&gt; Combiner -&gt; Partitioner -&gt; Sort -&gt; Shuffle -&gt; Sort -&gt; Reduce(这里对shuffle的定义有点不同，个人认为从map的输出到reduce的输入这段过程可以称之为shuffle。同时根据官方文档，combiner是在mapper最终输出前多次调用的，以及在reducer里也有调用) 下图中，input files在进入到Mapper之前时，会对这些文件split，因为一个mapper一般是64MB或者128MB，当大于的时候需要对输入文件处理，然后传给RecordReaders，以key-value pairs的形式传给mapper。 对于所有Mapper的output，先会对其进行parition操作，也就是决定去哪一个Reducer。当确定好哪些Reducer，这些key-value paris 就会传入到该Reducer相应的分区，然后对齐进行排序。最后将sort好的key-valueLists 传入到Reducer进行处理。 7. ShuffleHadoop The Definitive Guide P197 Mapper 根据官方图，Mapper的output出来的key-values pairs会先进入到buffer memory(默认100MB大小)，但buffer到80%容量的时候，buffer里面的内容会spill到disk去，如果此时buffer还未慢的情况下，mapper继续输出到buffer，如果满了的话mapper会被block，直到可以写入。 在buffer中的内容spill到disk之前，还有一个partitioner的步骤。对这些即将写入到disk的内容分组，同一个key和同一个reducer的ker-value pairs会在一起。然后这些key-value pairs排序(sort)。如果此时我们定义了combiner function，在输出前，这些pairs会先combine，然后输出。也就是说combiner function是在partitioner之后? (Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer) 每当buffer达到那个threshold的时候，buffer里面的内容写入到disk中(注意是每个cluster自己的local disk，而不是HDFS)，此时会新建一个临时的spill文件，每次写入都会新建一个，然后这个map结束前这些spill files会被merge到一个partitioned和sorted的文件里去。除了之前partition的输出后调用了一次combiner一次之外，但合并这些个spill files( &gt;=3 )的时候，会继续调用combiner去合并同一个reducer里的同一个key的value，所以combiner在output file被生成之前，会被调用许多次，以减少之后io的次数。但当spill files只是1个或则两个的时候，并不会调用combiner。注意有些情况下combiner并不适合使用，比如求平均值。 而不同mapper生成的spill文件最终会被merge成 {key:[v1, v2, v3…], …}这种形式。(这里有疑问，博客图和官方图有点不一样。按照官方图的理解，一个mapper对应的是一个spill file，所以最终是多个spill files？还是这些spill files在传给Reducer之前会被merge成上述的list形式？一个cluster一个最终的output file)​ Reducer 之前mapper端的所有工作已经完成了，所有的mapper的output都已经被写入到了一个output文件里面去了。那么Reducer就是要把这个文件里面的key-valueLists pairs 分给不同的reducers，而这个过程称之为Fetch，就是将相应的key-valueList pairs 拉取到相应的reducers里面去。 在Reducer阶段，每个reducer会调用线程从多个不同的cluster的output file里面fetch数据，然后对这些数据merge。这里的过程和之前的mapper有点像。reducer也有一个buffer memory(通过JVM的heap size来设置)，fetch的数据会被传到里面(如果放得下)，当buffer达到threshold的值的时候，buffer里面的内容会被merge然后spill到disk里面去(实际上有多种形式存放这个文件，这里不讨论)，如果之前我们已经定义了combiner，这里combiner也会被调用。直到所有的mapper的output file都被fetch到一个文件里去，reducer会在输入前sort里面的内容(实际上merge的时候已经sort了)，然后这个已经排好序的file就会被传到reducer里面执行，最终输出到HDFS上。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.cc/tags/MapReduce/"}]},{"title":"Spark笔记","date":"2017-04-10T06:55:40.000Z","path":"2017/04/10/Spark笔记/","text":"1. Resilient Distributed Dataset (RDD)RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators. RDD是一个容错的、并行的数据结构，可以让用户显示地将数据存储到磁盘和内存中，并能控制数据的分区。 Resilient RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. Formally, an RDD is a read-only, partitioned collection of records. Rdds can be created throught deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel. There two ways to create RDDs. Parallelizing parallelizing an existing collection in your driver program Referencing reference a dataset in an external storage system, such as a shared file system, HDFS, HBase, or ant data source offering a Hadoop Input Format Spark提供了RDD上的两类操作：transformation 和 action Transformation: return a new RDD 当对RDD进行transformation操作的时候，这些操作不会立刻就执行，而是将这些操作记录下来。如果有多个transformation操作时，每一次变换都是一个接着一个，此时就形成了一个有向无环图。这个图就是数据容错的关键所在。如果出现数据丢失的时候，只需要查找这个图就能根据丢失的RDD进行数据恢复 Action: evaluates and returns a new value 当对RDD做action操作的时候，这类action一般作用是返回一个值或者数组等，或者是将数据持久化到磁盘当中。此时执行action，会真正的提交job，执行之前的transformation记录的DAG。执行DAG策略中，会有多个stage，每一个stage钟有多个task，这些task就会被分配到各个nodes进行执行 The difference between flatMap and Map flatMap Example map: 对RDD每个元素转换，将函数用于RDD中的每个元素，将返回值构成新的RDD。flatMap: 对RDD每个元素转换, 然后再扁平化（即将所有对象合并为一个对象）。将函数应用于rdd之中的每一个元素，将返回的迭代器的所有内容构成新的rdd Example1： 123456// data 有两行数据，第一行 a b c，第二行 1 2 3scala&gt; data.map(line1 =&gt; line1.split(\",\")).collect()res11: Array[Array[String]] = Array(Array(a, b, c),Array(1, 2, 3))scala&gt; data.flatMap(line1 =&gt; line1.split(\",\")).collect()res13: Array[String] = Array(a, b, c, 1, 2, 3) Example2： 1234567891011scala&gt; val rdd = sc.parallelize(List(\"coffee panda\",\"happy panda\",\"happiest panda party\"))scala&gt; rdd.map(x=&gt;x).collectres0: Array[String] = Array(coffee panda, happy panda, happiest panda party) scala&gt; rdd.map(x=&gt;x.split(\" \")).collectres1: Array[Array[String]] = Array(Array(coffee, panda), Array(happy, panda), Array(happiest, panda, party))// 相比之前，flatMap将几个array合并成了一个arrayscala&gt; rdd.flatMap(x=&gt;x.split(\" \")).collectres2: Array[String] = Array(coffee, panda, happy, panda, happiest, panda, party) 2. References 理解Spark的核心RDD Spark RDD API Examples","tags":[{"name":"Spark","slug":"Spark","permalink":"http://chenson.cc/tags/Spark/"}]},{"title":"字符串搜索算法 - BM","date":"2017-04-02T04:28:44.000Z","path":"2017/04/02/字符串搜索算法-BM/","text":"当我们要在某一个文本中要匹配某一个字符串的时候，我们最简单的方法就是一个一个匹配，也就是从头开始匹配文本和字符串，但发现不同的时候，就把整个字符串右移一位，然后从头重新和文本比较。当全部相同的时候，则右移字符串的长度。在之前的 KMP 算法中，我们是找到尽可能右移的最大位数，而不是一位一位的移动，这样相比原先效率已经提高了很多了。但在大部分的编辑器中，“查找”功能使用的不是 KMP ， 而是 BM 算法。 1. Boyer-Moore Algorithm和 KMP 不同的是，BM 是从 Pattern P 倒着匹配上来的 首先定义两个预处理的方法 坏字符 (Bad Character Heuristic) 当文本 T 中的某个字符跟 Pattern P 的某个字符不匹配时，我们称文本 T 中的这个不匹配的字符为坏字符。 好后缀 (Good Suffix Heuristic) 当文本 T 中的某个字符跟 Pattern P 的某个字符不匹配时，我们称文本 T 中的已经匹配的字符串为好后缀。(因为算法从尾巴到头部比较) 好后缀的位置以最后一个字符为准。假定”ABCDEF”的”EF”是好后缀，则它的位置以”F”为准，即5（从0开始计算）。 如果”好后缀”在搜索词中只出现一次，则它的上一次出现位置为 -1。比如，”EF”在”ABCDEF”之中只出现一次，则它的上一次出现位置为-1（即未出现）。 如果”好后缀”有多个，则除了最长的那个”好后缀”，其他”好后缀”的上一次出现位置必须在头部。比如，假定”BABCDAB”的”好后缀”是”DAB”、”AB”、”B”，请问这时”好后缀”的上一次出现位置是什么？回答是，此时采用的好后缀是”B”，它的上一次出现位置是头部，即第0位。这个规则也可以这样表达：如果最长的那个”好后缀”只出现一次，则可以把搜索词改写成如下形式进行位置计算”(DA)BABCDAB”，即虚拟加入最前面的”DA”。 按照正常比较方法，当发现一个坏字符的时候，会出现一下三种情况 坏字符完全不出现在 Pattern P 中，那么就可以完全跳过这个字符，因为我们根本不可能匹配到这个字符。 坏字符出现在还未比较的前缀当中 (只出现在前缀，出现在前缀和后缀) 此时将 Pattern P 右移对齐这两个字符，然后从尾部继续比较 坏字符只出现在之前的好后缀当中（也就是已经比较过了） 对这种情况不做处理 根据上面几种情况，我们可以总结出以下坏字符规则： 后移位数 = 坏字符的位置 - 搜索词中的上一次出现位置 这里有两种特殊情况 如果坏字符不存在于 Pattern P 中，则最后一次出现的位置为 -1。 如果坏字符在 Pattern P 中的位置位于失配位置的右侧，则此启发法不提供任何建议。 除了坏字符，我们同样可以利用好后缀来提高查找效率 模式后移位数 = 好后缀在模式中的当前位置 - 好后缀在模式中最右出现且前缀字符不同的位置 举个栗子计算： 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 坏字符的移动 此时坏字符的位置是4，坏字符 I 不在字符串当中，移动的个数为： 4 - (-1) = 5 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 好后缀的移动 此时好后缀是 MPLE，其中 PLE 出现在头部，最右位置为 (01) 2。 此时好后缀的字符是8(以最后一个为准)，移动的个数为：8 - 2 = 6 12345Text: H E R E _ I _ S A _ S I M P L E _ E X A M P L EPattern: P L E X A M P L E 0 1 2 3 4 5 6 7 8 相比之下，我们可以发现好后缀的查找效率更高。 如何计算好后缀我们之前已经提过了，继续以上面最后一个例子讲解。 MPLE : 未出现，最右出现的位置为 -1； PLE : 未出现在头部，最右出现的位置为 -1； LE : 未出现在头部，最右出现的位置为 -1； E : 出现在头部，补充虚拟字符 ‘MPL’E，前缀字符为空，最右出现的位置为 0； 此时，所有的”好后缀”（MPLE、PLE、LE、E）之中，只有”E”在”EXAMPLE”还出现在头部，所以后移 6 - 0 = 6位。 如果之前我们只利用坏字符的话，移动的位数只有 2 - (-1) = 3位 所以我们最终的移动位置，是在这两者之间取最大值来移动。 所以在查找之前，我们需要预计算生成坏字符规则表 和 好后缀规则表，到时候只需要查表就可以了。继续上面的例子。 根据上面计算结果，我们将 Pattern P 右移6位。然后重新从尾部开始比较 文本中P不匹配，此时只能使用坏字符规则，P上一次出现的位置为4，则移动6-4=2位 移动2位后，发现文本中的E 与 Pattern P中的E 匹配，则继续倒序比较，直到发现全部匹配，则匹配到的第一个完整的模式 P 被发现。 继续下去则是依据好后缀规则计算好后缀 “E” 的后移位置为 6 - 0 = 6 位，然后继续倒序比较时发现已超出文本 T 的范围，搜索结束。 2. References 字符串匹配的Boyer-Moore算法 Boyer-Moore 字符串匹配算法","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.cc/tags/Algorithm/"}]},{"title":"PostgreSQL复习笔记","date":"2017-03-23T23:38:55.000Z","path":"2017/03/24/PostgreSQL复习笔记/","text":"1 常见问题1.1 匹配一致的编码规则123SET character_set_client=&apos;utf8&apos;; -- gbkSET character_set_connection=&apos;utf8&apos;; -- gbkSET character_set_results=&apos;utf8&apos;; -- gbk 1234-- 创建数据库时候设置CREATE DATABASE `test`CHARACTER SET &apos;utf8&apos;COLLATE &apos;utf8_general_ci&apos;; 1.2 如何理解索引索引字面上理解就是对数据所建立目录，它可以加快我们的查询速度，但是同时也降低了增删改的速度。 创建原则 不要过度使用索引 最好在查询频繁的列上使用索引 如果构建索引，这一列尽量是离散值，而不要过于连续的区间 索引的类型 普通的索引 index 唯一的索引 unique index 一张表上，只能有一个主键，但是可以有一个或是多个唯一索引 主键索引 primary key 不能重复 12-- 查看一张表上的所有索引show index from TABLE_NAMES; 1.3 模糊查询 % 匹配任意字符 _ 匹配单个字符 举个栗子 1234567891011SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC%&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;%ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC&apos;;SELECT * from TABLE_NAMESWHERE patter like &apos;_ABC_&apos;; 1.4 理解 COUNT 见3.1 聚集函数 1.5 理解 UNION 和 UNION ALL UNION 用于合并两个或是多个SELECT语句的结果集 注意： SELECT语句必须拥有相同的数量的列 列的需要拥有相似的数据类型 每条SELECT语句中的列的顺序必须是一致的 结果不允许有重复，否则使用 UNION ALL 123456SELECT mid, sex, ageFROM TABLE_1UNION -- UNIAON ALL -- 允许有重复的值出现在结果集中SELECT mid, sex, ageFROM TABLE_2;-- ORDER BY mid; -- 可以对其结果进行排序，注意的是排序只是针对合并后的结果集排序 1.6 理解 JOIN（左链接，内链接和外链接） 不同的几种JOIN类型，以及之间的差异 JOIN：如果表中至少有一个匹配，则返回行 LEFT JOIN：即使右表中没有匹配，也从左边返回所有的行 RIGHT JOIN：即使左表中没有匹配，也从右表中返回所有的行 FULL JOIN：只要其中一个表存在匹配，就返回行 INNER JOIN 平常我们需要链接两个表的时候，可以用以下方法 123SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM Persons, OrdersWHERE Persons.Id_P = Orders.Id_P 同时，我们也可以使用JOIN来实现上面的语句 12345SELECT Person.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName LEFT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意到上面的，左边所有的行都返回了，即使没有出现在右表当中，没有的值为NULL RIGHT JOIN 12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 注意，即使左边没有全部匹配到右边，依然在最后的OrderNo中，返回了右表所有的行数，没有的值为NULL 1.7 理解 HAVING 见3.2 在SQL中增加HAVING子句的原因是由于WHERE中无法使用聚合函数 2 数据库的基本操作2.1 表的使用 使用列约束建表 12345678CREATE [TEMPORARY] TABLE 表名 ( -- TEMPORARY 建立一张临时的表col_not_null INT NOT NULL, -- 列名 类型 &#123;约束1 约束2 ...&#125;col_unique INT UNIQUE,col_prikey INT PRIMARY KEY, -- NOT NULL + UNIQUE 主键col_default INT DEFAULT 42,col_check INT CHECK(col_check &lt; 42)col_ref INT REFERENCES -- 约束这个值必须是另一独立的表的某个列中的某个值); 使用表级约束建表 1234567CREATE TABLE 表名 ( myKey_1 INT, myKey_2 INT, myString varchar(15), CONSTRAINT cs1 CHECK (myString &lt;&gt; &apos;&apos;), -- 不能位空字符串 CONSTRAINT CS2 PRIMARY KEY (myKey_1, myKey_2)); -- 修改表结构 添加新列 1ALTER TABLE 表名 ADD COLUMN 列名 类型; 重命名新添加的列 1ALTER TABLE 表名 RENAME COLUMN 列名 TO 新列名; 改变一些约束和其他规则 12ALTER TABLE 表名 DROP CONSTRAINT cs1; -- DROP约束ALTER TABLE 表名 ADD CONSTRAINT cs3 UNIQUE(列名); --添加新的约束 修改列的类型 1ALTER TABLE 表名 ALTER 列名 TYPE 新类型; 重命名表名 1ALTER TABLE 表名 RENAME TO 新表名; 使用临时表 临时表的功能基本和表是差不多的，区别在于当你的会话结束时，你与数据库连接断开后，临时表会自动被删除。 键的约束 作为一个列的约束的外键（列约束） 1234567CREATE TABLE 表名 ( ... ... customer_id INTEGER NOT NUll REFERENCES customer(customer_id), -- 关联到customer表 ... ...);-- REFERENCES 外表名(外表名中的列) 表级约束 123456CREATE TABLE 表名( ... ... customer_id INTEGER NOT NULL, ... ... CONSTRAINT 表名_列名_fk FOREIGN KEY(列名) REFERENCES 外表名(外面中的列)) 注意：比较推荐的是使用表级约束，而不是混和私用表级和列级约束 ​ 约束名表名_列名_fk允许外面更容易定位错误资源 2.2 视图 建立视图 123CREATE VIEW 视图的名字 AS select系列语句;-- 例子CREATE VIEW item_price AS SELECT item_id, description, sell_price FROM item; 当视图建立好的时候，我们可以像使用表一样来查询这个视图，可以使用SELECT或WHERE语句等。 每次在视图中执行SELECT时，数据都会被重建，所以数据总是最新的，而不是一个在视图被建立的时候冻结的拷贝。也就是当与之相关的表的数据发生该表的时候，VIEW里面的数据也随之改变，而不是储存了建立VIEW的时候的拷贝对象。或者也可以理解类似指针指向原先的表，当原先的表发生变化，这边的数据自然而然的就能够读取出来。 当然，SELECT语句是可以在多个不同的表中提取数据的。 删除和替换VIEW 12DROP VIEW 名字; -- 不影响我们已有的数据CREATE OR REPLACE VIEW 名字 AS 新的select系列语句; 一些与VIEW常用的指令 12\\dv -- 查看当前数据库中的所有的VIEW\\d VIEW的名字 -- 查看具体的某一个VIEW的结构 2.3 INSERT语句 基本插入语句 123INSERT INTO 表名 VALUES (每列的值的列表);INSERT INTO customer VALUES(18, &apos;Mr&apos;, &apos;Jeff&apos;, &apos;Baggott&apos;, &apos;Midtown Street A\\\\33&apos;, &apos;Milltown&apos;, &apos;MT9 8NQ&apos;, &apos;746 3956&apos;); PS：这种操作很危险，SQL注入攻击 推荐的安全方法 12345INSERT INTO 表名(列名的列表) VALUES(跟之前的列的列表对应列的数值);INSERT INTO customer(customer_id, title, fname, lname, addressline, ...)VALUES(19, &apos;Mrs&apos;, &apos;Sarah&apos;, &apos;Harvey&apos;, &apos;84 Willow Way&apos;, ...); PS：避免在插入数据的时候为serial类型的数据提供数值，因为这个是系统自动添加的 访问序列生成器 序列生成器总 是被命名为&lt;表名&gt;_&lt;列名&gt;_seq 123currval(&apos;序列生成器名&apos;);nextval(&apos;序列生成器名&apos;);setval(&apos;序列生成器名&apos;, 新的值); 插入空值 123INSERT INTO customer VALUES(16, &apos;Mr&apos;, NULL, &apos;Smith&apos;,&apos;23 Harlestone&apos;, &apos;Milltown&apos;, &apos;MT7 7HI&apos;, &apos;746 3725&apos;); 使用 \\copy 命令 步骤 先生成如下格式的数据 ​ 再生成如下格式的数据，保存成.sql拓展名的文本文件 ​ 使用\\copy命令导入数据 ​ PS：SQL 里头的 COPY 命令有一个优点：它明显比\\copy 命令快，因为它直接通过服务器进程执行。\\copy 命令是在客户进程中执行，有可能需要通过网络传输所有数据。而且 COPY 在发生错误的时候会更可靠。除非你有大量的数据，否则区别不会太明显。 2.5 从数据库中删除数据 DELETE语句 语法类似于UPDATE语句 1DELETE FROM 表名 WHERE 条件; TRUNCATE语句（不推荐，因为不安全） TRUNCATE语句是把表中所有的数据都删除，但是保留这张表的结构，也就是说最后剩下了一张空表，所有的行都被删除了。 1TRUNCATE TABLE 表名; DROP语句 DROP语句就是删除了整张表的内容，包括表的结构。DROP完之后，这张表就是不存在的了 1DROP TABLE 表名; 2.6 修改数据库中的数据 UPDATE语句 1UPDATE 表名 SET 列名 = 值 WHERE 条件; 1UPDATE customer SET town = &apos;Leicester&apos;, zipcode = &apos;LE4 2WQ&apos; WHERE 一些条件; 如果没有WHERE子句的话，会导致表中的很多甚至是所有的行都被同时更新了 通过另一个表更新 1UPDATE 表名 FROM 表名 WHERE 条件; 3 高级数据选择3.1 聚集函数 Group By and count(*) 错误使用 1SELECT count(*), town FROM customer; 正确使用 1SELECT count(*), town FROM customer GROUP BY town; 结果是获得一个城镇的列表以及每个城镇的客户数量（count(*)) 同时我们也可以用两个columns name在GROUP BY中，然后用ORDER BY指定排列顺序。没有GROUP BY的话按照GROUP BY中的town，lname排序 Having Having是一种用于聚集函数的WHERE从句，我们使用HAVING来约束返回的结果为针对特定的聚集的条件为真的行，比如count(*) &gt; 1 PS：聚集函数无法在WHERE从句中使用，只能用在HAVING从句中 举个栗子： 选出有超过一个客户的城镇，在里使用一个HAVING从句来约束大一的行 SELECT中从句的优先度 1234567SELECTFROM WHEREGROUP BYHAVINGORDER BY -- DESC 下降LIMIT -- 用于限制rows是行数 mysql count(Column_Name) count(*) 统计所有的行 count(column_name) 统计所这个列中值不是NULL的行 count(Distinct column) 只统计这个列中唯一的情况，不重复统计 min min 函数使用一个列名做参数且返回这个列中最小的值。对于 numeric 类型的列，结果应该和预期一 样。对于时态类型，例如 date 的值，它返回最小的日期，日期既可以是过去也可以是未来。对于变长的字符串（varchar 类 型），结果可能和预期有点不同：它在字符串右边添加空白后再进行比较。 min 函数忽略 NULL 值。忽略 NULL 值 是所有的聚集函数的一个特点，除了 count(*)（当然，是否一个电话号码是最小值又是另一个问题了 PS：小心在 varchar 类型的列中使用 min 或者 max，因为结果可能不是你预期的。 max sum Sum 函数使用一个列名作为参数并提供列的内容的合计。和 min 和 max 一样，NULL 值被忽略。 和 count 一样，sum 函数支持 DISTINCT 变体。你可以让它只统计不重复值的和，所以多条值相同的行只会被加一 次 avg 我们要看的最后一个聚集函数是 avg，它使用一个列名做参数并返回这个列数值的平均值。和 sum 一样，它忽略 NULL 值。这里是一个示例 ​ 3.2 子查询 问题一 找到价格比平均价格高的商品项目 方法一（土方法） 方法二（用嵌套WHERE从句） 问题二 找到那些成本高于平均成本但售价低于平售价的产品 方法一（土方法） 方法二（用嵌套WHERE从句） 问题三 - 返回多行记录的子查询 之前的两个问题中，WHERE中的子查询中的SELECT字句返回的最后只有一个值——因为用了count()聚集函数。如果WHERE中的SELECT子句返回多个结果值呢？ 答案是用 WHERE column_name IN (RESULTS) 当然也可以使用NOT IN 来排出选项 3.3 相关子查询​ 在之前的例子中，这里的两个SELECT实际上是不相关的，也就是在内部的SELECT的结果基础上，外部SELECT再做继续查询 ​ 但是相关子查询则是内外的SELECT中，表与表之间是有关系的 格式 PS：建议在相关子查询中使用表的别名 3.4 UNION链接 格式 12345SELECT town FROM tcustUNIONSELECT town FROM customer;SELECT town, zipcode FROM tcust UNION SELECT town, zipcode FROM customer; PS：UNION 连接的使用有一些限制。你要连接的两个从两个表中查找列表的列必须有相同列数，而且选择的每个列必须都有相兼容的类型。 这个查询，虽然非常无意义，但是是有效的，因为 PostgreSQL 可以连接这两个列，即使 title 是一个固定长度的列而 town 是一个变长的列，因为他们都是字符串类型。例如如果我们尝试连接 customer_id 和 town，PostgreSQL 会告诉我们 无法做到，因为这两个列的类型不同。 3.5 自连接 3.6 外链接4. 表的管理​ 5. 事务和锁6. References","tags":[{"name":"SQL","slug":"SQL","permalink":"http://chenson.cc/tags/SQL/"}]},{"title":"Machine Learning - Linear Regression and Logistic Regression","date":"2017-03-17T01:06:30.000Z","path":"2017/03/17/Machine-Learning-Linear-Regression-and-Logistic-Regression/","text":"1. 三要素当一开始接触Andrew在Coursera上的ML公开课的时候，对线性回归和逻辑回归这两种模型有个大体的认识。但是在上完cs229的前三节课，初步了解了这两种模型背后的数学模型，Linear Regression和Logistic Regression背后的概率分布，了解到了这两种概率分布其实只是exponential family中的特例。但同时也开始对一些概念性的东西感觉很模糊，所以觉得有必要好好整理一下这部分的内容。 1. 1 Hypothesis首先对于样本数据，输入x和输出$\\hat y$之间是通过Target function在转换的，也就是 Target function f(x) = $\\hat y$。但是我们并不知道这个f(x)都是怎样的，所以我们假设了这么一个Hypothesis function去模拟这个Target function，使得我们用同样的输入x会的一个预测值$\\hat y$，使得这个不$\\hat y$断逼近真实值y。 1.1.1 Linear Regression H(x) = \\sum_{i=0}^n \\theta_i x_i = \\theta^Tx1.1.2 Logistic Regression H(x) = g(\\theta^Tx) = \\frac 1 {1 + e^{-\\theta^Tx}} \\tag{1}Logit上面公式(1)等价于（即log odds，logit） \\ln \\frac y {1 - y} = \\theta^T x = \\ln \\frac {p(y=1| x; \\theta)} {p(y=0| x; \\theta)} \\tag{2} 公式(1)的推导过程如下 \\begin{align} y &= \\frac 1 {1 + e^{-\\theta^Tx}} \\\\ &= \\frac {1+e^{-\\theta^Tx} - e^{-\\theta^Tx}} {1+e^{-\\theta^Tx}} \\\\ &= 1 - \\frac {e^{-\\theta^Tx}} {1+e^{-\\theta^Tx}} \\end{align}此时有 1-y = \\frac {e^{-\\theta^Tx}} {1 + e^{-\\theta^Tx}}此时用$\\frac {y} {1-y}$，且将上述的式子代入到里面，则有 \\begin{align} \\frac {y} {1-y} &= \\frac {\\frac 1 {1 + e^{-\\theta^Tx}}} {\\frac {e^{-\\theta^Tx}} {1 + e^{-\\theta^Tx}}} \\\\ &= \\frac 1 {e^{-\\theta^Tx}} \\\\ &= e^{\\theta^Tx} \\end{align}两边同时取以e为底的对数，此时有 \\begin{align} \\ln {\\frac y {1-y}} &= \\ln {e^{\\theta^Tx}} \\\\ &= \\theta^Tx \\end{align}似然函数根据上面公式，有如下假设 \\begin{align} p(y^{(i)}=1|x^{(i)};\\theta) &= h_{\\theta}(x^{(i)}) \\\\ p(y^{(i)}=0|x^{(i)};\\theta) &= 1- h_{\\theta}(x^{(i)}) \\\\ p(y^{(i)}|x^{(i)};\\theta) &= h_\\theta(x^{(i)})^{y^{(i)}} \\cdot (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\end{align}当$y_i$ = 0 时候有 \\begin{align} p(y^{(i)}=0|x^{(i)};\\theta) &= h_\\theta(x^{(i)})^{0} \\cdot (1-h_\\theta(x^{(i)}))^{(1-0)} \\\\ &=1-h_\\theta(x^{(i)}) \\end{align}当$y_i$ = 1 时候有 \\begin{align}p(y^{(i)}=1|x^{(i)};\\theta) &= h_\\theta(x^{(i)})^{1} \\cdot (1-h_\\theta(x^{(i)}))^{(1-1)} \\\\&= h_\\theta(x^{(i)})\\end{align}此时可以写出对于所有样本的似然函数 \\begin{align} \\mathcal L(\\theta) &= \\prod_{i=1}^m p(y^{(i)}|x^{(i)};\\theta) \\\\ &= \\prod_{i=1}^m h_\\theta(x^{(i)})^{y^{(i)}} \\cdot (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\end{align}两边同时取对数，此时有 \\begin{align} L(\\theta) &= \\log(\\mathcal L(\\theta)) \\\\ &= \\log(\\prod_{i=1}^m h_\\theta(x^{(i)})^{y^{(i)}} \\cdot (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}) \\\\ &= \\sum_{i=1}^m \\log(h_\\theta(x^{(i)})^{y^{(i)}} + (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}) \\\\ &= \\sum_{i=1}^m y^{(i)}\\log(h_\\theta(x^{(i)}) + (1-y^{(i)})(1-h_\\theta(x^{(i)}))) \\end{align}1. 2 Cost functionCost function呢，实际上也可以叫做Error function，就是用我们上面假设的Hypothe function所预测出来的值$\\hat y$和真实值y之间的误差。而我们需要做的是根据假设出的Hypothesis function，取一个合适的权重值，即theta的值，使其取的一个较低的cost，也就是这预测值与真实值之间的误差最小。 Ordinary Least Squares (Square Loss Function) 常用的方法是最小二乘法 J(\\theta) = \\frac 1 2 \\sum_{i=i}^m (h_\\theta(x^{(i)}) - y^{(i)})^2​ 当然我们也可以从概率的角度来理解这个问题 y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}​ 这里的$\\epsilon$是我们预测值与实际值之间的误差，这个问题我们会留到后面重点讲解。 0-1 Loss Function l(h_\\theta(x^{(i)}), y^{(i)}) = -y^{(i)}\\log(h_\\theta(x^{(i)})) - (1-y^{(i)})\\log(1 - h_\\theta(x^{(i)}))当$y_i$ = 0 时候有 \\begin {align} l(h_\\theta(x^{(i)}), y^{(i)}=0) &= -0\\log(h_\\theta(x^{(i)})) - (1-0)\\log(1 - h_\\theta(x^{(i)}))\\\\ &=\\log(1-h_\\theta(x^{(i)})) \\end {align}当$y_i$ = 1 时候有 \\begin {align} l(h_\\theta(x^{(i)}), y^{(i)}=0) &= -1\\log(h_\\theta(x^{(i)})) - (1-1)\\log(1 - h_\\theta(x^{(i)}))\\\\ &=-\\log(h_\\theta(x^{(i)})) \\end {align}则对于全部样本的损失函数有 J(\\theta) = -\\frac 1 m\\sum_{i=1}^m [y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]仔细看看这公式，是不是和之前的最大似然函数基本一致？所以在这种情况下，有最大化似然函数和最小化损失函数是等下的。 Absolute Loss Function Log Loss Function 1.3 Otimization Algorithm至于怎么使得上面的cost function最小呢，因为对于某些数据，其features有成千上百个，我们很难去找到这个最小的极值点，使得cost function最下，所以这个Algorithm就是优化函数，用来找cost function的最小值的。常用的方法有如下 Gradient Descent 在梯度下降中，我们采用的是 LMS update rules(Least Mean Squares) \\theta_j := \\theta_j + \\alpha(y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)} = \\alpha e^{(i)} x^{(i)} 当我们的预测值与实际值之前的误差ϵ很小时，我们就只需要对θ做出很小的调整，反之，说明当前的θ不对，需要调整的幅度比较大。直到最后收敛为止。 上面Repeat中的步骤实际是等同于cost function对θ求导的过程，所以为了保证收敛的效果，cost function应该是要 convex fuction，就不会导致停留在了local optim点。 推导过程如下： 可视化后大概的过程如下： 根据对于哪些θ求导，Gradient Descent还可以继续分成不同的几种方法 Batch Gradient Descent Stochastic Gradient Descent Mini Batch Gradient Descent SGD with mini-batch 问题：为什么下降是 - ，θ大J一定大吗？ Normal Equation (linear regression) 推导过程比较复杂，需要的数学知识比较多，这里只给出结论。想要看具体推导过程的还说看cs229的第二节课吧 ：）(cs229-notes-1, p11) \\theta = (X^TX)^{-1}X^T\\vec y Newton BGFS Simulated Annealing 2. Generalized Linear Model (GLM) 广义线性模型之前我们在cost function中提到过，我们可以从概率的角度来理解误差这个问题。对于Linear Regression和Logistic Regression，我们都可以假设： y^{(i)} = h_\\theta(x^{(i)}) + \\epsilon^{(i)}这里误差ϵ假设为IID (independently and identically distributed) 2.1 Linear Regression在Linear Regression中，y是连续的值，所以误差ϵ也是一个连续的值。假设误差ϵ是符合Gaussian Distribution (Normal Distribution)，所以有 Gaussian Distribution y | x; \\theta ∼ N (μ, σ^2) Probability of error ​ σ实际上是不影响概率的分布的，所以假设σ = 1，所以这里可以忽略了。因此也就是等同于如下 Likelihood L(\\theta) = L(\\theta; X, \\vec y) = p(\\vec y | X; \\theta) ​ 以上是在给定输入x和权重θ下，我们的预测值是真实值y的概率，所以这个概率呢，当然是越高越好啦。我们就是要想办法去 maximum likelihood。 ​ 对这个概率取个log（不影响结果），有 ​ 可以看到最终的式子里面，我们就是要求cost function的最小值。 2.2 Logistic Regression在Logistic Regression中， y是离散的值 {0, 1}，所以误差ϵ也是一个离散的值 {0, 1}。假设误差ϵ是符合Bernoulli Distribution，所以有 Bernoulli distribution Probability of error 把误差ϵ代入到上面的bernoulli function，可得 Likelihood ​ 同样，对上面去log之后有 ​ 同样，我们尽量要maximize the likelihood，就相当于要最小化后面的那部分（cost function）。这里可以用Gradient Ascent算法来求最大值，但是和Gradient Descent不一样（为什么） \\theta := \\theta + \\alpha∇_\\theta l(\\theta)​ 注意这里是加号，在求cost function的最小值时，用的是减号。 ​ 对其求导可得 ​ 所以有 ​ Digression Perception ？ ？ ？ 2.3 GLM似乎到现在，讲了半天都也没讲什么是广义线性模型，实际上呢，上面我们已经从误差概率的角度上来分析了线性回归和逻辑回归两种特例，因为他们误差服从的概率分布都是属于Exponential Family中的一种。 The Exponential Family η 被称作natural parameter，它是指数分布族唯一的参数T(y) 被称作sufficient statistic，很多情况下T(y)=y a(η) 被称作 log partition functionT函数、a函数、b函数共同确定一种分布 那么这个模型和上面我们提到过的Gaussian Distribution 和Bernoulli Distribution有什么关系呢？其实上面的这几个参数取不同的值的时候，即可得到不同的分布模型 Gaussian Distribution Bernolli Distribution 2.4 如何构建一个GLM模型在上面我们只是看到了一个通用的GLM概率模型 实际上对于构建这么一个概率模型，需要作出三个假设作为前提条件： p(y | x; θ) ∼ ExponentialFamily(η). 对于给定的输入x，θ和输出y需要服从某一种指数分布，这个指数分布由η 决定的 对于给定的输入x，预测T(y)的值，且经常T(y) = y。而我们是预测是H(x) 需要满足 H(x) = E[y|x] 对于自然参数η和输入x之间，需要存在相关性关系的，即：η = θT x","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"}]},{"title":"字符串搜索算法 - KMP","date":"2017-03-15T06:48:47.000Z","path":"2017/03/15/字符串搜索算法-KMP/","text":"1. 理解一：部分匹配表+已匹配数字1.1 Partial Match Table 上来先上个结论，这个先暂时不管怎么生成，用于KMP表的移动。 移动位数 = 已匹配的字符数 - 对应的部分匹配值 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是6 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位。 匹配到了第三个字符C，C的前一个字符B在上表中的值是0，已经匹配的字符数是2 所以移动的位数是 2 - 0 = 2，将搜索词向后移动2位 因为第一个字符不匹配，就将整个字符串向后移一位 匹配到了第六个字符B，B在上表中的值是2，已经匹配的字符数是6 所以移动的位数是 6 - 2 = 4，将搜索词向后移动4位 逐个比较，直到完全匹配 如果还需要继续搜索的话，D在上表中的值为0，匹配到的个数为7，移动的位数= 7 - 0 = 7，将整个字符串往后移动7位。接着就是重复之前的比较步骤了。 1.2 计算 Partial Match Table ​ 这里需要理解两个概念：前缀和后缀 ​ “前缀” 指除了最后一个字符以外，一个字符串的全部头部组合； ​ “后缀” 指除了第一个字符以外，一个字符串的全部尾部组合。 ​ 而我们需要的Partial Match Table就是前缀和后缀的最长共有元素的长度 ​ 继续以上面的例子讲解 “A”的前缀和后缀都为空集，共有元素的长度为0；“AB”的前缀为[A]，后缀为[B]，共有元素的长度为0；“ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；“ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；“ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1；“ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2；“ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。 了解了KMP的原理之后，来看一下代码该怎么写。 举个栗子： Text = a b a c a a b a c c a b a c a b a a b b Pattern = a b a c a b 根据前面的Partial Match Table, 我们可以算出Pattern的这个表 P a b a c a b steps 0 0 1 0 1 2 此时我们用两个指针 i 和 j 来表示 Text 和 Pattern 中的字符。 当 T[ i : i + j ] == P[ 1 : j ] 的时候，就是 Text 中包含了我们需要查找的 Pattern 先让 i 和 j 都从 1 开始（python代码中从0开始） 当T[ i ] = P[ j ]的时候，此时指针在 Text 和 Pattern 上都往前各走一步，即j+1，i+1 当 i = 6，j = 6 的时候，我们可以看出T[ i ] != P[ j ]，此时 j 就不能再继续往前走了，需要退回去几步。 那么到底是几步呢，经过上面查表，此时匹配到5，重复的字符串个数为1，意思是对于这个字符串 abaca，abaca 和 abaca 中有一个重复了，我们就不需要再比较这个，跳过这个字符，移动的个数为 6 - 1 = 5，将字符串 Pattern 向前挪5位，新的 j 就等于1了，然后重复之前的步骤。 1.3 Python代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091DEBUG = TrueDEBUG = False# 生成 partial match tabledef PMT(sent): length = len(sent) result = [0] for i in range(2, length + 1): sub_sent = sent[0 : i] sub_len = len(sub_sent) surfix = [] prefix = [] for j in range(sub_len): prefix.append(sub_sent[0 : j]) surfix.append(sub_sent[j + 1: sub_len ]) prefix.remove('') surfix.remove('') all_fix = list(set(prefix + surfix)) max_len = 0 com_str = '' for fix in all_fix: if fix in prefix and fix in surfix: if len(fix) &gt; max_len: max_len = len(fix) com_str = fix result.append(max_len) return result# KMP算法def KMP(text, pattern): if (len(text) &lt; len(pattern)): return 'Not Match' pmt = PMT(pattern) if DEBUG: print('Current Pattern is = ', pattern) print('Partial Match Table = ', pmt) i = 0 j = 0 while(i &lt; len(text)): if (text[i] == pattern[j]): if (j == (len(pattern) - 1)): return 'Match, the position in text is: &#123;&#125; - &#123;&#125; = &#123;&#125;'.format(i - j, i, text[i-j: i]) i += 1 j += 1 continue else: if j &gt;= 1: j = pmt[j - 1] else: i += 1 return 'Not Match'# 测试函数def test(text, pattern): result = KMP(text, pattern) print('Resutl = ', result) print('Pattern =', pattern) if DEBUG: for i in range(len(text)): print('&#123;:3s&#125;'.format(str(i)), end='-') print('') for i in range(len(text)): print('&#123;:3s&#125;'.format(text[i]), end='-') print('') print('\\n')# 测试部分text = 'BBC ABCDAB ABCDABCDABDE'pattern = 'ABCDABD'test(text, pattern)text = 'a b a c a a b a c c a b a c a b a a b b'pattern = 'a b a c a b'test(text, pattern)text = 'BBC ABCDAB ABCDABCDABE'pattern = 'ABCDABD'test(text, pattern)text = 'BBC AB'pattern = 'ABCDABD'test(text, pattern)text = 'ABCDABD'pattern = 'ABCDABD'test(text, pattern) 1.4 测试结果12345678910111213141516171819202122232425Text = BBC ABCDAB ABCDABCDABDEPattern = ABCDABDResutl = Match, the position in text is: 15 - 21 = ABCDABText = a b a c a a b a c c a b a c a b a a b bPattern = a b a c a bResutl = Match, the position in text is: 20 - 30 = a b a c a Text = BBC ABCDAB ABCDABCDABEPattern = ABCDABDResutl = Not MatchText = BBC ABPattern = ABCDABDResutl = Not MatchText = ABCDABDPattern = ABCDABDResutl = Match, the position in text is: 0 - 6 = ABCDAB[Finished in 0.1s] 2. 理解二：部分匹配表+idx2.1 Partial Match Table首先生成和上面一样的前缀后缀表(0-base 和 1-base) 2.2 Example 0-base 找到原文和搜索词不同的那个字符里面的匹配值，然后把搜索字符串右移到idx=匹配值的位置，过程如下图 1-base 与0-base不同的是，找的不是最后一个不同的字符 而是最后一个相同的字符里面的匹配值，然后把搜索字符串右移到idx=匹配值的位置，过程如下图 3. References 阮一峰 - 字符串匹配的KMP算法","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://chenson.cc/tags/Algorithm/"}]},{"title":"Hadoop权威指南笔记（一）","date":"2017-03-06T13:44:33.000Z","path":"2017/03/06/Hadoop权威指南笔记（一）/","text":"1. Hadoop1.1 初识Hadoop非常好的Tutorial 在学习hadoop之前，我觉得有必要了解一下hadoop的基本构成以及一些术语。 Block HDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks. By making a block large enough, the time to transfer the data from the diskcan be made to be significantly larger than the time to seek to the start of the block.Thus the time to transfer a large file made of multiple blocks operates at the disk transferrate.A quick calculation shows that if the seek time is around 10 ms, and the transfer rateis 100 MB/s, then to make the seek time 1% of the transfer time, we need to make theblock size around 100 MB. The default is actually 64 MB, although many HDFS installationsuse 128 MB blocks. This figure will continue to be revised upward as transferspeeds grow with new generations of disk drives.This argument shouldn’t be taken too far, however. Map tasks in MapReduce normallyoperate on one block at a time, so if you have too few tasks (fewer than nodes in thecluster), your jobs will run slower than they could otherwise. Node 简单的说就是一台主机，一台电脑。在hadoop中，有NameNode, DataNode, Secondary NameNode, JobTracker Node, TaskTracker Node, CheckpointNode 和 BackupNode。对一个cluster，NameNode只能有一个，DataNode可以有多个 Rack 中文机柜/机架，就是用来存放node的storage，通常一个rack有几十个nodes组成，这些nodes存放在同一个机柜，连接一个交换机 A Node is simply a computer. This is typically non-enterprise, commodity hardware for nodes that contain data. Storage of Nodes is called as rack. A rack is a collection of 30 or 40 nodes that are physically stored close together and are all connected to the same network switch. Network bandwidth between any two nodes in rack is greater than bandwidth between two nodes on different racks.A Hadoop Cluster is a collection of racks. 1.2 Major Components Distributed Filesystem — hadoop中是HDFS MapReduce 1.5 Install configuration123456# 待整理hadoop fshadoop dfs# fs refers to any file system, it oucld be local or HDFS# but dfs refers to only HDFS file system 2. MapReduce2.1 初识MapReduce 整个过程可以分为三个阶段，Input， MapReduce and Output 在input和output阶段，数据是存在HDFS文件系统中，其系统的block size大小默认是64/128MB。 在MapReduce中，又可以分为两个阶段，Map and Reduce，数据从map function到reduce function是存在local disk中，(soreing in HDFS with replication would be overkill)，然后通过network传输数据. 在每个阶段中，input和output的数据都是以 (key, values) 格式进行处理的，然后通过 map function 和 reduce function 进行处理。在本例中，input data的key是从数据文件开始处的行数的偏移量，但是map function输出的key是年份数据，以及reduce function输出的key是也不同的。所以这三个key-value pairs是不同的。 原始数据 Key-Values 以上为原始数据中input进来后的key-values的数据。然后map function阶段，提取出上面文件中的 1950 和 0001 之类的数据，组成新的key-values作为输出给下一阶段。 Key-Values in Map Function 在将Map Function的输出传给Reduce Function之前，实际上MapReduce Framework还是有对数据进行一个处理步骤。从最上的图一中，我们仍然可以看到Map和Reduce之间有一个 Shuffle 的过程。因为之前我们提到了，Map的过程中，只是实现了一个key-value匹配的过程，所有出来的数据也是无序的，而 Shuffle 就是对这个输出 sort &amp; group 的过程，然后将输出传给 Reduce Function 进行处理 当数据从Reduce Function中处理完后出来的大概如下，注意这个reduce只是选择最大值，其他reduce function可能做的是统计或者实现其他功能。 现在再看另外一个经典的WordCount的例子 在Hadoop系统中，处理一个wordcount的任务可以大致分成四个主要阶段，input，map，reduce，output。其中 Map 和 Reduce 可以继续细分，即分成多个 map tasks 和 reduce tasks。 这些tasks然后被 YARN 给分配集群中多台不同的机器处理。这其中的细节等到往后再讨论。 上面提到的分成多个tasks时，应该是input data切片分给多个maps（而不是一个大的map分成多个小的tasks）， 每个MapReduce分到一个fixed-sized 的数据，通常是64/128MB，这个过程叫做 input splits。然后每个split分配一个map task，同时运行在不同的机器上处理。这样划分的好处是有利于load-balancing，对于性能较好的机器可以处理更过是splits。 2.2 Data Flow 上图可以看出hadoop的整个数据流向，其中虚线代表是在一个node，实线代表的是不同node之间。在同一个node之间，数据的读取存储就有速度上的优势，不同node之间，也就是不同主机之间，就必须通过network进行传输，速度较慢。 Partition 当只有一个reduce的时候，map function的output当然就直接传给这个reduce了。但是当有多个reduce的时候，怎么办呢？此时map会将其输出进行partition(分区)，每一个reduce的任务都会创建一个分区，且每一个reduce task都会有一个partition (There can be many keys (and their associated values)in each partition, but the records for any given key are all in a single partition)，也就是说同一个key会在同一个partition中。 Shuffle and Sort 在map和reduce之间的data flow是Shuffle，从上图可以看出，一个reduce可以接受来自多个不同的map的output，其中包含了sort，partition等过程。 Combiner Functions 之前我们讨论过，data flow在map和reduce之间是通过network进行传输的，但我们知道map function的output是一个个key-value的键值对的，这些key-value paris中，有些是可以通过combiner function进行combine的，这样做的目的是减小map和reduce之间传输的数据大小，加快传输数据。 Combiner Function在许多情况和 Reduce Function是很像的，因为做的工作和reduce是比较类似的，只是处理的是局部map的output(因此Combiner是运行在map output端)，减少data flow的size。但是对于是否调用combiner function，这个是不确定的。因为有些情况下的output是不适合进行combine，有些则又是要多次调用进行合并。因为这个，Combiner是可选的，即可以调用，也可以不调用，当不调用的时候，就必需不能影响程序的正常运行。所以Combiner的input和output是一样的，和Mapper的output、Reducer的input一样。 对于有些特殊情况，甚至连reduce function都不需要。 举个栗子： 适用情况（Commutative &amp; Associative） ​ Reduce Function Combiner Function ​ Commutative: max(a, b) = max(b, a) ​ Associative: max(max(a, b), c) = max(a, max(b, c)) 不适用情况： 伪代码 In-Combiner Function Advantage 相比Combiner，In-Combiner的效率更高。 可以减少一些Mapper和Reducer之间的key-value pairs，可以减少处理这部分的开销。因为Combiner只是减少了一些Mapper和Reducer之间的intermediate data，但是并没减少从Mapper的output出来的key-value pairs的数量。但是In-Combiner是是Mapper 的一部分，也就是说key-value pairs在Mapper 输出前就已经减少了。 减少了key-value pairs可以减少系统的object serialization and deserialization 的开销，即垃圾回收机制 Disadvantage 内存使用，因为要保存一个array在内存中，当数据量很大的时候有可能会爆了。解决方案 有两个，第一是限制array的个数，第二是限制内存的使用。当这俩到达某一个阈值的时候，就发送给Reducer。 第二是讲一个Map的过程分成几个部分，导致debug中可能出现oedering-dependent bugs，调试可能比较困难。 ​ ​ ​","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://chenson.cc/tags/Hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://chenson.cc/tags/MapReduce/"}]},{"title":"Machine Learning - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Recommender-Systems/","text":"1. What is Recommender Systems对于推荐系统的定义，我们先举几个例子来理解一下。 电影网站给用户推荐电影，可以根据该用户以往的评分，比如给浪漫爱情电影评分高，给动作片评分较低，那么系统可以根据这些信息，给用户推荐偏向浪漫爱情的电影 如果是新用户呢？我们没有该用户的评分信息。那么我们可以根据整个系统中，某些电影评分较高进行推荐 那么如果是新网站，新用户呢？ 以上例子，我们可以把推荐系统分成两类。 Content-based systems Content-based，就是基于已有的信息进行推荐。具体哪些信息呢？在上面的电影推荐系统中，有两类信息需要分析。 第一，是User的评分信息，比如给爱情片评分高，给动作片评分低。 第二，是Movie的特征信息，比如这部电影偏向爱情片多一些，但也有一部分搞笑。所以在A（爱情片）和B（搞笑片）中， A的权重更高，B的较低 基于以上两部分信息，我们可以给用户推荐他所喜欢的电影。 Collaborative filterring systems 协同过滤器，则是基于用户/物品之间的相似度进行推荐的。即用户A和用户B都喜欢爱情、浪漫电影，我们就可以把用户A评分过的爱情浪漫电影，推荐给用户B。 2. Content-based systems2.1 Problem Analysis以电影推荐系统为例，假设我们已经对系统中的电影特征有了较为完善，即我们知道某部电影属于爱情片多少分，属于动作片多少分。 那么我们现在以Alice为例，她对两部爱情片评分比较高，对于两部动作片评分为0。那么系统就可以给Alice推荐偏向爱情浪漫的，且不怎么属于动作片的电影。 Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love ？ 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 2.2 Optimization Objective实际上我们已经假设之前对所有电影的特征进行了统计，所以此时有电影特征向量X，以及用户对于电影的评分Y向量。根据此时已有的信息，我们需要求出theta的值。所以能够对于那么没有评分过的电影，根据theta和x求出分数y。 因为一开始theat的值是随机的，所以我们用Linear Regression的方法，不断减少cost function的值求出theta。 值得注意的是，因为这里是多个用户，每一个用户我们求出一个theta值。最后对于多个用户，我们需要求出多个theta值。 Actually, we can assume that we have known all features about the all movies, that is x1, x2, …, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 2.2 Gradient descent update 2.3 Gradient descent in Logistic Regression Question: Why we don’t need $\\frac 1 m$? Anwser: As there are only one user However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to learn theta and features at the same time 3. Collaborative filtering3.1 Proble motivation Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love ? 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 3.2 How to do 在之前部分中，我们了解到了content-based，是已知 x 和 y，求 theta。 Assume: \\theta^{(1)} = \\begin{bmatrix} 0 \\\\ 5 \\\\ 0 \\end{bmatrix}, \\space\\space \\theta^{(2)} = \\begin{bmatrix} 0 \\\\ 5 \\\\ 0 \\end{bmatrix}, \\space\\space \\theta^{(3)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 5 \\end{bmatrix}, \\space\\space \\theta^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 5 \\end{bmatrix}, \\space\\space x^{(1)} = \\begin{bmatrix} 1 \\\\ 1.0 \\\\ 0.0 \\end{bmatrix}For Movie 1, we can calculate the result of Movie1 rating by all users. \\theta^{(1)} \\* x^{(1)} \\approx 5 \\\\\\ \\theta^{(2)} \\* x^{(1)} \\approx 5 \\\\\\ \\theta^{(3)} \\* x^{(1)} \\approx 0 \\\\\\ \\theta^{(4)} \\* x^{(1)} \\approx 0 但是对于有些情况，我们并不知道x的特征值，该怎么办呢？ 逆向思考，我们也可以通过 theat 和 y，来求 x 的值。 那么对于 theta和x的值都不知道的情况下呢？ 对比特征 Linear Regression Collaborative filtering 特性向量X 已知数据 待求解数据 权重 θ 待求解数据 待求解数据 y值 已知数据 已知数据 3.3 Optimization Algorithm For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. θ -> x -> θ -> x -> θ -> x -> θ -> x -> ...Actually, these two steps are Linear Regression, we shoud do that simultaneously to update theta and x. 3.4 Collaborative filtering Optimization Algorithm 实际上，上面是两个 LR的问题，我们可以将上面两步合并到一起，这个就是collaborative filterring， 此时的optimizatino object 就从 J(theta) 和 J(X) 变为了 J(theta, X)。 具体步骤如下 3.5 Vectorization: Low rank matrix factorization首先，我们先把评分Y用向量表示出来，同时表示为Theta和X两个矩阵的乘积 Y= \\begin{bmatrix} 5 & 5 & 0 & 0 \\\\ 5 & ? & ?& 0 \\\\ ? & 4 & 0 & ? \\\\ 0 & 0 & 5 & 4 \\\\ 0 & 0 & 5 & 0\\end{bmatrix} = \\begin{bmatrix} (\\theta^{(1)})^T(x^{(1)}) & (\\theta^{(2)})^T(x^{(1)}) & ... & (\\theta^{(n_u)})^T(x^{(1)}) \\\\ (\\theta^{(1)})^T(x^{(2)}) & (\\theta^{(2)})^T(x^{(2)}) & ... & (\\theta^{(n_u)})^T(x^{(2)}) \\\\ ... & ... & ... & ... \\\\ (\\theta^{(1)})^T(x^{(n_m)}) & (\\theta^{(2)})^T(x^{(n_m)}) & ... & (\\theta^{(n_u)})^T(x^{(n_m)}) \\end{bmatrix} = X * \\Theta', R \\in (n_m × n_u) X = \\begin{bmatrix} ---(x^{(1)})^T--- \\\\ ---(x^{(2)})^T--- \\\\ ... \\\\ ---(x^{(n_m)})^T--- \\end{bmatrix}, x^{(n_m)} = \\begin{bmatrix} x^{(n_m)}_1 \\\\ x^{(n_m)}_2 \\\\ ... \\\\ x^{(n_m)}_n \\end{bmatrix}, R \\in (n_m × n) \\Theta = \\begin{bmatrix} ---(\\theta^{(1)})^T--- \\\\ ---(\\theta^{(2)})^T--- \\\\ ... \\\\ ---(\\theta^{(n_u)})^T--- \\end{bmatrix}, \\theta^{(n_u)} = \\begin{bmatrix} \\theta^{(n_u)}_1 \\\\ \\theta^{(n_u)}_2 \\\\ ... \\\\ \\theta^{(n_u)}_n \\end{bmatrix}, R \\in (n_u × n)3.6 Mean Normalization对于那些新注册用户，系统中没有记录他们的偏好，则采用以下方法。 先计算出每部电影评分的平均值mu，然后把所有的评分都减去平均值（此后处理过的评分平均值为0）。虽然这样做对有评分记录用户是多余的，但却可以吧没有评分记录的用户给统一进来，避免全是0的情况。 4. Implement Algorithm4.1 Cost Function without Regularization Tips：这里需要计算的只是针对那些已经评分过的电影，对于用户没有评分过的不需要计算。 4.2 Collaborative filtering gradient \\frac {\\partial J} {\\partial x_k^{(1)}} , \\frac {\\partial J} {\\partial x_k^{(2)}} , ..., \\frac {\\partial J} {\\partial x_k^{(n_m)}} \\space\\space for \\space each \\space movie\\\\ \\frac {\\partial J} {\\partial \\theta_k^{(1)}} , \\frac {\\partial J} {\\partial \\theta_k^{(2)}} , ..., \\frac {\\partial J} {\\partial \\theta_k^{(n_u)}} \\space\\space for \\space each \\space userTips： 对于使用vectorization方法，最终只有两个for-loop，一个计算$X_{grad}$，一个计算$Theta_{grad}$ 如何对X和Theta求偏导数？ (Theta_{grad}(i, :))^T = \\begin{bmatrix} \\frac {\\partial J} {\\partial \\theta^{(i)}_1} \\\\ \\frac {\\partial J} {\\partial \\theta^{(i)}_2} \\\\ ... \\\\ \\frac {\\partial J} {\\partial \\theta^{(i)}_n} \\end{bmatrix} 同样，我们只需考虑用户已经评分过的电影，用其作为训练样本 因为Vectorization非常容易搞乱各个matrix，所以建议先整理一下各个matrix的size，计算时可以根据matrix的size进行计算。 4.3 Implementation注意这里并没有给出完整的代码 (Octave/Matlab)，都只是主要的部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950% Theta : nu x n% X : nm x n% Y : nm x nu% R : nm x nupred = X * Theta'; %nm x nu 'diff = pred - Y;% Cost Function with regularizationJ = 0.5 * sum(sum((diff.^2) .* R));J = J + (lambda * 0.5) * sum(sum(Theta.^2)); % regularized term of theta.J = J + (lambda * 0.5) * sum(sum(X.^2)); % regularized term of x.% calculate Xfor i = 1 : num_movies, % the row vector of all users that have rated movie i idx = find(R(i, :) == 1); % (1 * r) % the list of users who have rated on movie i Theta_temp = Theta(idx, :); % (r * n) Y_temp = Y(i, idx); % (1 * r) X_temp = X(i, :); % (1 * n) % ((1 * n) * (n * r) -(1 * r)) * (r * n) = (1 * n) X_grad(i, :) = (X_temp * Theta_temp' - Y_temp) * Theta_temp; %' % regularization X_grad(i, :) = X_grad(i, :) + lambda * X_temp;end% calculate Thetafor i = 1 : num_users, % the row vector of all movies that user i has rated idx = find(R(:, i) == 1)'; % (1 * r) ' Theta_temp = Theta(i, :); % (1 * n) Y_temp = Y(idx, i); % (r * 1) X_temp = X(idx, :); % (r * n) % ((r * n) * (n * 1) - (r * 1)) * (r * n) = (1 * n) Theta_grad(i, :) = (X_temp * Theta_temp' - Y_temp)' * X_temp; % regularization Theta_grad(i, :) = Theta_grad(i, :) + lambda * Theta_temp;endgrad = [X_grad(:); Theta_grad(:)]; ​​​","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"}]},{"title":"Machine Learning - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-SVM/","text":"1. Optimisation Objective h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\\\ z = -\\theta^Tx Why we need do that? 2. Hypothesis Function2.1 Logistic Regression \\frac 1 m \\sum\\_{i=1}^m [ y^{(i)} (-log(h\\_\\theta(x^{(i)})) + (1 - y^{(i)}) (-log(1 - h\\_\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum\\_{j=1}^n \\theta\\_j^22.2 Support Vector Machine C \\sum\\_{i=1}^m [y^{(i)} cost\\_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost\\_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum\\_{j=1}^n \\theta\\_j^2, \\space \\space \\space \\space \\space C = \\frac 1 \\lambdaAnalysis： 为了使得cost function取得最小值，我们令C*W + P部分中，C*W为零。即： 当 y = 1时， cost1 = 0，所以 z &gt;= 1 当 y = 0时， cost0 = 0，所以 z &lt;= -1 Note：1. cost0 and cost1 对应的是上图中左右两边的cost function，因为y=0和y=1的目标函数。 常数C取一个很大的值时比较好。因为C*W + P， 所以C大则W会变小，即相对penality就会变大，W会变小 为什么要重新选定一个cost function ？（逻辑回归的临界点为0，但是SVM的临界点是1，所以SVM更加精确。 ） 对应的线性逻辑回归？即次数不大于1的？ Decision Boundary 不是一条直线的情况 3. Large Margin Classifier12结论：常数C取一个比较大的值比较容易获得Large Margin ClassifierC大，则比较容易获得 以上为两类分布比较均匀的时候，Decision Boundary为图中黑色的线，所有点离黑色的距离都相对比较大比较均匀，但是当存在干扰点的时候如下图，Decision Boundary会由黑色变为粉红色。所以C的取值不能太大，也不能太小。需要求出最优解 4. Mathmatics Behind Large Margin Classification4.1 Vector Inner Product Note： 如何求投影p的值？ 当角度 &lt; 90°，p为正数。当角度 &gt; 90°时，p为负数。 向量内积 u^Tv = ||u|| · ||v|| · cosθ = ||u|| · p\\_{v,u} = ||v|| · p\\_{u,v} = u\\_1v\\_1+u\\_2v\\_2 4.2 SVM Cost Function C \\sum\\_{i=1}^m [y^{(i)} cost\\_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost\\_0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum\\_{j=1}^n \\theta\\_j^2, \\space \\space \\space \\space \\space C = \\frac 1 \\lambda当C取一个一个很大的值时，cost function只剩下后面P的部分。 假设θ0 = 0 \\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + ... + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + ... + \\theta^2_n) = \\frac 1 2 ||\\theta||^2 所以： \\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\\\ p^{(i)}||\\theta|| >= 1, if \\space\\space y^{(i)} = 1 \\\\ p^{(i)}||\\theta|| = 1, if \\space\\space y^{(i)} = 1 \\\\ p^{(i)}||\\theta|| = 0 \\\\ \\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + ... >= 0即将fn定义为x的幂次项组合，如下： f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, ... 但是在SVM中，我们要重新定义fn，引入Kernel的概念，即用 kernel function来表示fn。 Note: l 是landmark，且如果training sets里面的数量为n的话，则landmark的数量也为n。 假设training sets数量为n，则对于一个新的example来说，可计算出n个新的特征f1…fn。然后用新的特征，对该example进行判断（低维转为高维的过程） kernel function为guassian function。当x与landmark l越接近时，两点的距离越小，值接近1 5.2 SVM with Kernels 对比之前的cost function，可以发现这里θ和f(x)跟之前的不同。 在logistic regression 中，θ的维度为(n+1) x 1, 包含θ0， 且n为单个example的特征个数 在SVM with kernel中，f(x)的个数为m，其中m是training sets中的个数，所以θ的维度应该是(m+1)x1 Steps 给定一组training sets，根据每个example，选取m个landmark点 计算每一个example与所有landmark的相识度，相同为1，非常不同接近为0。计算相识度的kernel function为Gaussian Function 最终，对于每一个example里面都可以计算出m个新的feature，所以对于这个training sets而言，会得到一个m*m的矩阵？ 将得到的m*m的矩阵，代入到Hypothesis中，计算出θ的值。 5.4 SVM parameters C = 1/λ Large C Small λ Large θ Lower Bias High Variance Over Fitting Small C Large λ Small θ Higher Bias Low Variance Under Fitting σ Large σ more smoothly Higher Bias Lower Variance Under Fitting Small σ less smoothly Lower Bias Higher Variance Over Fitting","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.cc/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.cc/tags/SVM/"}]},{"title":"Linux常用指令","date":"2017-02-19T01:16:24.000Z","path":"2017/02/19/Linux常用指令/","text":"有时候记东西老是记不住，虽然是经常使用，所以对常用的linux指令做个简单记录，方便以后使用（不断更新… …）。 1. scp文件传输 (security copy)12# eg. scp chenson@127.0.0.1:~/Home/test . ~/Home/local_test_pathscp PATH_OF_SRC PATH_OF_DST 2. 测内存的使用率12345678valgrind --tool=massif --pages-as-heap=yesms_print massif 文件名valgrind --tool=massif --pages-as-heap=yes ./bwtsearch testfiles/shopping.bwt l.idx \"system\"ms_print massif.out.3116 1find &lt;PATH TO FOLDER&gt; -ls | tr -s &apos; &apos;|cut -d&apos; &apos; -f 7| paste -sd+ | bc 3. 强制写入123456789# vii commond model, Try the below command:w !sudo tee %Explanation:w – write!sudo – call shell sudo commandtee – the output of write (:w) command is redirected using tee% – current file name 4. 查看二进制文件vim下 1:%!xxd linux下 1xxd -b file_name 5. 查看公网IP1curl http://members.3322.org/dyndns/getip 6. 查看那个进程占用：1lsof -i:9000 # MAC 7. 查看监听端口：12netstat -ntlp # linuxlsof -i:port_num # MAC 8. 测文件夹大小123456find &lt;PATH TO FOLDER&gt; -ls | tr -s ' '|cut -d' ' -f 7| paste -sd+ |bcdu -ah --max-depth=1，其中a表示显示目录下所有的文件和文件夹（不含子目录），h表示以人类能看懂的方式，max-depth表示目录的深度。du -sh : 查看当前目录总共占的容量。而不单独列出各子项占用的容量 9. 文件系统空间使用情况1234567891011# 查看系统中文件的使用情况df -h# 查看当前目录下各个文件及目录占用空间大小du -sh *# 查看指定文件夹下文件夹的大小du -h folder_name# 查看该文件夹的大小df -hs folder_name 可以看到下面有一块数据盘，挂载在/data1下 如果系统有单独的数据盘，且数据盘没有分区和挂载，使用df -h命令是看不到的，可以使用fdisk -l查看，可以看到有哪些硬盘 下图可看到数据盘为Disk /dev/vdb 可以看到上图有53.7G的未分区数据盘，然后有1771.7G的已分区的数据盘。 References 分区教程 Ubuntu查看磁盘使用情况 10. 加密文件大小12# oJ5A93Fhzip -r -P password utils.zip utils 11. 打包+压缩 压缩 12345678910111213tar czvf my.tar.gz file1 file2 ....fileNtar -cvf jpg.tar *.jpg //将目录里所有jpg文件打包成tar.jpg tar -czf jpg.tar.gz *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz tar -cjf jpg.tar.bz2 *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2tar -cZf jpg.tar.Z *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Zrar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linuxzip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux 解压 1234567891011tar -xvf file.tar //解压 tar包tar -xzvf file.tar.gz //解压tar.gztar -xjvf file.tar.bz2 //解压 tar.bz2tar -xZvf file.tar.Z //解压tar.Zunrar e file.rar //解压rarunzip file.zip //解压zip 12. 查找文件12345678910111213141516171819202122$find ~ -name \"*.txt\" -print #在$HOME中查.txt文件并显示$find . -name \"*.txt\" -print$find . -name \"[A-Z]*\" -print #查以大写字母开头的文件$find /etc -name \"host*\" -print #查以host开头的文件$find . -name \"[a-z][a-z][0–9][0–9].txt\" -print #查以两个小写字母和两个数字开头的txt文件$find . -perm 755 -print$find . -perm -007 -exec ls -l &#123;&#125; \\; #查所有用户都可读写执行的文件同-perm 777$find . -type d -print$find . ! -type d -print $find . -type l -print$find . -size +1000000c -print #查长度大于1Mb的文件$find . -size 100c -print # 查长度为100c的文件$find . -size +10 -print #查长度超过期作废10块的文件（1块=512字节）$cd /$find etc home apps -depth -print | cpio -ivcdC65536 -o /dev/rmt0$find /etc -name \"passwd*\" -exec grep \"cnscn\" &#123;&#125; \\; #看是否存在cnscn用户$find . -name \"yao*\" | xargs file$find . -name \"yao*\" | xargs echo \"\" &gt; /tmp/core.log$find . -name \"yao*\" | xargs chmod o-w 13. 递归删除所有文件夹下的特定类型文件1find . -name '*.ttteset' -type f -print -exec rm -rf &#123;&#125; \\; 14. 后台运行指定程序（不挂断地运行命令） nohub 123456789101112131415161718192021nohup python main.py 80 &gt; ~/log/WeChatAccountLog.file 2&gt;&amp;1 &amp;该命令的一般形式为：nohup command &amp;如果使用nohup命令提交作业，那么在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中，除非另外指定了输出文件：command&gt;out.file是将command的输出重定向到out.file文件，即输出内容不打印到屏幕上，而是输出到out.file文件中。2&gt;&amp;1 是将标准出错重定向到标准输出，这里的标准输出已经重定向到了out.file文件，即将标准出错也输出到out.file文件中。最后一个&amp;， 是让该命令在后台执行。试想2&gt;1代表什么，2与&gt;结合代表错误重定向，而1则代表错误重定向到一个文件1，而不代表标准输出；换成2&gt;&amp;1，&amp;与1结合就代表标准输出了，就变成错误重定向到标准输出.# 可以将一个正在前台执行的命令放到后台，并且处于暂停状态。ctrl + z # 终止前台命令。Ctrl+c # 查看当前有多少在后台运行的命令jobs 使用 fg %n 关闭 screen 和nohup不太一样的，screen是可以把整个会话的session保存起来，类似平常我们用界面的时候，最小化在后台，用的时候又调出来 15. 后台上传/下载数据1另外有两个常用的ftp工具ncftpget和ncftpput，可以实现后台的ftp上传和下载，这样就可以利用这些命令在后台上传和下载文件了。 16. Docker 加速器地址12https://cr.console.aliyun.com/cn-qingdao/mirrorshttps://q90v31vp.mirror.aliyuncs.com 17. 查找指定内容1find . | xargs grep &quot;keyword&quot; | egrep &quot;keyword&quot; # 当前文件目录下 18. 挂载数据盘+自动重启 创建挂载脚本 123456#!/bin/bashmount /dev/vdm /home/mount /dev/vdl /data1mount /dev/vdk /data2mount /dev/vdj /data3mount /dev/vdi /data4 往/etc/rc.d/rc.local里面添加启动上面脚本的代码 12# mount diskbash /root/mount_disk.sh 给上面脚本执行的权限 1234chmod +x /etc/rc.d/rc.local# 重启机器reboot 重启之后重新连该机器，数据盘已自动挂载了 19. 查看系统glibc支持的版本123strings /lib64/libc.so.6 |grep GLIBC_rpm -qa |grep glibc 20. 开机启动linux设置开机自启动脚本的最佳方式 vsftpd, kaiqi ftp fuwu choongqi dashuju taojian ambari-agent restart","tags":[{"name":"Linux","slug":"Linux","permalink":"http://chenson.cc/tags/Linux/"}]},{"title":"Python数据分析笔记（一）","date":"2016-12-15T11:37:30.000Z","path":"2016/12/15/Python数据分析笔记（一）/","text":"1. 常见问题 Pandas.dataframe里面 .values, .iloc, .ix, .loc 的区别 Different Choices for Indexing loc: only work on index / labeliloc: work on position, from 0, 1, 2, … …ix: You can get data from dataframe without it being in the indexat: get scalar values. It’s a very fast lociat: Get scalar values. It’s a very fast iloc 12345678910111213141516171819202122232425262728&gt;&gt;&gt; df = pd.DataFrame(&#123;'A':['a', 'b', 'c'], 'B':[54, 67, 89]&#125;, index=[100, 200, 300])&gt;&gt;&gt; df A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df.iloc[0] # 用位置来索引A aB 54&gt;&gt;&gt; df.loc[100] # 用初始化时设置的index来索引，也就是自己给row设置的labelA aB 54&gt;&gt;&gt; df.loc[100:300] A B100 a 54200 b 67300 c 89&gt;&gt;&gt; df['A'] # 索引 columns100 a200 b300 cName: A, dtype: object&gt;&gt;&gt; df.A # 索引 columns100 a200 b300 cName: A, dtype: object Pandas 和 Numpy之间的转换 np.ndarray 转化为 pd.dataframe 1pd.DataFrame(example) pd.dataframe 转化为 np.ndarray 1example.values[:, :] 读写效率的对比 npy读写效率最高，但最费硬盘空间，比如np.load(), np.save() csv其次，比如pd.Dataframe.to_csv()，pd.load_csv() txt读写，当然也可以很快，但是需要频繁的split，对格式规范的数据比较麻烦 至于简单的excel和word，可以用xlrd,xlwt来操作 2. Numpy N维数组对象，可以利用这种数组对象对整块数据进行一些科学运算，就是把array当做一种对象里操作。这和Python中的array是不同的。 举个栗子： 在Python中 123&gt;&gt;&gt; data = [1, 2, 3]&gt;&gt;&gt; data * 10[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3] Numpy的ndarray 12345&gt;&gt;&gt; data2 = np.array([1, 2, 3])&gt;&gt;&gt; data2array([1, 2, 3])&gt;&gt;&gt; data2 * 10array([10, 20, 30]) 轴(axes) 和 秩(rank) 轴表示的是一种维度，如一维的数据，二维的数据，三维的数据等 12345678910111213141516171819202122&gt;&gt;&gt; data3 = np.array([1, 2, 3]) # 注意这里的**方括号**&gt;&gt;&gt; data3.ndim # 查看维度1&gt;&gt;&gt; data3 = np.array([[1,2,3], [1,2,3], [1,2,3]])&gt;&gt;&gt; data3.ndim2&gt;&gt;&gt; data3 = np.array([[[1], [1], [1]], [[1], [1], [1]],[[1], [1], [1]]])&gt;&gt;&gt; data3.ndim3&gt;&gt;&gt; data3.shape(3, 3, 1) # 维度从最外层到里层&gt;&gt;&gt; data3.size9 # 3 * 3 * 1 = 9#一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。&gt;&gt;&gt; data3.dtype# 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8).&gt;&gt;&gt; data3.itermsize# 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。&gt;&gt;&gt; data3.data 常用的数组创建函数 打印数组 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(6) # 1d array&gt;&gt;&gt; print(a)[0 1 2 3 4 5]&gt;&gt;&gt;&gt;&gt;&gt; b = np.arange(12).reshape(4,3) # 2d array&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]&gt;&gt;&gt;&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4) # 3d array&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 如果一个数组用来打印太大了，NumPy自动省略中间部分而只打印角落 1234567891011&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt;&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] 禁用这种reshape来打印整个数组，需要对printoption参数进行设置 1&gt;&gt;&gt; np.set_printoptions(threshold='nan') 基本的数据运算 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; b = np.array(list(range(5, 10)))&gt;&gt;&gt; a + barray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; a - barray([-5, -5, -5, -5, -5])&gt;&gt;&gt; a * barray([ 0, 6, 14, 24, 36])&gt;&gt;&gt; a += b&gt;&gt;&gt; aarray([ 5, 7, 9, 11, 13])&gt;&gt;&gt; c = np.ones(5)&gt;&gt;&gt; carray([ 1., 1., 1., 1., 1.])&gt;&gt;&gt; c + aarray([ 6., 8., 10., 12., 14.])&gt;&gt;&gt; c = np.ones(5, dtype=int)&gt;&gt;&gt; c + aarray([ 6, 8, 10, 12, 14]) 数组的运算 这就类似在Matlab/Octave中，对matrix/array中的数据执行批量运算，即Vectorization，前提是matrix/array的大小必须满足对应的要求。 用数组表达式可以代替循环操作，矢量化的运算是Numpy的优势。 数组转置和轴对换 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.T # 数组转置，轴对换array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]])&gt;&gt;&gt; np.dot(a.T, a) # 内积array([[125, 140, 155, 170, 185], [140, 158, 176, 194, 212], [155, 176, 197, 218, 239], [170, 194, 218, 242, 266], [185, 212, 239, 266, 293]])&gt;&gt;&gt; b = np.arange(16).reshape((2, 2, 4))&gt;&gt;&gt; barray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt;&gt;&gt; b.transpose((1, 0 , 2)) # 对高维数组，transpose需要得要一个由轴编号组成的元组才能对这些轴进行转置array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) 索引和切片 1234567891011121314151617181920212223242526272829303132333435# 一维数据&gt;&gt;&gt; a = np.arange(5)array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:]array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[1 : 4]array([1, 2, 3])&gt;&gt;&gt; a[1]1# 二维数据&gt;&gt;&gt; b = np.array([[1,2,3], [4,5,6]])array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; b[:, 1:3]array([[2, 3], [5, 6]])&gt;&gt;&gt; b[1][2]6&gt;&gt;&gt; b[1, 2]6# 三维数组&gt;&gt;&gt; c = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10, 11, 12]]])array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; c[1]array([[ 7, 8, 9], [10, 11, 12]]) 通用函数 P111 给 array 添加 columns 和 rows 123456789101112# 方法一np.c_[array1, array2] # 添加 columnsnp.r_[array1, array2] # 添加 row# 方法二 被插入的行np.insert(a, 2, values=b, axis=1) # 添加 columns# 方法三a = np.concatenate((a, -np.ones((a.shape[0], 1))), axis=1) # 添加 columns# 方法四np.column_stack((a,b)) 3. Pandas在Pandas中，Series和DataFrame是两个主要的数据结构 Series 类似一维数组，由一组数据（各种Numpt数据类型 ( list, dict等 )）和一组对于的数据标签组成 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&gt;&gt;&gt; obj = pd.Series(list(range(3, 8)))&gt;&gt;&gt; obj0 31 42 53 64 7dtype: int64&gt;&gt;&gt; obj.valuesarray([3, 4, 5, 6, 7])&gt;&gt;&gt; obj.index # 这里就是 index 索引，没有设置的是时候系统会自动设置为integer indexRangeIndex(start=0, stop=5, step=1)&gt;&gt;&gt; obj[1] # 可以用这索引，跟list用法类似，但是list中只能是数字，但pandas中可以自定义index的索引4# 自定义索引，普通的list和Numpy的数组就不行&gt;&gt;&gt; obj2 = pd.Series([1,3,4], index=['a', 'b', 'c'])&gt;&gt;&gt; obj2['a']1# 索引可以直接用来数组运算，这些在数据清洗的时候比较常用&gt;&gt;&gt; index = obj2 &gt; 2&gt;&gt;&gt; indexa Falseb Truec Truedtype: bool &gt;&gt;&gt; obj2[index]b 3c 4dtype: int64# 其他操作&gt;&gt;&gt; np.log(obj2)a 0.000000b 1.098612c 1.386294dtype: float64 # 当做字典来index&gt;&gt;&gt; 2 in obj2False &gt;&gt;&gt; 3 in obj2False&gt;&gt;&gt; 'a' in obj2 # 只能是index (key)，不能是valuesTrue# 用字典来初始化 Series&gt;&gt;&gt; obj3 = &#123;'one': 1, 'two': 2, 'three' : 3&#125;&gt;&gt;&gt; obj3&#123;'one': 1, 'three': 3, 'two': 2&#125;&gt;&gt;&gt; index = ['One', 'Two', 'Three']&gt;&gt;&gt; value = [1, 2, 3]&gt;&gt;&gt; obj4 = pd.Series(value, index)&gt;&gt;&gt; objOne 1Two 2Three 3dtype: int64 # 手动修改索引，且索引的值是不能重复的&gt;&gt;&gt; obj4.index = ['one', 'two', 'Three']&gt;&gt;&gt; obj4one 1two 2Three 3dtype: int64 # 重建索引，如果索引不存在的值，则引入NaN&gt;&gt;&gt; obj4.reindex(['one', 'two', 'Threeee']) # fill_value=0one 1.0two 2.0Threeee NaNdtype: float64# 可选 ffill/pad 向前填充或者bfill/backfill 向后填充&gt;&gt;&gt; obj4.ffill()one 1.0two 2.0Threeee 2.0dtype: float64# 当然也可以drop columns的内容&gt;&gt;&gt; obj4.drop('Threeee')one 1.0two 2.0dtype: float64 # 但是不能这么修改数值&gt;&gt;&gt; obj4.values = [4, 5, 6]---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2702 else:-&gt; 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):AttributeError: can't set attributeDuring handling of the above exception, another exception occurred:AttributeError Traceback (most recent call last)&lt;ipython-input-41-b4392e8960b0&gt; in &lt;module&gt;()----&gt; 1 obj4.values = [4, 5, 6]/Users/Chenson/anaconda/lib/python3.5/site-packages/pandas/core/generic.py in __setattr__(self, name, value) 2703 object.__setattr__(self, name, value) 2704 except (AttributeError, TypeError):-&gt; 2705 object.__setattr__(self, name, value) 2706 2707 # ----------------------------------------------------------------------AttributeError: can't set attribute ​ DataFrame DataFrame是一个表格型的数据结构，包含了一组有序的列，每列可以是不同的值类型。所有可以看做这是一个二维的数组，有行索引和列索引 创建DataFrame和基础操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; dates = pd.date_range('20170305', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6, 4), index=dates, &gt;&gt;&gt; columns=list('ABCD'))&gt;&gt;&gt; df A B C D2017-03-05 -1.616361 0.027641 0.328074 1.0386272017-03-06 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 -0.350594 0.231137 -0.004755 0.6014602017-03-08 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 -0.227631 -0.186816 -0.370522 0.343896# 当然也可以手动传进来创建df&gt;&gt;&gt; df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)&gt;&gt;&gt; df2Ones A B C D2017-03-05 1 -1.616361 0.027641 0.328074 1.0386272017-03-06 1 0.776285 -0.459325 -2.188099 -1.1356422017-03-07 1 -0.350594 0.231137 -0.004755 0.6014602017-03-08 1 -0.953833 0.377934 -0.583865 -0.3654432017-03-09 1 1.054860 -0.155754 -0.001311 -0.4198052017-03-10 1 -0.227631 -0.186816 -0.370522 0.343896# 看DataFrame的attributes&gt;&gt;&gt; df.&lt;TAB&gt;df.&lt;TAB&gt;df2.A df2.boxplotdf2.abs df2.Cdf2.add df2.clipdf2.add_prefix df2.clip_lowerdf2.add_suffix df2.clip_upperdf2.align df2.columnsdf2.all df2.combinedf2.any df2.combineAdddf2.append df2.combine_firstdf2.apply df2.combineMultdf2.applymap df2.compounddf2.as_blocks df2.consolidatedf2.asfreq df2.convert_objectsdf2.as_matrix df2.copydf2.astype df2.corrdf2.at df2.corrwithdf2.at_time df2.countdf2.axes df2.covdf2.B df2.cummaxdf2.between_time df2.cummindf2.bfill df2.cumproddf2.blocks df2.cumsumdf2.bool df2.D 查看DataFrame里面的数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;&gt;&gt; df.head() # 默认5行 A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401&gt;&gt;&gt; df.tail(3) # 手动设置打印的行数 A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988&gt;&gt;&gt; df.index # index 是 row 的 索引DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df.columnsIndex([u'A', u'B', u'C', u'D'], dtype='object')&gt;&gt;&gt; df.valuesarray([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]])&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804&gt;&gt;&gt; df.sort_index(axis=1, ascending=False) # 是否按照 columns的值下降来排序 D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690&gt;&gt;&gt; df.sort_values(by='B') # 按照columns B 升序来 A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 索引columns和rows 4. References python，numpy，pandas数据处理之小技巧","tags":[{"name":"Python","slug":"Python","permalink":"http://chenson.cc/tags/Python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://chenson.cc/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://chenson.cc/tags/Pandas/"}]}]