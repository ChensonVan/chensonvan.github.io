[{"title":"Machine Learning Week9 - Recommender Systems ","date":"2017-03-02T11:05:21.000Z","path":"2017/03/02/Machine-Learning-Week9-Recommender-Systems/","text":"9 Recommender Systems9.1 9.2 Problems9.2.1 Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action -x2 Love at last 5 5 0 0 1.0 0.0 Romance forever 5 ? ? 0 0.9 0.1 Cute puppies of love 5 4 0 ? 0.99 0.01 Nonstop car chases 0 0 5 4 0.0 1.0 Sword vs. karate 0 0 5 ? 0.2 0.8 9.2.2 PurposeActually, we can assume that we have known all features about the all movies, that is x1, x2, …, xn. And we want to initiate some random values for all theta for all users. As the feature values was fixed, we training the training set by minimizing cost function to get right value of theta. 9.3 Algorithm9.3.1 Optimization Objective 9.3.2 Gradient descent update12345Question: 1. Why we don&apos;t need (1/m) Anwser: As there are only ***one user*** 2. Reguration Answer: (λ/1) = λ 9.3.3 Gradient descent in Logistic Regression 12Question: However, in the above example, we have known the values of all features, but for sometime, we have no idea about that. It means that we have to *** learn theta and features at the same time *** 9.4 Collaborative filtering9.4.1 Proble motivation Movies Alice - θ(1) Bob - θ(2) Carol - θ(3) Dave - θ(4) romance - x1 action - x2 x(1) - Love at last 5 5 0 0 ? ? x(2) -Romance forever 5 ? ? 0 ? ? x(3) -Cute puppies of love 5 4 0 ? ? ? x(4) -Nonstop car chases 0 0 5 4 ? ? x(5) -Sword vs. karate 0 0 5 ? ? ? 9.4.2 How to do12Question: How can we get the value of x? Assume:$$\\theta^{(1)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(2)} = \\begin{bmatrix} 0 \\ 5 \\ 0 \\end{bmatrix}, \\space\\space\\theta^{(3)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\space\\theta^{(4)} = \\begin{bmatrix} 0 \\ 0 \\ 5 \\end{bmatrix}, \\space\\spacex^{(1)} = \\begin{bmatrix} 1 \\ 1.0 \\ 0.0 \\end{bmatrix}$$For Movie 1, we can calculate the result of Movie1 rating by all users.$$\\theta^{(1)} x^{(1)} \\approx 5 \\\\theta^{(2)} x^{(1)} \\approx 5 \\\\theta^{(3)} x^{(1)} \\approx 0 \\\\theta^{(4)} x^{(1)} \\approx 0$$ 9.4.3 Optimization Algorithm12345Idea: For a given value of theta, we can minimize the cost function to learn the value of xi For a given value of xi, we can also do that to learn the value of theata. θ --&gt; x --&gt; θ --&gt; x --&gt; θ --&gt; x --&gt; θ --&gt; x --&gt; ... But this two steps, we shoud do that *** simultaneously *** Origional Algorithm Collaborative filterin Optimization Algorithm ​","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"}]},{"title":"Machine Learning Week7 - SVM","date":"2017-03-02T11:02:59.000Z","path":"2017/03/02/Machine-Learning-Week7-SVM/","text":"7 SVM - Support Vector Machine7. 1 Optimisation Objective$$h_\\theta (x) = \\frac 1 {1 + e^{-\\theta^Tx}} \\z = -\\theta^Tx$$ Why we need do that? 7.2 Hypothesis Function7.2.1 Logistic Regression$$\\frac 1 m \\sum{i=1}^m [ y^{(i)} * (-log(h\\theta(x^{(i)})) + (1 - y^{(i)}) * (-log(1 - h\\theta(x^{(i)}))) ] + \\frac \\lambda {2m} \\sum{j=1}^n \\theta_j^2$$ 7.2.2 Support Vector Machine$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ Analysis： 为了使得cost function取得最小值，我们令C*W + P部分中，C*W为零。即： 当 y = 1时， cost1 = 0，所以 z &gt;= 1 当 y = 0时， cost0 = 0，所以 z &lt;= -1 Note：1. cost0 and cost1 对应的是上图中左右两边的cost function，因为y=0和y=1的目标函数。 常数C取一个很大的值时比较好。因为C*W + P， 所以C大则W会变小，即相对penality就会变大，W会变小 为什么要重新选定一个cost function ？（逻辑回归的临界点为0，但是SVM的临界点是1，所以SVM更加精确。 ） 对应的线性逻辑回归？即次数不大于1的？ Decision Boundary 不是一条直线的情况 7.3 Large Margin Classifier12结论：常数C取一个比较大的值比较容易获得Large Margin ClassifierC大，则比较容易获得 以上为两类分布比较均匀的时候，Decision Boundary为图中黑色的线，所有点离黑色的距离都相对比较大比较均匀，但是当存在干扰点的时候如下图，Decision Boundary会由黑色变为粉红色。所以C的取值不能太大，也不能太小。需要求出最优解 7.4 Mathmatics Behind Large Margin Classification7.4.1 Vector Inner Product Note： 如何求投影p的值？ 当角度 &lt; 90°，p为正数。当角度 &gt; 90°时，p为负数。 $$向量内积：u^Tv = ||u|| · ||v|| · cosθ = ||u|| · p{v,u} = ||v|| · p{u,v} = u_1v_1+u_2v_2$$ 7.4.2 SVM Cost Function$$C \\sum_{i=1}^m [y^{(i)} cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)}) cost0(\\theta^Tx^{(i)})] + \\frac 1 2 \\sum{j=1}^n \\theta_j^2,\\space \\space \\space \\space \\space C = \\frac 1 \\lambda$$ 当C取一个一个很大的值时，cost function只剩下后面P的部分。 假设θ0 = 0$$\\frac 1 2 \\sum_{j=1}^n\\theta^2_j = \\frac 1 2 (\\theta^2_0 + \\theta^2_1 + … + \\theta^2_n) = \\frac 1 2 (\\theta^2_1 + … + \\theta^2_n)= \\frac 1 2 ||\\theta||^2$$ 所以：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\p^{(i)}是点到向量\\theta的projection，即点到Decision Boundary的距离$$上面我们讨论了，当C取到一个合适的、较大的数值时，SVM的cost function就只剩下后面P的部分，即$$\\frac 1 2 ||\\theta||^2$$我们要减小cost function，所以需要减小θ的值。 当θ取到一个比较小的值的时候，还需要满足上面讨论的：$$\\theta^Tx^{(i)} = p^{(i)}||\\theta|| \\p^{(i)}||\\theta|| &gt;= 1, if \\space\\space y^{(i)} = 1 \\p^{(i)}||\\theta|| &lt;= -1, if \\space\\space y^{(i)} = 0 \\$$所以θ比较小时，只能增加p的值去满足p*||θ|| &gt;= 1 或者 p*||θ||&lt;= -1。 这样就保证了p的值比较大，即点到Decision Boundary的大间距。 7.5 Kernels7.5.1 Kernels &amp; Similarity首先，我们回想一下之前的logistic regression中对于non-linear 情况的拟合。 Predict y = 1, if$$\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3x_2 + \\theta_4x_1^2 + \\theta_5x_2^2 + … &gt;= 0 \\\\theta_0 + \\theta_1f_1 + \\theta_2f_2 + \\theta_3f_3 + \\theta_4f_4 + \\theta_5f_5 + … &gt;= 0$$即将fn定义为x的幂次项组合，如下：$$f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, …$$ 但是在SVM中，我们要重新定义fn，引入Kernel的概念，即用 kernel function来表示fn。 Note: l 是landmark，且如果training sets里面的数量为n的话，则landmark的数量也为n。 假设training sets数量为n，则对于一个新的example来说，可计算出n个新的特征f1…fn。然后用新的特征，对该example进行判断（低维转为高维的过程） kernel function为guassian function。当x与landmark l越接近时，两点的距离越小，值接近1 7.5.2 SVM with Kernels 对比之前的cost function，可以发现这里θ和f(x)跟之前的不同。 在logistic regression 中，θ的维度为(n+1) x 1, 包含θ0， 且n为单个example的特征个数 在SVM with kernel中，f(x)的个数为m，其中m是training sets中的个数，所以θ的维度应该是(m+1)x1 Steps 给定一组training sets，根据每个example，选取m个landmark点 计算每一个example与所有landmark的相识度，相同为1，非常不同接近为0。计算相识度的kernel function为Gaussian Function 最终，对于每一个example里面都可以计算出m个新的feature，所以对于这个training sets而言，会得到一个m*m的矩阵？ 将得到的m*m的矩阵，代入到Hypothesis中，计算出θ的值。 7.5.4 SVM parameters C $$C = \\frac 1 \\lambda$$ Large C Small λ Large θ Lower Bias High Variance Over Fitting Small C Large λ Small θ Higher Bias Low Variance Under Fitting σ | Large σ | more smoothly | Higher Bias | Lower Variance | Under Fitting || ———– | —————– | ————– | ——————- | —————- || Small σ | less smoothly | Lower Bias | Higher Variance | Over Fitting | ​","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://chenson.com/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","permalink":"http://chenson.com/tags/SVM/"}]}]